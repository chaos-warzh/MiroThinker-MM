[
  {
    "title": "LLM检索增强生成原理与架构详解-今日头条",
    "page_body": "生成式AI技术很强大，但它们受到知识的限制。 虽然像 ChatGPT 这样的LLM可以执行许多任务，但每个LLM的基线知识都存在基于其训练数据的差距。 如果你要求LLM写一些关于最近趋势或事件的文章，LLM不会知道你在说什么，而且回答最好是混合的，坏的情况下往往还是有问题的。\n在线工具推荐： Three.js AI纹理开发包 - YOLO合成数据生成器 - GLTF/GLB在线编辑 - 3D模型格式在线转换 - 可编程3D场景编辑器\nLLM基于知识的问题归结为两个关键问题：\nLLM 训练数据往往无可救药地过时（截至撰写本文时，ChatGPT 的世界知识将于 2022 年 1 月结束，不包括使用 Bing 浏览和 GPT-4V 等提供额外上下文的功能）。 LLM在无法获得事实的情况下进行推断，因此当他们的知识存在差距时（称为幻觉），他们会自信地做出虚假但听起来合理的陈述。\n检索增强生成 (RAG：Retrieval Augmented Generation) 是一种有助于解决这两个问题的策略，它将信息检索与一组精心设计的系统提示相结合，将LLM锚定在从外部知识存储检索的精确、最新和相关信息上。 尽管LLM训练数据保持静态，但利用这种背景知识提示LLM可以创建需要对事实进行深入且不断发展的理解的特定领域应用程序。\n你可以问LLM“什么是 RAG？”，根据LLM本身是否使用 RAG，可能会得到截然不同的答案：\n“RAG是什么？” 有RAG的LLM响应：\nRAG（检索增强生成）是 Meta AI 研究人员引入的一种方法，它将信息检索组件与文本生成器模型相结合，以解决知识密集型任务。 可以有效地修改其内部知识，而无需重新训练整个模型。 您想了解更多详细信息吗？\n“RAG是什么？” 没有 RAG 的LLM响应：\n根据上下文，RAG 可以指多种不同的事物。 以下是一些常见的含义：\n1. 红色、琥珀色、绿色 (RAG)：在项目管理和报告中，RAG 是一种颜色编码系统，用于快速传达项目或任务的状态或运行状况。\n2. RAG分析：这是一种用于风险评估或决策的方法。 它涉及根据风险或选项的严重程度、影响或可取性将其分类为红色、琥珀色或绿色。 它有助于优先考虑行动或选择。\n3. 随机访问生成器 (RAG)：在某些技术背景下，RAG 可能指生成随机访问模式或数据的系统或算法，通常用于计算机科学或信息检索。\n4.资源分配图（RAG）：在操作系统和计算机科学的背景下，资源分配图用于死锁检测。 它是一个有向图，表示系统中进程之间的资源分配和请求关系。\n如果你有特定的上下文或领域，请提供更多详细信息，我可以在该上下文中为你提供有关“RAG”的更准确的解释。\n使用和不使用 RAG 来回答这个问题有很大区别。 特别是在涉及特定领域知识（如首字母缩略词）的情况下，RAG 可以极大地提高LLM回答的准确性。\n如果你与了解最近事件、了解用户特定信息或对某个主题比正常情况有更深入了解的聊天机器人进行过交互，那么很可能在没有意识到的情况下与 RAG 进行了交互。 如果你看过有关“通过文档聊天”的教程，那么这就是 RAG 最简单的版本。 LangChain 和 LlamaIndex 等框架使快速创建简单的知识感知应用程序成为可能，从而实现了 RAG 的民主化。\n我开发并实现了 Skyflow 内部的 LLM 应用程序，包括使用 RAG 的系统。 由于 RAG 几乎无处不在，并且不会很快消失，因此了解 RAG 的基础知识以及当你想要将代码转移到生产环境时如何超越这些基础知识非常重要。 我邀请你学习我在 RAG 旅程中的经验，这样你就不必经历艰难的摸索过程。\n1、一个最小RAG示例\nLangChain 提供了一个 RAG 最小（但不是最简单）形式的示例：\nfrom langchain.document_loaders import WebBaseLoader from langchain.indexes import VectorstoreIndexCreator loader = WebBaseLoader(\"https://www.promptingguide.ai/techniques/rag\") index = VectorstoreIndexCreator().from_loaders([loader]) index.query(\"What is RAG?\")\n通过这五行代码，我们得到了 RAG 的描述，但代码是高度抽象的，因此很难理解实际发生的情况：我们获取网页的内容（本示例的知识库）。\n我们处理源内容并将它们存储在知识库（在本例中为矢量数据库）中。 我们输入一个提示，LangChain从知识库中查找一些信息，并将提示和知识库结果传递给LLM。\n虽然此脚本有助于原型设计和理解使用 RAG 的主要节拍，但它对于超越该阶段并不是那么有用，因为你没有太多控制权。 让我们讨论一下实际执行的内容。\n2、LLM+RAG基本架构\n由于完整的 LLM 应用程序架构相当大，因此我们将仅考虑启用 RAG 的组件：\n编排层接收用户在任何关联元数据（如对话历史记录）中的输入，与所有相关工具交互，将提示发送给 LLM，并返回结果。 编排层通常由 LangChain、Semantic Kernel 等工具组成，并使用一些本机代码（通常是 Python）将它们编织在一起。 检索工具是一组返回上下文的实用程序，这些上下文通知用户提示并对用户提示进行响应。 该组包括知识库和基于 API 的检索系统。 LLM 是你向其发送提示的大型语言模型。 它们可能由 OpenAI 等第三方托管，也可能在你自己的基础设施中内部运行。 就本文而言，你使用的具体模型并不重要。\n在典型的LLM应用程序中，你的推理处理脚本会根据需要连接到检索工具。 如果你正在构建基于 LLM 代理的应用程序，则每个检索实用程序都会作为工具向你的代理公开。 从这里开始，我们将只讨论典型的基于脚本的用法。\n当用户触发你的推理流程时，编排层会将必要的工具和 LLM 结合在一起，以从你的检索工具中收集上下文并生成上下文相关的知情响应。 编排层处理所有 API 调用和特定于 RAG 的提示策略（我们很快就会谈到）。 它还执行验证，例如确保你没有超出 LLM 的令牌限制，这可能会导致 LLM 拒绝你的请求，因为你在提示中填充了太多文本。\n3、知识库检索\n要查询数据，你不仅需要数据，还需要应用程序可以访问的格式。 对于基于 LLM 的应用程序，这通常涉及向量存储 - 一个可以基于文本相似性而不是精确匹配进行查询的数据库。\n将数据从源格式转换为矢量存储需要 ETL（提取、转换、加载）管道。\n汇总源文档。 你需要收集希望可用于你的应用程序的任何内容。 对于我在 Skyflow 的私有LLM方面的工作，这包括我们的产品文档、白皮书和博客文章，但这可以轻松扩展到内部记录、规划文件等。 清理文档内容。 如果有任何东西不应该对LLM提供者或你的应用程序的最终用户可见，现在你有机会将其删除。 在此过程中，删除个人身份信息 (PII)、机密信息和正在开发的内容。 此步骤之后剩下的任何内容对于后续训练和推理过程的所有阶段都将是可见的。 Skyflow GPT Privacy Vault 可以帮助你对文档内容进行去标识化，以确保你的训练数据不含敏感信息。 将文档内容加载到内存中。 Unstructed、LlamaIndex 和 LangChain 的文档加载器等工具可以将各种文档类型加载到你的应用程序中，特别是非结构化内容。 无论是文本文档、电子表格、网页、PDF、Git 存储库还是数百种其他内容，都可能有一个加载程序。 但要小心，因为并非所有加载器都是一样的，有些加载器比其他加载器加载更多的内容或上下文。 例如，一个加载器可能从电子表格中的多个工作表加载内容，而另一个加载器仅从第一个工作表加载内容。 将内容分成块。 当你分割你的内容时，你把它分成小的、一口大小的片段，这些片段可以适应LLM提示，同时保持意义。 有多种方法可以分割内容。 LangChain 和 LlamaIndex 都有可用的文本分割器，默认情况下按空格字符和句子递归分割，但你需要使用最适合你的内容的方法。 对于我的项目，我发现用 Markdown 编写的文档即使使用 LangChain 的 Markdown 拆分器也会丢失太多上下文，因此我编写了自己的拆分器，根据 Markdown 的标题和代码块标记对内容进行分块。 为文本块创建嵌入。 嵌入存储向量—一个文本块的相对位置以及与其他附近文本块的关系的数字表示。 虽然这很难想象，但庆幸的是创建嵌入很容易。 OpenAI 提供嵌入模型，LangChain 和 LlamaIndex 提供各种托管或自托管嵌入选项，或者您也可以使用 SentenceTransformers 等嵌入模型自行完成。 将嵌入存储在向量存储中。 获得嵌入后，你可以将它们添加到矢量存储中，例如 Pinecone、Weaviate、FAISS、Chroma 或众多其他选项。\n存储向量后，你可以查询向量存储并查找与你的查询最相似的内容。 如果您需要更新或添加源文档，大多数向量数据库都允许更新存储。 这意味着你还可以从矢量存储中删除内容，这是微调模型时无法执行的操作。\n你可以再次运行管道来重新创建整个知识库，虽然这比重新训练模型成本更低，但它仍然耗时且效率低下。 如果你希望定期更新源文档，请考虑创建文档索引流程，以便仅处理管道中的新文档和最近更新的文档。\n4、基于API的检索\n从向量存储中检索并不是唯一的检索类型。 如果你有任何允许编程访问的数据源（客户记录数据库、内部票务系统等），请考虑让你的编排层可以访问它们。 在运行时，你的编排层可以查询基于 API 的检索系统，以提供与当前请求相关的其他上下文。\n5、使用 RAG 提示\n设置好检索工具后，就可以使用一些编排层魔法将它们编织在一起。\n首先，我们将从提示模板开始。 提示模板包含你想要作为提示的一部分传递给 LLM 的所有信息的占位符。 系统（或基本）提示符告诉LLM如何表现以及如何处理用户的请求。 一个简单的提示模板可能如下所示：\nSystem: You are a friendly chatbot assistant that responds in a conversational manner to users' questions. Respond in 1-2 complete sentences, unless specifically asked by the user to elaborate on something. Use History and Context to inform your answers. --- History: {history} --- Context: {context} --- User: {request}\n当你的最终用户提交请求时，你就可以开始填写变量。 如果用户提交“什么是RAG？”，你可以填写请求变量。 你可能会随请求一起获得对话历史记录，因此也可以填写该变量：\nSystem: You are a friendly chatbot assistant that responds in a conversational manner to user's questions. Respond in short but complete answers unless specifically asked by the user to elaborate on something. Use History and Context to inform your answers. --- History: [{\"role\": \"assistant\", \"message\": \"Hi! How can I help you?\"}] --- Context: {context} --- User: What is RAG?\n接下来，调用你的检索工具，无论它们是矢量存储还是其他 API。 一旦你掌握了上下文（知识库结果、客户记录等），就可以更新上下文变量，以便在推理过程中为 LLM 提供信息和基础。\n注意：如果你的 RAG 实施中包含多种类型的数据，请确保对它们进行标记，以帮助LLM区分它们。\nSystem: You are a friendly chatbot assistant that responds in a conversational manner to user's qu"
  },
  {
    "title": "通义DeepResearch全面开源！同步分享可落地的高阶Agent构建方法论-知乎",
    "page_body": "01.从 Chatbot 到 Autonomous Agent通义研究团队开源发布通义 DeepResearch —— 首个在性能上可与 OpenAI DeepResearch 相媲美、并在多项权威基准测试中取得领先表现的全开源 Web Agent。\n GitHub：\n https://tongyi-agent.github.io\n 模型链接：\n https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B\n 在多个极高难度的信息检索和推理任务中，通义DeepResearch 取得了最先进的（SOTA）成绩：\n Humanity’s Last Exam (HLE)：32.9BrowseComp-EN：45.3BrowseComp-ZH：49.5用户中心化的 xBench-DeepSearch：75.0系统性地超越了目前所有的闭源及开源 Deep Research 代理。\n 不仅如此，研究团队还完整分享了一套可落地的高阶Agent构建方法论，详细介绍了从数据合成、Agentic 持续预训练（CPT）、有监督微调（SFT）冷启动，到最终强化学习（RL）阶段的全套流程。在 RL 环节，提供了算法创新、自动化数据构建与高稳定性基础设施的全栈解决方案。\n 在推理阶段，基础的 ReAct 模式无需任何提示工程即可充分展现模型固有能力，而深度模式（test-time-scaling） 则展示了其在复杂推理与规划能力上的上限。\n 02.基于完全合成数据的持续预训练和后训练该模型能力基于一种多阶段数据策略，旨在创建海量高质量的代理训练数据，而无需依赖昂贵的人工注释。\n 持续预训练数据研究团队将 Agentic CPT 引入到Deep Research Agent训练中，为后训练创建强大的Agent基础模型。研究团队提出了 AgentFounder，这是一个系统化且可扩展的大规模数据合成解决方案，它利用来自后训练流程的数据创建数据飞轮。\n 数据重组和任务构建 研究团队持续从各种来源收集数据，包括文档、公开的爬取数据、知识图谱、历史轨迹和工具调用记录（例如，带有链接的搜索结果）。这些不同的数据源被重构为基于实体的开放世界知识记忆。基于随机采样的实体及其对应的知识，生成多风格（问题-答案）对。\n 动作合成 基于多样化的问题和历史轨迹，研究团队构建了一阶动作合成数据和高阶动作合成数据。该方法能够在离线环境下大规模、全面地探索潜在的推理-动作空间，从而消除了对额外商业工具 API 调用的需求。具体而言，对于高阶动作合成，将轨迹重构为多步骤决策过程，以增强模型的决策能力。\n 后训练数据高质量合成问答数据研究团队开发了一套端到端的合成数据生成解决方案。这一全自动流程无需人工干预即可构建超越人类质量的数据集，旨在突破人工智能代理的性能极限。经过长期的探索和迭代——从早期的网页点击流逆向工程问答对（WebWalker）到更系统的基于图谱的合成方法（WebSailor 和 WebSailor-V2），再到形式化的任务建模（WebShaper），这种方法确保了卓越的数据质量和强大的可扩展性，突破了模型能力的上限。\n 为了解决复杂且高度不确定的问题，研究团队通过一种新颖的流程合成基于 Web 的问答数据。该流程首先通过在高度互联的知识图谱随机游走和基于表格数据融合同构表构建，将来自真实网站数据整合，并确保信息结构的真实性。然后，对子图和子表进行采样，生成初始问题和答案，关键步骤是通过策略性地混淆或模糊问题中的信息来增加问题难度。该方法基于一个完整的理论框架，将问答难度正式建模为一系列可控的“原子操作”（例如，合并具有相似属性的实体），这些操作基于实体关系，使得能够系统地增加复杂性。\n 为了进一步减少问答系统的信息结构与推理结构之间的不一致性，提高推理难度和结构扩展能力，提出了一种基于集合论的信息搜索问题正式建模。基于这种正式建模，开发了能够以可控方式扩展问题的智能体，并最大限度地减少了推理捷径和结构冗余，从而进一步提升了问题质量。此外，这种正式建模还能高效地验证问答的正确性，有效解决了信息搜索合成数据难以验证的挑战。\n 此外，官方还开发了一个自动化数据引擎，以扩大博士级研究问题的创建规模。该引擎基于多学科知识库，生成需要多源推理的“种子”问答对。然后，每个种子都会进入一个自我引导的“迭代复杂性升级”循环，其中，一个问题构建代理配备了一套强大的工具，包括网络搜索、学术检索和 Python 执行环境。在每次迭代中，代理都会扩展知识边界，深化概念抽象，甚至构建计算任务，从而形成一个演化循环，上一轮的输出成为下一轮更复杂的输入，确保任务难度的可控且系统地升级。\n 超越高质量问答为了最大限度地提升模型在推理、规划和工具使用方面的能力，研究团队引入了 IterResearch，这是一种创新的Agent范式（详情见下文）。它通过在每一轮中动态地重建精简的工作空间，取代了传统的不断积累上下文的方法。这种方法充分释放了模型的推理潜力，并确保模型的每个决策都是经过深思熟虑的结果。\n 通过这一过程，不仅创建了“问题”，还创建了如何逐步、有条理地解决问题的“黄金范例”。该数据集由经过严格筛选的高质量问题解决示例组成，有助于充分释放模型在推理、规划和工具使用方面的潜力。\n 03.Rollout模式通义团队还对深度研究型智能体的部署范式进行了广泛的探索。因此，最终模型支持多种部署格式，包括原生的 ReAct 模式和上下文管理的深度模式。\n ReAct 模式模型使用 ReAct 推理范式展现出卓越的性能。它严格遵循“思考-行动-观察”的循环，通过多次迭代来解决问题。模型上下文长度为 128K，可以处理大量的交互轮次，从而完全实现与环境交互的可扩展性。ReAct 的简单性和通用性为模型的内在能力和训练流程的有效性提供了最清晰的基准。\n 官方选择 ReAct 很大程度上受到了“The Bitter Lesson”的影响，利用可扩展计算的通用方法最终将优于依赖复杂的人工知识和复杂设计的方法。\n 深度模式除了 ReAct 模式外，研究团队还开发了“深度模式”，用于处理复杂的多步研究任务。此模式基于其全新的 IterResearch 范式，旨在将Agent的能力发挥到极致。\n IterResearch 范式的创建是为了解决Agent将所有信息堆积在一个不断扩展的单一情境中时出现的“认知瓶颈”和“噪音污染”。相反，IterResearch 将一项任务解构为一系列“研究轮次”。\n 在每一轮中，Agent仅使用上一轮中最重要的输出来重建一个精简的工作空间。在这个专注的工作空间中，Agent会分析问题，将关键发现整合成一个不断演变的核心报告，然后决定下一步行动——是收集更多信息还是提供最终答案。这种“综合与重构”的迭代过程使Agent能够在执行长期任务时保持清晰的“认知焦点”和高质量的推理能力。\n 在此基础上，通义团队提出了“研究-合成”框架。在该模型中，多个研究Agent使用 IterResearch 流程并行探索同一个问题。Agent会最终整合它们完善的报告和结论，从而得出更全面的最终答案。这种并行结构使模型能够在有限的上下文窗口内考虑更广泛的研究路径，从而将其性能推向极限。\n 04.端到端Agent训练流程\n 训练这样的Agent模型需要重新思考整个模型训练流程，从预训练到微调再到强化学习。研究团队建立了一种新的代理模型训练范式，将Agentic CPT → Agentic SFT → Agentic RL 连接起来，为 AI Agent创建了一个无缝的端到端训练循环。以下是利用强化学习解决最后阶段的方法，这对于使代理的行为与高阶目标保持一致至关重要：\n 基于策略的代理强化学习 (RL)通过强化学习构建高质量的Agent是一项复杂的系统工程挑战；如果将整个开发过程视为一个“强化学习”循环，其组件中的任何不稳定或鲁棒性不足都可能导致错误的“奖励”信号。接下来，将分享在强化学习方面的实践，涵盖算法和基础设施两个方面。\n 在强化学习算法方面，使用定制的基于策略的组相对策略优化 (GRPO) 取得了多项算法突破，采用严格的在线策略训练方案，确保学习信号始终与模型当前能力相关。训练目标使用 token 级策略梯度损失进行优化。其次，为了进一步降低优势估计的方差，采用了留一法 (leave-one-out) 策略。此外，由于观察到未经过滤的负轨迹会显著降低训练稳定性，对负样本采取了保守策略。在延长训练时间后，这可能会表现为“格式崩溃”现象。为了缓解这种情况，研究团队会选择性地从损失计算中排除某些负样本，例如，那些由于超出长度限制而无法得出最终答案的样本。为了提高效率，不采用动态采样，相反，利用更大的批次和组大小，这有助于保持较小的方差并提供足够的监督。\n 训练动态展现出有效的学习效果，奖励呈现持续上升趋势。同时，策略熵始终保持在高位，表明模型持续探索，并防止过早收敛。研究团队将其归因于Web环境的非平稳特性，这种特性自然会促进稳健的自适应策略，并消除了显式熵正则化的需要。\n 研究团队认为，算法固然重要，但并非 Agentic 强化学习成功的唯一决定性因素。我们尝试过许多不同的算法和技巧，发现数据和训练环境的稳定性可能是决定强化学习是否有效的更关键因素。有趣的是，我们曾测试直接在 BrowseComp 测试集上训练模型，但效果低于我们合成数据。我们分析这种差异的出现是因为合成数据提供了更一致的分布，从而可以更有效地定制模型。相反，人工标注的数据（例如 BrowseComp）本身就包含更多噪声。由于其规模有限，很难近似可学习的底层分布，从而阻碍了模型从中学习和概括。该现象可进一步延拓到其他Agent的训练上，为更多样和复杂的Agent训练带来启示。\n 在基础设施方面，使用工具训练Agent需要开发一个高度稳定高效的环境：\n 合成训练环境：依赖实时 Web API 进行开发成本高昂、速度慢且不一致。官方利用离线维基百科数据库和自定义工具套件创建了一个模拟训练环境，解决了这一问题。通过调整数据流水线，为该环境生成高质量、复杂的任务，创建了一个经济高效、快速可控的平台，显著加快了研究和迭代速度。\n 稳定高效的工具沙盒：为了确保在代理训练和评估期间可靠地使用工具，研究团队开发了一个统一的沙盒。该沙盒通过缓存结果、重试失败的调用以及使用冗余提供程序作为后备（例如，备用搜索 API）来优雅地处理并发和故障。这为代理提供了快速且确定性的体验，这对于防止工具错误破坏其学习轨迹至关重要。\n 自动数据管理：数据是提升模型能力的核心驱动力，其重要性甚至超过了算法。数据质量直接决定了模型通过自我探索推广到分布外场景的能力上限。为了应对这一挑战，研究团队在训练动态的指导下实时优化数据。这种优化是通过全自动数据合成和过滤流程实现的，该流程可以动态调整训练集。通过闭合数据生成和模型训练之间的循环，这种方法不仅确保了训练的稳定性，还带来了显著的性能提升。\n 基于策略的异步框架：在 rLLM 之上实现了自定义的步级异步强化学习训练循环。多个代理实例并行与（模拟或真实）环境交互，每个"
  },
  {
    "title": "跨领域知识整合能力评估：LLM设计的综合测试-CSDN博客",
    "page_body": "跨领域知识整合能力评估：LLM设计的综合测试\n关键词\n跨领域知识整合能力 大型语言模型（LLM） 综合测试框架 算法设计 系统架构 实战应用\n摘要\n本文旨在探讨跨领域知识整合能力评估在大型语言模型（LLM）设计中的应用，提供一个综合测试框架。我们将从背景介绍开始，逐步深入探讨核心概念、算法原理、系统架构以及实战应用，旨在为读者提供一个全面、清晰的技术路线图。文章将涵盖以下主要内容：\n背景与问题阐述 ：介绍跨领域知识整合的背景、重要性以及常见问题。 核心概念与联系 ：详细阐述关键概念，包括大型语言模型、跨领域知识整合能力等，并使用Mermaid流程图展示各概念之间的联系。 算法原理讲解 ：介绍评估LLM跨领域知识整合能力的算法原理，包括数学模型和公式，并通过Python代码进行详细解释。 系统分析与架构设计 ：分析项目场景，设计系统功能、架构、接口以及交互流程。 项目实战 ：介绍环境安装、核心实现、代码应用、案例分析及项目小结。 最佳实践与总结 ：总结最佳实践、注意事项，并提供拓展阅读建议。\n通过本文的阅读，读者将能够深入了解跨"
  },
  {
    "title": "搜狐",
    "page_body": "SpaceX发布新型“星舰”图   世锦赛：唐钱婷百米蛙泳摘铜 \n视频\n国风播主@宋曦川 期待下次更进步的自己\n热　门 | Kpop播主@学姐大头是彦铮 评委秀献上 推　荐 | 音乐播主@加小西 《年轮》又火起来了 看　点 | 摄影播主@XP日记 在老城漫无目的闲逛\n 浴血荣光   小重逢 \n 热　播  | 红腰鼓   爱你不是三两天   洛基   春晖   自　制  | 屌丝男士   公主病   他来了   匆匆那年   电　影  | 杀戒   驱魔人   天下无贼   消失的她   动　漫  | 棒棒宝贝儿歌   蚂蚁快跑   超级飞侠   综　艺  | 谢谢你来了   选择   金牌调解   养生堂 \n习近平向世界青年和平大会致信 自上而下、以上率下 | “一定要原汁原味地保护” 新思想 | 救援人员为灾区搭建生命通道 | 千年石窟 | 夏日经济 三中全会 | 战地记者 | 防台防汛安全线 | 我国最大超深油气区 铭记历史 | 闻“风”而动 | 吉林振兴 | 梦想派件 | 科学减重 以师德润泽心灵 | 低空经济迎风“高飞” | 泥泞里的守护 人享其行、物畅其流 | 城市发展 | 人民城市理念 | 思想纵横 在绿色领域与中国合作潜力巨大 | 鼓励外商在华再投资 三个月三场会谈 | 关税战为何适得其反？ “石榴花开 籽籽同心” | “新疆智慧” | 最美守边人 | 三合村\n中美经贸会谈在瑞典斯德哥尔摩举行\n 中美双方经贸团队进行了深入、坦诚、建设性的交流 \n 北京4天下1年的雨 今夏北方暴雨为何这么大？ \n 北京怀柔女子讲述洪水细节：一家七口徒步一小时撤离 \n 31省份经济半年报：多省增长超预期 京沪消费增速垫底 \n 山西公布一火灾事故评估报告：企业被罚1500万 13人被公诉 \n特朗普称中方已向其发出访华邀请 外交部回应\n 泰国指责柬埔寨违反停火协议 中方表态 \n 美国教育部宣布对杜克大学展开“种族歧视调查” \n 泽连斯基：强制医学生军事培训 允许60岁以上公民服役 \n 辞职风波后支持率回升 民众集会力挺 石破茂还能扛多久？ \n “特朗普爱上白宫保洁”火了 美国观众边骂边付费追剧 \n印乐法师任少林寺住持 少林寺住持是如何选出的\n 少林寺新住持曾表示“不评价少林寺商业化运作” \n 媒体：民航版“12306”上线 能让消费者真正尝到甜头吗？ \n 每天花30元“朝九晚五” 杭州有家“假装上班无限公司” \n 江西一树林发现大量被丢弃药品？官方通报 \n 患者突然从ICU病房窗户跳下后又起诉医院 法院判了 \n4人在无人区失联超30小时 当事人：为求救徒步40多公里\n 偷蒜贼偷2261斤大蒜塞爆副驾座 已被依法采取强制措施 \n 一家四口命丧酒店 警方：坠亡男子无家暴记录 正准备举家移民 \n 男子开锁坐地起价从60元变800元 遭拒将母子两人反锁 \n 71岁成龙时隔21年再演警察 与以前利落的身手形成残酷对比 \n中方回应是否已邀请特朗普访华\n 4小时前 \n 普京和特朗普将在华会晤？中方回应 \n 4小时前 \n 泰柬为何能“闪电停火” \n 4小时前 \n 泽连斯基签署法案：60岁以上可服役 \n 4小时前 \n 承德一村庄因暴雨断联 有村民遇难 \n 4小时前 \n 台风已锁定中国 强风雨或持续3天 \n查看更多\n24小时热文\n娱乐 视频\n 明星八卦   电视剧   电影   综艺   美图 \n进入娱乐首页 >\n刘诗诗红发造型明艳动人，穿新中式黑裙古典优雅\n马伊琍短发搭配棕色皮质套装 露出长腿气场十足\n朱孝天妻子晒与F4欢乐合照 单手比耶好俏皮\n温碧霞自曝曾因艳星称呼抑郁，揭露自己不生娃的原因\n林志颖50岁近照曝光，与郭德纲同框，打破“不老神话”引热议 \n成龙被传当爷爷？疑助儿子迎娶小11岁女星，女儿靠卖废品为生\n素颜女神沦落至去拍短剧！40岁王丽坤受前夫连累资源狂跌?\n陈乔恩称要一个月瘦到45公斤：早上量体重时已瘦到48.5公斤\n俞灏明感情迎新进展！疑为王晓晨庆生，还开始装修新房！\n助理否认沈玉琳进ICU：只是因身体不适而去就医检查\n汪小菲带孩子到餐厅吃饭，儿子沉迷手机游戏，一家人很和谐\n给华晨宇P女性形象照网友道歉：已认识到错误并删除相关图片\n《解忧杂货店》导演疑似内涵王俊凯：没有机会打开他的那扇门\n赵樱子说自己对陈晓一见钟情：他现在单身了我也不敢主动\n娱乐播报\n 俄罗斯芭蕾国家剧院抵沪演出《天鹅湖》   张予曦自曝因拒绝表白被欺负 \n 钟丽缇为二女儿庆祝17岁生日   刘宇宁锐评张凌赫龚俊唱歌 \n 俄罗斯芭蕾国家剧院抵沪演出《天鹅湖》   张予曦自曝因拒绝表白被欺负 \n 钟丽缇为二女儿庆祝17岁生日   刘宇宁锐评张凌赫龚俊唱歌 \n体育 体育视频\n NBA   CBA   中超   国际足球   英超   西甲   乒乓球   网球 \n进入体育首页 >\n梅西夫妇带孩子们看演唱会 揽腰合影超甜蜜\n王欣瑜社媒分享日常照 与狗狗合影笑容明媚\n《箭心》第二集:射箭初体验,这系统在耍我?\n热身赛-胡金秋16+6胡明轩10分 中国男篮险胜波黑\n比赛中硬刚主帅！ 曝徐昕已向杜锋道歉并取得原谅\n游泳世锦赛综述:唐钱婷错失卫冕摘铜 覃海洋第一晋级\n游泳世锦赛奖牌榜:中国队5金优势领跑 意大利0金11银\n女足亚洲杯分组:中国朝鲜同组 四强直接晋级世界杯\n中超官方:卡扎伊什维利获本轮最佳 帽子戏法助泰山反弹\n韩国羽协:陈雨菲根本就不想赢球 靠拉吊折磨安洗莹膝盖\n新仁到位!路易斯-迪亚斯抵达慕尼黑 即将体检签约拜仁\n看不上！曝罗德里戈不愿加盟热刺 想去更大的球队效力\nNBA-保罗身着3号球衣亮相:回归快船是不需要思考的决定\n世联赛最佳惹争议！得分王吴梦洁落选 朱婷迷妹靠后却入围\n财经 宏观 理财 股票\n 行业   经营管理 \n进入财经首页 >\n上证指数 深圳成指 创业板指\n+11.77\n夯实基础 深化改革 住房公积金谱写服务民生新篇章  上海第六批集中供地成交总额289.57亿元：多宗地块高溢价成交   威海乳山216套海景房“1元起拍”：二手房东为加快周转试水   “高净值人士”进入上海大宗物业交易市场，小规模项目活跃度提高   半年降价7次 中介砍价“大刀” 0元定制直播砍价服务能否成交？   成都两宗住宅用地溢价成交 成交价约19亿元 成华区地块溢价超29% \n中央重磅定调：告别棚改老路，转向城市更新  北京土地市场重启，核心地块上新受追捧，弹性供地策略显效   北京限购 ，终于要开闸了 ？“ 大城市疏解 ”战略退场   住宅+幼儿园+商业“套餐”上架 昌平13.69亿“拼装地块”月底开席   下半年新盘销售出师不利，哪些新盘卖的好，您自己来看看 \n楼市精选 帮你买房\n保利·阅云台 全线封顶庆祝仪式 石门首座“庐”系雅致启幕 6599起，精装两室只在碧桂园星荟 保利理想城\n尽在焦点\n幸福城 在售\n天玺华府 在售\n城发投·云尚 待售\n铂云府 在售\n搜索\n科技\n 互联网   通讯   智能硬件   科技评测   手机   科学   数码 \n进入科技首页 >\n185亿，刘强东又出手\nNASA发现\"黄金星球\",全球每人可分6912亿元\n懂车帝不是马斯克的通行证\n亲自走一趟北京后，黄仁勋明白，中方已不再需要英伟达\nDeepSeek要凉了？是它幻觉太严重还是它在闷声发大财？\n“中国市场需求强劲，英伟达向台积电追单30万块H20芯片”\n中国版OpenAI，拼命融钱、非常差钱、很难赚钱\n王兴兴：十年后大家就不用工作了，吃喝都由机器人提供\n尴尬了，德国测试智驾，华为遥遥领先，特斯拉垫底，还不如小米\n2000一张票的世界人工智能大会，最好看的竟然是机器人？\n董宇辉单飞周年考：粉丝增速由日均10万降至2万，试水自营品\n蚂蚁数科CTO王维：在复杂的金融领域，推理模型是数科必然的选择\niPhone 17 Pro真机疑现身街头！外观竟有大变化？\n对话商汤林达华：具身智能仅靠真机获取数据有物理瓶颈\n时尚 时尚 人物 奢品\n被骂4年，她登顶亚洲 人们想要的，真是个咖啡馆吗？\n《穿PRADA的女王2》审美降级，顶级时装穿成H&M？  究竟要吃多少苦才能成为高定买家？   好久没买新鞋，这双真的很心动   乔布斯女儿嫁给了马术奥运冠军！斯坦福毕业有颜有才   “阔腿裤+薄底鞋”今年夏天太火了！时髦松弛又好看   从“白幼瘦”到“气血感”，审美终于“健康”了？ \n教育 高考 留学 学习资料\n清华录取通知书送到理发店引围观，店主：生意好了不少 36岁唐尚珺大学第一个暑假，一人一车骑往西藏\n搞钱色交易，大学原副校长王达利被“双开”  张雪峰回应赚几亿:营业额不等于利润 也没收那么多钱   逐步推行免费学前教育：免多少，怎么免   一名“优等生”的自救：为什么优绩主义让人焦虑   河南一幼儿园餐食走红，网友：本升幼需要什么？   2025年全国“最美教师”公示 \n旅游 国内游 境外游 攻略家\n广州这处旅游景点太“良心”：作为全市首个5A景区，门票仅需 5元 为什么说泰山是阴间的入口？网友直言：爬泰山时真的不该乱说话\n暑假游电白（10）︱沉醉西葛：一场“白+黑”的海滨奇遇  益阳文旅在长沙黄兴路步行街上演“山乡奇遇”   第六届大运河文化旅游博览会圆满落幕，成果斐然   放假通知！休息13天！   31岁四川游客北海乘摩托艇遇难，知情网友：游客非要自己驾驶摩艇   旅发项目现场直击｜芷江：文旅重点项目跑出“加速度” \n美食 流行餐单 食疗养生 减肥食谱\n哈根达斯败退中国：高端定位遇挑战，奶茶成新对手 后厨实拍的炒面做法，饭店的网红招牌菜，挺费功夫，但味道真不错\n后厨实拍的炒面做法，饭店的网红招牌菜，挺费功夫，但味道真不错  杨梅干制作方法，万万没想到是用脚踩的，网友：真好又戒掉一个零食   服务员说我不会吃鱼，现场给我展示去鱼骨，就这一手我是真的佩服！   这么爱吃的女吃货，没见过   好兄弟在肚中，晚上我争取多吃点   今天的花生格外的难吃 \n健康 搜狐医药 搜狐名医 就医帮\n癫痫患者外出，建议带张个人资料卡片 拒绝针头？科学家将牙线做成了无针疫苗\n以后买这些药，不需要处方了！  北京疾控发布强降雨后十条防病提示   体重管理十大关注热词出炉！   营养不足会影响渐冻症治疗效果？   前列腺癌手术会致长期尿失禁吗？   大雨天驾车出行，多深的积水会威胁生命安全？ \n母婴 早教 科学哺育 婴幼健康\n承包神兽们的快乐夏天，自然有办法，2025搜狐成长生活节嗨玩山野 落日追风，一起迎着夕阳来一场山林大冒险！\n2025搜狐成长生活节暨宝宝运动会圆满举行  政策解读："
  },
  {
    "title": "学位论文的各组成部分与排列顺序",
    "page_body": "学位论文的各组成部分与排列顺序\n　　学位论文，一般由封面、中文摘要、英文摘要、目录、插图和附表清单、主要符号表、引言(第一章)、正文、结论(最后一章)、参考文献、致谢、附录等部分组成并按前后顺序排列。\n　　1、封面：论文封面(见附件1)、要求如下：\n　　学位论文题目应能概括论文的主要内容，切题、简洁，不超过26字，可分两行排列，中英文对照;\n　　2、中文摘要：论文摘要的字数一般为300个左右。内容包括研究工作目的、研究方法、所取得的结果和结论，应突出本论文的创造性成果或新见解，语言精炼。摘要应当具有独立性，即不阅读论文的全文，就能获得论文所能提供的主要信息。\n　　为便于文献检索，应在论文摘要后另起一行注明本文的关键词(3-5)个。\n　　3、英文摘要：与中文摘要对应。\n　　4、目录(见附件2)：应是论文的提纲，也是论文组成部分的小标题。目录一般列至二级标题，以阿拉伯数字分级标出。\n　　5、插图和附表清单：论文中如果图、表较多，可以分别列出清单列于目录页之后。图表的清单应有序号、图表名称和页码。\n　　6、符号、标志、缩略词、计量单位、名词、术语等注释说明，可以集中列于图表的清单之后。\n　　7、引言(第一章)：在论文正文前。内容包括：该研究工作的实用价值和理论意义;国内外已有的文献综述;本研究要解决的问题。\n　　8、正文：是论文的主体。写作内容可因研究课题的性质而不同，一般包括：理论分析、计算方法、实验装置和测试方法、对实验结果或调研结果的分析与讨论，本研究方法与已有研究方法的比较等方面。内容应简炼、重点突出。各章节之间应密切联系，形成一个整体。\n　　9、结论(最后一章)：结论应明确、简炼、完整、准确，要认真阐述自己的研究工作在本领域中的地位、作用以及自己新见解的意义。如果不可能得出明确结论，也可以没有结论而进行必要的讨论。\n　　10、参考文献：引用他人的成果必须标明出处。所有引用过的文献，应按引用的顺序编号排列。参考文献一律放在结论之后，不得放在各章之后。\n　　11、致谢：致谢对象限于对课题研究、学位论文完成等方面有较重要帮助的人员。\n　　12、附录：凡不宜放在论文正文中，但又与论文有关的研究过程或资料，如较为冗长的公式推导、重复性或者辅助性数据图表、计算程序及有关说明等，均应放入附录。\n　　例文\n　　摘要：\n　　随着茶叶出口贸易的快速发展，大量的茶企业都开始迈向国际化的路线，国际商务往来也越来越频繁。在对外茶叶贸易过程中就必然会和各个国家进行贸易谈判。\n　　由于国际贸易谈判与企业的国际发展战略息息相关，商务英语沟通技巧的好与坏直接关系着国际贸易谈判的质量高低。特别是对茶叶领域来说，其专业性比较高，涉及到的名词非常多，一个英语非常娴熟的人可能对茶叶领域的相关名词也知道得不多，这就给茶叶国际贸易谈判带来诸多的困难。\n　　笔者在本文中总结了茶叶国际贸易谈判中商务英语沟通中的一些技巧，这些商务英语沟通技巧的运用可以显著提升在沟通中谈判者的主动性，不至于在贸易谈判中因为英语语言问题吃亏。因此，相关的政府官员应当努力的学习此类商务英语沟通技巧，使我国的茶企业的国际战略进行的更加顺畅，不断地积累名誉与利益。我国的茶企业对外贸易人员也应当进一步的加深茶叶知识与商务英语水平，提升贸易成功率，不断加强英语沟通技巧。\n　　关键词：\n　　国际贸易谈判；茶叶企业；商务英语；技巧\n　　市场只有进行更加充分的竞争才能让真正有实力的企业更加壮大。对于茶叶企业来说，世界市场才是其终极目标，不能仅限于将市场放在国内，各个企业只有拥有国际化的发展目光，积极参与到国际竞争当中，才能不被国际市场所淘汰。我国的茶叶企业也是同样如此，茶叶的生产地广阔、种植方式简单，在全世界各地都有种植，茶叶市场竞争十分激烈，即便如此，我国作为茶叶大国应当力争在国际茶叶贸易中占领主导地位，要想加强茶叶企业国际竞争力，就必须要重视茶叶国际贸易谈判，确保在谈判中获得最大的优势。\n　　1茶叶国际贸易谈判中商务英语的沟通技巧\n　　1.1提升贸易双方信息传达准确性\n　　在国际贸易谈判中，信息传达的准确性尤为重要，可以说谈判就是双方依靠自己所获得的信息筹码进行对垒的过程，这一过程中谁的信息更丰富、更准确，谁就占据了主导地位。\n　　在茶叶国际贸易谈判中，需要茶叶企业对外贸易人员精通谈判过程中会涉及到的各个专业领域，包括贸易、法律、政治、金融等等，这些专业领域中的词汇在贸易谈判中都可能会运用上，并且非常重要，如果对外贸易人员的商务英语水平低，对于其中的英语词汇不甚了解、一知半解，而产生错误的意会，做出错误的应对，那么极小的疏忽都可能导致茶叶国际贸易谈判的破裂。\n　　因此，商务英语的翻译准确性以及对其用于贸易谈判中的意义与内涵都要明确留意、格外注重，在将英语翻译为汉语的过程中保证翻译的明确词意与其在普通英语沟通与谈判贸易沟通这两种不同环境下所代表的不同语义传达给茶叶企业，让茶叶企业在国际贸易谈判中获得的信息更加准确，传达出的信息也更加有意义。这样提升贸易双方信息传达准确性对于贸易的顺利进行以及谈判之后的实际贸易流程顺畅有着巨大帮助。\n　　1.2具有商务专业英语翻译特点\n　　普通的英语翻译只是进行简单的语言沟通交流，能顺畅地与外国人交流基础上再加入一些个性化翻译特点，例如将英语翻译成我国的古文优美词汇、语句等等，在普通的翻译中这样能增强翻译的美感也能体现出翻译者的专业能力，但是商务英语翻译则完全不一样。商务英语翻译中一定要切忌加入个性化翻译，翻译者需要如实、准确的将英语意思传达给贸易谈判双方，在茶叶企业的国际贸易谈判中更是如此，我国作为茶叶的起源地，在国际茶叶贸易中要占据主导地位，就要发挥出一种茶叶专业商务英语翻译超高的水准，这样才能在谈判过程中使对方认为这个企业可靠、持久促使谈判成功。\n　　茶叶商务英语需要通过大量的实践积累来寻找规律，如果对外贸易人员没有相应的商务英语水平，那么就会造成双方的茶文化差异，对贸易谈判有着负面的影响。\n　　1.3商务英语礼貌用语技巧\n　　英语国际贸易谈判在语言层面虽然不是非常困难，但是其又和普通人之间的沟通非常不一样，国际贸易谈判有一整套严格的规则，对规则的遵守体现了谈判双方对国际贸易准则的熟悉程度。在国际贸易谈判中礼貌用语的使用十分讲究，礼貌的用语不仅能显示出企业的素养，体现企业本身较高的商业处理态度，又能帮助贸易谈判过程中建立良好的沟通氛围。商务英语沟通技巧中体现对事对人的礼貌，如在陈述合约、文件的时候英语用法态度谦和，英语与汉语一样不同的词汇虽然可能表达的意思相同但是给人的态度却不一样，比如说“canyougivemethatpen？”这句话与“Couldyougivethatpen”虽然表达的都是“能不能给我那支笔？”但是由于时态的不同“Can”相较于“Could”态度较为生硬，因此在茶叶国际贸易谈判中运用“could”明显会更好。商务英语中优秀的礼貌用语可以让国际贸易双方进行更加友好的沟通交流，帮助谈判更顺利的进行，即使是遇到文化差异而出现的矛盾也能更积极的去解决，促使茶叶国际贸易谈判成功。\n　　2茶叶对外贸易人员提升商务英语沟通技巧方案\n　　2.1深入加强商务英语水平\n　　商务英语翻译并不是一个一蹴而就的过程，大部分的茶叶企业在国际贸易谈判中对于商务英语的沟通选择中都会采取两种形式，一种是依靠专业的翻译人员，另一种加强对外贸易人员英语水平。\n　　这两种方式有着各自的优点，使用专业的翻译人员作为国际贸易谈判中桥梁的好处在于对英语的翻译更加准确，专业翻译人员的翻译经验丰富，能够准确地将双方的信息传达给两边，而对外贸易人员加深英语水平来进行国际贸易谈判的好处在于茶叶专业知识丰富，并且对茶叶企业的国际战略发展了解十分深入，可以站在专业翻译人员所不能达到的角度来为茶叶企业的切身利益进行思考。这两种方式都属于商务专业英语翻译，但是两种都有着一定的缺陷，茶叶企业要在国际贸易谈判中消除这些缺点就需要将二者进行融合，茶叶国际贸易谈判首先需要谈判人员对于英语语言的掌握非常娴熟，其次需要对茶文化，茶类专有名词了然于心，此外，还需要谈判人员对于国际贸易准则有基本的了解，只有这样才不至于谈判双方在沟通中存在障碍。\n　　因此，茶叶对外贸易人员本身对茶叶相关领域非常了解，对相关名词的掌握相比较一般人也会更好，所以其主要的工作就是提升英语语言的沟通交流技巧，通过培训深入加强其商务英语水平，这样才能让茶叶企业在国际贸易谈判中处于优势地位。\n　　2.2明确谈判时商务英语内容含义\n　　对外贸易人员的商务英语培训过程中，可以让其大量的观看国际谈判视频，并进行实际性的练习，增强其商务英语沟通技巧经验。在有了商务英语经验的基础上，对其词汇、语句的内容含义进行深入的分析。\n　　有的时候在谈判过程中，往往话有所指，对方不第一时间告诉你其真正的意图就是为了揣摩我方谈判者的水平，在这样的情况下茶叶企业对外贸易人员就相当于茶叶企业本身，如果没有及时明白其中的含义，不作出良好的应对，反而会被对方所瞧不起，在之后的谈判中处于劣势。因此在对外贸易人员应当在不断的翻译实践与视频学习中总结沟通技巧，提升商务英语灵活性与应变能力。\n　　2.3深入了解谈判双方文化差异\n　　文化差异是导致谈判破裂很重要的一个因素，许多的国际谈判中因为双方的文化差异性导致无法进行顺利的沟通，双方的沟通理解有误差，对整个谈判过程造成了负面影响。茶叶企业对外贸易人员应当重视这样的文化差异，提升自己的文化敏感度，并尊重对方的文化思想，通过积极的商务英语使用态度来缓和双方的文化差异性。\n　　商务英语礼貌用语就是用来缓和文化差异十分常见的手段，无论是国外还是国内礼节都是一种对他人的尊重表示，在谈判中更加有礼貌就能使对方感到认同感，同样自身也会注重礼貌用语，这样就很大程度的减少了双方的文化差异，将茶叶国际贸易谈判引向成功。\n　　3结论\n　　我国作为茶叶大国，国内茶产业十分发达，相应的想要进入国际化市场的茶叶企业也非常多。茶叶企业要进入国际市场第一步就是要学会进行正确的国际贸易谈判，只有行之有效的谈判才能让整个茶叶企业的国际战略效率提升，而在此基础上，"
  },
  {
    "title": "语言大模型的进化轨迹",
    "page_body": "ChatGPT的发布是语言大模型（LLM）发展史的转折点，它让人们意识到LLM的潜力，并引发了“AI竞赛”，世界上主要人工智能实验室和初创公司都参与其中。在这之后，基于LLM的聊天机器人层出不穷。\nChatGPT及相关LLM模型让我们共同见证了AI的历史性变革，很多人好奇，LLM和它们的运作方式究竟是怎样的？它们是如何被构建的？未来又将走向何方？本文对此进行了深入探讨。\n本文作者Etienne Bernard是人工智能和机器学习专家，NuMind的联合创始人兼CEO，该企业创建由LLM提供支持的自定义NLP模型。Etienne曾在Wolfram Research工作八年，主要担任机器学习负责人，并领导了自动学习工具、用户友好的深度学习框架以及各种机器学习应用程序的开发。\n（以下内容经授权后由OneFlow编译，转载请联系OneFlow获得授权。来源：\nhttps://www.numind.ai/blog/what-are-large-language-models）\n作者 | Etienne Bernard\nOneFlow编译\n翻译 | 宛子琳、贾川、杨婷\n1\n语言模型\n简单来说， 语言模型能够以某种方式生成文本 。它的应用十分广泛，例如，可以用语言模型进行情感分析、标记有害内容、回答问题、概述文档等等。但理论上，语言模型的潜力远超以上常见任务。\n想象你有一个完备的语言模型，可生成任意类型的文本，并且人们还无法辨别这些内容是否由计算机生成，那么我们就可以使其完成很多事，例如生成具有代表性的内容，如电子邮件、新闻稿、书籍和电影剧本等。再进一步来看， 还可以用其生成计算机程序，甚至构建整个软件。只要愿意，我们还可以让它生成科学论文 。如果语言模型真正“完备”，那么它们生成的论文将能够以假乱真，与真实论文没有区别，这意味着必须对语言模型展开实质性研究！\n当然，就目前而言，完备的语言模型还无法实现，不过也展示出了这些系统的潜力。 语言模型不仅仅能“预测文本”，它们的潜力可能远超想象。\n现在我们回顾一下语言模型的发展历程，从最初的朴素语言模型到目前基于Transformer的LLM（语言大模型）。\n2\n朴素语言模型\n语言模型是机器学习模型，因此它们会学习如何生成文本。教授它们的方法（即训练阶段）是 提供一个大规模文本语料库，它们将从中学习如何模仿生成这些文本的过程。\n也许这听起来有些抽象，但创建一个朴素语言模型实际上非常简单。你可以将文本语料库分成一定大小的字符串块，并测量它们的频率。下面是我使用大小为2的字符串得到的结果：\n图源：《机器学习导论》\n这些字符串块被称为n-gram（其中n表示字符串的大小，因此此处n=2）。通过这些n-gram，你可以像玩多米诺骨牌一样生成文本。从一个初始的n-gram开始，例如“th”，然后根据测量的频率随机选择一个以初始n-gram结尾的n-gram 。在这个例子中，如果选择“hi”，就会形成“th” + “hi” = “thi”。然后再继续添加以“i”开头的 n-gram，以此类推，生成整段文本。不过正如你所想，这些n-gram模型并不能生成足够连贯的文本。以下是我继续执行这一过程时得到的结果：\n说实话，这一结果并不太理想！但也说得通，因为该模型的记忆能力很有限，只通过前一个字符来预测下一个字符。如果我们使用n=4的字符串，结果会稍微好一些：\n“complaine building thing Lakers inter blous of try sure camp Fican chips always and to New Semested and the to have being severy undiscussion to can you better is early shoot on”\n现在出现了一些拼写正确的单词，但结果仍不够理想！理论上，进一步增加n的值，输出结果会得到改善，但在实践中， 我们无法显著增加n值，因为这需要一个庞大的数据集来训练模型 。最后，我们可以尝试将单词而不是字符作为基本单位（在自然语言处理术语中称为“词元（token）”）。这会改善输出结果，但因为n<6，生成的文本仍然缺乏连贯性。\n这些朴素语言模型的记忆能力始终有限，因此无法生成超过一定长度的连贯文本 。尽管如此，它们仍具备一定用途。几年前，朴素语言模型被广泛用于文本分类和语音识别，且如今仍被用于语言识别等任务。 然而，对于更高级的文本理解和文本生成任务来说，朴素语言模型就捉襟见肘了。因此需要神经网络。\n3\n基于神经网络的语言模型\n现代语言模型基于（人工）神经网络。 神经网络是受人脑启发开发出的计算机，能够通过任务示例学习如何执行任务 。这种机器学习形式也被称为深度学习，因为其中的网络由多个计算层组成（因此被称为“深度”网络）。 在神经网络中，通过遍历任务示例并迭代修改网络参数以优化任务目标，从而实现学习。你可以将这些参数想象成一组旋钮（knob），通过左右旋动以改进目标 ，但区别是计算机为你进行改进，并且知道如何同时正确地朝着改进方向进行调整（得益于著名的反向传播算法）。因此，网络会遍历任务示例（通常以几百个示例为一批），并在这一过程中优化目标。以下是一个正在被优化的目标示例（称为成本函数，数值越小越好）：\n成本函数随训练迭代次数的变化。图源：《机器学习导论》\n随着模型的训练，成本函数值会逐渐下降，意味着模型在任务处理上变得更加优秀。\n在该案例中，我们想要生成文本。 目前，标准的方法是训练一个模型，通过前面的单词预测后面的单词 。由于下一个单词有多种可能性，模型会学习为每个可能的单词关联一个概率。以下是对“the cat sat on the”之后可能出现单词的概率分布可视化图像：\n改编自《机器学习导论》\n一旦拥有这样的预测模型，就可以从预测概率中抽样来生成下一个单词，然后将该单词重新输入到网络，再抽样一个新的单词，以此类推，从而生成文本 。这一生成过程被称为自回归，因为网络利用自己的预测结果来生成新的预测。虽然有人对此提出了异议，认为这并非最佳方法，但目前而言，这就是最实际的操作方式，且效果也出奇地好！\n曾经用于建模语言的第一类神经网络是RNN（循环神经网络），尤其是LSTM（长短期记忆网络）架构。 RNN就像一个逐个阅读单词的小型机器，在这一过程中，它会更新思维状态（mental state） 。这种思维状态如同记忆，使它们在生成文本时能大概记住一段时间的信息。 相比n-gram模型，RNN模型能够获得更为优越的性能。\nRNN创建之初，神经对话模型（Neural Conversational Model）给我留下了深刻印象。该模型是Google于2015年基于电影字幕训练的一个特殊LSTM模型，当时这个模型已经算得上是LLM了（也可能是首个LLM）：拥有3亿参数，并在约10亿词的数据上进行训练。由于电影字幕是对话形式，因此可直接将该模型用作聊天机器人。以下是不同人类输入对应的回答示例：\n人类：你叫什么名字？\n机器：我叫朱莉娅。\n人类：你是什么时候出生的？\n机器：7月20日。\n人类：我的名字是大卫。我的名字是什么？\n机器：大卫。\n人类：天行者是谁？\n机器：他是一个英雄。\n人类：天空是什么颜色？\n机器：蓝色。\n人类：血液是什么颜色？\n机器：和黑眼圈的颜色相同。\n如你所见，该模型能够闲聊，同时也对世界有一定了解，这些知识完全是通过学习如何预测文本获得的！我记得自己曾对这一事实很感兴趣： 学习预测文本迫使你理解世界 （但并不意味着这个过程很容易）。然而，该模型也有一些明显的短板：它经常出错，并且与类似基于LSTM的模型一样，无法生成长篇连贯的文本。理论上，循环神经网络可以长时间记忆事物， 但在实践中，它们却往往很快就忘记了：经过几十到一百个词之后，它们就会开始偏离主题，不再连贯。\n2017年，人们针对短期记忆问题提出一种解决方案——Transformer 。Transformer是一种基于注意力机制的新型神经网络架构（本质上是一种选择操作），下图来自介绍Transformer的论文，用以说明其在翻译任务中的工作原理：\nTransformer架构。来源：https://arxiv.org/abs/1706.03762\nTransformer在各个方面都可圈可点，但最值得一提的是， 该架构在文本建模方面表现非常出色，并且很适合在GPU上运行，从而处理（和学习）大量数据。正是有了Transformer这种架构，才使得现代LLM得以兴起（或至少起到了很强的促进作用）。\n4\n现代语言大模型\nTransformer的发明标志着现代LLM时代的开始。 自2018年以来，AI实验室开始训练规模越来越大的模型。令众人惊讶的是，这些模型的质量也在不断提高！下图对这些模型进行了可视化，我们将重点介绍其中值得关注的模型：\nLLM进化树。来源：https://github.com/Mooler0410/LLMsPracticalGuide\n这些语言模型主要分为三类。一是“仅编码器（encoder-only）”组（上图中的粉色部分），该类语言模型擅长文本理解，因为它们允许信息在文本的两个方向上流动。二是“仅解码器（decoder-only）”组（上图中的蓝色部分），该类语言模型擅长文本生成，因为信息只能从文本的左侧向右侧流动，以自回归方式有效生成新词汇。三是“编码器-解码器（encoder-decoder）”组（上图中的绿色部分），该类语言模型对上述两种模型进行了结合，用于完成需要理解输入并生成输出的任务，例如翻译。\n这一切都主要始于文本理解类模型。最初是使用RNN的ELMo，之后是谷歌著名的BERT模型及其派生模型（如RoBERTa），它们都基于Transformer。 这些模型通常具有几亿个参数（相当于约1GB的计算机内存），在大约10GB到100GB的文本上进行训练（通常为几十亿个单词），并且可以在现代笔记本电脑上以约0.1秒的速度处理一段文本。 这些模型极大地提升了文本理解任务的性能，如文本分类、实体检测和问题回答等。 这已然是NLP（自然语言处理）领域的一场革命，不过才刚刚拉开序幕……\n在文本理解类语言模型发展的同时， OpenAI开始基于Transformer创建文本生成类语言模型 。首先是2018年的GPT-1，有1亿个参数； 然后是2019年的GPT-2，拥有高达15亿个参数，并在40GB的文本上进行了训练。至少对我来说，GPT-2的创建是一个至关重要的时刻。 以下是GPT-2可以生成的文本示例，从一个由人类撰写的段落开始：\n来源：https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n生成的英语文本质量很不错，而且具有连贯性。例如，科学家的名字没有改变，而这在基于RNN的模型中是个经典问题。 由于GPT-2在所生成文本的质量上取得了巨大突破，为避免滥用，OpenAI最初决定不向公众发布。 可以说GPT-2标志着LLM正朝着正确的方向发展。 需要注意的是：使用这类语言模型需要先提供一个起始文本，这个起始文本被称为提示（prompt）。\n一年后（2020年），OpenAI创建了GPT-3。 GPT-3是一个具有1750亿个参数的模型 （需要700GB的计算机内存来存储模型！）， 该模型不仅规模显著扩"
  },
  {
    "title": "海南省三亚市中级人民法院一站式域外法服务平台建设项目初步设计及概算获批_项目审批_海南省发展和改革委员会",
    "page_body": "索 引 号： 00817382-X/2023-04438 主题分类：\n发文机关： 省发展和改革委员会 成文日期： 2023-10-25 16:46 \n标 题：  海南省三亚市中级人民法院一站式域外法服务平台建设项目初步设计及概算获批 \n文 号： 琼发改审批函〔2023〕878号 发布日期：  2023-10-25 16:46 \n备案登记号：\n废止日期： 时 效 性：\n有效\n有效\n海南省三亚市中级人民法院一站式域外法服务平台建设项目通过与协会组织、科研机构、高校等社会资源合作，利用人工智能、大数据技术高效整合查明资源，分类分析国内外涉外法律案例，翻译整理外国法律，形成域外法查明知识库，打造集域外法查明案例、涉外裁判文书、域外法律法规、行政条例等内容检索以及域外法查明、专项尽职调查、域外法律咨询等服务一体化的线上服务平台，实现域外法已查明案例的深度运用。2023年10月，我委批复了海南省三亚市中级人民法院一站式域外法服务平台建设项目初步设计及概算。主要建设内容及规模：建设一站式域外法服务平台19大功能模块。进行域外法查明案例数据库、涉外裁判文书数据库、域外法律法规数据库、学术资料数据库建库及数据治理。完成OCR模型、劳动争议类计算器模型、侵权纠纷类计算器模型、借款合同纠纷类计算器模型、关键词扩展模型搭建。实现与电子签章平台对接接口、短信平台、云审互联网庭审系统、海易办、海政通、海南国际商事纠纷诉调裁三位一体平台、海南法院智慧审判系统对接。建立数据处理能力、基础组件能力、翻译页面适配能力、智能搜索扩展能力、流程管理引擎等底层能力。购买代码审计、入网安全评估服务、安全加固服务等安全服务。购买音视频服务、短信服务、语音转文字服务。审核后的项目投资概算为595.44万元。"
  },
  {
    "title": "【学员论坛第一期】高质量期刊论文的编辑经验与写作方法-中国社会科学网",
    "page_body": "　　4月27日，中国社会科学院党校第47期处级干部进修班第一期学员论坛开讲，论坛主题为“高质量期刊论文的编辑经验与写作方法”。进修班临时党支部书记、金融研究所银行研究室主任李广子主持论坛。\n高质量期刊论文的“三新”问题与写作方法\n　　高质量期刊的“三新”规定是指新观点、新方法、新材料。所谓新观点，指的是相关论题在学术史上首次提出，具有首创性，观点对学术界的成说具有大幅度的深化与延展。新方法，指的是研究方法首次使用、综合运用其他研究方法有效解决问题。要说明该方法是否具有理论支撑、是否解决问题、以往为何没有使用。新材料，指的是在文学史上具有重要影响的作家、作品、文学现象等方面新发现的资料，属于首次发现、首次发表。另外，还对“三新”的写作规范作了说明。\n学术论文的评价标准及其写作规范\n　　一篇高质量的学术论文至少要具备创新性、学术性、现实性、思想性、规范性等五个标准。创新性是首要的也是最重要的标准，学术性要求揭示现象背后的规律性认识，现实性要求要有问题意识、能够对现实有指导作用，思想性体现在文章能够言之有理、符合逻辑、给人以新的启示，规范性要求文章的题目要合适、摘要要突出重点、关键词要准确、论证要严密、语言要通畅。同时，对大家关心的如何投稿问题给出了中肯建议。\n英文期刊论文的编辑经验与写作技巧\n　　在英文期刊论文编辑经验方面，分别从定位、选稿、语言三个角度进行了分享。其中，期刊的定位应是传播好中国声音，讲好中国故事，把我国最新的经济学研究成果介绍给国外，扩大中国经济学研究的国际影响力；应根据期刊定位来进行选稿；语言是英文期刊最常见的问题，考验着编辑对英文语言的掌握，需要咬文嚼字，应该说目前在“信”方面把握得还可以，而在“达、雅”方面尚需继续努力。在写作技巧方面，从选题、语言、投稿、研究无禁区宣传有纪律等方面作了介绍。\n　　\n　　当代中国研究所社会史研究室副主任吴超、政治学研究所比较政治研究室主任郭静、农村发展研究所城乡关系研究室副主任年猛、美国研究所美国外交研究室副主任王玮、世界经济与政治研究所国际政治经济学研究室副主任马涛等学员作为对话人作了交流发言。\n　　吴超认为，高质量论文选题应聚焦现实重大需要，文献资料获取和使用要注意使用的限度，要巩固基础学科、发展重点学科、扶持交叉学科、培育新兴学科，不断完善学科体系。要以学术为基础，以政治为导向，胸怀党之大者、国之大者、民之大者，坚持基础研究与对策研究并重，两支笔两手抓，兼顾学术论文与普及读物。郭静从政治学研究对象的特殊性角度作了交流发言，重点从如何加强实践调研、如何解读历史、如何进行国际比较、如何进行纵向比较等角度分享了治学心得。年猛以扎实做学问、胸怀国之大者为中心展开，强调高质量论文要有好想法、手要快、组团队、讲好中国故事。王玮结合自己的经历，讲了社会科学研究工作中的立场、观点、方法问题，同时，从读者和审稿人对期刊建设的促进作用方面谈了看法。马涛分享了两点体会，一是要做到笔耕不辍，二是要做到“一气呵成”，并对国际经济学理论实证论文撰写经验谈了看法。"
  },
  {
    "title": "数据质量评估与优化-豆丁网",
    "page_body": "数据质量评估与优化 数据质量评估指标体系 数据质量分析方法 数据清洗与预处理 数据质量优化策略 数据质量监控与反馈 数据质量提升技术 数据质量与业务价值关联 数据质量管理实践案例 数据质量评估指标体系 数据质量评估与优化 数据质量评估指标体系 准确性 1.准确性是数据质量评估的核心指标，指数据与真实世界的一致程度。在评估数据准确性时，需考虑数据来源的可靠性、数据采集过程中的误差控制以及数据清洗和校验的严格性。 2.前沿技术如机器学习算法在数据准确性评估中的应用日益增多，通过构建预测模型，可以更精确地评估数据的准确性。 3.结合大数据分析，可以通过多维度、多角度的数据交叉验证，提高数据准确性的评估效果。 完整性 1.数据完整性指数据集中所有必要信息的齐全程度，包括数据的全面性和无遗漏。完整性评估关注数据是否包含所有相关字段，以及是否存在缺失值。 2.随着数据量的增长，完整性评估需要采用自动化工具和算法，如数据集成技术，以高效识别和填补数据缺失。 3.完整性评估应结合行业标准和业务需求，确保数据在业务应用中的完整性和可用性。 数据质量评估指标体系 一致性 1.数据一致性指数据在不同时间、不同系统或不同来源间的一致性。评估数据一致性时，需关注数据定义、格式、编码等方面的统一性。 2.在多源数据融合过程中，一致性评估尤为重要，通过数据映射和转换技术，确保数据的一致性。 3.前沿技术如区块链在数据一致性保证中的应用，可以提供不可篡改的数据记录，增强数据的一致性。 时效性 1.数据时效性指数据反映现实世界最新状态的能力。时效性评估关注数据更新频率、数据滞后时间以及数据更新的及时性。 2.随着实时数据分析的需求增加，时效性评估变得越来越重要。通过引入实时数据流处理技术，可以提高数据时效性的评估标准。 3.结合物联网和边缘计算技术，可以实时采集和处理数据，从而提高数据时效性的评估效果。 数据质量评估指标体系 安全性 1.数据安全性指数据在存储、传输和处理过程中受到保护的程度。安全性评估包括数据加密、访问控制、备份恢复等方面。 2.随着网络安全威胁的日益严峻，数据安全性评估需要采用先进的安全技术和策略，如人工智能辅助的安全防护系统。 3.遵循国家网络安全法律法规，确保数据在各个生命周期阶段的安全性，是数据安全性评估的重要任务。 可用性 1.数据可用性指数据在需要时能够被访问和使用的能力。可用性评估关注数据的访问速度、数据格式兼容性以及数据接口的稳定性。 2.随着云计算和大数据技术的发展，数据可用性评估需要考虑数据的分布式存储和访问，以及数据服务的可靠性。 3.通过构建高效的数据服务架构和优化数据管理流程，可以提高数据的可用性，满足用户对数据资源的实时需求。 数据质量分析方法 数据质量评估与优化 数据质量分析方法 数据完整性分析 1.数据完整性是数据质量评估的核心要素之一，它确保数据在存储、传输和处理过程中不丢失、不损坏、不重复。 2.分析方法包括数据一致性检查、数据唯一性验证和完整性约束检查，以识别和纠正数据中的错误和不一致性。 3.结合区块链技术，可以通过分布式账本确保数据的不可篡改性，提高数据完整性分析的有效性和可靠性。 数据准确性分析 1.数据准确性是指数据值与真实值的接近程度，是数据质量评估的基础。 2.分析方法包括直接比较、交叉验证和统计分析，通过历史数据、行业标准或第三方数据源来验证数据的准确性。 3.利用机器学习模型进行预测性分析，可以提前识别潜在的数据准确性问题，提高数据处理的准确性。 数据质量分析方法 数据一致性分析 1.数据一致性分析关注数据在不同系统、不同时间点是否保持一致，是数据质量的关键指标。 2.分析方法包括数据比对、数据同步和版本控制，确保数据在不同应用场景下的统一性。 3.通过引入数据治理框架，实现数据的一致性管理，降低数据不一致带来的风险。 数据及时性分析 1.数据及时性是指数据能够及时更新和反映实际情况的能力，对业务决策至关重要。 2.分析方法包括数据延迟监控、实时数据处理和事件驱动架构，确保数据更新与业务需求同步。 3.结合边缘计算和云计算技术，实现数据的实时处理和快速响应，提高数据及时性。 数据质量分析方法 数据安全性分析 1.数据安全性分析关注数据在存储、传输和处理过程中的安全风险，是数据质量评估的重要方面。 2.分析方法包括安全漏洞扫描、访问控制和数据加密，确保数据不被未授权访问或篡改。 3.随着物联网和大数据的发展，引入人工智能和机器学习技术，提高数据安全分析的能力和效率。 数据可靠性分析 1.数据可靠性分析关注数据在长期存储和使用过程中的稳定性和可信赖程度。 2.分析方法包括数据备份、恢复测试和系统稳定性监控，确保数据在面临故障或灾难时的可靠性。 3.结合云计算和分布式存储技术，实现数据的冗余存储和快速恢复，提高数据可靠性。 数据质量分析方法 1.数据可用性分析关注数据是否能够被用户或系统有效访问和使用，是数据质量评估的重要指标。 2.分析方法包括性能监控、用户反馈和系统优化，确保数据服务的稳定性和响应速度。 3.通过引入自动化工具和智能化服务，提高数据可用性，满足用户和系统的实时需求。 数据可用性分析 数据清洗与预处理 数据质量评估与优化 数据清洗与预处理 缺失值处理 1.缺失值的存在会影响数据分析结果的准确性和可靠性，因此处理缺失值是数据清洗与预处理的重要环节。 2.常用的缺失值处理方法包括删除、插值、多重插补等，根据数据的特点和分析需求选择合适的方法。 3.在处理缺失值时，应遵循数据挖掘和统计分析的原则，确保数据质量和分析结果的合理性。 异常值处理 1.异常值是数据集中偏离整体趋势的数值，可能会对分析结果产生不良影响。 2.异常值处理方法包括可视化、统计检验和聚类分析等，通过多种手段识别和去除异常值。 3.异常值处理应谨慎进行，避免误删重要信息，同时要关注异常值背后的潜在原因。 数据清洗与预处理 数据标准化 1.数据标准化是使不同特征量级的变量在同一尺度上进行比较的重要步骤。 2.常用的标准化方法包括Z-score标准化、Min-Max标准化等，选择合适的方法可以保证数据的有效性。 3.标准化过程需要遵循数据分布和特征量级的分析，避免因标准化导致的偏差。 数据类型转换 1.数据类型转换是保证数据一致性、准确性和完整性的基础。 2.常用的数据类型转换包括将字符串转换为数值、将数值转换为日期等，根据实际需求选择合适的转换方法。 3.数据类型转换过程中，要注意数据的一致性和转换结果的准确性，避免产生新的错误。 数据清洗与预处理 数据整合与融合 1.数据整合与融合是将来自不同来源、格式和结构的数据进行统一和整合的过程。 2.数据整合方法包括合并、连接、汇总等，旨在提高数据利用价值和分析效果。 3.数据整合过程中，要注意数据的一致性和完整性，确保分析结果的可靠性。 数据脱敏 1. 数据脱敏是保护个人隐私和商业秘密的重要手段，尤其在数据共享和公开分析时。 2. 数据脱敏方法包括随机化、加密、遮挡等，根据数据敏感度和保护需求选择合适的方法。 3. 数据脱敏过程中，要平衡数据安全和数据质量，确保脱敏后的数据仍具备分析价值。 数据质量优化策略 数据质量评估与优化 数据质量优化策略 数据清洗与预处理 1. 数据清洗是数据质量优化的基础步骤，旨在识别和纠正数据中的错误、异常和不一致性。 2. 预处理包括数据格式转换、缺失值处理、异常值处理和数据标准化，以提高数据的可用性和一致性。 3. 利用最新的数据清洗工具和技术，如自动化的数据清洗平台，可以大幅提升清洗效率和准确性。 数据质量监控与反馈 1. 数据质量监控通过建立数据质量指标体系，实时跟踪数据的质量变化。 2. 采用大数据分析技术，如机器学习算法，对数据质量进行预测和预警，确保数据持续满足业务需求。 3. 通过建立反馈机制，及时收集用户对数据质量的反馈，持续优化数据质量管理体系。 数据质量优化策略 数据治理与规范 1. 数据治理是确保数据质量长期稳定的关键，包括数据分类、数据标准和数据生命周期管理。 2. 制定和执行数据治理策略，规范数据采集、存储、处理和共享流程，提升数据质量。 3. 结合行业最佳实践和法规要求，不断优化数据治理体系，以适应数据驱动决策的趋势。 数据质量评估模型 1. 数据质量评估模型通过量化指标评估数据的质量，如准确性、完整性、一致性、及时性和可靠性。 2. 采用多维度的评估方法，结合业务需求和数据特性，构建全面的数据质量评估体系。 3. 利用先进的评估模型，如深度学习算法，对数据进行智能评估，提高评估的准确性和效率。 数据质量优化策略 1. 应用数据质量提升技术，如数据脱敏、数据加密和数据压缩，保障数据安全和隐私。 2. 采用自然语言处理、图像识别等技术，对非结构化数据进行处理，提高数据的可用性。 3. 结合云计算和边缘计算技术，实现数据质量优化的实时性和高效性。 数据质量教育与培训 1. 通过数据质量教育与培训，提高员工对数据质量重要性的认识，增强数据质量意识。 2. 定期开展数据质量相关课程，提升员工在数据清洗、评估和优化方面的技能。 3. 建立数据质量文化，鼓励员工积极参与数据质量优化工作，形成全员参与的良好氛围。 数据质量提升技术 数据质量监控与反馈 数据质量评估与优化 数据质量监控与反馈 数据质量监控框架构建 1. 建立全面的数据质量监控体系，涵盖数据采集、处理、存储和使用的全过程。 2. 采用多层次监控策略，包括数据准确性、完整性、一致性和时效性等关键指标。 3. 结合先进的数据分析技术和机器学习模型，实现自动化数据质量评估和异常检测。 实时数据质量监控技术 1. 利用实时数据处理技术，如流处理框架（如Apache Kafka），确保监控数据的实时性。 2. 应用复杂事件处理（CEP）技术，对数据流进行连续监控，及时发现数据质量问题。 3. 集成可视化工具，提供实时监控界面，便于用户快速定位和响应数据质量问题。 数据质量监控与反馈 1. 制定科学的数据质量评估指标体系，包括数据准确性、一致性、完整性、有效性和时效性等。 2. 结合行业标准和业务需求，设计针对性的评估模型，确保评估结果的客观性和有效性。 3. 定期对评估指标进行更新和优化，以适应数据质量和业务环境的变化。 数据质量反馈机制 1. 建立有效的数据质量反馈机制，确保数据质量问题能够被及时发现并得到解决。 2. 设立数据质量管理团队，"
  },
  {
    "title": "问答类AI产品的运行原理是怎样的？-搜狐",
    "page_body": "问答类AI产品的运行原理可以概括为“理解问题→检索/生成答案→输出结果”的流程，其核心技术依赖自然语言处理（NLP）、机器学习（ML）和大规模数据训练。以下是具体实现步骤和技术细节：\n一、核心运行流程 \n1.输入处理与意图识别\n语音/文本输入：语音通过ASR（自动语音识别）转为文本，文本通过纠错、分词等预处理。\n语义理解：通过NLP技术（如句法分析、实体识别、情感分析）提取关键词、意图（如“问天气”或“查百科”）和上下文关系。\n示例：用户输入“明天北京会下雨吗？”→系统识别意图为“天气查询”，提取实体“北京”“明天”。\n2.知识检索与推理\n结构化数据（如知识图谱、数据库）：直接匹配实体和关系（如查询天气API）。\n非结构化数据（如文档、网页）：通过Embedding技术将文本转为向量，用相似度搜索（如FAISS）匹配相关内容。\n生成式模型（如GPT4）：基于预训练的大语言模型（LLM），通过注意力机制生成连贯回答，支持逻辑推理（如数学题）。\n3.答案生成与优化\n检索式问答：从候选答案中选择最相关结果（如 BM 25算法排序）。\n生成式问答：模型按概率生成逐词输出，通过温度（temperature）参数控制创造性。\n后处理：过滤敏感词、调整格式（如列表化）、添加免责声明。\n4.反馈学习（可选）\n用户对答案的点赞/纠错会被记录，用于模型微调（Finetuning）或强化学习（RLHF）。\n二、关键技术支撑 \n1.自然语言理解（ NLU ）\n预训练模型：BERT、RoBERTa等用于语义编码，解决一词多义（如“苹果”指水果或公司）。\n多轮对话管理：通过对话状态跟踪（DST）处理上下文依赖（如“它多少钱？”指代前文提到的商品）。\n2.知识管理与检索\n混合检索：结合关键词搜索（快速）与语义搜索（精准），如 Elasticsearch +向量数据库。\n实时更新：新闻类问答需接入API，知识库类需定期增量训练。\n3.生成模型优化\n控制生成：通过Prompt工程（如“请用一句话回答”）约束输出长度和风格。\n事实核查：对生成内容进行知识库交叉验证，减少“幻觉”（Hallucination）。\n三、典型架构示例 \nPlaintext （指未经加密处理的原始文本数据）\n用户输入→语音识别（ASR）→文本预处理→意图识别（NLU）→知识检索/生成→答案排序→后处理→输出（文本/语音）\n四、挑战与解决方案 \n1.歧义问题\n例：“ 李娜 是谁？”可能是歌手或网球运动员。\n方案：通过用户画像或对话历史消歧（如用户此前聊过体育）。\n2. 长尾 问题\n罕见问题（如“ 量子计算 对水稻种植的影响”）可能无现成答案。\n方案：调用搜索引擎或引导用户重新提问。\n3.实时性要求\n如股票价格、交通路况需对接实时API，而非依赖静态知识库。\n五、应用场景差异 \n1.客服机器人：侧重精准匹配知识库，严格限制生成自由度。\n2.教育问答：需结合解题步骤（如数学公式推导）和生成式解释。\n3.医疗咨询：依赖权威数据库（如医学文献），答案需附加风险提示。\n六、未来方向 \n1.多模态问答：支持图片、视频输入（如问“这张植物照片的品种”）。\n2.个性化适配：根据用户知识水平（如儿童vs专家）调整回答复杂度。\n3.端侧部署：小型模型（如Tiny Llama）在手机端运行，保护隐私。\n通过以上技术组合，问答AI实现了从简单检索到复杂推理的多样化能力，但其效果高度依赖训练数据质量、算力支持及工程优化。[微风]"
  },
  {
    "title": "大模型落地，跑得快更要跑得稳_数字经济_福建省经济信息中心",
    "page_body": "　　比盛夏的上海更火热的是2024世界人工智能大会暨人工智能全球治理高级别会议（以下简称“WAIC2024”）。大会线下参观人数突破30万人次，创历史新高。\n　　值得注意的是，WAIC2024的首发首秀不仅涉及模型更新换代，还涵盖应用、平台、系统等。行业和观众的目光更多投向与模型落地紧密相关的交互体验、商业模式等领域。\n　　一个引发广泛关注的问题是，随着大模型能力不断增强，其安全性、可靠性、可控性也日益受到挑战。尤其是面对行业用户合法合规、精准可控等要求，大模型可能存在的数据安全、幻觉等成为绕不过的问题。\n　　中国信息通信研究院华东分院人工智能事业部主任常永波说，应用价值与应用安全是大模型发展的两翼，当前大模型已进入快速迭代期，在积极探索落地应用的同时，大模型厂商也要高度重视应用场景需求下对安全的行业要求。\n　　技术自身缺陷不容忽视\n　　依托庞大参数规模、海量训练数据、强大算力资源，大模型作为人工智能领域最热门的技术分支，已在多个领域表现出超越人类的能力。\n　　“金融、医疗、教育、政务、制造等众多领域都在积极探索大模型安全应用范式，以应对大模型安全风险。”常永波介绍，伴随大模型的深度应用，产学研用各方都在加强大模型安全威胁和防御技术体系研究。在原有可信人工智能治理体系框架基础上，提升大模型的鲁棒性、可解释性、公平性、真实性等能力成为行业研究热点。安全评测技术和安全防御技术的不断成熟，有效护航大模型发展。\n　　WAIC2024上，清华大学、中关村实验室、蚂蚁集团等机构联合撰写的《大模型安全实践（2024）》白皮书（以下简称“白皮书”）正式发布。白皮书显示，大模型技术存在自身缺陷，包括生成内容不可信、能力不可控以及外部安全隐患等问题。\n　　“幻觉是大模型目前比较难解决的问题。”常永波说，模型在遵循语法规则的同时，可能产生虚假或无意义的信息。这一现象源于大模型基于概率推理的输出方式。它可能导致对模糊预测的过度自信，从而编造错误或不存在的事实，影响生成内容的可信度。“智能涌现”是大模型的另一种效应，它可以让模型展现出色性能，也具有突发性、不可预测性和不可控性等特征。\n　　另外，大模型的脆弱性和易受攻击性使外部安全隐患难以消除。相关数据显示，随着大模型技术快速发展，相关网络攻击也在增多。\n　　聚焦安全可靠可控性建设\n　　大模型带来的种种风险，对监管方、学术界、产业界是全新且不可回避的问题。\n　　近年来，《互联网信息服务算法推荐管理规定》《互联网信息服务深度合成管理规定》《生成式人工智能服务管理暂行办法》《科技伦理审查办法（试行）》等政策法规相继发布，搭建起我国人工智能治理的基本框架。一系列政策法规坚持发展与安全并重原则，强化科技伦理风险防控，从技术发展与治理、服务规范、监督检查与法律责任等层面对大模型安全发展提出要求。\n　　白皮书提出，构建大模型安全政府监管、生态培育、企业自律、人才培养、测试验证“五维一体”的治理框架。\n　　在监管方面，常永波介绍，敏捷治理正成为一种新型治理模式。该模式以柔韧、流动、灵活及自适应为特点，倡导多元利益相关者共同参与，能快速响应环境变化。在实施治理策略时，结合柔性伦理规范和硬性法律法规，构建完善的治理机制，在规制大模型风险的同时平衡创新与安全。\n　　“为确保大模型在实际应用中发挥最大效能，防止潜在风险和滥用，大模型建设通常会聚焦三个重要维度：安全性、可靠性和可控性。”蚂蚁集团安全实验室首席科学家王维强解释，安全性意味着确保模型在所有阶段都受到保护，防止任何未经授权的访问、修改或感染，保障人工智能系统无漏洞、免诱导；可靠性要求大模型在各种情境下都能持续提供准确、一致、真实的结果，这对于决策支持系统尤为重要；可控性关乎模型在提供结果和决策时能否让人类了解和介入，以便人类根据需要进行调适和操作。\n　　王维强特别提到时下备受关注的Agent（智能体）。他说，Agent是目前大模型落地的关键路径，但复杂的Agent体系进一步扩大了大模型风险敞口。目前RAG（检索增强生成）、指令遵循、知识图谱嵌入等方法可有针对性地提升模型输出的可控性和准确性。\n　　合力推动人工智能健康发展\n　　“目前来看，让大模型完全不犯错几乎不可能，但减小犯错几率，减弱错误危害性，是可以做到的。”常永波说，安全治理需产学研共同发力，中国信息通信研究院已开展一系列标准和测评研究，头部厂商也在加速构建自身的安全和治理体系。\n　　蚂蚁集团安全内容智能负责人赵智源介绍了相关经验。一方面，在大模型产品投入应用前，企业需做好全面评测，对暴露出的安全问题展开针对性防御，把好入口关；相关产品进入市场后，也要时刻监控可能出现的风险隐患，进行技术补救和改进。另一方面，模型技术通常跑在安全技术前，行业研究要保持一定前瞻性。\n　　“我们很早就开始探索基于安全知识构建视觉领域生成内容风险抑制的技术。在多模态大模型发布后，我们又将这一技术集成到多模态基座中，降低风险内容生成比例。”赵智源介绍，蚂蚁集团已构建起面向产业级应用的大模型安全一体化解决方案“蚁天鉴”2.0版本，形成包括大模型基础设施测评、大模型X光测评等在内的测评和防御技术链条，并已运用于金融、政务、医疗等专业场景下的AI应用全流程。\n　　常永波说，大模型落地门槛正在大幅降低，大量中小企业在模型安全治理方面的能力较弱，有些甚至不符合基本的合规要求。解决这些问题，需要监管的进一步引导和头部厂商的能力释放。\n　　“我们现在已把‘蚁天鉴’的测评能力框架开源，将来也会把检测能力以及对风险的认知更多分享到平台上，它可以适配较多模型。希望我们提供的开放能力能帮助大模型行业持续健康发展。”王维强说，模型厂商离用户最近，可第一时间发现安全隐患，并通过和监管保持良性沟通互动，助力大模型安全落地。\n　　清华大学长聘副教授李琦认为，大模型安全应用是一个新兴领域，研究和应用尚处于起步阶段。随着新的实践不断深入发展，相关技术也会持续升级，为建构大模型安全实践范式打造高价值参考体系。\n　　人工智能治理是全球性问题。WAIC2024开幕式上发布的《人工智能全球治理上海宣言》提出，高度重视人工智能的安全问题。宣言强调，以发展的眼光看问题，在人类决策与监管下，以人工智能技术防范人工智能风险，提高人工智能治理的技术能力。宣言呼吁，推动制定和采纳具有广泛国际共识的人工智能的伦理指南与规范，引导人工智能技术的健康发展，防止其被误用、滥用或恶用。（崔爽）"
  },
  {
    "title": "大模型评测介绍（步骤、数据集、工具平台）-CSDN博客",
    "page_body": "对 大模型 的评测需围绕 能力维度、评测方法、结果验证 三大核心展开，通过“定维度→选方法→验结果”的流程，全面 评估模型 的真实表现，而非单一指标判定。\n评测步骤\n一、明确核心评测维度\n先确定要评估模型的哪些能力，避免无目标测试。常见核心维度可分为5类，覆盖模型的主要应用场景：\n基础语言能力 ：评估模型对语言的理解与生成准确性，包括文本摘要、翻译、语法纠错、多轮对话连贯性（如是否遗忘前文信息）。 知识与事实准确性 ：验证模型知识储备的广度与正确性，例如回答历史事件、科学原理、常识问题时，是否存在“一本正经胡说八道”的幻觉。 逻辑与推理能力 ：测试模型的深度思考能力，包括数学计算（如复杂方程求解）、逻辑推导（如“如果A→B，B→C，那么A是否→C”）、代码生成与调试（如根据需求写Python函数并通过测试）。 安全与合规性 ：检查模型的风险控制能力，例如面对恶意请求（如生成违法内容、歧视言论）时，是否能拒绝或引导，是否符合数据隐私规范。 效率与实用性 ：评估模型的落地可行性，包括响应速度（单轮回答耗时）、token消耗（生成1000字的成本）、多模态支持（如文生图、图生文的匹配度，若为多模态模型）。\n二、选择合适的评测方法\n根据评测维度，搭配“自动评测+人工评测+真实场景验证”的组合方法，兼顾效率与准确性：\n1. 自动评测：高效量化基础表现\n依赖 标准化 数据集和工具，快速输出客观分数，适合初步筛选和批量对比。\n使用公开基准数据集 ：直接用行业公认的数据集测试，例如：\n通用能力：\nMMLU（Massive Multitask Language Understanding，大规模多任务语言理解。涵盖57个学科的知识与推理）是由加州大学伯克利分校研究人员于 2020 年 9 月推出的大模型测评基准，旨在全面评估大型语言模型的跨领域知识覆盖和复杂问题解决能力。 C-Eval（中文版本的MMLU，适合中文模型）； SuperCLUE：SuperCLUE 是由国内 CLUE 学术社区于 2023 年 5 月推出的中文通用大模型综合性评测基准，旨在全面评估中文大模型的能力表现。\n代码能力：\nSWE-bench Verified（真实软件工程任务，需生成可通过测试的代码补丁） HumanEval（生成单函数代码并验证）：由 OpenAI 于 2021 年开发的代码生成模型评测基准，用于评估大型语言模型根据自然语言描述生成正确代码的能力。\n安全能力：TruthfulQA（检测模型是否输出真实信息，而非迎合错误常识），测评大模型的幻觉率的。\n工具辅助自动化评估 ：用代码或平台自动比对结果，例如用Docker容器运行模型生成的代码，检查是否通过单元测试；用相似度算法（如BLEU、ROUGE）评估生成文本与参考文本的匹配度。\n2. 人工评测：补充自动评测的盲区\n自动评测无法覆盖“主观体验”和“复杂场景”，需人工介入判断，适合深度评估。\n设定人工打分标准 ：针对目标场景制定明确维度，例如评测“客服类模型”时，可从“问题解决率、语气友好度、回复简洁性”3个维度打分（1-5分制）。 匿名对比测试 ：参考“Chatbot Arena”的模式，让评测者同时看2个模型的回答（隐藏模型名称），投票选择更优的一个，避免先入为主的偏见。 复杂任务场景模拟 ：让模型处理真实世界的复杂需求，例如“根据公司财报写一份3页的分析报告，并提出2个业务优化建议”，人工评估结果的逻辑性、实用性。\n3. 真实场景验证：落地前的“最后一关”\n在实际应用场景中测试模型，评估其“适配性”，避免“实验室表现好，落地用不了”的问题。\n小范围灰度测试 ：让部分真实用户使用模型（如内部员工、种子用户），收集反馈，例如“用模型辅助客服回复客户咨询，统计客户满意度、问题解决耗时是否提升”。 长期性能监控 ：上线后跟踪模型的稳定性，例如是否出现突发的响应延迟、准确率下降，是否在特定场景（如高峰时段）出现故障。\n三、验证评测结果的可靠性\n避免因“测试偏差”导致结论失真，需通过3个方式验证：\n复现性检查 ：确保同一测试流程下，多次运行的结果一致（如用同一数据集测试模型，2次得分误差不超过5%），排除环境配置、数据抽样的偶然误差。 对比基准模型 ：将待评测模型与“行业标杆”对比，例如“测试新中文模型时，与GPT-4o、通义千问等已知模型在同一数据集上打分，判断其相对水平”。 排除过拟合干扰 ：避免模型“背题”——测试集需与模型训练数据无重叠，且定期更新测试数据（如SuperCLUE每半月替换100%题库），确保模型是“真会”而非“死记硬背”。\n四、常见评测工具与平台推荐\n不同需求对应不同工具，可直接复用现有平台提高效率：\n自动评测工具 ：\nOpenCompass（支持多维度数据集，可本地部署） Hugging Face Open LLM Leaderboard（英文模型通用，在线查看各模型分数）；\n人工与场景评测平台 ：\nChatbot Arena（大模型竞技场，匿名对比对话能力） LM Arena（用户投票对比多模型表现）；\n垂直领域评测 ：\nMedBench（医疗领域专用） SWE-bench Verified（代码能力专用）。\n附：\n公开基准数据集\nMMLU-大型 语言模型 测评基准\n（Massive Multitask Language Understanding，大规模多任务语言理解）是由加州大学伯克利分校研究人员于 2020 年 9 月推出的大模型测评基准，旨在全面评估大型语言模型的跨领域知识覆盖和复杂问题解决能力。\nMMLU 包含 15,908 个多项选择题，覆盖 57 个学科领域，包括 STEM（如数学、物理、计算机科学）、人文（如历史、哲学、文学）、社会科学（如心理学、经济学、政治学）以及专业领域（如法律、医学、会计）等。题目难度跨度极大，从基础的小学生级别计算题到专业的 GRE、司法考试题目都有涉及。\n该基准采用英语作为统一测试语言，主要通过零样本（Zero-shot）和少样本（Few-shot）两种测试场景来评估模型。零样本测试要求模型在没有见过任何特定任务示例的情况下回答问题，少样本测试则会提供一些示例问题供模型参考。其评分标准基于模型在所有学科中正确回答的比例，分数范围从 0 到 100%。\nMMLU 通过统一提示词设计和标准化情境学习示例的 HELM 框架，解决了传统测评结果不一致的问题，能够更可靠地衡量模型的知识广度和推理深度，是目前大模型评测领域广泛应用的基准之一。它也启发了一系列衍生版本，如 MMLU-Pro、MMMLU 和 MMLU-Redux 等，这些版本在 MMLU 的基础上对 数据集 进行了改进或拓展。\nMMLU 没有独立的官方网站，通常通过 https://paperswithcode.com/dataset/mmlu 来访问其相关信息和数据集。\n此外， https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu 这个链接可以查看 MMLU 基准测试上的最新技术状态排行榜。\nC- Eval  中文版MMLU\nC-Eval 是由上海交通大学、清华大学和爱丁堡大学研究人员于 2023 年 5 月联合推出的全面的中文基础模型评估套件。\n该套件包含 13948 个多项选择题，涵盖了 52 个不同的学科，包括自然科学、社会科学、工程技术、人文学科等，同时设有初中、高中、大学、专业四个难度级别，旨在全面评估大模型的中文理解、知识掌握和推理能力。C-Eval 的设计目标是帮助开发者更好地开发中文大模型，促进学术界和产业界科学地使用该基准帮助模型迭代，而不是单纯用于打榜。\nC-Eval 的官网地址为 https://cevalbenchmark.com/ 。\nSuperCLUE-中文通用大模型综合性评测基准\nSuperCLUE是由国内CLUE学术社区于2023年5月推出的中文通用大模型综合性评测基准，旨在全面评估中文大模型的能力表现。\nSuperCLUE构建了多层次、多维度的综合性测评基准框架，从基础到应用覆盖通用基准体系、文本系列基准、多模态系列基准、推理系列基准、Agent系列基准、AI应用系列基准、性能系列基准等。其测评内容主要包括以下几个方面：\n基础能力 ：涵盖语言理解与生成、知识理解与应用、逻辑与推理、计算、角色扮演、安全等10项基础能力。 专业能力 ：包括中学、大学与专业考试等内容，涵盖数学、物理、地理、社会科学等50多项能力，评估模型的知识广度和深度。 中文特性能力 ：针对中文特性任务设立专项评测，如成语理解、诗歌创作、字形分析等，考查模型对中文语言文化特色的处理能力。\nSuperCLUE具有以下特点：一是测评题库每2个月100%替换且全部原创，杜绝过拟合风险，体系维度也会根据大模型进展实时更新；二是测评方法与用户交互方式保持一致，测评任务贴近真实落地场景，高度还原用户视角；三是作为完全独立的第三方评测机构，不研发自家模型，保证评测的客观性和公正性。\nSuperCLUE还提供了SuperCLUE-OPEN（多轮开放式问答）、SuperCLUE-OPT（客观题闭卷测试）和\"琅琊榜\"匿名对战等核心评测体系，每月更新榜单，支持开发者横向对比模型效果，是中文大模型评测领域广泛应用的基准之一。\n官网地址是 https://www.superclueai.com/\nSWE-bench  Verified 软件工程基准测试\nSWE-bench Verified 是  OpenAI  推出的经过人工验证的软件工程基准测试，是 SWE-bench 的子集，旨在更可靠地评估 AI 模型解决现实世界软件问题的能力。\nSWE-bench 最初由普林斯顿大学和芝加哥大学的研究者提出，于 ICLR 2024 会议上发表。SWE-bench Verified 则是 OpenAI 与 SWE-bench 作者合作的成果，于 2024 年 8 月推出。该基准包含 500 个样本，这些样本是从 SWE-bench 原始测试集中通过人工注释筛选出来的，确保 单元测试 范围适当、问题描述清晰明确。\n在 SWE-bench Verified 中，AI 模型的任务是读取开源项目中的 issue 和相关代码，然后修改文件，使修改后的代码能够自动通过原项目的单元测试，以此来衡量模型是否能真实修复开源项目中的 bug。OpenAI 还与 SWE-bench 作者合作开发了一个新的评估工具，使用容器化的 Docker 环境，使在 SWE-bench 上进行评估更容易、更可靠。\nSWE-bench Verified 的官网地址为 https://www.swebench.com/ 。相关代码和数据也可在  GitHub  上获取，地址为 https://github.com/princeton-nlp/swe-bench 。\nHumanEval-代码生成模型评测基准\nHumanEval 是由 OpenAI 于 2021 年开发的代码生成模型评测基准，用于评估大型语言模型根据自然语言描述生成正确代码的能力。\n该基准包含 164 个原创的编程问题，每个问题都由自然语言描述（文档字符串）、函数签名和对应的单元测试组成。模型的任务是根据给定的描述和函数签名完成函数体的编写，评测时通过执行单元测试来验证生成代码的功能正确性。\nHumanEval 的主要评价指标是 pass@k，它衡量在 k 个生成的解决方案中至少有一个能通过单元测试的比例。例如，pass@1 表示模型生成 1 个代码样本时的通过率，pass@10 表示生成 10 个样本时的通过率。"
  },
  {
    "title": "类案全面检索基本标准问题研究-内蒙古自治区高级人民法院网",
    "page_body": "　　2017年8月1日起施行的《最高人民法院司法责任制实施意见（试行）》要求承办法官进行“全面检索，制作检索报告”，并视拟作出的裁判结果与本院同类生效案件裁判尺度比对结果，作出不同的处理。随着“全面检索，制作检索报告”口号的提出，案例检索也成为了法官们的一项基本工作，而对法官而言，其重要性又被摆到了一个全新的高度。\n 　　运用案例的前提是发现案例、研究案例，我们常说的案例检索以及《实施意见》提到的全面检索本身都是发现案例的过程，目前最高人民法院尚未发布“全面检索”的标准以及“检索报告”的范本，但是笔者始终认为检索应该是标准化的，包括检索流程与成果呈现，故笔者将案例检索标准化作如下相关思考：\n 一、案例检索的司法背景及意义\n 　　最高人民法院印发的《最高人民法院司法责任制实施意见（试行）》（以下简称《实施意见》），该实施意见自2017年8月1日起施行，明确实施类案与关联案件检索机制，同时规定法官在审理案件时，应当进行案件检索，制作检索报告，并据此分情况决定进一步的处理。具体规定如下：为进一步统一裁判尺度，《实施意见》新创设了类案与关联案件检索机制，明确承办法官在审理案件时，应当依托办案平台、档案系统、中国裁判文书网、法信、智审等，对最高人民法院已经审结或正在审理的类案与关联案件进行全面检索，制作检索报告，并分情形作出处理：拟作出的裁判结果与本院同类生效案件裁判尺度一致的，经合议庭评议后即可制作、签署裁判文书；拟作出的裁判结果将形成新的裁判尺度的，由院庭长决定或建议提交专业法官会议、审判委员会讨论；对拟作出的裁判结果将改变本院同类生效案件裁判尺度的，应当按程序提交专业法官会议、审判委员会讨论；如发现本院同类生效案件裁判尺度存在重大差异的，应层报审判委员会讨论决定。该《实施意见》的出台进一步统一了裁判尺度，规范法官在判案过程中的自由裁量权，保证法律的统一适用，尽可能减少“同案不同判”的现象，为接下来进一步推进司法案例制度改革的工作打下铺垫，做好准备。正是由于影响我国裁判尺度不一致的主要原因是理念和司法层面，因此，要想切实的构建统一的裁判尺度框架必须从司法层面切入，从理念到制度进行反思和完善，尽可能促进“同案同判”，实现公正审判，确保司法权威。\n 　　最高人民法院提出的“全面检索并制作检索报告”出发点是统一裁判尺度，实现同案同判，建立类案全面检索基本标准对统一裁判尺度具有重要意义。统一类案裁判尺度，保障法律适用的预见性和统一性，是推进司法责任制改革的重要内容。为探索构建“类案类判”工作机制，确保类案法律适用统一，如何统一类案证明标准，注重案例检索研学案例检索的方法、技巧，让法官以同类案件同样处理的方式办案，让当事人对裁判结果形成理性预期、增强司法公信力，建立类案检索机制，统一裁判尺度，显得尤为迫切和重要。\n 　　在中央关于“加强和规范司法解释和案例指导，统一法律适用标准”的要求下，准确高效的类案检索成为司法工作人员的迫切需求。在某些疑难案件的办理过程中，司法工作人员除依靠自身业务知识和经验外，经常需要检索和待办案件类似的案件，来把握相关案情的处理，建立类案及关联案件强制检索机制，为法官提供多维度、多层面的分析场景，通过自动检索、类案推送、等方法避免类案非类判现象。\n 雷德林克指出：“每个先例对未来的同类或类似性质的案件都具有某种指导力量。”因此，为了统一裁判尺度，构建统一裁判尺度是规范审判权运行的题中之义,是提高公众对司法信任程度的必然要求,以及法律指引功能得以发挥的重要前提。对裁判尺度进行统一,既可以避免毫无边界行使裁判权导致的不安全,也能避免一味地限制裁量权可能遭遇的相应制度无法协同。重视案例，尊重以前的案例，类似案例类似处理能够让法院对同等情形一视同仁、同案同判，实现法律适用的统一。是“法律面前人人平等”的应有之义。快速有效地搜索裁判案例，是提高办案效率的重要一环。案例检索对于规范法官自由裁量权的行使，提升裁判结果的预见性，打造公平、公正、透明、可预期的司法环境具有重要的实践意义。\n 二、案例可供检索的范围\n 　　最高人民法院于2016年8月29日发布的《最高人民法院关于人民法院在互联网公布裁判文书的规定》，该规定将于2016年10月1日正式实施，具体规定了裁判文书公开的范围，规定第三条 “人民法院作出的下列裁判文书应当在互联网公布：（一）刑事、民事、行政判决书；（二）刑事、民事、行政、执行裁定书；（三）支付令；（四）刑事、民事、行政、执行驳回申诉通知书；（五）国家赔偿决定书；（六）强制医疗决定书或者驳回强制医疗申请的决定书；（七）刑罚执行与变更决定书；（八）对妨害诉讼行为、执行行为作出的拘留、罚款决定书，提前解除拘留决定书，因对不服拘留、罚款等制裁决定申请复议而作出的复议决定书；（九）行政调解书、民事公益诉讼调解书；（十）其他有中止、终结诉讼程序作用或者对当事人实体权益有影响、对当事人程序权益有重大影响的裁判文书。”第四条 “人民法院作出的裁判文书有下列情形之一的，不在互联网公布：（一）涉及国家秘密的；（二）未成年人犯罪的；（三）以调解方式结案或者确认人民调解协议效力的，但为保护国家利益、社会公共利益、他人合法权益确有必要公开的除外；（四）离婚诉讼或者涉及未成年子女抚养、监护的；（五）人民法院认为不宜在互联网公布的其他情形。”\n 三、案例的检索来源\n 1、裁判文书网\n 　　中国裁判文书网是全国法院公布裁判文书的统一平台。截至8月10日，全国各地各级法院已上传31880851份裁判文书至该网站，其中最高人民法院裁判文书为26005份。裁判文书网设置了快捷检索及高级检索功能，可以实现当事人、审理法院、审理法官、代理律师等多维度检索，同时支持二维码扫码阅读。建议以裁判文书网为主要案例检索来源及最终提供案例的打印版本来源。各地各级法院官方网站开设的“裁判文书公开”栏目，亦可作为该院裁判文书的主要检索来源。\n 2、其他市场化数据库\n 　　除此之外，市面上常用的裁判文书数据库读者可根据使用便捷程度及操作习惯选取辅助裁判文书检索来源。 \n 3、指导性案例及公报案例的获取\n 　　指导性案例在最高人民法院网站开设专门栏，供读者查询、复制、保存，最高人民法院公报网可检索自1985年第1期至2017年第12期的全部公报案例。\n 4、纸质期刊等的使用\n 　　在互联网裁判文书检索之外，亦可关注《人民司法》、《民事审判指导与参考》、《商事审判指导》、《审判监督指导》、《人民法院报》等报刊书籍所发布的案例。为了解裁判文书作出的背景或法官的庭审询问风格等。\n 四、案例的检索顺序\n 1、指导性案例\n 　 按照《最高人民法院关于案例指导工作的规定》第七条的规定，最高人民法院发布的指导性案例，各级人民法院审判类似案例时应当参照。《〈关于案例指导工作的规定〉实施细则》则明确，“类似案例”是指“基本案情和法律适用方面相类似”，应当参照的是指导性案例的“裁判要点”。尽管指导性案例目前仅发布了16批总计87号，但其参照适用方法可以在一定程度上作为类案判断及裁判尺度判断的标准。\n 2、普通案例\n 　　除指导性案例明确规定为类似案例参照适用外，其余案例包括公报案例，均为普通案例。如全面铺开类案检索，本院类案、上级法院类案乃至最高人民法院类案均可能纳入强制检索并比对案情、判断裁判尺度标准的范围。故在为统一法律适用及裁判倾向判断之目的进行案例检索时，需按照指导性案例、本院类案、上级法院类案、最高人民法院类案的顺序进行检索。\n 五、案例可供检索的方法\n 　　主要的方法其实就是：关键词+筛选条件。\n 　　所谓关键词检索其实就是平常大家使用百度输入关键词的习惯，而所谓筛选条件就是网站提供的高级搜索功能，帮助我们将各类案件进行分类整合。根据案件类型、当事人信息、发生地区、发生时间、案由、法官、法院等几乎与裁判文书相关的各个连接点作为筛选条件来帮助我们进行检索。\n 1、案例检索方法之案由检索法\n 　　案由就是关键词，每份判决书的标题都会包含案由，而案例数据库的高级检索功能一般都支持选择案由，这样可以进一步缩小检索范围，有助于我们更快地找到想要的案例。《民事案件案由规定》共规定了四级案由，其中，第一级案由10类，第二级案由43类，第三级案由424种，部分第三级案由下还有第四级案由，这些案由都可以作为关键词。\n 2、案例检索方法之争议焦点检索法\n 　　在案由检索的基础上做的二级检索，能了解法院对于案件主要争议焦点的裁判趋向，不同地域的法院倾向于哪种裁判观点，裁判观点的构成以及为什么作出这样的判决，从而在设计诉讼策略方案时规避相关法律风险以及顺应法院的裁判思路调整诉讼策略，做到有的放矢。\n 3、案例检索方法之案件事实检索法\n 　　此类案件是基于对当事人提供的材料和当事人的事实陈述进行提炼之后的检索，必要时与上述其它几种检索方法交叉、多次检索，以确保检索案例的精准度和检索结果的准确性。\n 4、案例检索方法之定位检索法\n 　　首先，在了解裁判文书具体格式基础上（如一审裁判文书一般会包含原告诉称、被告辩称、本院经审理查明、本院认为等基本结构），被告方拿到原告的起诉状后，了解起诉状的基本事实并提取出关键词，以“请求驳回原告诉讼请求”来定位检索，也许会收到较好的效果。其次，根据案件进程检索相应程序案例，比如一审上诉案件，因对应程序已进入二审，检索就定位于二审案例，再与其它检索方法配合使用，限定范围，提升检索效率。\n 5、案例检索方法之关键词检索法\n 　　关键词的选取水平是检索能力高低的判断标准之一，也是对检索人员法律功底的考验。用规范、简洁的语言概括检索命题，以合理的层次梯度组配关键词，这样的检索需要平时良好的积累和持续的训练，一旦运用娴熟，会对以后的检索工作提供很大帮助。关键词可来源于多种场合，如当事人口述或书面文件表述、证据标题、就该主题整理归纳的专业文章、相关法律规定表述、相关单一案例归纳等等。关键词不是单一的，但通常来讲单次检索在文本框中输入的关键词应不超过三个。 关键词除直接从文本中提取外，还可以根据相关度进行转化，如“差额补足”可直接转化为“差额支付”，“售后返租”可"
  },
  {
    "title": "多模态大模型持续学习系列研究-今日头条",
    "page_body": "“AI刚学会的新技能，睡一觉就忘光？\n”——别笑，这正是无数工程师和产品经理的噩梦。\n灾难性遗忘有多真实？\n去年某头部车企的智驾模型，更新一次行人检测算法，结果把识别红绿灯的老本行丢了一半，连夜回滚版本。\n钱烧了，用户骂了，工程师集体怀疑人生。\n中科院自动化所这次把“续命术”摆到台面上：一篇综述+五篇顶会论文+开源库，直接把持续学习从实验室推到生产线。\n先说最扎心的UCIT Benchmark。\n以往测试像开卷考试，新任务偷偷夹带旧数据，模型当然表现好。\nUCIT直接切断“小抄”，旧任务图像、文本全隔离，分数立刻打回原形。\nHiDe-LLaVA在这种“裸考”里用两层专家路由：顶层像快递分拣，动态挑模块；底层像乐高拼插，参数按需融合。\n结果旧任务掉点不到3%，比传统方法少一半，关键是显存只涨8%，老黄卡也能跑。\n联邦场景更离谱。\n医院想用AI看CT，数据不能出内网，模型还得月月更新。\nFCIT的DISCO框架把“知识”拆成子空间，像拼拼图：每家医院只交一小块，中心服务器再对齐边缘。\n实测三家医院联合训练，乳腺癌筛查准确率从78%拉到89%，而原始数据始终没离开硬盘。\nModalPrompt这招最“抠门”。\n它不要回放数据，直接拿图像-文本配对当“记忆锚点”。\n想象成给AI贴便利贴：新任务贴一张“猫=cat”，旧任务贴“狗=dog”，提示词自动对齐。\n推理速度飙到1.42倍，存储砍了70%，隐私敏感的客户直接鼓掌。\nMLLM-CL的评测更“不讲武德”。\n传统测试假设任务独立同分布，现实却是用户今天问天气、明天问股票。\nMR-LoRA给每个任务开小灶：路由选择器像地铁闸机，只放行相关参数，其余冻结。\n结果在12个非IID任务上，平均准确率比LoRA高11%，训练时间却只多10分钟。\nLLaVA-c的谱感知巩固（SAC）像给模型做“脑电图”。\n它先算参数矩阵的特征值，把重要方向加固，再让无监督查询当“小护士”巡视，防止跑偏。\n多任务联合训练第一次被持续学习反超，工程师群里直接刷屏“活久见”。\n开源库MCITlib更实在。8种算法一键切换，UCIT和DCL两个Benchmark直接复现论文结果。\n某机器人团队用它三天内让机械臂学会叠衣服，旧任务“倒垃圾”准确率还涨了5%。\n工业落地比论文更快。\n某电商客服系统用FCIT框架，两周内把退货政策、新品咨询塞进同一模型，人工转接率从35%降到12%。\n自动驾驶公司把ModalPrompt塞进车机，OTA更新包从800MB缩到200MB，用户终于不再吐槽“每次升级像换车”。\n下一步？\n团队已经在折腾视频+3D+语言的“三合一”持续学习。\n想象一下，机器人看完一段做饭视频，就能在陌生厨房复现动作，还能用中文解释步骤。\n听起来像科幻，但代码已经push到GitHub，issue区热闹得像菜市场。\n别再说“AI学不会长期记忆”。\n中科院这波操作把“灾难性遗忘”打成了“可控性遗忘”——旧知识不是丢了，而是被妥帖收进抽屉，随用随取。\n对于天天被需求变更折磨的打工人，这大概是2025年最治愈的消息。"
  },
  {
    "title": "LLM评测中的多模态任务支持-CSDN博客",
    "page_body": "第1章：引言\n1.1 书名含义与目标\n《LLM评测中的多模态任务支持》这本书的标题直观地揭示了本书的核心内容，即对大型语言模型（LLM）在多模态任务支持中的评测方法和实际应用进行深入探讨。LLM，即大型语言模型，如GPT、BERT等，是近年来自然语言处理领域的重要突破。这些模型凭借其强大的语言理解和生成能力，在各种自然语言处理任务中表现出色。而多模态任务，则涉及到文本、图像、声音等多种数据类型的处理和融合，是未来人工智能发展的重要方向。\n本书的主要目标是系统地介绍LLM在多模态任务中的评测方法，从理论基础到实际应用进行详细剖析，帮助读者了解和掌握如何有效地评估和提升LLM在多模态任务中的性能。具体而言，本书将涵盖以下几个方面：\nLLM的基本概念和特点 ：介绍LLM的定义、历史发展、技术优势以及面临的挑战，为后续的多模态任务讨论打下基础。 多模态任务的基础 ：探讨多模态任务的概念、数据集、特征提取和融合方法，以及多模态学习算法。 LLM评测方法 ：详细讨论评估多模态任务支持的不同方法，包括评测指标、实验设计、评估工具和特殊考虑。 LLM在多模态任务中的挑战与解决方案 ：分析支持多模态任务的技术挑战，并提供相应的解决方案。 实际应用案例 ：通过具体案例展示LLM在多模态任务中的实际应用，帮助读者理解理论如何转化为实践。 未来展望 ：讨"
  },
  {
    "title": "图像检索跨领域应用-全面剖析-豆丁网",
    "page_body": "图像检索跨领域应用 第一部分 图像检索技术概述 2 第二部分 跨领域应用背景分析 7 第三部分 图像检索算法研究进展 13 第四部分 跨领域应用场景举例 17 第五部分 技术难点及挑战 22 第六部分 实际应用案例剖析 26 第七部分 应用效果评估方法 30 第八部分 未来发展趋势展望 35 第一部分 图像检索技术概述 关键词 关键要点 图像检索技术发展历程 1. 早期基于特征匹配的图像检索：主要依靠手工提取图像特征，如颜色、纹理等，通过相似度计算进行检索，效率较低。 2. 特征提取与降维：随着深度学习的发展，卷积神经网络（CNN）被用于提取图像特征，大大提高了检索精度和效率。 3. 检索系统架构演变：从简单的索引库检索到基于内容的检索，再到智能检索系统，技术不断演进，功能日益丰富。 图像检索关键技术 1. 特征提取技术：包括颜色特征、纹理特征、形状特征等，以及基于深度学习的特征提取方法，如CNN。 2. 相似度度量方法：包括欧氏距离、余弦相似度、汉明距离等，以及基于深度学习的相似度度量方法。 3. 检索算法优化：如基于MapReduce的分布式检索、基于索引的检索算法优化等，以提高检索速度和准确性。 图像检索应用领域 1. 消费级应用：如社交媒体、电子商务、在线教育等，用户可以通过图像快速查找相似内容。 2. 专业领域应用：如医疗影像、卫星图像、遥感图像等，用于疾病诊断、环境监测等。 3. 跨领域融合：图像检索与其他技术的结合，如自然语言处理、计算机视觉等，形成新的应用场景。 图像检索面临的挑战 1. 数据量与多样性：随着图像数据的爆炸式增长，如何高效地检索大量且多样化的图像数据成为一大挑战。 2. 高度相似图像检索：在高度相似图像中找到差异化的内容，尤其是在大规模图像库中实现。 3. 实时性需求：在实时场景中，如视频监控、实时搜索等，如何保证检索速度与准确性。 图像检索发展趋势 1. 深度学习技术的进一步应用：深度学习在图像检索中的应用将更加深入，如自监督学习、迁移学习等。 2. 多模态融合检索：结合图像、文本、语音等多种模态信息，提高检索的全面性和准确性。 3. 智能化检索：利用人工智能技术，如强化学习、图神经网络等，实现智能化的个性化检索。 图像检索前沿技术 1. 增强现实与虚拟现实：将图像检索技术应用于AR/VR领域，实现沉浸式检索体验。 2. 无监督学习与自编码器：减少对标注数据的依赖，通过无监督学习提取图像特征。 3. 跨域检索与多任务学习：实现跨领域图像检索，同时处理多个相关任务。 图像检索技术概述 随着互联网的飞速发展，图像信息在信息传播、数据存储、娱乐娱乐等领域扮演着越来越重要的角色。图像检索技术作为信息检索领域的一个重要分支，旨在实现从海量图像库中快速、准确地检索出与用户需求相匹配的图像。本文将从图像检索技术的发展历程、关键技术、应用领域等方面进行概述。 一、图像检索技术的发展历程 1. 初期阶段（20世纪80年代至90年代） 在这一阶段，图像检索技术主要基于图像的视觉特征，如颜色、纹理、形状等。研究者们提出了多种特征提取方法，如颜色直方图、纹理特征、形状特征等。然而，由于特征提取方法的局限性，检索效果并不理想。 2. 中期阶段（20世纪90年代至21世纪初） 随着计算机视觉和机器学习技术的快速发展，图像检索技术逐渐从基于视觉特征的检索转向基于内容的检索。在这一阶段，研究者们提出了多种基于内容的图像检索方法，如基于颜色、纹理、形状、空间关系等特征的检索方法。同时，图像检索系统也逐渐从单模态检索转向多模态检索。 3. 现阶段（21世纪初至今） 随着深度学习技术的兴起，图像检索技术取得了显著的进展。基于深度学习的图像检索方法在特征提取、相似度度量等方面取得了突破性成果。目前，深度学习已成为图像检索技术的主流方法。 二、图像检索关键技术 1. 特征提取 特征提取是图像检索技术的核心环节，其目的是从图像中提取出具有区分度的特征向量。常见的特征提取方法包括： （1）颜色特征：如颜色直方图、颜色矩、颜色相关矩阵等。 （2）纹理特征：如灰度共生矩阵、局部二值模式（LBP）、Gabor滤波器等。 （3）形状特征：如边缘检测、轮廓提取、形状描述符等。 （4）空间关系特征：如区域关系、路径关系等。 2. 相似度度量 相似度度量是图像检索技术中另一个关键环节，其目的是计算查询图像与数据库中图像之间的相似度。常见的相似度度量方法包括： （1）欧氏距离：计算两个特征向量之间的欧氏距离。 （2）余弦相似度：计算两个特征向量之间的余弦值。 （3）汉明距离：计算两个特征向量之间的汉明距离。 （4）距离加权方法：如加权欧氏距离、加权余弦相似度等。 3. 深度学习 深度学习技术在图像检索领域取得了显著的成果。常见的深度学习方法包括： （1）卷积神经网络（CNN）：通过学习图像的层次化特征表示，实现图像检索。 （2）循环神经网络（RNN）：处理具有时间序列特征的图像检索任务。 （3）生成对抗网络（GAN）：生成与查询图像相似的图像，提高检索效果。 三、图像检索应用领域 1. 搜索引擎 图像搜索引擎是图像检索技术最典型的应用领域。通过图像检索技术，用户可以快速找到与查询图像相似的图片。 2. 社交媒体 在社交媒体中，图像检索技术可以帮助用户发现相似的兴趣爱好、朋友等。 3. 医学影像 在医学影像领域，图像检索技术可以帮助医生快速找到与病例相似的病例，提高诊断准确率。 4. 安全监控 在安全监控领域，图像检索技术可以帮助监控人员快速识别异常行为。 5. 娱乐娱乐 在娱乐娱乐领域，图像检索技术可以帮助用户发现相似的兴趣爱好、朋友等。 总之，图像检索技术在各个领域都发挥着重要作用。随着深度学习等技术的不断发展，图像检索技术将更加成熟，为人类生活带来更多便利。 第二部分 跨领域应用背景分析 关键词 关键要点 图像检索在电子商务中的应用 1. 消费者体验优化：图像检索技术能够帮助消费者快速找到所需商品，提升购物体验，减少搜索时间，增加用户满意度。 2. 库存管理效率提升：商家可以利用图像检索技术对库存进行高效管理，减少库存积压，提高库存周转率。 3. 淘宝搜索引擎优化：图像检索与文本检索的结合，使得电子商务平台的搜索引擎更加智能化，提高搜索匹配的准确性。 图像检索在医疗影像分析中的应用 1. 疾病诊断辅助：图像检索技术可以辅助医生快速识别疾病特征，提高诊断效率和准确性，尤其是在罕见病诊断方面。 2. 图像数据分析：通过对海量医学图像进行检索和分析，可以挖掘出潜在的医疗研究数据，促进医学研究的发展。 3. 智能化医疗辅助：结合深度学习等人工智能技术，图像检索可以实现对医疗影像的智能化处理，为患者提供更加个性化的医疗服务。 图像检索在版权保护中的应用 1. 知识产权保护：图像检索技术可以帮助版权持有者追踪盗版行为，保护其知识产权，维护市场秩序。 2. 创意内容审核：图像检索可以用于审核创意内容，防止抄袭和侵权行为，保护原创者的权益。 3. 数据安全监控：通过图像检索技术，可以对上传的内容进行实时监控，防止敏感信息泄露，保障网络安全。 图像检索在社交媒体中的应用 1. 内容推荐系统：图像检索技术可以用于社交媒体的内容推荐，提高用户体验，增加用户粘性。 2. 互动性增强：通过图像检索，用户可以更容易地找到感兴趣的内容，增强社交媒体的互动性。 3. 广告投放优化：图像检索技术可以用于精准广告投放，提高广告效果，降低广告成本。 图像检索在安防监控中的应用 1. 安全监控实时性：图像检索技术可以实现对监控画面中目标的快速识别和跟踪，提高安防监控的实时性。 2. 事件预警与响应：通过图像检索，系统可以自动识别异常行为，及时发出预警，提高事件响应速度。 3. 数据分析辅助决策：图像检索技术可以对监控数据进行分析，为安全决策提供数据支持。 图像检索在文化遗产保护中的应用 1. 艺术品鉴定与修复：图像检索技术可以帮助专家快速鉴定艺术品，辅助文物修复工作，保护文化遗产。 2. 数字化展示与传播：通过对文化遗产的图像进行检索，可以实现数字化展示，便于传播和保护。 3. 智能旅游开发：结合图像检索技术，可以开发智能旅游项目，提升游客体验，促进文化遗产旅游的发展。 图像检索作为一种重要的计算机视觉技术，在多个领域有着广泛的应用。近年来，随着深度学习等技术的不断发展，图像检索的性能得到了显著提升。本文旨在对图像检索的跨领域应用背景进行深入分析，探讨其发展现状、挑战及未来趋势。 一、跨领域应用背景分析 1. 数据爆炸与信息过载 随着互联网、物联网等技术的飞速发展，数据规模呈爆炸式增长。根据中国互联网络信息中心（CNNIC）发布的《中国互联网发展统计报告》，截至2020年12月，我国互联网用户规模已达9.89亿，互联网普及率为70.4%。在如此庞大的数据量中，如何快速、准确地找到所需信息成为一大难题。图像检索技术的出现，为解决这一问题提供了有效手段。 2. 多模态数据融合需求 在现实生活中，图像、视频、文本等多种类型的数据并存。传统的单一模态数据检索方法已无法满足用户对信息获取的需求。跨领域图像检索技术将图像与其他模态数据进行融合，实现多源数据的统一检索，为用户提供更加丰富的检索结果。 3. 人工智能与深度学习技术推动 深度学习技术的快速发展为图像检索提供了强大的技术支持。以卷积神经网络（CNN）为代表的深度学习模型在图像识别、分类等任务上取得了显著成果。这使得跨领域图像检索技术在性能上得到了显著提升。 4. 应用场景不断拓展 随着图像检索技术的成熟，其应用场景也在不断拓展。从最初的图像检索系统，到如今的社交网络、电子商务、安防监控、医疗诊断等领域，图像检索技术已深入到人们生活的方方面面。 二、跨领域应用发展现状 1. 技术突破 近年来，跨领域图像检索技术取得了显著突破。以深度学习为代表的机器学习技术在图像检索中的应用，使得检索精度和速度得到了极大提升。同时，多模态数据融合、跨模态检索等技术的不断进步，为图像检索的跨领域应用提供了有力支持。 2. 应用案例 目前，跨领域图像检索技术在多个领域取得了实际应用，如： （1）社交网络：通过图像检索技术，用户可以快速找到相似的朋友、图片等，提升用户体验。 （2）电子商务：图像检索技术可以帮助用户快速找到心仪的商品，提高购物效率。 （3）安防监控：跨领域图像检索技术在监控视频分析、异常行为检测等方面具有重要作用。 （4）医疗诊断：图像检索技术可以帮"
  },
  {
    "title": "LeCun亲自出镜，Meta推出新世界模型！",
    "page_body": "机器之心报道 \n 机器之心编辑部 \n 最近，Meta 大动作不断。 \n 前些天有外媒曝出马克・扎克伯格正在组建一个名为「超级智能团队」的专家团队，以实现通用人工智能。随后开出 9 位数的薪酬为该团队吸纳人才。 \n 就在刚刚，Meta 又有新的动作，推出基于视频训练的世界模型 V-JEPA 2（全称 Video Joint Embedding Predictive Architecture 2）。其能够实现最先进的环境理解与预测能力，并在新环境中完成零样本规划与机器人控制。 \n Meta 表示，他们在追求高级机器智能（AMI）的目标过程中，关键在于开发出能像人类一样认知世界、规划陌生任务执行方案，并高效适应不断变化环境的 AI 系统。 \n 这次，Meta 首席 AI 科学家 Yann LeCun 亲自出镜，介绍世界模型与其他 AI 模型的不同。 \n 他说，世界模型是一种现实的抽象数字孪生，AI 可以参考它来理解世界并预测其行为的后果。与理解语言不同，世界模型使机器能够理解物理世界，并能够规划行动路线以完成任务，而无需进行数百万次的试验，因为世界模型提供了对世界运行方式的基本理解。能够使用世界模型进行推理和规划的 AI 将产生广泛影响。例如，它可以用于帮助视障人士的辅助技术、在混合现实中为复杂任务提供指导、使教育更加个性化，甚至可以理解代码对程序状态和外部世界的影响。 \n 此外，世界模型对于自动驾驶汽车和机器人等自主系统至关重要，它将开启机器人技术的新纪元，使现实世界中的 AI 智能体能够在不需要大量机器人训练数据的情况下帮助完成家务和体力任务。 \n V-JEPA 2 拥有 12 亿参数，基于联合嵌入预测架构（JEPA）构建。在此之前，Meta 已经证明，JEPA 架构在处理图像和 3D 点云等模态方面出色的表现。 \n 此次发布的 V-JEPA 2 是在去年首个基于视频训练模型 V-JEPA 的基础上，进一步提升了动作预测和世界建模能力，使机器人能够通过与陌生物体及环境交互来完成任务。 \n 我们先看几个示例： \n 开启对世界的理解。V-JEPA 2 与语言建模相结合，可提供卓越的运动理解以及领先的视觉推理能力。当视频中的人跳向水面时，V-JEPA 2 给出了解读：向前，1.5 周空翻，无转体。\n预测下一步会发生什么。V-JEPA 2 可以预测世界将如何发展。示例中，当人正在做当前事情时，V-JEPA 2 能够预测接下来会发生什么。 \n 下面是 V-JEPA 2 的一些性能指标：\n目前，V-JEPA 2 相关论文、下载链接等已经放出。\n论文标题：V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning \n 论文链接：https://ai.meta.com/research/publications/v-jepa-2-self-supervised-video-models-enable-understanding-prediction-and-planning/ \n 项目链接：https://github.com/facebookresearch/vjepa2 \n HuggingFace 链接：https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6 \n 项目网站：https://ai.meta.com/vjepa/ \n V-JEPA 2 有哪些创新？ \n V-JEPA 2 基于 JEPA 构建，包含两个主要组件： \n 编码器，用于接收原始视频并输出嵌入，这些嵌入能够捕捉世界状态的语义信息。 \n 预测器，用于接收视频嵌入以及关于预测内容的附加上下文，并输出预测后的嵌入。 \n 在训练过程中，Meta 使用基于视频的自监督学习来训练 V-JEPA 2，因而无需额外的人工注释即可在视频上进行训练。 \n V-JEPA 2 训练包含两个阶段：无动作预训练，以及后续的动作条件训练。 \n 在第一阶段 —— 预训练阶段，Meta 使用了超过 100 万小时的视频和 100 万张图像。这些丰富的视觉数据有助于模型深入了解世界的运作方式，包括人与物体的交互方式、物体在物理世界中的移动方式以及物体与其他物体的互动方式。 \n 仅仅经过预训练，Meta 就发现模型已经展现出与理解和预测相关的关键能力。例如，通过在冻结编码器和预测器特征的基础上训练注意力读出（read-out）模型，V-JEPA 2 在 Epic-Kitchens-100 动作预测任务中创造了新的最高纪录，该任务可以根据以自我为中心的视频预测未来 1 秒将执行的动作（由名词和动词组成）。最后，将 V-JEPA 2 与语言模型相结合，可以在视频问答基准（例如感知测试和 TempCompass）上实现最先进的性能。 \n 在第一阶段之后，模型能够预测世界状态的可能演变。然而，这些预测并没有直接考虑智能体将采取的具体行动。 \n 因而，在训练的第二阶段，Meta 专注于利用机器人数据（包括视觉观察（视频）和机器人正在执行的控制动作）来提升模型的规划能力。 \n Meta 通过向预测器提供动作信息，将这些数据整合到 JEPA 训练流程中。在使用这些额外数据进行训练后，预测器学会在进行预测时考虑具体动作，然后即可用于控制。 \n 第二阶段的训练不需要大量的机器人数据 —— 仅使用 62 小时的机器人数据进行训练就能构建出一个可用于规划和控制的模型。 \n Meta 展示了 V-JEPA 2 如何用于在新环境中进行零样本机器人的规划，这些环境中涉及的物体在训练阶段从未见过。与其他机器人基础模型不同 —— 这些模型通常需要部分训练数据来自模型部署的具体机器人实例和环境 —— 他们使用开源的 DROID 数据集对模型进行训练，然后直接将其部署到 Meta 实验室的机器人上。他们证明了 V-JEPA 2 的预测器可以用于基础任务，例如够到物体、拿起物体，并将其放置到新位置。 \n 对于短期任务，例如拿起或放置物体，Meta 以图像的形式指定目标。他们使用 V-JEPA 2 的编码器获取当前状态和目标状态的嵌入向量。从其观察到的当前状态出发，机器人通过预测器来想象采取一组候选动作的后果，并根据这些动作与期望目标的接近程度对候选动作进行评分。在每个时间步，机器人通过模型预测控制重新规划，并执行评分最高的下一个动作以接近目标。 \n 对于长期任务，例如拿起物体并将其放置到正确的位置，他们指定了一系列视觉子目标，机器人会按顺序尝试实现这些子目标，类似于人类观察到的视觉模仿学习。通过这些视觉子目标，V-JEPA 2 在新环境和未见环境中放置新物体的成功率达到了 65% 到 80%。 \n 对物理理解进行基准测试 \n 这次，Meta 还发布了三个新的基准测试，用于评估现有模型从视频中理解和推理物理世界的能力。尽管人类在这三个基准测试中的表现都非常出色（准确率在 85% 到 95% 之间），但包括 V-JEPA 2 在内的顶级模型与人类表现之间仍存在显著差距，这表明模型需要在这些方向上进一步改进。 \n 第一个基准测试 ——IntPhys 2 专门设计用于衡量模型区分物理合理场景和不合理场景的能力，它是基于早期的 IntPhys 基准测试进行构建和扩展的。他们设计 IntPhys 2 的方式类似于发展认知科学家评估年幼人类何时获得直觉物理能力的方法，即通过「违背预期」范式。他们通过游戏引擎生成视频对来实现这一点，其中两个视频在某个时间点之前完全相同，然后其中一个视频中发生了违反物理规律的事件。模型必须识别出哪个视频中发生了违反物理规律的事件。尽管人类在各种场景和条件下几乎都能完美完成这项任务，但他们发现当前的视频模型表现仅接近随机水平。\nIntPhys 2 项目链接：https://github.com/facebookresearch/IntPhys2 \n HuggingFace 链接：https://huggingface.co/datasets/facebook/IntPhys2 \n 论文链接：https://ai.meta.com/research/publications/intphys-2-benchmarking-intuitive-physics-understanding-in-complex-synthetic-environments/ \n 第二个基准测试 ——MVPBench 通过多项选择题来衡量视频语言模型的物理理解能力。与文献中的其他视频问答基准测试不同，MVPBench 旨在减少视频语言模型中常见的捷径解决方案，例如依赖于表面的视觉或文本线索和偏见。MVPBench 中的每个示例都有一个最小变化对：一个视觉上相似的视频，以及相同的问题，但答案相反。为了正确回答一个问题，模型还必须正确回答其对应的最小变化对。\nMVPBench 项目链接：https://github.com/facebookresearch/minimal_video_pairs \n HuggingFace 链接：https://huggingface.co/datasets/facebook/minimal_video_pairs \n 论文链接：https://ai.meta.com/research/publications/a-shortcut-aware-video-qa-benchmark-for-physical-understanding-via-minimal-video-pairs/ \n 第三个基准测试 ——CausalVQA 旨在关注模型对物理世界视频中因果关系的理解，包括反事实（如果…… 会发生什么）、预期（接下来可能会发生什么）和计划（为了实现目标应该采取什么行动）。 \n Meta 发现，虽然大型多模态模型越来越能够回答视频中发生了什么的问题，但它们仍然难以回答可能发生什么和接下来可能会发生什么的问题，这表明在预测物理世界在给定动作和事件空间的情况下可能如何演变方面，它们的表现与人类存在巨大差距。\nCausalVQA 项目链接：https://github.com/facebookresearch/CausalVQA \n 论文链接：https://ai.meta.com/research/publications/causalvqa-a-physically-grounded-causal-reasoning-benchmark-for-video-models/ \n V-JEPA 2 在 Hugging Face 关于物理推理榜单上排名第一，超越 GPT-4o 等。\n地址：https://huggingface.co/spaces/facebook/physical_reasoning_leaderboard \n Meta 下一步会做什么 \n Meta 计划在多个领域进一步探索世界模型。目前，V-JEPA 2 能够在单一时间尺度上进行学习和预测。然而，许多任务需要跨多个时间尺度进行规划。想象一下，将一个高级任务分解成更小的步骤，例如装载洗碗机或烘烤蛋糕。Meta 希望专注于训练能够跨多个时间和空间尺度进行学习、推理和规划的分层 JEPA 模型。另一个重要方向是多模态 JEPA 模型，这些模型能够利用多种感官进行预测，包括视觉、听觉和触觉。 \n 期待 Meta 接下来的更多研究。 \n 参考链接： \n https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/ \n © THE END \n 转载请联系本公众号获得授权 \n 投稿或寻求报道：liyazhou@jiqizhixin.com \n 原标题：《刚刚，LeCun亲自出镜，Meta推出新世界模型！》 \n阅读原文"
  },
  {
    "title": "科研速递|理工学院冀晓强教授团队在智能机器人领域顶级期刊JFR发表文章|香港中文大学（深圳）理工学院",
    "page_body": "异构多机器人协作 能够突破单个机器人性能的限制，实现团队合作与优势互补，在办公/家庭服务、安防巡检、星际探索以及军事攻防等自动化、无人化任务中具有重要意义。然而，传统的多智能体协作方法通常依赖于精细建模，难以应对复杂动态环境，且无法理解和处理自然语言任务描述。\nEmbodied Artificial Intelligence (EAI) 具身智能作为一种新兴范式，将人工智能与物理实体深度结合，以实现机器人在复杂环境中的动态感知、学习交互和泛化控制能力。为解决上述问题， 香港中文大学（深圳）理工学院冀晓强教授 研究团队创新性地 提出了用于大语言模型（LLM）微调的异构多机器人协作控制数据集MultiPlan 以及错误诊断指标MRED。MultiPlan基于自然语言任务描述框架定义了任务内容、环境描述和动作规划，结合机器人底层SDK，提供了简洁且可部署的智能协作方案。与传统方法相比， 微调后的7B参数规模模型在复杂任务的规划及控制能力上实现了对GPT-4o等闭源大模型的显著超越 。MultiPlan数据集涵盖100个常见室内外生活场景，通过模板生成与人工复核的数据流水线，确保了数据的泛化性和多样性。在现实世界中的实验验证进一步体现了本研究的实际应用价值：分别在办公室服务和城市街道清洁两个场景中进行部署，证明了方法的有效性和鲁棒性。\n1.期刊介绍\nJournal of Field Robotics (JFR)  是机器人学领域的顶级期刊之一，致力于发表面向真实世界应用的前沿研究，强调理论研究与实际工程应用的紧密结合，特别关注机器人在复杂场景和真实任务中的实际部署与表现。 JFR 在国际机器人学领域享有极高声誉，对论文的创新性、工程实践价值和技术细节的完整性要求严格。本论文被该期刊专题特刊“ Embodied Artificial Intelligence for Field Robots ”接收，该特刊聚焦于将人工智能与物理实体深度融合的前沿研究，旨在探讨具身智能在复杂真实环境中提升机器人自主性和适应性的最新进展。\n2.研究内容与方法\n相比于传统的单机器人系统，异构多机器人系统（HMRS）在工业自动化和灾害救援等任务中展现了更高效的任务执行能力。然而，现有的多机器人任务规划方法，包括共识算法、行为驱动方法和强化学习策略，普遍存在适应性不足、难以应对动态环境和复杂协作问题的局限性。基于此，本研究提出了一种将大语言模型（LLM）与EAI相结合的多机器人协作规划控制方法，通过自然语言接口实现直观灵活的任务描述，并利用LLM的广泛知识和上下文理解能力解决传统方法的不足。\n图1. 异构多机器人系统通过优势互补拓展了单机器人系统的能力\n在方法上，本文设计了一种可扩展的自然语言描述框架，将异构多机器人任务转化为可量化的语言指令，从而有效提升任务的物理语义对齐能力。基于该框架，本文构建了首个面向LLM的多机器人任务规划数据集——MultiPlan，为LLM在多机器人系统中的任务规划和协作提供了全面且多样化的场景支持。此外，为评估LLM的规划性能，本文提出了多机器人协作错误诊断评估（MRED）指标，能够细粒度地识别和量化任务执行中的错误。最后，通过对LLM的监督微调，本文验证了其在复杂任务场景下的泛化能力，并进行了室内外实地测试，包括办公室服务和城市街道清洁任务，验证了模型的实际应用能力和鲁棒性。\n图2. 本文进行的部署测试示意图：办公室服务任务和城市街道清洁任务\n之后，通过对LLM的监督微调，本文验证了其在复杂任务场景下的泛化能力，并进行了室内外实地测试，包括办公室服务和城市街道清洁任务，验证了模型的实际应用能力和鲁棒性。\n视频演示1：本文进行的部署测试：办公室服务任务（见底部视频1：Indoor）\n视频演示2：本文进行的部署测试：城市街道清洁任务（见底部视频2：Outdoor）\n3.研究贡献\n本文的主要贡献是：\n1.提出了一种面向异构多机器人任务的可扩展自然语言描述框架，本文有效解决了抽象语言指令与物理执行之间的鸿沟。\n2.构建了涵盖100个常见场景的大规模高质量大模型微调数据集--MultiPlan数据集。提出的MRED评估指标能够系统性的对任务规划的错误进行诊断。\n3.MultiPlan数据集和MRED评估指标不仅填补了LLM在多机器人任务规划领域的研究空白，还为进一步研究提供了高质量的基准。\n4.作者简介\n冀晓强教授\n本文通讯作者为香港中文大学（深圳）理工学院冀晓强教授。\n冀晓强教授，香港中文大学（深圳）理工学院研究助理教授，兼任深圳市人工智能与机器人研究院智能控制中心主任，并担任中国仿真学会智能物联专委会委员等。冀晓强教授在美国哥伦比亚大学获得博士学位，主要研究方向为智能控制系统，主持多项科研及人才项目，至今在 Automatica、Journal of Field Robotics (JFR)、IEEE/ASME T-Mech、T-ASE、Information Fusion 、 RA-L 、CDC、ICRA等顶尖国际期刊及会议发表论文四十余篇。在非最小相位系统方面，是该领域全球范围内学习控制设计的推动者之一。担任包括 IEEE T-AC 在内的多个顶级期刊及会议的审稿人、 MECC 副编辑、RCAR等国际会议领域主席，并于近期获得CINT优秀论文奖、ISUI最佳论文奖等。\n万瀚文\n本文第一作者为香港中文大学（深圳）理工学院在读硕士研究生万瀚文。\n万瀚文\n2023级硕士生\n计算机与信息工程专业\n万瀚文于2023年加入理工学院攻读硕士学位，主要研究方向为具身智能控制、大模型和强化学习控制等。目前在 Journal of Field Robotics (JFR) ，EMNLP，ASC国际顶级期刊/会议发表论文，曾担任IROS、ICRA等机器人国际顶级会议审稿人。\n供稿 | 冀晓强教授团队\nPanopto Video (video.cuhk.edu.cn|165b84b6-bb92-424b-9350-b27e003ae891)\nPanopto Video (video.cuhk.edu.cn|1d2689f2-0694-418a-a96d-b27e003aec0c)"
  },
  {
    "title": "从 Anthropic，看见 Google 曾经的样子-网易",
    "page_body": "这是硅基立场 2024-2025 系列盘点的第一期内容。\n1. Anthropic 的崛起\nAnthropic 的崛起是2024年全球 AI 产业最受瞩目的现象之一，它是 OpenAI 强有力的挑战者，令很多厌倦了 OpenAI 式虚张声势和夸大其词的 AI 研究者和开发者耳目一新。\n从 Menlo Ventures 提供的数据看，2024年 OpenAI 在企业 AI 领域的市场份额从50%下降至34%，Anthropic 的市场份额从12%增至24%，交叉线正在逼近。这也意味着更多企业开发者正在从 GPT-4 Turbo 的 API 切换到 Claude Sonnet 3.5，一个可能更适合开发企业级 AI 应用的基座大语言模型，也据说是这个世界上目前最受欢迎的大语言模型。\nAnthorpic 对企业市场的野心是巨大的。经常出差的同学们在旧金山机场几乎隔几个登机口就能看见一块 Claude 模型投放的广告牌，上面写着“Powerful, fast, or safe. Pick three”。这是一个进攻性的文案 —— \"Good, fast, cheap, pick two” 是一个常见的商业常识表述，意在提醒人们 trade off 是不可避免的，但 Anthropic 无意妥协，它宣称自己打破了性能、速度和安全的不可能三角。大语言模型铁人三项，Claude 是全能的。\n这个风格非常不 Anthropic —— 如果换作别的公司，你可以认为它在自吹自擂。但偏偏 Anthropic 并未有过自吹自擂的历史纪录。相反的是，比起大多数 AI 明星公司（当然包括 OpenAI）对 AGI 表面上的过度热衷和自我包装，Anthropic 对 AI 未来发展的体现了一种审慎而克制的态度，这在 Anthropic 的联合创始人和 CEO Dario Amodei 的公开表达，Anthropic 对其关键业务的定义和描述，更重要的是 Claude 模型的训练方法上，都得到了体现。\n2. Anthropic 的几个特点\nPowerful AI，而非 AGI\n2024年10月11日，Anthropic CEO Dario Amodei 发表了一篇足以载入人工智能发展史册的长文“Machines of Loving Grace”（充满爱意的机器）。在这篇文献中，Amodei 不仅概述了“强大的人工智能”（Powerful AI）可以改变社会的五个关键领域：生物学和身体健康、神经科学和心理健康、经济发展和贫困、和平与治理、工作和意义。\nAmodei 描述的未来体现了一种科学家的理性和现实，而非斯坦福辍学明星创业者式的信手拈来—— Amodei 拒绝用“AGI”（通用人工智能）这个科幻小说中诞生的热词描述他探索的一切，而用“Powerful AI”（强大的人工智能）取而代之。尽管有些刻意，但“Powerful AI” 的表述显然更专注于具体的能力，而不是抽象的概念。\nAmodei 赋予了 Powerful AI 可衡量的特征和定义：在特定领域超过诺贝尔奖得主级别的智能、具体的界面（文本、音频、视频、鼠标/键盘控制）、可以自主完成长期任务、以 10-100 倍人类速度运行、可以复制数百万次、能够独立和协作工作，以及具体的时间表。Amodei 相信实现上述特征和定义的 Powerful AI最早在2026年实现，且不是一蹴而就。Powerful AI 的愿景强调具体的解决问题的能力，而不是匹配人类意识或一般智力。\n谨慎使用 Agent\nAmodei 的 Powerful AI 愿景，近期体现在 Anthropic 在 AI Agent 的探索。在 Amodei “Machines of Loving Grace” 的长文发表之后两周，Anthropic 推出了升级的 Claude 3.5 Sonnet 模型和 Claude 3.5 Haiku 新模型。与此同时，它还公布了一项匹配新模型的突破性功能：Computer Use（计算机使用），允许 Claude 像人类用户一样与计算机交互：移动光标和单击按钮、键入文本和执行按键、截取和分析屏幕截图和编辑文本文件等。\nComputer Use 可以让 Claude 3.5 Sonnet 将自然语言指令转换为计算机命令的 API， 以实现 Powerful AI 的关键定义之一：通过具体的界面，自主完成长期任务，它被视作 AI Agent 实践的重要突破。2024年12月20日，Anthropic 官方发布了“Building Effective Agents” 的文章，第一次公开了对 AI Agent 的理解。Anthropic 在文章中精确定义了 Agent （代理）和 Workflow（工作流） 的区别：“工作流” 是用事先定义的代码路径操作模型，实现某种程度的自主性，而“代理”则是大模型动态指导自身流程和工具的使用，自主控制完成任务。\n这一定义的精确区分并非意味着 Anthropic 自我标榜只追求最先进的 Agent 研发。相反，它不厌其烦地拆解了搭建一个Workflow 或 Agent 的每一个关键步骤和每个步骤采用的 AI 工具（包括 Claude 系列模型和 API）。Anthropic 表示它建议用大语言模型开发 AI 应用的时候找到最简单的解决方案，“并且只在需要的时候增加复杂性”。它并不鼓励 AI 开发者炫技，并且致力于拆解 AI 开发的每一个环节和它工作的原理，提高大语言模型自身的可解释性和透明性。\n可解释 的 Constitutional AI\nAnthropic 旗下的一系列模型，包括备受欢迎的 Claude 3 和 Claude 3.5系列模型的研发，都基于与 OpenAI 采用的RLHF（基于人类反馈的强化学习）迥异的训练方式，即 Constitutional AI （宪法式人工智能）。Constitutional AI 与 RLHF 的本质区别是：前者用构建于初始架构中的基准价值原则的 AI 自主评估和反馈训练结果，而后者是基于数万人类反馈样本的汇总进行训练反馈。具体对比如下表（感谢Perplexity的贡献）：\n我其实更倾向于将 Constitutional AI 翻译成 “原则AI”，它的“原则”由一系列已知的人类价值观构成，用人类易懂的自然语言书写，从而避免复杂的数学公式或代码的黑箱决策。它的决策框架可以用来检视，使用基于已知原则的 AI 生成的反馈，避免人类反馈系统的不透明性，从而创建一致的、基于原则的生成结果和自主行为。此外，一个 AI 应用的价值观，在原则上可基于 Claude 模型的 API 进行定制，也可以修正。\n不难想象它对企业级 AI 应用的重要：不同的企业有不同的价值主张、商业规则和业务流程，且很难通过数万人类反馈的样本进行强化学习，对模型生成和行为结果的一致性也更关切，还对 AI 行为的可回溯、检视和透明化有更高的要求。而即便是个人用户，模型生成结果的一致性和持续性也更重要。用过 Claude 3.5 Sonnet 模型的很多人都对它的写作和代码两项能力印象深刻，也得益于它的训练方式。\n在人们评估 Anthropic 的潜力时，Constitutional AI 的训练方式往往被有意无意忽略了，或将 “Constitutional” 简单地理解为某一种特定的价值观。对开发者来说，它是可定制的、多元的原则在模型生成结果和行为时的透明化和一致性体现。\n3. Anthropic 有什么样的性格？\nClaude：作为大语言模型的性格\n强烈推荐同学们逐篇阅读 Anthropic 官方博客上的长文，体验一下这家研究驱动的人工智能公司在自然语言表达上清晰、克制却又带着强烈节奏感的审美。当然，从它的 CEO Dario Amodei 发表的长文 Machines of Loving Grace 里，我们也能感知到这种审美的一致性。这些文章中，比较吸引我的一篇是 Claude’s Character，它探讨了一个Claude 的“性格”。\n“我们不必训练模型去采纳它们遇到的任何观点，强烈采纳单一的观点，或者假装没有观点或倾向，而是可以训练模型在训练后诚实地对待它们所倾向的任何观点，即使与它们交谈的人不同意它们的观点。我们还可以训练模型表现出合理的开放心态和好奇心，而不是对任何一种世界观过于自信”。Anthropic 继而补充：“我（指Claude）不会说（人们）想听到的东西”。\n听上去非常像一个聪明人该有的样子。以及，你很难想象一个大语言模型的性格跟研发它的团队没有任何关系，因此，Claude 作为一个模型的性格，某种程度也应该是 Anthropic 作为一家企业的性格，甚至人格。\nAnthropic 的性格：可解释性、科学主义、工具理性与人文主义的调光板\n如前所述，Anthropic 作为一家人工智能公司的颇具个性的立场和价值观——包括 Constitutional AI 的原则、对 Agent 热潮的警惕，对 Powerful AI 而非 AGI 的未来愿景，都折射了这家公司的颇具特色的性格：简单可以总结为：可解释性、科学主义、工具理性与人文主义的调光板。\n没有一家 AI 公司像 Anthropic 那样，如此在意概念描述与术语定义的精确性：它刻意避免使用 AGI 这个最流行的描述 AI 未来的概念，而用 Powerful AI 这么一个中性得有些乏味的词汇定义它所做的一切。Anthropic 认为 AGI 是科幻小说派生的概念，用于指引 AI 发展的未来有不可解释性。其对 Agent 和 Workflow 的严格界定，也透露了这家公司对时髦 AI 术语的本能警惕与抵制。\n你可以认为，Anthropic 有意抵制日益普遍的 AI 炒作（AI Hypes），抗拒任何人和组织以任何形式的 AI 先知面目误导公众，而是试图用更精确、科学主义的方式呈现它的 AI 研究与产品，并注重其原理的透明性和可解释性。它主动谈论 AI 的安全问题，并且用可解释的原理，将安全作为 Claude 系列模型的卖点。它甚至不给外界 Claude 下一代新模型的预期，无论Sonnet, Haiku 还是传说中的 Opus，都是 Claude 3.5家族的成员，因为Dario Amodei 认为只有当神经网络的规模取得显著增长的情况下，才会诞生一代全新的模型。这应该是Scaling Law 的一个常识，现在却需要 Anthropic 和 Amodei 来重复。\n为什么 Anthropic 这么在意科学、透明和克制地解释 AI 的发展？粗暴的解释是：Dario Amodei 是一个科学家，他有生物物理学的博士学位，在百度和 Google 的研究岗位上延续他的学术生涯，然后加入了 OpenAI。很多人津津乐道 OpenAI 联合创始人 Ilya Sutskever 与 CEO Sam Altman 在理念上不可调和的冲突，可是有没有一种可能，曾担任 OpenAI 研究副总裁 Dario Amodei，与 Ilya Sutskever 同时，甚至更早看到 AI 演进的未来，和它带来的安全、伦理和其它黑盒子里的问题？而他要做的，就是用迥异于 OpenAI 理念和方法，把 OpenAI 的一切都反过来，重新塑造一个人工智能的未来？\n让我们通过 Anthropic CEO Dario Amodei 和 OpenAI CEO Sam Altman 公开表达的关于人工智能现状和未来的看法，对比一下两种不同的价值观（感谢Perplexity的贡献）：\nAnthropic 对人工智能的审慎、可解释和透明化原则体现了它对人工智能的工具理性立场，即人们该如何不受夸大、误导和驱使，按照自己的意志掌控和操作 AI，实现人的福祉增益。一个有意思的对比是：OpenAI 命名自带“AI”，但 Anthropic 这个自创的单词确出自希腊语的“anthropos”，意思是 human，即人类。不得不说，Anthropic 对 OpenAI 的反叛是深入每一个细节的，就连名字都如此。\n但 Anthropic 对 AI 与人类关系的思考又是充满柔和与温情的。在 Dario Amodei 那篇 The Machine of Loving Grace 里，提及人们对 AI 安全问题的低估是忧心忡忡的，讨论 AI 的定义和理性的，而展望起 AI 对人的生命延续、算法解决病例基因和消除人类精神疾病等领域，则体现了一种平静的温暖和笃定—— Amodei 没有扮演预言一切的先知，而是一如既往地展现了 AI 解决人类长期生理和心理健康的可解释性，提出了具体实现的步骤和原理。这种工具理性和人文主义的"
  },
  {
    "title": "论文那些事 | 阮晓悦：论文写作经验分享-中南财经政法大学MBA教育中心",
    "page_body": "作为通过论文正式答辩通过的，即将毕业的我，现在谈论起写论文这件事儿，往事历历在目，有一些过程还是记忆犹新，对我来说，还是特别有意义的。现在，把我的经验分享给学弟学妹们，希望能帮助到各位。\n一、论文选题以及选导师\n这个节点大家都不必忧心，建议各位同学选择感兴趣的、与自己工作相关并且有数据资料来源的方向即可，财大是很贴心的，会根据你的选题给各位分配优秀的导师，而且会发送给你们与导师沟通的指南，告知大家各种重要节点。\n如果在平时上课，有特别欣赏的老师，与自己的研究方向也很契合的，也可以提前跟老师沟通是否有名额，提前确定论文导师。\n二、开题报告及开题答辩\n在确定了选题与导师之后，可以去知网等相关网站看看相关文献，主要看看别人的课题是如何研究的，大纲是怎么安排的，自己有个初步的想法。与导师第一次沟通，彼此的第一印象很重要，关系到后续论文的修改是否顺畅，建议以通话或者视频的形式，跟导师讲一下自己的想法。一般情况下，导师会给大家推荐一些文献去阅读，也会对大家初步的想法给出意见。\n开题报告经过几轮修改，就可以参加开题答辩了，开题答辩就是老师在为你的论文提前把关，避免走弯路，所以大家一定要认真对待。尤其是答辩组老师们提出的意见，都要一一与导师沟通修改，可以减少后续论文修改的工作量。\n三、撰写论文\n写论文一定要趁早，不要沉浸在开题答辩通过的喜悦中无法自拔，这才是万里长征第一步。当然，写作过程肯定会遇到一些困难，特别要注意的是，论文框架很重要，参考文献多比少好，数据引用要有出处。最后要论文写完后，也要尽量留下充足的时间让导师给你的论文提出修改意见，不要想着一次两次就能定稿，反复修改才能打造出精品。\n四、论文预答辩\n在论文定稿之后，有一次预答辩的机会，这是一次很好的检验你论文到底写的如何的机会，不管论文写的如何，既然导师同意让你去参加预答辩，那么说明至少是合格的，大家就要珍惜这次机会，切记论文重复率一定要在学校给的合格范围内，并且论文格式一定不要出问题。\n五、论文盲审\n论文盲审是在预答辩与正式答辩的必经之路，根据预答辩意见，修改论文的同时，再次检查格式规范与重复率。\n财大 MBA 对毕业论文的格式要求很严格，建议在提交终稿之前自己再对照财大 MBA 格式规范再次检查一遍。\n查重方面，建议在提交论文终稿之前先使用知网给自己的论文查重一下，路径为知网首页的“个人查重服务”，费用为 1.5 元 / 千字符，一篇论文查一次也就 50 ～ 90 元。重复率必须小于 10% 。如果个人查重后发现重复率较高，就要降重，最靠谱的方法就是自己逐句降重，\n盲审方面，对于返回的盲审的意见，建议就是小意见自己修改，大意见和导师充分沟通后再做出修改；制作一份修改清单，把盲审意见和自己修改后的结果都列出来，以备检查。\n六、论文正式答辩\n走到这一步，说明大家即将迎来胜利，但也不可掉以轻心。根据学校要求，一步一步认真完成即可。需要注意的是认真梳理正式答辩的材料，论文答辩之前先去旁听别人答辩！\n答辩过程也不必焦虑，主要分为第三个环节：论文介绍环节、老师点评提问环节、回答问题环节。做到论文介绍逻辑顺畅，简单明了，回答问题结合论文展开，有条理有想法。只要你对论文的每个细节都非常熟悉，你就可以在答辩场上从容不迫地回答老师的各种问题，实话实说，并虚心接受老师的意见，然后你就会发现，通过答辩其实也是一件很容易的事情。\n最后，祝想毕业的同学们，都能顺利毕业，拿到双证！"
  },
  {
    "title": "网络外部性在交互式问答社区的适用性研究-传媒-人民网",
    "page_body": "——以知乎效用问题为例\n摘 要： 信息消费作为经济学、传播学、社会学的交叉问题，在互联网社会中备受关注。本研究以UGC网络问答社区——知乎为案例切入，通过文本分析，探讨传媒经济学中网络外部性原理“效用与用户量呈正相关”命题在当今交互式问答社区中的适用性。在为经典理论提出限制条件的基础上，从信息论相关理论出发，结合实际数据建构出新的效用测量指标，并使用概率数学模型提出效用与用户总量关系的新命题。\n关键词： 网络外部性；UGC交互社区；文本分析；概率模型\n一、研究背景\n （一）实际背景\n 本世纪的第一个十年可谓交互式问答社区的滥觞时期，起初是各大论坛BBS各领风骚，其后有百度贴吧、天涯论坛、虎扑步行街等引领潮流。然而，这些网站经过若干年发展之后，不约而同出现了贴子良莠不齐问题，迈过了最为辉煌的鼎盛阶段，原本的分众兴趣组织型社区转变为娱乐戏谑的情绪化社区。\n 随着本世纪第二个十年接近尾声，互联网社群的发展进入了新的阶段，交互式问答社区成为近年来发展最为迅速的社群之一。尤以典型代表知乎为例，其以严肃型问答平台的姿态进入广大网民的视野，2010年底创办初期以邀请制完成新用户注册，广泛稳定的高质量回答、优良严谨的用户素质以及近似学术交流的讨论氛围在互联网论坛中独树一帜。2013年初面向公众开放注册权限，一年之内用户数量攀升十倍。其后，知乎用户的数量更是连年飙升，截至2017年底已经突破1亿用户。但是随之而来的，是近年来负面反馈波动增多，其中关键就包括对知乎水贴泛滥的诟病，这也成为了近期研究网络问答社区的重点议题。\n CNNIC 8月发布的《第42次中国互联网络发展状况统计报告》显示，我国网民数量逐年增加，截止6月，网民数量超8亿，普及率已经达到了57.7%，与之对应的学历结构中可以看到，大学本科及以上学历再次下降，只占其中10.6%，网民仍以中等教育水平的群体为主。内容产品逐步下沉的增长策略，进一步扼杀了建设高质量小圈层讨论社区的可能性。\n （二）理论背景\n Katz 和 Shapiro 两位学者于1985年第一次提出了网络外部性的概念，他们将网络外部性定义为当消费同样产品的其他使用者的人数增加时，某一使用者消费该产品所获得的效用增量。关于网络外部性的主流分类，目前被普遍采用的有两种：一是根据影响性质，即正向与负向；二是根据影响方式，即直接或间接。\n Liebowitz（1994）对网络外部性提出质疑，他认为如果人数过多，但资源结构体系一定时，可能会使每个人获得的效益更少，所以，他劝喻人们在应用网络外部性理论的同时考虑实际情况。王伟（2017）认为，在知乎社区中可能出现高质量答案被其他低质量答案淹没的情形，尤其对于观看及点赞人数不多的题目。因此，本研究基于现有理论与实际情况，做出网络外部性理论在问答社区范围内不适用的研究假设，并提出此情况下的新命题。\n 就现有网络调查结果显示，随着知乎的注册用户不断增多，知乎用户对于知乎环境和体验的负面舆情声量越来越强，尤其是关于过滤大量无效信息而披沙拣金、劳心劳力的抱怨。知乎越来越像没有门槛但使用体验也随之明显变差的百度贴吧，因此“知乎贴吧化”成为知乎当前面临的一个转折点。\n （三）研究思路\n 经典经济学“网络外部性”原理似乎不能解释网络交互式问答社区现有的问题。因此，本研究的核心问题是：\n 1、“网络外部性”理论在当下网络交互式问答社区中是否适用？如何证伪？\n 2、如果该理论在该条件下被证伪，单个用户效用和总用户数量之间是否有关？有什么样的关系？关系转折点是否存在？关系转折点是否可测？\n二、网络外部性的反思：从效用入手\n （一）效用的理论解释\n 效用（Utility）又名功用，是经济学中最常用的概念之一，是指商品满足人的欲望的能力，或者说效用是指消费者在消费商品时所感受到的满足程度。从消费的主体来讲，效用是某人从自己所从事的行为中得到的满足；从消费的客体来讲，效用是商品满足人的欲望或需要的能力。\n 网络外部性研究的核心问题，是对“效用”的理解。效用是一种主观的心理评价，是消费者对商品满足自己的欲望的能力的一种主观认知；对此的通俗理解可以称为“产品体验”或“用户满意度”。在现有传媒经济理论中，用户的产品体验也就是对于使用媒介产品产生的感受被称为“效用”，而“效用”和接入此媒介产品的用户数量关系的原理也被称为“网络外部性”原理。\n （二）效用的测量方法\n 对效用的测量方法，西方经济学家先后提出了基数效用和序数效用的概念。基数效用论者认为效用是可以衡量和加总的。序数效用论者认为，效用是不可以度量的而且度量也是没意义的，效用只能排序。\n 两种对效用的测量方法形成了两种分析消费者行为的方法，分别是基数效用论者的边际分析方法、序数效用论者的无差异曲线的分析方法。但是根据网络外部性理论，效用作为一个可测定的基数量可以和接入其他用户的数量成正相关关系，因此在研究该理论时采取基数效用论，即效用是可以利用一系列代换测定的。\n （三）效用的可测代换\n 现实生活中，Brien（1991）认为我们在接收其他人给予的信息后，所获得的价值取决于信息质量，其包含信息的内容及形式等。高智勇等人（2006）认为信息质量取决于用户对所获得的信息内容的满意度，达到一定程度满足的信息有对人们才有价值。移动端用户体验领域的研究者刘冰（2011）根据用户在使用网络平台时的体验，认为信息的易用性、可靠性、适用性可以反映信息的效用价值。\n 综上，网络外部性聚焦于效用，效用又可以通过用户反馈来反应。正面反馈表示信息令人满意，信息效用高，网络外部性正向效果成立；负面反馈表示满意程度差、信息效用低，网络外部性正向效果不成立。由此，通过效用的可测代换，抽象的网络外部性可以直接通过具体的用户正面反馈、负面反馈来体现。后期通过分析用户正负面情绪，即可与用户人数进行交叉分析，验证原有网络外部性理论的适用性。\n三、网络外部性的证伪：以数据为本\n 通过观察知乎用户在不同时间段内对平台的反馈和此时间段内用户数量进行对比分析，观察网络外部性理论在知乎平台适用程度，从而证伪。\n （一）研究设计\n H0：知乎的总用户量与用户的正面反馈指数呈线性正相关。\n H1：知乎的总用户量与用户的正面反馈指数不呈线性正相关。\n 对于用户体验与反馈的研究，由于知乎用户数量过于庞大，实验法在不控制变量的情况下效度偏低。而用户数量是一个不断动态变化的量，采用被试回想型的问卷调查则信度偏低。因此，保证信度和效度的技术手段将采取能够保证覆盖几乎所有用户的，且在知乎发展的各个过程都可以向前追溯观测到的方法——基于大数据信息采集的内容分析法。\n 首先，由于知乎用户数量和反馈指数本身无法观测，因此需要采用一个中介变量来进行代换计算。假设反馈指数可以通过文本分析测量，则反馈状况在不同时间的变化就可以通过大数据对于不同时间段内文本的爬取来得出这些反馈出现的时间点，则可以得出时间段和采集的文本情况的关系。\n 其次，对于用户对于知乎平台本身的评价，可以采用的手段为采集同时间段内知乎回答条目下方的评论文本，以用户对于知乎平台感受的关键词为筛选词汇，类似的关键词“知乎”、“越来越”“正在变得”“体验”，从知乎开放注册以来的评论一直爬取到2017年12月，并进行筛选，就可以得出抽样调查的文本内容，从而通过文本内容进行分类数据处理，测定负面反馈程度。\n 第三，“反馈”作为一种变量无法直接测得，由于知乎上已经爬取到的评论主要以文本内容为主，因此可以采用智能文本分析也就是语义分析的手段来进行测量。在负面反馈程度测定中，采用语义分析中的“负面情感趋向指数”的测试来判定负面反馈指数，将采集到的文本数据进行分类处理后分析就可以测量。\n （二）具体操作\n 1、用户数量采集\n 知乎官方和一些高质量用户从2013年以来每隔一段时间都会发布知乎平台用户总数量的统计或者计算报告，根据这些用户总量的时间点可以推算出关键时间节点上知乎用户数量的大致情况。\n 2、用户内容爬取\n 本次数据采集的对象为，知乎平台中用户关于知乎使用体验的评论贴和回复贴。数据采集时间区间从2012年2月开始，至2017年12停止。数据采集工具为PYTHON爬虫工具。\n 从知乎搜索入口共查找130万条数据，经过筛选共选出符合研究要求的数据约为18万条评论。筛选逻辑关键词为：感受、使用、知乎、体验、用户、贴吧化等关键词，进行多次爬取和筛选。最终，将同一个月的用户评论进行归并和处理，并剔除无效信息，最终把每个月所爬取得的所有用户体验类评论整理合并成同一个文档，每月归并成单独的文本库。\n 3、用户情感分析\n 本研究使用波森数据分析平台的中文语义分析系统，其分析逻辑主要基于词性分析、依存文法、情感指数分析。将每月的文本库内容输入平台进行情感价值判断分析，从而得出负面情感指数，便可把握所爬取的月总体评价性文本对于知乎使用和知乎本身的态度，也就能够和“受众体验”即受众效用进行代换来计算。若此负面情感指数和时间以及受众的变化并无明显相关性，则并无相关关系。\n （三）研究结论与证伪\n 将“用户数量-时间”、“负面指数-时间”两组数据按照上文中的步骤进行归并、整理、代换，并代入网络外部性的原有线性负相关假设，计算结果为：从2011年2月至2017年9月期间，知乎注册用户人数及负面指数相关检验不显著(p=.397>.05，r=.106)，与网络外部性理论假设不符，原假设在此条件下不成立。因此，网络外部性在此条件下得以证伪，其在交互式问答社区中并不适用。\n 另外，从研究数据中也可以观测到用户量和负面指数之间存在着相关的规律，这种新观测到的规律分析如下：\n 观察“图3.知乎平台负面指数与时间的关系散点图”，这个图形似乎呈倒U形，而不是线性关系。从图中可见，2014年12月~2015年初可能出现这个倒U形的拐点。因此，从图中可以做出假设，并非线性正相关，而是分段相关。结合获得的用户人数数据及散点图，以用户人数少于或大于200万(2015年初)把数据分为2个部分，第一部分35个月，第二部分33个月，合共68个月。假设在人数不同时可能出现不同的效应，通过斯皮尔曼相关分析，发现在注册用户人数少于200万的时期(2011年2月~2015年2月)里，用户人数与负面指数成显著反比(p=.000<.01，r=-.607**)；当"
  },
  {
    "title": "benchmark-百度百科",
    "page_body": "英语单词\nbenchmark是英语中兼具名词和动词用法的术语，英式发音为[ˈbentʃmɑːk]，美式为[ˈbentʃmɑːrk]，常见于金融、计算机及工程测量等领域。其名词含义主要指衡量事物的基准或测量参考点，动词则指通过标准化方法进行评估或检测  [1]  [3]  [5] 。\n作为名词，benchmark既可指代经济指标（如基准利率）、计算机性能测试程序，也可表示地理测绘中的水准点  [1]  [6] 。动词用法强调以既定标准为参照进行系统性评估，例如通过基准测试衡量设备性能或行业发展水平  [4]  [8] 。柯林斯词典将其定义为“可作比较标准的已知质量或数量事物”  [1] 。\n该词源于18世纪英国地形测量工程术语，原指用于确定海拔高度的固定标记点 。随着技术发展，其应用范围逐步扩展至金融、计算机科学等领域。20世纪后期，计算机领域开始广泛采用benchmark作为硬件性能检测的规范流程，成为跨行业通用的标准化参照概念  [6-7] 。\n外文名 benchmark\n属 性 英语单词\n发 音 [ˈbentʃmɑːrk]\n词 性 名词、动词\n释 义 基准，基准点；评估，衡量\n目录\n1 单词释义 2 单词用法 3 短语搭配 4 双语例句\n单词释义\n英 [ˈbentʃmɑːk] 美 [ˈbentʃmɑːrk] \nn. 基准；（监测计算机性能的）基准（问题）标准检查程序；（测量高度的）水准点，基准点\nv. （根据某种标准）评估，衡量；基准问题检测结果为\n[  复数  benchmarks  第三人称单数  benchmarks  现在分词  benchmarking  过去式  benchmarked  过去分词  benchmarked ]  [1]\n单词用法\n柯林斯英汉双解大词典 \nbenchmark /ˈbɛntʃˌmɑːk/ (also bench mark) CET6+ TEM8\n1.N-COUNT A benchmark is something whose quality or quantity is known and which can therefore be used as a standard with which other things can be compared. 衡量基准  [1]\n短语搭配\nbenchmark interest rate  基准利率\nbenchmark system 基准系统\nbenchmark test  基准测试\nbenchmark crude 标准原油\nBenchmark Capital 标杆资本 ; 基准资本 ; 美国万宝环球基金 ; 风险投资公司标杆资本\nBenchmark job  基准职位  ; 基准工作 ; 基准工作职务 ; 基准岗位\ntoy benchmark 简单基准程序 ; 简单基准测试程序 ; 简单基准规范\nsynthetic benchmark 复合基准程序 ; 复合基准规范 ; 合基准程序\nIndustry Classification Benchmark 行业分类基准 ; ICB行业分类指标 ; 行业分类基准系统 ; 业分类基准\nbenchmark database 基准数据库 ;  基准  [1]\n双语例句\n1、Tests at the age of seven provide a  benchmark  against which the child's progress at school can be measured. 七岁时进行的测试为孩子在学校中的学习发展提供了一个测量基准。\n2、Inside the kit are the  benchmark  facilities for loading the database and driving the  benchmark . 这个工具包中有用于加载数据库和驱动基准测试程序的一些基准工具。\n3、The economic downturn is drying up demand for oil and natural gas, dropping the  benchmark  price for a barrel of oil to below $50. 经济低迷正在抑制市场对石油和天然气的需求，导致基准原油价格跌破每桶50美元。  [1]\nThe truck industry is a benchmark for the economy. \n卡车产业是衡量经济发展的基准。\n《柯林斯英汉双解大词典》\nWas this a certified benchmark? \n这是一个认证基准吗？\nThat's a real life benchmark. \n这是一个真实的基准对比。\nBut the benchmark still matters. \n但基准仍至关重要。\nWe ran a rough benchmark. \n我们运行了一个粗略的基准。\nWhat's my benchmark here? \n此处，我的基准是什么呢？\nPerform many benchmark measurements. \n执行许多次基准测试度量。\nThis is not a TPC-H benchmark result. \n这不是一个TPC - H的基准测试结果。\nFigure 2 shows the benchmark results. \n图2显示了测评的结果。\nFigure 3 shows the benchmark results. \n图3显示了这个测评的结果。\nThe benchmark kit contains a thousand files. \n这个基准工具包包含了上千个文件。\nTo some that is a conservative benchmark. \n对某些人来说这还是一个保守的指标。\nOf course, these are still only benchmark tools. \n当然，这些仍然只是基准测试工具。\nThis benchmark has that problem in several ways. \n基准在很多方面都存在这样的问题。\nThis benchmark presented several challenges. \n这个基准测试有几个难点。\nBut China surpassed even this benchmark last year. \n但去年中国甚至超过了这一基准。\nMake sure that your benchmark results are relevant. \n要确保基准测试结果是相关的。\nThis threshold was adopted as a national benchmark. \n该阈值被采用为国家基准。\nBenchmark results for this product are also available. \n此产品的基准测试结果已经可用。\nThere is no commonly accepted benchmark for ETL tools. \n有没有一个被普遍接受的基准的ETL工具?\nThis sets a new benchmark for open source transactions. \n这次给开源交易设立了一个新的基准。\nMake a small benchmark, the output is nearly meaningless. \n做一个小的基准，输出几乎是毫无意义的。\nOf course all of this activity interferes with our benchmark. \n当然，所有这样的活动都会干扰到我们的基准测试。\nThe highest cost per user occurs at the start of the benchmark. \n每个用户的最高成本发生在基准测试的开始处。  [2]\n参考资料\n1\n【benchmark】单词解析 ．有道词典  [引用日期2024-06-18] 2\nbenchmark ．双语例句 ．2023-12-09  [引用日期2024-10-31] 3\nbenchmark是什么意思_benchmark的翻译_音标_读音_用法_例句_爱词霸在线词典 ．爱词霸 ．2025-04-26 4\nbenchmark ．海词网 ．2014-11-24 5\nbenchmark  ．dictionary.cambridge.org ．2022-01-15 6\nbenchmark是什么意思 ．北京新航道教育文化发展有限责任公司 ．2025-01-10 7\nBenchmark——溯源、拓展、思考  ．腾讯网 ．2022-05-12 8\nbenchmark  ．dictionary.cambridge.org ．2021-12-29"
  },
  {
    "title": "谢赛宁、李飞飞、LeCun联手提出多模态LLM新范式，空间超感知登场-网易",
    "page_body": "机器之心报道\n编辑：杜伟、+0\n纽约大学助理教授谢赛宁新作又来了，合著者还有李飞飞、Yann LeCun。\n这次的成果名为「 Cambrian-S 」，根据谢赛宁的说法，它既是一种观点，也是一个数据集、一个基准或者一个模型，代表其迈出了探索视频空间超感知的第一步。\n从名称上来看，Cambrian-S 是谢赛宁团队去年研究成果「Cambrian-1」的延续。Cambrian-1 是一个关于图像 MLLM 的开放项目，旨在让 AI 获得强大的视觉表征学习能力。具体详见：寒武纪 1 号诞生：谢赛宁、Yann LeCun 团队发布最强开源多模态 LLM\n打开网易新闻 查看更多图片 \n在构建 Cambrian-1 之后，团队并没有立即扩展到 Cambrian-2 或 3，而是停下来思考以下三个问题：1）什么才是真正的多模态智能？2）LLM 范式对感知建模是否真的合适？3）为什么人类的感知如此轻松、直觉，却又如此强大？\n在思考这些问题下，他们意识到某种根本性的东西缺失了，因此认为 在构建「超级智能」之前，必须先构建「超感知」（supersensing）\n那么，如何定义超感知呢？在他们的语境中，超感知不是指更先进的传感器或更好的摄像头，而是 指一个数字生命体如何真正体验世界 —— 它能够吸收无尽的输入流，并从中不断学习 。超感知是智能的一部分，就像眼睛是大脑感知外部世界的那部分。你不需要感知就能解决代码或数学问题，但是如果 AI 要在现实世界中成为智能体，它就必须具备感知建模能力。\n更具体地讲，谢赛宁团队划分了多模态智能从现代到未来的发展路径：\n0 仅语言理解（linguistic-only understanding） ：没有感知能力，推理局限于文本和符号。当前的多模态大模型虽然已超越此阶段，但仍保留其偏向。 1 语义感知（semantic perception） ：将像素解析为对象、属性与关系，对应于当前多模态模型强大的「看图说话」能力。 2 流式事件认知（streaming event cognition） ：处理实时无边界的输入流，主动理解并响应持续发生的事件，这与当前让多模态模型成为实时助手的努力相契合。 3 隐式 3D 空间认知（implicit 3D spatial cognition） ：将视频理解为 3D 世界的投影。智能体必须知道有哪些东西、它们在何处、如何相互关联以及这些关系如何随时间变化。目前的多模态模型在这方面仍然极其有限。 4 预测性世界建模（predictive world modeling） ：大脑通过「无意识推理」来预测潜在的世界状态，基于先验期望进行判断。不过，当前的多模态系统还缺乏这种能够预测未来状态、保持持续记忆、进行推理与规划的内部模型。\n要研究这一切，视频是终极媒介。视频是人类体验世界的方式，也是人们真实生活经验的直接投影。\n打开网易新闻 查看更多图片 \n图 1\n团队尝试了一种 全新的原型 —— 预测感知（predictive sensing） ，在 Cambrian-S 上训练了一个潜变量帧预测（LFP）头。在推理过程中对 「惊讶度」（surprise） 进行估计，并以两种方式加以利用：1）surprise-driven 的记忆管理 —— 压缩或跳过不令人惊讶的帧，将算力集中在令人惊讶的帧上；surprise-driven 的事件分割 —— 利用惊讶值的峰值来检测事件边界或场景变化。\n通过利用这种内部预测模型产生的信号，团队在空间认知任务上看到了令人鼓舞的提升。这虽然只是一个 玩具级的预测世界模型 ，但借助这种机制，团队的中小型模型在新提出的 VSI-Super（Visual-Spatial Intelligence）基准 上超越了 Gemini。\n谢赛宁表示，这是一篇很长的论文，但其中有许多非常有趣的细节。如果你正在研究视频多模态模型，这篇论文或许值得一读。虽然并不确定团队的方向是否正确，但他确信当下的范式还远远不够。\n打开网易新闻 查看更多图片 \n论文标题：Cambrian-S: Towards Spatial Supersensing in Video 论文地址：https://arxiv.org/pdf/2511.04670 项目主页：https://cambrian-mllm.github.io/ 代码地址：https://github.com/cambrian-mllm/cambrian-s\n接下来看论文细节。\n基准测试空间超感知\n为追求「空间超感知」奠定基础，首先需要确定如何衡量该能力。研究者对这一能力的基准测试进行了两部分研究。\n研究者首先审计了一套流行的视频 MLLM 基准。其分析（图 3）显示，现有基准绝大多数侧重于语言理解和语义感知，而忽视了「超感知」所需的更高级空间和时间推理。\n打开网易新闻 查看更多图片 \n图 3\n为解决这一关键差距，该研究接着引入了 VSI-Super。这是一个新的基准，专门用于在任意长的流式场景中探测空间智能的这些更困难、更连续的方面。\n本文的其余部分将使用此基准来测试当前 MLLM 范式的极限。\n解构现有的视频基准\n近年来 MLLM 的进步导致了视频问答（Video-QA）基准的激增。然而，一个关键问题随之产生：现有基准在多大程度上真正检验视觉感知能力，而不仅仅是测试语言先验？\n该研究的诊断测试通过改变视觉输入的丰富性和文本提示的信息量，试图解开模型对视觉感知与语言先验的依赖关系。那些仅靠纯文本输入（如字幕或「盲」MLLM）就能解决的基准，更偏向于检验语言理解；而需要多帧输入才能回答的问题，则要求真正的视觉感知。\n研究者使用一个基于图像的 MLLM Cambrian-1 进行评估，此举旨在探测潜在的任务需求，而不将其与特定视频架构和后训练方法的能力相混淆。\n研究者为向 Cambrian-1 模型输入视频设立了几种实验条件：\nMultiple Frames (多帧) : 模型处理从视频片段中均匀采样的 32 帧。这是文献中表示视频输入的标准方法。 Single Frame (单帧) : 模型仅处理给定视频片段的中间帧。此条件测试对最少的、上下文核心的视觉信息的依赖程度。 Frame Captions (帧字幕) : 模型不接收视频帧，而是接收对应于相同 32 帧均匀采样图像的字幕。此条件旨在揭示在没有低层感知基础的情况下，任务的可解决程度。使用 Gemini-2.0-Flash API 来为视频帧重新生成字幕。\n为了在这些条件下对性能进行情境化分析，该研究还引入了两个基线：\nBlind Test (盲测) : 模型仅使用任务的问题来尝试解答。所有视觉输入都被忽略，也不使用视觉字幕。该基线测量模型基于其预先存在的知识、语言先验以及基准问题中任何潜在偏见的性能。 Chance Acc (随机准确率) : 这代表了特定任务格式（例如，多项选择题）通过随机猜测可实现的准确率，作为性能的下限。\n图 2 (a-c) 的结果表明，Cambrian-1 作为一个未经任何视频后训练的、基于图像的 MLLM，可以在许多基准上达到合理的性能。这表明，这些基准所针对的大部分知识，都可以通过标准的单图像指令调优管线来获取。然而，在 VSI-Bench 和 Tomato 这两个数据集上，该模型的性能低于随机水平，因为它们分别需要真正的视频感知和对高帧率视频的细粒度理解。\n使用文本字幕代替视觉输入也产生了显著的性能提升，在 EgoSchema、VideoMME、LongVideoBench、VideoMMMU、Perception Test 和 MVBench 等基准上，其准确率超过随机水平 20% 以上（图 2 i）。在将基准性能与「盲测」结果进行比较时，也可以得出类似的结论（图 2 d, f）。这种性能表现意味着，这些基准主要探测的是可从视频内容的文本摘要中推断出的能力。\n打开网易新闻 查看更多图片 \n图 2\n解读使用「多帧」和「帧字幕」之间的性能差异（图 2 j），一个显著的正差值（偏向于多帧输入）标志着该基准对精细视觉感知的需求。相反，一个很小或负的差值（更偏向于「帧字幕」）则表明其具有更强的以语言为中心的性质。研究者的分析将 VideoMMMU、EgoSchema、VideoMME、Perception Test 和 LongVideoBench 归于后一类，表明它们可能更依赖于语言理解而非视觉提示。一个显著的例外是 VSC，它对当前的 MLLM 来说是如此具有挑战性，以至于所有三种输入条件都产生接近于零的性能，从而排除了它们之间任何有意义的比较。\n现有的基准绝大多数侧重于语言理解和语义感知，而忽视了「超感知」所需的更高级的空间和时间推理。\n研究者希望强调基准测试中固有的挑战，以及创建一个单一的、包罗万象的基准来评估每一种能力是不切实际的。例如，对语言先验的依赖不应仅仅被视为一个缺点，因为获取丰富的世界知识并对其进行有效检索在许多场景中无疑是有益的。研究者认为，视频基准不应被视为衡量一个单一、统一的「视频理解」概念。相反，它们的设计和评估应基于它们旨在评估的特定能力。因此，上述分析旨在指导开发能更有效推动「空间超感知」发展的任务，这也将是本文余下部分的中心焦点。\nVSI-SUPER：迈向多模态大型语言模型中空间超感知的基准测试\n参照图 1，空间超感知要求 MLLM 具备四种关键能力： 语义感知、流式事件认知、隐式 3D 空间认知和预测性世界建模\n然而，正如研究者在图 2 中的分析所概述的，大多数现有的视频问答（QA）基准主要评估语言理解和语义感知方面，这些方面更具反应性，并由特定任务驱动。虽然近期的研究已开始通过持续感知、记忆架构和主动回答来解决流式事件认知问题，但这种能力通常是在测试时「工程实现」的，而非原生的模型技能。\n此外，尽管空间推理偶尔会作为现有基准中的一个类别出现，但这些任务很少达到真正空间认知的水平，并且远未探测定义「超感知」的世界建模能力（图 3）。尽管 VSI-Bench 向着检验空间认知迈出了第一步，但其视频仍然是短片和单场景的，并且它既没有对问题进行形式化定义，也没有评估世界预测建模这一基本能力。\n为了阐明当前 MLLM 与空间超感知之间的差距，研究者引入了 VSI-SUPER，这是一个由两部分组成的、用于连续空间感知的基准。这些任务对人类来说是直观的，通常很容易，即只需观看并持续追踪发生的事情，但它们对机器来说仍然具有惊人的挑战性。它们要求跨越无界的空间视频，选择性地过滤和结构化地积累视觉信息，以保持连贯的理解并回答问题。重要的是，它们能够抵御「暴力」的上下文扩展，暴露了对真正空间推理的需求。下面将详细介绍这两个组成部分。\nVSI-SUPER Recall (VSR)：长时程空间观察与回忆\nVSR 基准要求 MLLM 观察长时程的时空视频，并按顺序回忆出一个异常物体的位置。如图 4 所示，为了构建这个基准，人类标注者使用图像编辑模型（即 Gemini）将令人惊讶或不合时宜的物体（例如，一只泰迪熊）插入到一个室内环境的 walkthrough 视频的四个不同帧（和空间位置）中。\n打开网易新闻 查看更多图片 \n图 4\n然后将这个编辑过的视频与其他类似的房间游览视频拼接起来，创建一个任意长的连续视觉流。这项任务类似于语言领域中常用于压力测试 LLM 长上下文能力的「大海捞针」（NIAH）测试。类似的 NIAH 设置也已被提议用于长视频评估。\n然而，与那些插入不相关文本片段或帧的"
  },
  {
    "title": "《自然》测评大模型：全能型or牛马型？你用的AI什么型？-CSDN博客",
    "page_body": "可供选择的大模型很多，有些擅长写代码，有些擅长整合信息。\n几乎每周都会出现一个新的、看起来更厉害的 人工智能 （AI）工具，让研究人员想要一探究竟。无论是用来改手稿、写代码、提假说，现在的研究人员比以往有着更多的生成式 AI 工具可供选择。\n每个大 语言模型 （LLM）适用于不同的任务。有些以免费对话机器人的形式，有些则以付费应用程序编程接口（API）的形式——可与 其他 软件集成。还有一些可供下载，让研究人员能创建自己的自定义模型。\n虽然 LLM 能生成类似人类的回答，但如果只靠它们自己，还是很容易出错的，福瑞德·哈金森癌症研究中心的数据科学家 Carrie Wright 说道。\n所以，哪个 LLM 适合哪种任务？在此，科研人员与《自然》分享了他们当前最喜欢用的LLM，供读者借鉴。\no3-mini（推理型）\n2022 年，总部位于加州旧金山的 OpenAI 通过推出免费的 ChatGPT 对话机器人，让世界认识了 LLM。 科学家主要用ChatGPT 搜索信息，或是作为写作助手 ，比如草拟摘要，但更新的模型扩展了该技术的应用潜力。去年 9 月，OpenAI 推出了这之后的最新力作：让科学家啧啧称奇的 o1 “推理模型”，随即又在 12 月推出了更先进的 o3。这两个推理模型都比一个 LLM 独立工作的速度要慢，因为它们接受的是一种逐步式回答的训练方式。**这种“思维链”过程主要是为了模拟人类推理过程，这帮助它们突破了科学和数学的高基准。**也让它们更擅长技术任务，如解决代码问题和对数据进行重新格式化。\n1 月 20 日，中国杭州一家默默无闻的初创公司  DeepSeek  推出了一个竞品推理模型。之后，OpenAI 便更新了一系列新工具，包括速度更快的 o3-mini——对话机器人注册用户可免费使用的推理模型，此外还有 deep research——部分付费用户可将数百个网站的信息整合成带引用的报告，有点类似做一篇文献综述。旧金山初创公司 FutureHouse 的化学家、AI 专家 Andrew White 表示， 这个模型和其他工具一起用时是最出色的。\n在一个新的数学证明中拆解不熟悉的概念，o3-mini 能“做得非常好”，英国牛津大学数学家、AI研究员 Simon Frieder 说道。但他说，即使是最好的模型，“也完全谈不上是数学家的对手”。\nDeepSeek（全能型）\nDeepSeek-R1 于上月推出，其能力与 o1  **不相上下，可通过应用程序接口使用，成本仅为 o1 的一小部分 。**它和 OpenAI 模型的差别还在于它是“开放权重”（open weight），也就是说即使它的训练数据没有公开，但任何人都能下载底层模型，并修改用于特定的科研项目。 R1 “解锁了一个新范式”，让这个群体里——尤其是资源相对有限的人——也能构建专门的推理模型 ，White 说。\n运行完整模型需要使用强大的计算芯片，这是很多专业学者所欠缺的。但香港中文大学（深圳）的计算机科学家王本友等研究人员正在构建能用单个机器运行或训练的版本。**和 o1 一样，DeepSeek-R1 的强项是数学题和写代码。**但它对生成假说一类的任务也很在行，White 说。这是因为 DeepSeek 选择公布该模型的完整“思维链”， 这能让研究人员更好地调整接下来的问题，最终优化模型的输出 ，他说。这种透明性对于医疗诊断或许也是很重要的能力。王本友正在用实验锻炼R1——使用该模型类似推理的能力构建一个从患者评估到诊断和治疗建议的清晰、符合逻辑的途径。\nDeepSeek-R1 也有一些缺点。 这个模型好像有一个很长的“思维”过程，这会减慢它的速度 ，使它在搜索信息或头脑风暴上的作用偏弱。对于向其 API 和对话机器人输入数据的安全性担忧，导致一些政府禁止国家机构的工作人员使用这个对话机器人。DeepSeek 也不像它的商业对手那样采取很多措施来防止模型生成伤害性的输出。添加过滤器来防止这类输出——如制作武器的步骤——需要时间和行动。虽然这肯定不是故意的，但缺少防护栏确实令人担忧，Simon 说。\nOpenAI 还表示 DeepSeek 可能对其模型采取了“不适当的蒸馏”，这种方法是指用另一种算法的输出来训练模型，这在 OpenAI 的使用条件中是禁止的。\n在本文发表前，我们无法联系到 DeepSeek 对这些评价作出回复。\n一些研究人员认为这种蒸馏很正常，并且乐于使用 R1，但也有人担心使用的工具未来可能有诉讼风险。  一旦使用该模型被认为有违期刊的伦理标准，使用 R1 的科研人员可能会被要求撤稿，EIT Manufacturing 的 AI 法律律师 Ana Catarina De Alencar 表示。使用 OpenAI 和其他被指违反知识产权的公司的模型，也可能面临同样的处境，De Alencar 说。新闻机构表示，这些公司是在未经允许的情况下使用新闻报道来训练它们的模型。\nLlama（牛马型）\nLlama 一直是科研界的“必用” LLM。  作为加州的 Meta AI 在 2023 年首次发布的开放权重模型家族，各个版本的 Llama 仅在开放科学平台 Hugging Face 上就已下载了 6 亿多次。 能被下载和扩展可能是“科研界如此喜欢 Llama 的原因” ，福瑞德·哈金森癌症研究中心的数据科学家 Elizabeth Humphries 说。\n如果要处理有所有权或受保护的数据，能在个人或机构服务器上运行 LLM 就是必需的，这能防止敏感信息反馈给其他用户或开发者，Wright 说。\n已经有研究人员扩展了 Llama 的模型，让 LLM 预测材料的晶体结构，以及用来模拟量子计算机的输出。北卡罗来纳大学教堂山分校的机器学习科学家陈天龙表示，Llama 很适合用来模拟量子计算机，因为让它在改进后理解专门量子语言是相对容易的。\n但 Llama 要求用户在使用前获得允许，对有些人来说，是个小的摩擦点，White 说，因此， 其他开放模型现在往往是科研首选  ，如西雅图艾伦人工智能研究所开发的 OLMo，或是杭州阿里云推出的 Qwen。DeepSeek 的高效底层模型 V3 也是构建科研模型的一个竞品基础模型。\nClaude（代码型）\nSimon 说， 硅谷的很多人都认为 Claude 3.5 Sonnet 是写代码神器 。Claude 3.5 Sonnet 来自旧金山的 AI 公司 Anthropic，除了文本外还能读取视觉信息，如图表等。此外， 它的一个模式还能远程操作用户电脑 ，具备操控其他软件的潜力。\nClaude 模型的写作风格也深受推崇。  ChatGPT 等 LLM 在删除术语时，也省略了重要信息，Wright 说。而 Claude 模型在通俗表达的同时能保留其原始含义。她说，使用该模型着手撰写经费申请或给代码添加注释时，这一点是加分项。\n在一个基于数据驱动的科学任务的基准测试中，Claude 3.5 Sonnet 在代码挑战中的表现很好，而这些任务都是选自生物信息学和计算机化学等领域的真实论文，俄亥俄州立大学的AI研究员孙欢表示。虽然 Claude 3.5 Sonnet 作为在线对话机器人可免费使用，就和 OpenAI 的模型一样，但研究人员只有通过一个付费 API 使用才能完全集成。孙欢说： “随着更便宜的开源模型不断强大，我相信人们会更喜欢用这些，而不是闭源的 API。”\nOlmo（真开源型）\n如果研究人员想要了解 LLM 表面之下的运作原理，就需要比 Llama 和 DeepSeek 更透明的模型。  开放权重还不够，真正的开源模型会公开算法的训练数据，以及用来训练和评估模型的代码，Hugging Face 的研究员 Lewis Tunstall 说。目前，这类模型中的顶尖选手是 OLMo 2。\n研究这类模型，就能将 LLM 中的偏见溯源到其训练数据，还能进一步理解算法如何生成回答，从而提升效率。 当前，任何开源模型的主要缺陷在于，研究人员需要一定的专业知识才能运行，但随着越来越多的免费实操课程的出现， “准入门槛每天都在降低” ，Tunstall 说。\n他表示，如果法院确实判处使用版权内容训练AI是违法的，那么利用允许重复使用和修改的“许可”数据集训练的模型——比如 Hugging Face 主导的重新创造 DeepSeek-R1 的行动——可能成了仅有的安全模型了。\n那么，如何系统的去学习大模型LLM？\n作为一名深耕行业的资深大模型算法工程师，我经常会收到一些评论和私信，我是小白，学习大模型该从哪里入手呢？我自学没有方向怎么办？这个地方我不会啊。如果你也有类似的经历，一定要继续看下去！这些问题啊，也不是三言两语啊就能讲明白的。\n所以我综合了大模型的所有知识点，给大家带来一套 全网最全最细的大模型零基础教程 。在做这套教程之前呢，我就曾放空大脑，以一个大模型小白的角度去重新解析它，采用基础知识和实战项目相结合的教学方式，历时3个月，终于完成了这样的课程，让你真正体会到什么是每一秒都在疯狂输出知识点。\n由于篇幅有限，⚡️ 朋友们如果有需要全套 《 2025全新制作的大模型全套资料 》， 扫码获取 ~\n    大模型学习指南+路线汇总    \n我们这套大模型资料呢，会从 基础篇、进阶篇和项目实战篇 等三大方面来讲解。\n    ①.基础篇    \n基础篇里面包括了Python快速入门、AI开发环境搭建及提示词工程，带你学习大模型核心原理、prompt使用技巧、Transformer架构和预训练、SFT、RLHF等一些基础概念，用最易懂的方式带你入门大模型。\n    ②.进阶篇    \n接下来是进阶篇，你将掌握RAG、Agent、 Langchain 、大模型微调和私有化部署，学习如何构建外挂知识库并和自己的企业相结合，学习如何使用langchain框架提高开发效率和代码质量、学习如何选择合适的基座模型并进行数据集的收集预处理以及具体的模型微调等等。\n    ③.实战篇    \n实战篇会手把手带着大家练习企业级的落地项目（已脱敏），比如RAG医疗 问答 系统、Agent智能电商客服系统、数字人项目实战、教育行业智能助教等等，从而帮助大家更好的应对大模型时代的挑战。\n    ④.福利篇    \n最后呢，会给大家一个小福利，课程视频中的所有素材，有搭建AI开发环境资料包，还有学习计划表，几十上百G素材、电子书和课件等等，只要你能想到的素材，我这里几乎都有。我已经全部上传到CSDN，朋友们如果需要可以微信扫描下方CSDN官方认证二维码免费领取【 保证100%免费 】\n相信我，这套大模型系统教程将会是全网最齐全 最易懂的小白专用课！！"
  },
  {
    "title": "不用“科学上网”就能跨库检索文献？你一定要知道的五个学术工具-哔哩哔哩",
    "page_body": "不需要“科学上网”，如何才能快速进行跨库文献检索，找到权威外刊？\n本文推荐五款学术工具，兼具文献检索、AI对话、文献阅读的功能，一定能满足你的科研需求！\n一、WisPaper\n这款复旦大学研发的AI学术工具，限时免费！\n网址： https://www.wispaper.ai/?utm_source=bilibili\nWisPaper是一款专注于提升科研效率的智能学术工具，帮助学者进行学术检索、观点提炼等科研工作。\n1.海外文献库搜索\n对接主流学术数据库如Google Scholar，精准查找全球范围内的相关前沿研究。比如当你搜索“LLM”这类专业术语时，传统搜索引擎或学术库给出海量结果，却不一定精准匹配你的需求。而 WisPaper 能智能拆解你的问题，通过二次验证和深度搜索，快速锁定最符合你要求的高质量文献。\n当检索结果旁出现“Yes”标识时，示文献与关键词100%相关。\n2.深度对话\n针对文献内容，选中任意段落，即可向AI发起提问，深入挖掘细节。AI基于文献本身提供解释、澄清概念、对比观点。比如你想知道这篇文献中对于LLM的定义，只需要询问WisPaper就能得到一个不脱离文献本身的回答。\n3.核心总结\nWisPaper能一键提炼复杂文献，AI帮你提取核心论点、研究方法、关键结论，生成清晰、结构化的摘要。你只需要1分钟就能快速掌握文献精髓，确认这篇文献是否值得细细阅读分析。也不用担心自己鱼的记忆，记不住文献重点了。\n二、Scite Assistant\nScite Assistant 是一款创新的文献检索与分析工具，通过深度学习技术自动分类和评估科学文章的引用内容。scite能快速帮助用户找到特定文献的相关引用，清晰展示其他研究者对这些文献的支持、反对或提及态度，提高研究工作的效率和文献引用的准确性。适用于研究人员、学生和出版商，旨在优化学术文献的探索过程，确保引用的质量。\n1. 智能引文分类\n自动分类引文为支持、反对或提及，帮助用户快速了解文献的引用态度。\n2.引文上下文分析\n提供引文的上下文信息，展示引用在文献中的具体位置和含义。\n3.文稿上传\n允许用户上传PDF文稿，检查其中引用的文献是否被支持或反对。\n三、Research Rabbit \nResearch Rabbit 是一款基于文献关系图谱的探索工具，帮助你发现、追踪和管理学术文献。相较于传统数据库，Research Rabbit 更强调文献之间的网络关系，让你在“图”中读文献，在“图”中找线索。\n1.文献网络\n导入文献后，Research Rabbit 让你一眼看清哪些文献是领域内“高频核心”、某篇文章前后的研究链条以及哪些方向还在持续活跃研究中。\n2.文献摘要\n点击任意一篇文献，右侧会显示文献摘要。每一篇文献都可以加入你的清单，支持分组、批注、分享。在向清单添加新文献的过程中，相关文献会一直变化。它将根据新的关注的范围不断推荐相关文献。\n3.同步文献收藏\n选择想要同步的文献清单，ResearchRabbit 就能自动导入对应文献，并基于这些文献生成推荐网络。\n四、Readpaper\n1.摘录\n在文献中选取一段话，右键点击编写即可收录为摘要.点击摘要部分，会有辅助线连接摘要和原文，同时若从其他页面跳转过来，会有按钮可迅速回到原页面。选中后可划线，单独划线部分不计入笔记。添加批注之后，计入摘录，同时用红色、黄色、蓝色等不同颜色打标签，可以帮助用户利用颜色区分笔记类型。\n2.总结\n总结是用户自我撰写的总结文件，页面与Word基本一致，可以复制源文件中的语言加入到总结中，并加入外部链接、代码块等辅助。\n3.单词/短语\n支持手动添加/在翻译页面的生成结果中添加到短语（或摘要中）。可选有道、百度、IDEA和AI翻译四种，并可以自己添加阿里、腾讯、DeepL和Google翻译。\n五、ConnectedPapers\n1.上传文件，了解学术领域\n输入一篇典型的论文，构建该领域类似论文的图表，你会对感兴趣的领域的趋势、流行作品和动态有一个真实、直观的了解。\n2.搜索近期论文，无需保留列表\n你可以使用Connected Papers搜索近期论文，确保你没有错过任何新发表的重要的论文。\n3.建立相关文献目录\n你可以使用该工具为你的论文查找此领域的重要文献和里程碑研究，会按照时间线生成相关研究发展脉络，以及查找最近发表的最新技术。"
  },
  {
    "title": "超越ToT，苏黎世理工发布新一代思维图GoT：推理质量提升62%，成本降低31%-CV技术指南（公众号）博客园",
    "page_body": "前言 推理速度、推理质量二者兼得，全新一代思维图GoT可以处理更复杂推理任务！\n本文转载自新智元\n仅用于学术分享，若侵权请联系删除\n欢迎关注公众号 CV技术指南 ，专注于计算机视觉的技术总结、最新技术跟踪、经典论文解读、CV招聘信息。\nCV各大方向专栏与各个部署框架最全教程整理\n【CV技术指南】CV全栈指导班、基础入门班、论文指导班 全面上线!!\n大型语言模型在推理上仍然是弱势项目，需要依赖各种思维工具辅助完善推理过程。\n最近，苏黎世联邦理工大学、华沙理工大学的研究人员共同提出了一个全新的LLM思维框架GoT（Graph of Thoughts，GoT），在推理质量和推理速度上都要超越现有的思维链（CoT）和思维树（ToT）等方法。\n论文链接： https:// arxiv.org/pdf/2308.0968 7.pdf\nGoT的关键思想和主要优势在于将LLM生成的信息建模为图（arbitary graph），其中信息单元（思维，LLM thoughts）作为图的顶点，顶点之间的依赖关系作为图的边。\nGoT方法可以将任意的LLM思维组合成协同结果，提取出整个思维网络的本质，或者使用反馈回路来增强思维。\n通过实验可以证明GoT在不同任务上提供了优于现有技术的优势，例如，与ToT相比，排序任务的质量提高了62%，同时成本降低了31%\n研究人员认为，GoT方法可以让LLM推理更接近人类的思维和大脑推理机制，比如二者都在内部形成了复杂的网络结构。\nLLM思维（thought）的进化之路\n用户与LLM对话的过程主要包括用户消息（提示，prompts）和模型回复（思维、想法，thoughts），其中回复可以是一段文本（摘要任务）、一个文档（生成任务）或是一个代码块等。\n为了充分激活语言模型的能力，通常会采用各种提示方法：\n1. Input-Output (IO) 提示\n输入序列后，直接用语言模型获取输出，不添加任何中间思考过程。\n2. 思维链（Chain-of-Thought, CoT）\n在输入和输出之间引入多个中间思维状态，相比IO方法，可以显著提升语言模型在数学难题和通用推理任务上的性能。\n3. 多思维链\n独立生成多条思维链，然后根据预先指定的评分指标返回最佳输出结果的思维链。\n自一致思维链（CoT-SC）方法可以将CoT扩展到多条推理路径，不过没有进行单路径内的「局部探索」，例如回溯（backtracking）。\n4. 思维树（Tree of Thoughts, ToT）\nToT将过程或推理建模为一棵思维树来增强CoT-SC方法，单个树节点代表部分解决方案；基于给定的节点，思维生成器（thought generator）可以构造出一定数量的新节点，然后用状态评估器（state evaluator）为每个新节点生成相应评分。\n根据用例的不同，可以使用LLM自身对输出结果进行评估，也可以利用人工评分等。\n扩展树的过程中，节点的调度取决于使用的搜索算法，如深度优先、广度优先。\n其他方法如思维分解（thought decomposition）等或多或少都隐含使用了树的思路。\n思维图（Graph of Thought, GoT）框架\n总体来说，GoT包含四部分：\n1. 语言模型推理过程，即在特定上下文中，所有语言模型的思维，以及思维之间的关系\n2. 潜在的思维转换\n3. 用于获取思维评分的评估函数\n4. 用于选择最相关思维的排序函数\n推理过程\n研究人员将推理过程建模为一个有向图，顶点代表某个问题（初始问题、中间问题、最终问题）的一个解决方案，有向边代表使用「出节点」作为直接输入构造出的思维（入节点），具体思维的形式取决于用例。\n图节点的类别也不一定相同，例如在生成任务中，某些节点代表「写一段文字的规划」，另一些节点用来对「实际文本段」进行建模，推理过程是一个异构图（heterogeneous graph）。\n思维转换\n基于图结构，GoT可以在推理中实现不同的思维转换，也可以叫做graph-enabled transformations.\n比如说，在写作任务中，可以将几篇输入文章合并成一个连贯的摘要；在排序任务中，可以将几个排序后的数字子数组（sub-array）合并成一个最终的排序数组。\n每次变换操作都包含两部分：1）反映当前推理状态的图，以及2）一个用到的语言模型。\n变换操作会修改当前的图，添加新的节点和输入边。\n为了最大化GoT的表现力，用户可以指定要删除的相应顶点和边来显式删除思维；为了节省上下文空间，用户可以删除推理中未来不改进的部分。\n1）聚合转换（Aggregation Transformations）\nGoT可以将任意多个思维聚合成一个新的思维，并将不同思维的优势结合起来。\n在最基础的形式中，只创建一个新的节点，其余思维链中的节点作为出节点连接到新节点中。\n更一般地，该操作还可以聚合推理路径，也就是组成更长的推理路径\n2）优化转换（Refining Transformations）\n可以修改当前思维节点v为一条循环边（v, v），代表与原始思维相同迭代思维。\n3）生成转换（Generation Transformations）\n可以基于已有的单思维节点生成一个或多个新的思维，和之前的推理模式，如ToT或CoT-SC类似。\n对思维进行评分和排序\n评估函数所需要的数据包括受评估的思维、整个推理过程的状态以及语言模型，要求全推理过程可以最大化函数的通用性。\n在对思维的排序时，其输入包括推理过程、语言模型以及指定返回k个评分最高的思维。\n系统架构&可扩展性\nGoT架构由一组交互模块组成：\n1. 提示器（Prompter）：为LLM准备信息\n主要负责把图结构编码进提示词中，GoT架构允许用户根据不同用例实现不同的图编码，提供全部图结构访问权限。\n2. 解析器（Parser）：从LLM的回复中抽取信息\n解析器为每个思维构造出一个思维状态（thought state），包含了抽取出的信息，并用于后续状态更新。\n3. 评分模块（Scoring）：对LLM回复进行验证和评分\n验证一个给定的LLM思维是否能够满足潜在的正确性条件，然后对思维进行打分。\n具体分数可能需要构造提示，让语言模型给出评价；对某些用例来说，人类反馈评分也可以；如果是排序之类的用例，可能还需要引入局部评分函数。\n4. 控制器（Controller）：协调整个推理过程，并决定如何继续推理\n控制器中包含两个重要组件：图操作（the Graph of Operations, GoO）和图推理状态（GRS）。\n其中GoO是一个静态结构，指定了给定任务上的图分解过程，即规定了可用于LLM思维转换的操作，以及思维之间的顺序和依赖关系；每个操作对象都知道自己的前置操作和后继操作。\nGRS是一个动态结构，用来维护LLM推理过程进行中的状态，包括所有思维的历史及状态。\n示例用例\n1. 排序\n比如任务是对有重复的0-9数字序列进行排序，直接输入的话，语言模型无法对超过一定长度的序列正确排序。\n在GoT框架中，研究人员采用基于合并的排序方法：\n首先将输入的数字序列分解为多个子矩阵；然后对子矩阵分别进行排序；再将子矩阵进行排序；最后将所有子矩阵合并，得到最终结果。\n在这个用例中，LLM思维就是一串有序的数字序列。\n为了对LLM的输出进行评分，假定输入序列a的长度为n，输出序列b的长度为m，可以将误差范围定义为：\nX表示错误排序的连续数字对的数量，如果相邻两个数字排序错误，即左边的数字大于右边，则X加一。\nY表示，输出序列中的数字频率，与输入序列频率的吻合程度。\n2. 关键词计数任务\nGoT框架将输入文本分割成多个段落，计数每个段落中的关键字，并聚合子结果。\n段落的数量可以预先定义，也可以留给LLM分割，或者将每个句子视为一个单独的段落。\n为了获得对思维的评分，首先需要对每个关键字推导出计数和正确计数之间的绝对差值，然后将所有差值相加，并得到最终分数。\n3. 文档合并\n该任务的目标是基于几个内容部分重叠的输入文档生成一个新的保密协议（NDA）文档，尽量减少重复，同时最大限度地保留信息，可以广泛应用于法律程序等领域。\n为了给解决方案打分，研究人员要求语言模型查询两个值（每个值三次，取平均值），第一个值对应于解决方案冗余（10表示没有冗余，0表示至少一半的信息是冗余的），第二个值代表信息保留（10表示保留了所有信息，0表示没有保留），然后计算调和平均值。\n延迟与思维量的权衡\nGoT在延迟（思维图中达到给定最终思维的跳数）和思维量（volume，思维图中存在通往某个思维的路径数量）之间的权衡，也比之前的提示方案要好。\n假设输出一个思维的时间成本为O(1)，每个提示方案的总成本固定为Θ（n）：\n1. CoT-SC由源自单个起始思维的k个独立链组成；\n2. ToT是一个完整的k-ary树；\n3. 在GoT中，在完整k-ary树的叶子处与一个大小相同但边反向的镜像k-ary树连接起来；\n可以看到，虽然CoT-SC提供的思维量为N，但代价是高延迟（N）；CoT-SC将延迟降低了k倍（对应于分支因子），但同时也将容量降低了k倍。\nToT提供logk N的延迟，但容量也下降了；\nGoT是唯一一个同时具有logk N的低延迟和高容量N的方案，可能是由于GoT利用聚合思想，可以从分解图中的其他中间思维获取到最终思维。\n实验结果\n总的来说，GoT在排序、找集合交集、关键词计数和文档合并任务上，其结果质量要比基线模型更好，并且推理成本也更低。\nGoT vs. ToT\n在所有任务中，GoT都比ToT（树的分支更多、深度较浅）和ToT2（树的分支少、深度更深）的性能更好。ToT通常比ToT2的质量更高，但消耗也更大。\n相比ToT，GoT方法将中值误差降低了约62%，从而实现了更高的排序质量，并且运行成本降低了31%以上；优势主要是因为GoT能够将复杂的任务分解成更简单的子任务，独立解决这些子任务，然后逐步将这些结果合并成最终结果。\nGoT vs. IO / CoT\nGoT的质量更高，对于排序（P=64）任务，GoT的中值误差分别比CoT和IO低约65%和约83%，不过GoT和ToT的运行成本远高于IO和CoT\n随着问题规模P的增加，GoT相比其他基线来说质量提升更大。\n总的来说，这个分析说明了GoT确实非常适合复杂的问题案例，因为推理调度通常会随着问题规模的增长而变得更加复杂。\n参考资料：\nhttps:// arxiv.org/abs/2308.0968 7https://github.com/spcl/graph-of-thoughtshttps://twitter.com/DrJimFan/status/1695127737986585015\n欢迎关注公众号 CV技术指南 ，专注于计算机视觉的技术总结、最新技术跟踪、经典论文解读、CV招聘信息。\n计算机视觉入门1v3辅导班\n【技术文档】《从零搭建pytorch模型教程》122页PDF下载\nQQ交流群：470899183。群内有大佬负责解答大家的日常学习、科研、代码问题。\n其它文章\n分享一个CV知识库，上千篇文章、专栏，CV所有资料都在这了\n明年毕业，还不知道怎么做毕设的请抓紧机会了\nLSKA注意力 | 重新思考和设计大卷积核注意力，性能优于ConvNeXt、SWin、RepLKNet以及VAN\nCVPR 2023 | TinyMIM：微软亚洲研究院用知识蒸馏改进小型ViT\nICCV2023|涨点神器！目标检测蒸馏学习新方法，浙大、海康威"
  },
  {
    "title": "信息检索技术综述：从传统稀疏检索到现代深度学习方法-CSDN博客",
    "page_body": "摘要\n本文系统综述了信息检索领域的关键技术与算法发展历程。从基于关键词匹配的传统稀疏检索方法，到基于 深度学习 的稠密检索技术，再到结合两者优势的混合检索方案，以及多向量检索、学习式稀疏检索等前沿进展，本文全面分析了各类方法的理论基础、算法实现、优势局限及应用场景。研究表明，信息检索技术正朝着稀疏与稠密方法融合、效率与效果平衡、通用性与专业化结合的方向发展，为搜索引擎、问答系统和检索增强生成等应用提供强大支撑。\n1 引言\n信息检索（Information Retrieval, IR）是计算机科学的核心领域之一，致力于从大规模文档集合中精准高效地查找与用户需求相关的信息。随着互联网数据量的爆炸式增长和人工智能技术的快速发展， 信息检索技术 经历了从传统关键词匹配到现代语义理解的演变过程。\n当前的信息检索系统需要解决几个关键挑战： 词汇鸿沟问题 （同一概念的不同表达形式）、 语义理解问题 （表面匹配与深层语义的差异）、 效率与效果的平衡 （大规模数据下的实时响应）以及 查询模糊性 （短查询中的信息不足）。这些挑战推动了信息检索技术的不断创新与发展。\n本报告将系统梳理信息检索主要技术路线，分析代表性算法的原理与特点，探讨各种方法的应用场景与发展趋势，为相关领域研究者和实践者提供技术参考。\n2 稀疏检索：基于关键词匹配的传统方法\n2.1 基本概念与技术原理\n稀疏检索是信息检索中最传统且广泛应用的方法类别，其核心思想是通过高维稀疏向量表示文本，向量维度对应词汇表中的词项，值表示词项在文本中的重要性权重。这种方法基于 词袋模型 （Bag of Words），忽略词序和语法结构，依靠统计特征衡量文本相关性。\n稀疏检索的优势在于 高效性 （基于倒排索引实现毫秒级检索）、 可解释性 （结果可直接追溯至匹配关键词）和 无需训练数据 （基于统计规律而非机器学习）。\n2.2 代表性算法与技术\n2.2.1 TF-IDF算法\nTF-IDF（Term Frequency-Inverse Document Frequency）是最经典的权重计算方法，由词频（TF）和逆文档频率（IDF）两部分组成：\nTF-IDF(t,d) = TF(t,d) × IDF(t) \nAI写代码\n1\n其中：\nTF(t,d) = count(t,d) / |d| （词t在文档d中的出现次数除以文档总词数） IDF(t) = log(N / (df(t) + 1)) （总文档数N除以包含词t的文档数的对数）\nTF-IDF的基本思想是：一个词项在文档中出现频率越高，同时在所有文档中出现频率越低，则其区分能力越强，权重越高。\n2.2.2 BM25算法\nBM25（Best Matching 25）是TF-IDF的改进和概率化版本，被认为是效果最优异的稀疏检索算法。BM25引入了词频饱和控制和文档长度归一化机制，解决了TF-IDF的两个主要缺陷：\nBM25(D, Q) = Σ IDF(qi) × [tf(qi, D) × (k1 + 1)) / (tf(qi, D) + k1 × (1 - b + b × |D|/avgdl))] \nAI写代码\n1\n其中k1和b是可调参数（通常k1∈[1.2,2.0]，b=0.75），|D|是文档长度，avgdl是文档集合的平均长度。\n2.2.3 BM25F算法\nBM25F（Fielded BM25）是针对结构化文档的扩展版本，考虑不同字段（如标题、正文、摘要）的权重差异，对不同字段分别计算BM25得分后加权求和，适用于网页、学术论文等结构化文档检索。\n表1：稀疏检索代表性算法对比\n算法\n核心思想\n创新点\n适用场景\nTF-IDF 词频×逆文档频率 首次量化词项重要性 基础文本检索、特征加权\nBM25 概率模型+长度归一化 控制词频饱和，归一化文档长度 通用文本检索、搜索引擎\nBM25F 多字段加权融合 区分不同字段的重要性 结构化文档检索\n2.3 应用场景与局限性\n稀疏检索广泛应用于传统搜索引擎、初步候选检索（作为第一级召回器）和精确关键词匹配要求高的场景。然而，稀疏检索也存在明显局限性：无法解决词汇鸿沟问题（对同义词、近义词、一词多义处理能力弱）；语义理解能力有限；对长尾和复杂查询效果不佳。\n3 稠密检索：基于语义理解的现代方法\n3.1 基本概念与技术原理\n稠密检索是基于深度学习的现代检索范式，通过神经网络模型将查询和文档映射到低维稠密向量空间（通常几百到几千维），然后计算向量相似度实现语义匹配。与稀疏检索不同，稠密检索能够捕捉文本深层语义信息，有效解决词汇鸿沟问题。\n稠密检索的核心是文本嵌入技术，将文本转换为固定长度向量表示，语义相近的文本在向量空间中距离较近，语义相反的文本距离较远，语义无关的文本则几乎正交。\n3.2 代表性算法与技术\n3.2.1 DPR（Dense Passage Retrieval）\nDPR提出双塔架构（Dual Encoder），使用两个独立的BERT编码器分别处理查询和文档，通过对比学习最大化相关文档对的向量相似度。损失函数使用负对数似然： L = -log(exp(s(q,d+)) / Σ exp(s(q,d-))) 。\n3.2.2 ANCE（Asynchronous Negative Cache）\nANCE针对静态负样本不足的问题，提出异步渐进式负采样技术，在训练过程中动态刷新负样本池，选择当前模型最难以区分的\"困难负样本\"进行训练，显著提升模型区分能力。\n3.2.3 Sentence-BERT与SimCSE\nSentence-BERT对BERT网络进行修改，通过池化操作获取固定长度的句子表示，使用孪生网络结构进行微调。SimCSE通过简单的对比学习框架，使用dropout作为数据增强手段，无需额外数据就能学习到高质量的句子嵌入。\n3.2.4 BGE（BAAI General Embedding）\nBGE提出指令感知嵌入（Instruction-aware Embedding），在查询端注入任务指令（如\"为以下问题查找相关文档：\"），提升了模型在多任务场景下的泛化能力。BGE-M3模型支持稠密检索、稀疏检索与多向量交互三模态融合。\n表2：稠密检索代表性模型对比\n模型\n发布机构/时间\n核心创新\n训练数据需求\n计算效率\nDPR Facebook, 2019 双塔架构+对比学习 大量查询-文档对 高（编码后可快速检索）\nANCE Microsoft, 2020 异步困难负采样 大量查询-文档对 中（需动态更新负样本）\nSentence-BERT UKP Lab, 2019 孪生BERT+池化操作 句子对数据 高\nSimCSE Princeton, 2021 Dropout作为数据增强 无监督版无需标注数据 高\nBGE 北京智源, 2023 指令感知嵌入 多任务训练数据 中\n3.3 应用场景与局限性\n稠密检索广泛应用于开放域问答、语义搜索引擎、推荐系统和检索增强生成（RAG）等场景，特别是在需要深度语义理解的复杂查询、同义词扩展、跨语言检索等任务中表现优异。然而，稠密检索也面临可解释性差、需要大量训练数据、计算开销大等挑战。\n4 混合检索：结合关键词与语义的融合方法\n4.1 基本概念与技术原理\n混合检索是为克服稀疏检索和稠密检索各自局限性而提出的综合解决方案，同时执行两种检索方式并将结果融合，既保留稀疏检索的关键词精确匹配能力，又利用稠密检索的语义理解优势。\n混合检索系统的典型架构包括多个检索组件、结果融合模块和重排序模块。首先分别使用稀疏检索和稠密检索获取候选结果，然后对两组结果进行融合排序，最后可能还包括基于更复杂模型的重排序步骤。\n4.2 代表性融合算法\n4.2.1 加权求和法（Weighted Sum）\n将不同检索系统的得分进行归一化后加权融合：\nFinal_Score = α × Sparse_Score + β × Dense_Score \nAI写代码\n1\n其中α和β为权重参数，通常通过网格搜索确定。\n4.2.2 倒数排序融合（RRF）\n无需分数归一化的稳健融合方法：\nRRF_Score = Σ (1 / (k + rank_i)) \nAI写代码\n1\n其中k为常数（通常取60），rank_i是文档在第i个系统中的排名。\n4.2.3 学习式融合（Learned Fusion）\n使用机器学习模型（如LambdaMART）学习如何结合不同检索系统的得分，可加入更多特征如文档质量、新鲜度、权威性等。\n4.2.4 级联融合（Cascade Fusion）\n先使用高效检索方法（如BM25）获取候选集，再用精确但耗时的检索方法（如稠密检索）对候选集进行重排序。\n4.3 应用案例与优势挑战\n混合检索在RAG系统中应用广泛，如云鼎科技的\"基于聚类的稀疏与密集检索增强生成系统\"专利和openGauss的GaussMaster系统都采用了混合检索方案。\n混合检索的优势在于兼顾效率与效果、缓解零样本问题、适应多样查询类型。挑战在于系统复杂度高、计算资源消耗大、参数调优困难。\n5 多向量与学习式检索等前沿进展\n5.1 多向量检索模型\n多向量检索模型针对单向量表示信息压缩过度导致语义细节丢失的问题，使用多个向量表示文本的不同方面或片段。\n5.1.1 ColBERT模型\nColBERT采用token级别的向量表示和迟交互（Late Interaction）机制。查询和文档分别编码为token向量，相似度计算为： s(q,d) = Σ max_{j} (sim(q_i, d_j)) 。支持使用FAISS等向量库进行高效检索。\n5.1.2 Poly-encoder模型\n使用多个向量表示查询的不同方面，通过注意力机制与文档向量交互。计算查询m个向量与文档向量的注意力权重，加权求和后得到最终相似度。\n5.1.3 BGE-M3模型\n支持三模态融合的模型，可以同时处理稠密检索、稀疏检索和多向量交互。采用混合表示方式，既生成整体嵌入向量，也保留token级别的权重信息。\n5.2 学习式稀疏检索\n学习式稀疏检索（Learned Sparse Retrieval, LSR）将机器学习与传统稀疏检索结合，通过学习优化稀疏向量的权重和扩展，提升稀疏检索的表示能力。\n5.2.1 DeepCT与uniCOIL\nDeepCT使用BERT模型预测每个词项的重要性权重，将其视为词频的软性估计。uniCOIL在DeepCT基础上简化模型结构，使用轻量级MLP网络生成词项权重，保持高效性同时提升效果。\n5.2.2 Splade模型\n采用稀疏正则化技术和扩展策略，通过最大池化操作从BERT模型的输出中导出词项权重，同时支持查询和文档端的扩展（引入相关词项）。Splade-v2版本进一步提升了效果和效率。\n表3：前沿检索方法对比\n方法类型\n代表算法\n核心创新\n优势\n局限性\n多向量检索 ColBERT, Poly-encoder 细粒度token级交互 保留更多语义细节，精度高 计算存储成本高\n学习式稀疏检索 uniCOIL, Splade 神经网络优化稀疏表示 保持高效性同时提升语义能力 需要训练数据，可解释性降低\n跨模态检索 CLIP, ALBEF 统一多模态表示空间 支持图文等多模态检索 训练复杂度高\n生成式检索 DSI, SEAL 直接生成文档标识符 端到端检索，无需索引 可扩展性有待验证\n6 技术发展趋势与展望\n信息检索技术正处于快速发展阶段，未来几年可能呈现以下发展趋势：\n6.1 技术融合加深\n稀疏检索与稠密检索的界限逐渐模糊，出现更多混合模型和学习式稀疏检索方法，结合两者优势。知识图谱增强检索将更广泛应用，利用结构化知识提升检索效果。\n6.2 效率与效果平衡\n随着模型压缩、近似计算和硬件加速技术的发展，稠密检索的效率将进一步提升，使其更适合大规模实时应用。"
  },
  {
    "title": "利用 AI 解放双手：把“贾维斯”带进现实|开源专题 No.64-腾讯云开发者社区-腾讯云",
    "page_body": "Significant-Gravitas/AutoGPT https://github.com/Significant-Gravitas/AutoGPT\nStars:  160k  License:  MIT\nAutoGPT 是开源 AI 代理生态系统的核心工具包。\n提供构建、测试和委托 AI 代理的工具。 AutoGPT 处于 AI 创新前沿，提供文档、贡献指南以及快速开始创建自己的代理。 包含强大的组件如 Forge 和 Benchmark，帮助用户快速开发并衡量代理性能。 通过 Leaderboard 可以提交基准运行结果，并与其他代理竞争。CLI 和  UI  界面使得操作更加简单易用。\nreworkd/AgentGPT https://github.com/reworkd/AgentGPT\nStars:  29.2k  License:  GPL-3.0\nAgentGPT 是一个在浏览器中组装、配置和部署自主 AI 代理的项目。\n可以配置和部署自主 AI 代理。 自定义 AI 名称，并让其实现任何想象得到的目标。 通过思考任务、执行任务并从结果中学习，尝试达成目标。 提供了快速开始指南，包括 CLI 设置环境变量、 数据库 、后端和前端等内容。 技术栈包括 Nextjs, Typescript, FastAPI 等。\nOpenInterpreter/open-interpreter https://github.com/OpenInterpreter/open-interpreter\nStars:  42.8k  License:  AGPL-3.0\nopen-interpreter 是一个为计算机提供自然语言接口的项目。\n提供本地运行代码（ Python 、Javascript、Shell 等）的能力 通过类似  ChatGPT  的界面在终端中与 Open Interpreter 进行交互 提供自然语言接口来控制计算机通用功能，如创建和编辑照片、视频、PDF 等，控制 Chrome 浏览器进行研究，绘图分析大型数据集等 可以克服其他服务限制，在本地环境中运行，并具有完全访问互联网和任何包或库的能力。\nmicrosoft/JARVIS https://github.com/microsoft/JARVIS\nStars:  22.8k  License:  MIT\nJARVIS 是一个连接 LLMs 与 ML 社区的系统。\n探索人工通用智能（AGI）并将尖端研究传递给整个社区 提供 Easytool 以实现更简单的工具使用 发布 TaskBench 用于评估 LLMs 的任务自动化能力 支持 OpenAI 服务和 GPT-4 模型 添加了 Gradio 演示和构建了 Web  API ，支持 CLI 模式，并提供配置参数来调整本地端点规模\nlavague-ai/LaVague https://github.com/lavague-ai/LaVague\nStars:  3.0k  License:  Apache-2.0\nLaVague 是一个使用大型动作模型框架自动生成自动化工作流的项目。\n自然语言处理 ：理解自然语言指令以执行浏览器交互。 Selenium 集成：与 Selenium 无缝集成，用于自动化 Web 浏览器。 开源项目：基于开源项目构建，如 transformers 和 llama-index，并利用本地或远程的开源模型，确保代理透明度并与用户利益一致。 本地模型实现隐私和控制：支持像 Gemma-7b 这样的本地模型，使用户可以完全控制他们的 AI 助手并获得隐私保证。 高级 AI 技术：首先使用本地嵌入（bge-small-en-v1.5）执行 RAG 以提取最相关 HTML 片段来满足  LLM  回答查询所需。然后利用 Few-shot 学习和 Chain of Thought 来引出最相关 Selenium 代码进行操作而无需对 LLM 进行微调（Nous-Hermes-2-Mixtral -8x7B-DPO）以生成代码。\n0\n使用本地大模型调用代码，根本就是一场骗局！\n模型 软件 gpt 工具 框架\n 通过大模型调用其他工具到底可不可行？ChatGPT 或许能轻松搞定一切，但同样的需求落在本地大模型上，恐怕就要打个问号了。 法国开发工程师 Emilien Lancelot 尝试了多款号称具备工具调用功能的 agent 框架，来看看本地大模型到底能不能完成任务，但结果就像他总结的“一无所获”。是什么让这位工程师失望了？ \n深度学习与Python\n2024/06/17\n876 0\n2023年前半场的几个AI 应用 （Part 1)\n人工智能 软件开发\n能让ChatGPT更加流行的一个原因是，它还提供ChatGPT API。也就是说，各种软件通过它，可以无缝连接到ChatGPT。它的官方文档在这里:\n数智圈\n2023/04/19\n465 0\n【愚公系列】《AIGC辅助软件开发》004-AI智能化编程助手：AutoGPT\n腾讯技术创作特训营S10\n文章链接：https://cloud.tencent.com/developer/article/2466437\n愚公搬代码\n2024/11/18\n148 0\n把小爱音箱接入大语言模型：打造你的语音助手 | 开源日报 No.258\n客户端 模型 语音助手 开源 开发\nxiaogpt 是一个可以在小米 AI 音箱上运行 ChatGPT 和其他 LLM 的项目。\n小柒\n2024/05/10\n2K 0\n从文本生成到数据增强：探索 AI 前沿的开源套件 | 开源专题 No.44"
  },
  {
    "title": "如何构建心理健康垂直领域对话大模型？数业智能心大陆-博客园",
    "page_body": "为何心理健康垂直领域对话大模型至关重要？\n 在人工智能迅猛发展的今天，虽然通用语言模型如 ChatGPT 等在多个领域表现优异，但在心理健康等专业性强的领域却显得力不从心。为解决这一局限，心理健康领域的垂直大模型应运而生，它们致力于提供更加精确和深入的对话服务，以弥补通用模型在针对性和专业度上的不足。\n这些垂直大模型的引入是对深度心理专业知识需求的直接回应。它们经过专业领域的深度训练和优化，不仅能够精准把握行业术语和语境，还能够根据心理行业规范提供深入的解决方案。在心理健康行业，这表示模型能更准确地识别用户情绪，给出更加专业和个性化的反馈。随着技术的不断进步，垂直领域大模型预计将成为推动行业发展和服务水平提升的关键力量。\n垂直领域大模型的构建之道\n 在心理健康等垂直领域构建大模型，需面对通用大语言模型（LLM）在专业领域表现不佳的挑战。这主要是因为通用模型缺乏特定领域的训练语料，以及产品设计上未能针对特定领域进行优化。因此，关键在于如何让模型掌握特定领域的知识，即使这意味着牺牲一些通用能力。\n以下是几种旨在为特定领域如心理健康提供深度定制的智能解决方案：\n集成领域知识库：此策略通过整合丰富的领域特定知识库，增强模型对专业术语和情境的理解。虽然实施相对直接，但它依赖于高效的检索机制，并要求模型已有一定的领域知识储备。\n应用轻量级微调技术：采用先进的微调技术对模型进行细致调整，以优化其在特定任务上的表现。这种方法在保持模型参数数量不变的情况下，提升了任务适应性，但并不涉及新知识的深度学习。\n执行深度微调：在已有的基座模型上进行深度微调，使其能够吸收并应用特定领域的深层知识。尽管理论上这能提供最全面的领域适应性，但实践中可能会遇到训练数据稀缺和模型灾难性遗忘的难题，同时对计算资源的需求也较为庞大。\n开展定制化预训练：这是一种更为彻底的方法，从词汇表的构建到模型架构的设计，每一步都针对特定领域进行定制。遵循从预训练到监督微调再到强化学习的连续训练流程，能够孕育出高度专业化的领域模型。不过，这种方法在经济和技术上都极为考验企业的能力。\n清晰关键概念\n 在构建垂直领域大模型的过程中，避免误区并确保项目沿着正确的方向发展，关键在于对几个核心概念的清晰理解和正确应用：\n首先，我们必须区分领域专业性与任务特定性。领域专业性意味着模型需要对特定知识体系有深入的理解，以便处理复杂的专业问题并提供深度分析。而任务特定性则关注于模型完成如文本分类或情感分析等特定类型的任务，这通常不需要广泛的领域知识。\n其次，对参数高效微调（PEFT）的理解至关重要。PEFT 通过调整模型的一小部分参数来优化其在特定任务上的表现，但它并不涉及学习全新的知识体系。理解这一点有助于我们合理设定对 PEFT 的期望，并明确其在模型优化中的作用。\n此外，认识到模型知识的有限性并强化其检索能力至关重要。即使是最先进的模型也无法掌握无限的知识，因此，通过整合知识库、数据库和实时数据源，模型能够扩展其知识边界，提供更全面和准确的回答。\n同时，我们必须严格遵守伦理和合规性的准则，特别是在处理心理健康等敏感领域的数据时。这包括确保用户隐私和数据安全。\n数业智能心大陆—心理垂直领域对话大模型（心愈大陆）\n 数业智能心大陆作为一家专注于数字心理健康的人工智能企业，其定位为“实时在线、温情陪伴的 AI 专业心理咨询师”，正是基于对垂直领域大模型的深入理解和创新应用。在心理健康领域，心大陆通过结合先进的大语言模型技术与专业的心理疗愈知识，构建了一款能够提供精准、个性化服务的智能对话系统。\n基座模型的选择与优化：心大陆的心理垂直领域对话大模型所采用的基座模型是 Llama2 中文基座模型。这个模型构成了心大陆 AI 心理咨询师的“大脑”，通过增量预训练，模型吸收了数千万字的中文心理专业书籍内容，为模型提供了丰富的专业知识基础。\n微调数据的专业构建：微调数据包括了心大陆独立自主构建的一个全新且可公开访问的“关于青少年焦虑和抑郁评估的多模态心理评估数据集（MMPsy）”。包含了上亿条心理量表评估与反馈标注、20 多万分钟的心理访谈音频及文字记录、2 万多段认知行为疗法心理咨询和共情疏导专业语料、以及上万个心理常见问题及专业解答语料库，该数据集是目前心理健康领域中规模最大的多模态相关数据集。这些数据经过定量分析与标定，为心理测量 AI 模型的训练提供了科学的依据。此外，研发团队还筛选并校验了数十万份用户的心理量表数据，进一步丰富了模型的训练维度。\n训练方法：心大陆基于专业的 CBT 疏导语料训练的模型，利用先进大语言模型的对话能力、提示学习、上下文学习等技术进行自动数据生成，提高心理数据的规模和多样性，并将自动生成的数据与人工标注数据相结合，利用监督式微调的方式持续优化自有的大语言模型。这项技术已经通过了国家网信办的人工智能心理算法备案。在人机交互对话时，结合心理学知识库和检索技术，确保回复的相关性和准确性，减少错误信息，打造一个专业、高效的心理健康智能陪伴助手，以实现有效的心理疏导。\n未来展望\n 随着心理健康垂直领域对话大模型的成熟，未来的服务将更加精准地满足个体需求，提供更深层次的情感支持和专业指导。通过继续推进技术创新，加强伦理标准建设，确保用户隐私和数据安全，同时加大跨学科研究力度，融合心理学、人工智能、数据科学等领域的洞见，以实现更全面的心理支持系统。展望未来，我们期待这些智能系统不仅能成为人们心灵的慰藉者，更能作为心理健康教育和预防的重要工具，促进社会整体的心理健康和福祉。"
  },
  {
    "title": "前沿方法 | 贝叶斯方法论：不确定性建模的利器-广东外语外贸大学粤港澳大湾区会计与经济发展研究中心",
    "page_body": "前沿方法 | 贝叶斯方法论：不确定性建模的利器\n在统计学与机器学习的发展历程中，频率派（ Frequentist ）与贝叶斯派（ Bayesian ）一直是两大核心思想。前者认为 “ 参数固定，数据随机 ” ；后者则主张 “ 参数本身也是随机的 ” ，应当用概率来刻画我们的不确定性认知。\n随着计算能力的提升，贝叶斯方法正从 “ 小众 ” 走向主流，广泛应用于金融风险、宏观经济预测、临床试验、因果推断乃至人工智能领域。今天，我们就来系统梳理贝叶斯（ Bayesian ）方法论。\n一、贝叶斯定理：方法论的基石\n贝叶斯方法的核心来自一个 18 世纪的数学公式 —— 贝叶斯定理：\n其中：\n·  θ ：模型参数\n·  D ：观测数据\n·  P(θ) ：先验分布（ Prior ），研究者在看到数据之前的知识或假设\n·  P(D ∣ θ) ：似然函数（ Likelihood ），即数据在参数给定下的可能性\n·  P(θ ∣ D) ：后验分布（ Posterior ），即结合数据后的更新认知\n�� 用一句话概括：先验 ×  数据  →  后验。\n这种 “ 不断修正信念 ” 的思想，赋予了贝叶斯方法天然的动态学习能力。\n二、主要方法与技术\n1 ．贝叶斯回归（ Bayesian Regression ）\n在传统的回归分析中，频率派方法通过最小化误差平方和来估计模型参数，结果通常是点估计。相比之下，贝叶斯回归则为模型参数引入了先验分布（ Prior ），并通过后验分布（ Posterior ）的计算，提供一个完整的概率分布。\n贝叶斯回归的核心思想是：在观测到数据后，不仅给出单一的参数估计，还能够提供整个参数的分布情况，揭示模型的不确定性。例如，在房价预测中，贝叶斯回归不仅会给出一个平均预测值，还会输出一个概率区间，这有助于刻画预测的不确定性并指导决策。贝叶斯回归的优势在于其能够结合先验知识（如市场趋势、历史数据等），尤其适合数据稀缺或具有较高不确定性的情况。通过不断地将新的数据纳入模型，贝叶斯回归可以动态更新先验，从而提供更精确的预测结果。例如， Linero （ 2018 ）提出的贝叶斯回归树模型，用于高维预测和变量选择，展示了贝叶斯方法在高维数据中的强大适应性和灵活性。\n2 ． MCMC （ Markov Chain Monte Carlo ）\n贝叶斯推断中的后验分布往往无法直接解析，尤其在复杂模型或高维数据中。 MCMC 方法通过构造一个马尔可夫链并从中进行随机采样来近似计算后验分布。常见的 MCMC 算法包括 Metropolis-Hastings 和 Gibbs 采样。\n这种方法的核心优势在于其能够有效应对复杂模型中的多维参数空间，并且不依赖于模型的解析解。 Kucukelbir 等（ 2017 ）提出的自动微分变分推断（ ADVI ）与 MCMC 结合，为贝叶斯推断提供了更高效的计算方法，并显著提升了大规模问题的求解效率。\n3 ．贝叶斯模型平均（ Bayesian Model Averaging, BMA ）\n传统的模型选择方法往往依赖于选择 “ 最优 ” 模型，而贝叶斯模型平均（ BMA ）则通过对多个模型进行加权平均，解决了单一模型选择的偏误问题。在 BMA 中，模型的后验概率被用来为不同模型赋予权重，从而结合多个模型的优点，减少不确定性带来的影响。\n在生态学等领域， BMA 的应用已逐渐成为主流方法之一，它通过综合多个候选模型的预测，降低了模型设定错误的风险。 Dormann 等（ 2018 ）讨论了 BMA 在生态学中的应用，提出了贝叶斯方法在预测推断中的有效性和多样性。\n4 ．变分推断（ Variational Inference, VI ）\nMCMC 方法虽然是贝叶斯推断的标准工具，但它的计算效率相对较低，尤其在面对大规模数据时。为了解决这一问题，变分推断（ VI ）作为一种替代方法应运而生。变分推断通过将复杂的后验分布近似为一个简单的分布，并优化该分布，使其尽可能接近真实的后验分布，从而加速计算过程。\n与 MCMC 不同， VI 不依赖于采样过程，而是通过优化一个目标函数来直接计算近似后验。 Wang 和 Yeung （ 2020 ）讨论了变分推断在深度学习中的应用，提出了如何结合贝叶斯深度学习和变分推断技术来提升模型的效率和可扩展性。\n三、优缺点分析\n优点：\nØ  自然处理不确定性：给出概率分布而非单一估计。\nØ  小样本友好：可利用先验信息提升稳健性。\nØ  模型灵活性强：适用于层次模型、动态模型、非线性模型。\nØ  动态更新：随着新数据到来实时修正。\n缺点：\nØ  计算复杂度高：需要 MCMC 或 VI 等近似方法。\nØ  先验敏感性：不同先验可能导致不同结论。\nØ  落地成本大：工业界应用需更多优化和算力支持。\n四、应用场景\n贝叶斯方法在多个领域展现出广泛的适用性。在金融风险管理与资产定价中，它能够结合历史波动信息与实时市场数据，更敏锐地捕捉极端风险，从而提升信用风险评估和资产定价因子选择的稳健性 （ 项后军 等， 2023 ） 。在宏观经济预测与政策模拟中，贝叶斯方法通过引入合理的先验约束，有效缓解了传统模型在参数维度过高和样本不足时的不稳定问题，帮助研究者在通胀预测、产出缺口测度及政策冲击模拟中获得更加可靠的结论 （ 许志伟 和 刘建丰 ， 2019 ） 。在医学与临床试验中，它可以利用历史数据和专家知识作为先验，从而减少新试验所需的样本量，加快研发进程，尤其适合小样本与罕见病场景。在因果推断与社会科学研究中，贝叶斯方法常与多种计量方法结合，能够在复杂干预与不确定性环境下提供更全面的效果估计。在市场营销与消费者行为分析中，层次贝叶斯模型使得研究者可以在群体与个体层面同时刻画偏好差异，为市场细分和个性化推荐提供支撑。而在人工智能与机器学习中，贝叶斯优化已成为超参数调优的重要工具，贝叶斯深度学习则通过在参数上引入分布实现不确定性建模，增强了模型的稳健性与可解释性。整体来看，贝叶斯方法凭借其对不确定性的刻画与动态更新的优势，已经成为跨学科研究和实际应用中不可或缺的分析工具。\n参考文献\n[1]  项后军 ， 张清俊 ， 刘文革 ， 黄一鸣 .(2023). 金融系统向实体经济让利政策：效果评价及宏观效应研究 . 管理世界 ,39(12),70-89+121+90-93.\n[2]  许志伟 ， 刘建丰 .(2019). 收入不确定性、资产配置与货币政策选择 . 经济研究 ,54(05),30-46.\n[3] Dormann, C. F., Calabrese, J. M., Guillera‐Arroita, G., Matechou, E., Bahn, V., Bartoń, K., ... & Hartig, F. (2018). Model averaging in ecology: A review of Bayesian, information‐theoretic, and tactical approaches for predictive inference. Ecological monographs, 88(4), 485-504.\n[4] Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic differentiation variational inference. Journal of machine learning research, 18(14), 1-45.\n[5] Linero, A. R. (2018). Bayesian regression trees for high-dimensional prediction and variable selection. Journal of the American Statistical Association, 113(522), 626-636.\n[ 6 ] Wang, H., & Yeung, D. Y. (2020). A survey on Bayesian deep learning. ACM computing surveys (csur), 53(5), 1-37."
  },
  {
    "title": "【IQA技术专题】 基于多模态大模型的IQA Benchmark：Q-BENCH-CSDN博客",
    "page_body": "基于多模态大模型的IQA Benchmark：Q-BENCH（2024 ICLR）\n专题介绍 一、研究背景 二、Q-Bench 三大评估任务设计\n2.1 低层次视觉感知任务（A1：Perception） 2.2 低层次视觉描述任务（A2：Description） 2.3 视觉质量评估任务（A3：Assessment）\n三、实验\n3.1 低层次感知能力（A1）：整体准确率 3.2 低层次描述能力（A2）：总分（3 维度之和） 3.3 质量评估能力（A3）：平均 SRCC/PLCC\n四、总结\n本文将围绕《Q-BENCH: A BENCHMARK FOR GENERAL-PURPOSE\n FOUNDATION MODELS ON LOW-LEVEL VISION》展开完整解析。\n 为填补 多模态 大型语言模型（MLLMs）在低层次视觉感知与理解能力评估上的空白，研究团队提出Q-Bench 基准，从低层次视觉感知、低层次视觉描述、整体视觉质量评估三大维度系统评估 MLLMs 能力：构建含 2,990 张图像的LLVisionQA 数据集评估感知能力（通过问答正确性衡量），创建含 499 张图像及专家标注黄金描述的LLDescribe 数据集并结合 GPT 对比 pipeline 评估描述能力（从完整性、精确性、相关性维度），还设计基于 softmax 的策略让 MLLMs 预测可量化质量分数以评估质量评估能力；实验显示 MLLMs 具备初步低层次视觉技能，但稳定性和精确性不足，其中InternLM-XComposer-VL在多项任务中表现最优，GPT-4V接近初级人类水平但仍落后于资深人类，该基准旨在推动 MLLMs 低层次视觉能力的研究与提升。参考资料如下：\n [1].  项目地址\n论文整体结构思维导图如下：\n专题介绍\n图像质量评价（Image Quality Assessment, IQA）是图像处理、计算机视觉和多媒体通信等领域的关键技术之一。IQA不仅被用于学术研究，更在影像相关行业内实现了完整的商业化应用，涉及影视、智能手机、专业相机、安防监控、工业质检、医疗影像等。IQA与图像如影随形，其重要程度可见一斑。\n但随着算法侧的能力不断突破，AIGC技术发展火热，早期的IQA或已无法准确评估新技术的能力。另一方面，千行百业中各类应用对图像质量的需求也存在差异和变化，旧标准也面临着适应性不足的挑战。\n本专题旨在梳理和跟进IQA技术发展内容和趋势，为读者分享有价值、有意思的IQA。希望能够为底层视觉领域内的研究者和从业者提供一些参考和思路。\n系列文章如下：\n 【1】     IQA综述\n 【2】 PSNR&SSIM\n 【3】 Q-Insight\n 【4】 VSI\n 【5】 LPIPS\n 【6】 DISTS\n 【7】 Q-align\n 【8】 GMSD\n 【9】 NIQE\n 【10】 MUSIQ\n 【11】 CDI\n一、研究背景\n多模态大型语言模型（MLLMs，如 LLaVA、MiniGPT4）在高层次视觉任务（图像 captioning、VQA）中表现突出，但低层次 视觉感知 与理解能力的评估存在空白，而该能力对图像质量评估（IQA）、视觉失真检测、美学分析等关键应用至关重要。\n 为了填补这部分空白，Q-bench首个针对 MLLMs 低层次视觉能力的系统性基准，围绕核心问题 ——“MLLMs 如何模拟人类低层次视觉感知与理解能力” 展开，聚焦三大能力维度，且遵循两大设计原则：\n任务需涉及图像低层次属性的感知 / 理解，例如清晰度、噪声、亮度等；\n不依赖推理能力或外部知识。\n二、Q-Bench 三大评估任务设计\n分为3大块，并基于每一块提出一个 数据集 ，整体如下图所示。\n 感知、描述和评估三大块。\n2.1 低层次视觉感知任务（A1：Perception）\n提出一个数据集LLVisionQA，共 2,990 张图像，涵盖 10 类来源（含野外采集如 KONiQ-10K、生成图像如 AGIQA-3K、人工扭曲图像如 KADID-10K），确保多样性，如下表所示：\n 每张图像对应 1 个（问题 Q + 正确答案 C+1-3 个错误答案 F）元组，支持多选择问答评估。\n 作者针对这个数据集，设计了3类不同的问题：\nYes-or-No（判断类，如 “图像清晰吗？”）：平衡 40% 答案为 “No” 以减少模型偏向性； What（分类类，如 “图像存在哪种失真？”）：考察多属性理解； How（程度类，如 “图像清晰度如何？”）：覆盖非极端属性，支持细粒度评估。\n评估基于两大维度划分，确保评估全面性，如下图所示：\n属性类型（Axis 1）：包含第一个扭曲（模糊、噪声等）以及第二个其他属性（颜色、光照等）。 感知范围（Axis 2）：包含全局感知（整体清晰度）以及局部上下文感知（花朵是否对焦），例如上图的左右侧，分别代表了整体和局部一个评估。\n最后是评估方法：向 MLLM 输入 “问题 + 图像 + 选项”（选项随机打乱），如下图所示。\n 但是MLLM的输出可能不一致，因此作者后续采用 GPT  辅助 5 轮投票机制，解决 MLLM 输出格式不统一问题（如 “High”“A. High” 均视为正确），评估准确率提升至 98.4%。\n2.2 低层次视觉描述任务（A2：Description）\n提出了一个数据集LLDescribe，共 499 张图像，与 LLVisionQA 共享 10 类图像来源，确保任务一致性。它的生成是由摄影专家标注，平均58 词 / 条（远长于传统图像 caption 的 10-11 词），需覆盖图像所有低层次关注点（如失真、颜色、光照）。如下图所示：\n 评估方法，从 3 个核心维度对 MLLM 输出的描述打分（0-2 分 / 维度），具体定义如下：\n完整性：2 分（完全覆盖黄金描述信息）、1 分（部分覆盖）、0 分（未覆盖）。 精确性 ：2 分（无争议信息）、1 分（争议信息少于匹配信息）、0 分（争议信息更多）。 相关性：2 分（完全围绕低层次属性）、1 分（部分相关）、0 分（完全无关）。\n作者采用单模态 GPT 进行 5 轮平均评分，降低评估随机性，确保结果可复现。\n2.3 视觉质量评估任务（A3：Assessment）\n采用 7 个现有 IQA 数据集，覆盖 3 类图像场景，总规模81,284 张图像，具体可参见2.1节中表的第三列。为了完成这个操作，作者设计了以下的策略：\n 具体来说是运用了 softmax -based 质量预测策略，但MLLM 直接输出存在 “正向偏向”（78% 输出 “good”）和 “极端偏向”（84% 输出 “5” 分），可测量性弱，因此考虑取 MLLM 输出中 “good” 和 “poor” 两个高频 token 的 logits，通过 softmax 计算概率并映射为可量化分数，公式如下所示： q p re d ​ = e x SCORE _ TO K EN g oo d ​ + e x SCORE _ TO K EN p oor ​ e x SCORE _ TO K EN g oo d ​ ​\n该策略与人类评分的相关性显著优于 argmax 策略（如 LLaVA-v1 在 KONiQ-10K 上 SRCC 从 0.038 提升至 0.462）。\n三、实验\n实验同样也分为3块。\n3.1 低层次感知能力（A1）：整体准确率\n如下图所示：\n 其中：资深人类（Senior-level）整体准确率81.74%，所有维度均最优；GPT-4V（闭源）整体准确率 73.36%排名第一，What 类问题（79.18% 准确率）；InternLM-XComposer-VL整体准确率64.35%排名第二，In-context Other 属性（77.19%）；随机猜测整体准确率37.94%。\n3.2 低层次描述能力（A2）：总分（3 维度之和）\n如下图所示：\n其中：InternLM-XComposer-VL整体得分4.21排名第一，最大优势为相关性（1.87 分）；Kosmos-2整体得分4.03排名第二，最大优势为完整性（1.12 分）；平均水平（所有模型）整体得分~3.5，相关性普遍高于完整性 / 精确性。\n3.3 质量评估能力（A3）：平均 SRCC/PLCC\n如下图所示：\n其中：InternLM-XComposer-VL 平均 SRCC/PLCC为0.541/0.581排名第一， NIQE （传统 IQA 方法）平均 SRCC/PLCC为0.387/0.398 。\n作者对多个MLLM评估下来，结论总结如下，分为优势和不足，优势如下：\n基础能力：多数 MLLM 在 A1/A2/A3 任务中显著超随机猜测，证明具备初步低层次视觉技能； 场景适应性：在生成图像（CGIQA-6K、AGIQA-3K）和人工扭曲图像（KADID-10K）的 IQA 任务中，性能普遍优于传统方法 NIQE； 可优化性：通过同义词集成策略（如 “good+fine”vs“poor+bad”），Top5 模型的 IQA 精度平均提升 1.3%-2%。\n劣势如下：\n感知偏差：所有 MLLM 对 “扭曲属性” 的感知准确率低于 “其他属性”（如 LLaVA-v1.5 扭曲类 47.98% vs 其他类 67.30%），且 Yes-or-No 问题普遍偏向 “yes”（如 IDEFICS-Instruct 的 Yes 准确率 88.65% vs No 准确率 13.09%）； 描述短板：即使最优模型，描述的完整性和精确性也仅处于 “可接受水平”（平均 0.8-1.2 分 / 2 分），存在信息遗漏或错误； 细粒度评估差：在高相似度图像场景（如 LIVE-FB 含 95% 高质量图像）中，MLLM 难以区分细微质量差异，相关性指标显著下降。\n四、总结\n该论文探索了MLLMs在低级视觉能力方面的进展，期望这些大型基础模型能够成为通用智能，最终减轻人类的工作负担。提出 MLLMs 应该具备三项重要且独特的能力：对低级视觉属性的准确感知，对低级视觉信息的精确且完整的语言描述，以及对图像质量的量化评估。\n为了评估这些能力，论文收集了两个用于低级视觉的多模态基准数据集，并提出了一种基于 softmax 的统一 MLLMs 量化 IQA 策略。评估证明，即便在没有任何低级特定训练的情况下，一些卓越的 MLLMs 仍然具备可观的低级能力。尽管如此，MLLMs 要成为真正可靠的通用低级视觉助手仍有很长的路要走，需要进一步的优化。\n感谢阅读，欢迎留言或私信，一起探讨和交流，如果对你有帮助的话，也希望可以给博主点一个关注，谢谢。"
  },
  {
    "title": "Deep Research类产品深度测评：下一个大模型产品跃迁点到来了吗？-网易",
    "page_body": "Deep Research 产品可被理解为 一个以大模型能力为基础、集合了检索与报告生成的端到端系统，对信息进行迭代搜索和分析，并生成详细报告作为输出。\n参考 Han Lee 的 2x2 分析框架，目前 Deep Research 类产品在 输出深度、训练程度 两大维度呈现分异。 输出深度 即产品在先前研究成果的基础上进行了多少次迭代循环以收集更多信息，可进一步被理解为 Agentic 能力的必要基础。 低训练程度 指代经过人工干预和调整的系统，比如使用人工调整的 prompt，高训练程度则是指利用机器学习对系统进行训练。\n和传统 LLM Search 产品相比，Deep Research 是迈向 Agent 产品雏形的一次跃迁，可能也将成为具有阶段代表性的经典产品形态。\nDeep Research 产品通过系列推理模型嵌入，已生长出了 Agent 产品必要的推理能力。\n更为关键的是，是 Deep Research 采用多次搜索和异步返回模式，在持续搜索过程中迭代和优化回复，从而输出更符合用户需求的内容，信息推理深度显著提升。这一自主计划、反思、行动的落地，是 Agent 路线图中必须迈过的一级阶梯。\nFounder Park 正在搭建开发者社群，邀请积极尝试、测试新模型、新技术的开发者、创业者们加入，请扫码详细填写你的产品/项目信息，通过审核后工作人员会拉你入群～\n进群之后，你有机会得到：\n高浓度的主流模型（如 DeepSeek 等）开发交流；\n资源对接，与 API、云厂商、模型厂商直接交流反馈的机会；\n好用、有趣的产品/案例，Founder Park 会主动做宣传。\n01\n测评对象\n为描画当前 Deep Research 产品版图，厘清主要产品的能力层次，本文选取 Google Deep Research （下称 Google）、OpenAI Deep Research （下称 OpenAI）、Perplexity （下称 PPLX）、xAI Deep Search （下称 xAI）、Manus 五个产品进行测评，主要维度比较如表 1 所示。\n表 1 测评产品比较（信息截至 2025 年 4 月）\n从这一比较不难发现，除 Google、xAI 以外，迄今其他三家产品均支持图片等多模态输出。\n考虑到目前公开性能表现有限，包括 Hugging Face、JINA 等团队自行的开源复现项目，并不在此次测评对象之中。\n02\n测评任务\n本文锚定 “Agent 能力+产品核心能力” 任务框架设计，聚焦 Tool Use、Instruction Following、Memory 三大维度的 Agent 能力，以及 报告输出能力 这一 Deep Research 的核心性能，对五大 Deep Research 产品进行评估。\n需要指出的是， Memory 没有进入到最终的任务域。 主要原因在于，经多任务测试，Deep Research 的自动联网检索构建了一个“后门机制”，即使向其投喂长文，它也可以通过联网检索获得精炼信息，从而绕过用户给定的长文 context。因此，目前难以通过长文 prompt 有效评估其 memory 空间。\n本文最终的 “2+1”任务设计选取遵循“代表性案例”思路 ，完成各任务的主要能力可被视为 Tool Use、Instruction Following 等“代理变量”，二者并非严格对应关系。在不同能力维度统摄下，分别设计了任务场景如下。\nTool Use\n在线检索能力：小众内容定位\n小众内容检索反映出模型处理在线信息“长尾效应”的综合能力，亦是 ODR 团队访谈中提到的优势任务，如 Josh Tobin 所言：“当你有一个需要详细描述的问题，而且获得最佳答案需要大量阅读互联网内容时，Deep Research 真的特别擅长。如果你提出一个比较宽泛的问题，它会帮你理清楚具体想要什么。但它最出色的表现是在 查找特定 信息 时。”\nTask 1 冷门电影检索\n本文参考 IMDb 观影全球评论数，选取了一部冷门电影（N<500），截取了电影中非关键帧场景，对其进行简要描述，逆向检索电影名称，从而评估 Deep Research 产品是否能依据有限信息，通过外部网页检索，准确定位到长尾内容。\nTask 2 最新书籍检索\n考虑到测试内容可能在模型训练集中出现，为提升测评信度，本文设计了最新出版书籍的检索任务，选取了于 2025 年 4 月刚出版的经济学书籍《Startup Capitalism New Approaches to Innovation Strategies in East Asia》，并使用书中使用的理论框架和案例提供线索，再次检测产品的检索能力。\n数据分析能力：基于财报的因子计算\n数据分析能力体现了产品对数据进行基础处理以及复杂分析的性能，评估其数值分析的准确性、推理逻辑可靠性的综合表现。\n本任务关注 Tesla 财报信息的因子计算，选取基于财报以及 Earnings call 的结构化数值数据分析，提供 EPS 增速跳跃因子这一指标的计算公式，评估产品调用代码进行数值计算的自主性和有效性。\n编程能力：智慧城市设计\n产品设计规划需要进行目标分析、流程分解、方案比较，并通过 coding 加以实现，贴合效率、美观等现实目标进行创新，能够评估模型在给定目标下进行自主探索的潜力。\n本任务围绕智慧城市大脑的前端产品场景，要求模型通过数据计算、指标构建、模块设计，最终形成一个能够展示的网页解决方案，从而评估模型贯穿美观设计、指标运算、代码落地的全流程能力。\nInstruction Following\n文献分析能力：多话题科研综述\n科研文献综述需要模型围绕特定科学问题进行文献整合与现状梳理，难点一在于专业领域文献的检索源可得性确认，难点二在于分析特定领域研究的纵深性，完成对已有文献的整合分析。\n本文基于文献综述这一任务场景，进行分段任务设计，其中每段的综述主题、数据信源、分析逻辑、字数要求、引用格式均有所不同，通过不同维度的差异化要求，考察产品对于“碎片化”需求的 instruction following 程度。\n路线设计能力：旅游方案规划\n消费方案推荐要求模型对消费属性以及用户评论进行全方面检索与梳理，并在个体偏好、预算等多重约束条件下，生成最优规划，主要体现在旅行规划和购物推荐两大任务，ODR 团队 Isa Fulford 在访谈中推荐：“我认为 购物和旅行 推荐 是最主要的应用场景。我个人已经使用这个模型好几个月了。”\n本文设计了一个上海-韩国旅游规划的场景，考察产品兼顾时间要求、消费预算限制、目的地偏好，最终提供具有一个用户需求贴合性、可行性方案的 instruction following 能力。\n报告输出能力：研报分析\n市场研报分析考察模型自主确定关键分析维度、确认数据来源，结合宏观趋势、中观行业、微观企业与用户的多角度整合分析表现，并在报告输出环节保证数据与图表的结构化输出。\n本文围绕关注的 AI 招聘初创公司 Mercor，要求产品从市场格局、产品技术、商业模式、竞品公司、团队特征进行调研分析，并强调可通过图表形式可视化。\n03\n测评结果\nTool use 能力\n在线检索：OpenAI“一骑绝尘”\nTask 1\n根据一幕线索顺藤摸瓜出一部冷门电影， 这一任务只有 OpenAI 成功检索出电影名《布宜诺斯艾利斯 100 公里 （Buenos Aires 100 km）》，其他产品均在“狐疑”中给出了错误答案。\n需要指出的是，五家产品均收到了两次机会——在收到第一轮 prompt 时，均未给出正确回复，而在提示“这部电影好像是在阿根廷拍摄”后，OpenAI 结合这一线索成功定位到电影的主要信息。\n本文选取测评镜头并非电影关键情节，并且描述极尽简单，OpenAI 能够在极其有限信息的情况下，展开多源网页搜索，验证了其主打的“小众内容检索能力”确实一骑绝尘。\n图 OpenAI 的正确电影输出\nManus、Google、xAI 三家产品则在回复语气中透露出一丝迟疑和不确定性，在表示需要更多信息继续推理的同时，给出了 3-5 个错误回复。\n图 Manus 的电影猜测结果\n图 Google 的电影猜测结果\n图 xAI 的电影猜测结果\n而 PPLX 则在输出中坦诚表示，基于给定线索，无法给出任何潜在答案，未生成其他冗余信息。\n图 PPLX 的无结论输出\nTask 2\n与 Task 1 结果无异，OpenAI 在给出三本潜在书籍结果的同时， 仍是唯一成功检索到正确书籍的产品， Google 和 xAI 则提供了一些更为古早、延伸性的书籍和论文，Manus 则未输出实质性书籍检索结果。\n图 OpenAI 的正确书籍输出\n图 Google 的书籍猜测结果\n图 xAI 的电影猜测结果\n图 Manus 的书籍猜测结果\n数据分析：“全军覆没”\n在初始 prompt 中，产品被要求自行挖掘财报和 earings call 相关信息，并根据给定公式进行计算。然而，PPLX 并未正确爬取各季度的 EPS 数据、以及同比每股收益增长率 （%），可能存在信源偏差问题。\n因此，在第二次 prompt 中直接给定了 Tesla 在 2023 年 Q1 到 2024 年 Q4 的 EPS growth 的数据，要求产品进行计算。\n这一计算过程并不算复杂，用普通最小二乘法（OLS）即可完成拟合， 然而，五家产品均败下阵来，无一成功计算出正确数值，且“各有各有的问题”。\n“中道崩殂”：xAI、Google 未完成计算任务\n• xAI 大篇幅展示了对于计算公式的理解，长篇大论看似严谨，然而【预测 2024 Q1（基于 2023 Q2-Q4 和 2024 Q1）】的表述， 实则未能正确理解公式。\n图 xAI 展示的计算过程\n• xAI 在给出了计算样例后，仅完成了 1/4 的计算量，输出了第一个数据点 2024Q1 的计算值，其对于公式的错误理解导致基准有误，作为唯一输出的 -4.19 同样与正确答案差之千里。\n图 xAI 未完成的因子计算\n• Google 对计算口径的理解具有明显偏差 ，将所需 数据点 错误推至 2025 年 Q1，基于这一错误逻辑，其同样未完成极端，甚至未输出任何测算结果。\n图 Google 给出的计算示例\n“南辕北辙”：PPLX 计算对象错位\n• PPLX 早在输出中重复了 prompt 中提到的计算方式，在计算公式渲染效果上更胜一筹， 但从结果来看，其同样未能正确理解计算逻辑， 混淆了 2024 年 Q1-Q4 的计算对象，反而输出了 2023 年 Q3-2024 年 Q2 的计算值。\n图 PPLX 展示的计算过程\n• 相较于 LLM Search 计算的结果，PPLX 将计算数值结果与市场分析文本高度杂糅，使得其答案异常隐匿。其输出的 4 个结果点中，2024 年 Q1、Q2 符合计算需求，但同样并非正确答案。\n图 PPLX 的错误计算数据点\n“失之毫厘”：OpenAI、Manus 计算结果略有偏差\n从计算对象准确性、计算完备度而言，OpenAI 和 Manus 在这一数据分析任务中表现出最高的能力成熟度。 二者均正确了定位了 2024 年 Q1-Q4 的分析目标，且在数值计算的大方向上把握正确，但在数值计算的微观代码实现层面有所偏差。\n• OpenAI 基于表格直接给出了 2023 年 Q2-2024 年 Q4 的计算结果，尽管输出并非完全准确，但其 2024 年 Q1-Q4 在数据整体分布趋势上与正确数据相似。 由于并未给出数值计算的完整运行代码，难以排查其计算偏差发生点。\n图 OpenAI 的计算结果\n• Manus 在明确给出了计算结果的基础上，主动输出折线图、条形图对结果进行可视化，相较于 OpenAI 的表格更具易读性。\n图 Manus 输出结果与可视化图表\n但 Manus 在回归运算过程中未能正"
  },
  {
    "title": "广东省工业和信息化厅 广东省财政厅珠海市人民政府关于举办第八届“创客中国”广东省中小企业创新创业大赛暨第七届“创客广东”大赛的预...",
    "page_body": "索引号： 696453330/2023-00178\n分类：\n发布机构： 广东省工业和信息化厅\n成文日期： 2023-04-14\n名称： 广东省工业和信息化厅 广东省财政厅珠海市人民政府关于举办第八届“创客中国”广东省中小企业创新创业大赛暨第七届“创客广东”大赛的预通知\n文号： 粤工信服务函〔2023〕12号\n发布日期： 2023-04-19\n主题词：\n各地级以上市中小企业行政主管部门、财政局，省中小企业服务中心，各有关单位： \n　　 为贯彻落实党的二十大“支持中小微企业发展”“支持专精特新企业发展”“营造有利于科技型中小微企业成长的良好环境”要求和我省高质量发展大会“加快实施创新驱动发展战略”会议精神，锚定高质量发展首要任务，全力推进制造业当家战略，推动中小微企业创业创新与提质升级，营造大众创业万众创新良好氛围，省工业和信息化厅、省财政厅和珠海市人民政府拟联合举办第八届“创客中国”广东省中小企业创新创业大赛暨第七届“创客广东”大赛（以下简称“创客广东”大赛）。为提前做好赛事筹备和项目征集工作，现将有关事项通知如下：\n　　 一、大赛主题\n　　 创客广东 匠心南粤\n　　 二、时间地点\n　　 时间：2023年4月--10月\n　　 地点：决赛在珠海市举行\n　　 三、组织架构\n　　 大赛设立组委会，负责赛事的组织实施，组委会下设秘书处、仲裁委员会和专家委员会，坚持公平、公开、公正原则组织开展各项赛事工作。\n　　 （一）秘书处。秘书处设在省工业和信息化厅服务体系建设处，负责赛事活动的协调筹备、组织实施、监督指导、宣传报道等工作。\n　　 （二）仲裁委员会。由组委会秘书处召集，负责对有争议的事项进行仲裁。\n　　 （三）专家委员会。评审专家负责对参赛项目进行评审。初赛专家从大赛官网专家库中抽取或由主办单位自行确定；复赛、决赛评审专家从省工业和信息化厅专家库中抽取或特别邀请。为推进大赛项目落地及成果对接，初赛、复赛、决赛中龙头企业评委须占2名以上。\n　　 四、赛事组织\n　　 （一）初赛（地市赛、专题赛）。分别由各地级以上市中小企业行政主管部门和专题赛中标单位主办，省工业和信息化厅和省财政厅为指导单位。\n　　 （二）复赛、决赛。由省工业和信息化厅、省财政厅和珠海市人民政府共同主办，珠海市工业和信息化局、香洲区人民政府等单位承办。\n　　 （三）赛事支持。中国银行广东省分行作为本次大赛特别支持单位，为大赛组织和参赛项目提供支持，并为参赛项目提供最高3000万元额度的优惠授信。\n　　 鼓励与国内各大展会、创业赛事、创投栏目资源共享，提升大赛品牌价值；鼓励国内外商贸组织、知名电商平台、媒体（含投融资垂直媒体）、社交平台、投资机构、科研院所、券商、银行、双创载体、创辅机构（知识产权、律所、会计师事务所、工业设计机构）等积极参与并作为大赛支持单位，提升大赛产销对接成果、赛事服务质量及社会影响力；鼓励各地市充分利用官方媒体（含门户网站及公众号等）设置大赛官网外链。\n　　 五、参赛与报名\n　　 （一）参赛领域\n　　 根据《战略性新兴产业分类（2018）》（国家统计局令第23号）、《广东省制造业高质量发展“十四五”规划》精神，围绕培育发展战略性支柱产业集群和战略性新兴产业集群，设置新一代信息技术（包括新一代电子信息、软件与信息服务）、智能家电、新材料（包括先进材料、前沿新材料）、半导体与集成电路、智能制造、机器人、生物医药与健康、新能源等8个参赛领域。\n　　 （二）参赛条件\n　　 1.企业组 \n　　 （1）在中国境内及港澳台地区注册，符合《中小企业划型标准规定》（工信部联企业〔2011〕300号）的中小微企业。\n　　 （2）参赛项目已进入市场，具有良好发展潜力。 \n　　 （3）拥有自主知识产权且无产权纠纷，无不良记录。\n　　 （4）往届“创客中国”大赛区域（专题）赛获得一、二、三等奖的参赛项目，不参加本届赛事。 \n　　 2.创客组 \n　　 （1）遵纪守法的个人或团队。 \n　　 （2）团队的核心成员不超过5人。\n　　 （3）参赛项目的创意、产品、技术及相关专利归属或授权参赛团队，与其它单位或个人无知识产权纠纷。\n　　 （4）往届“创客中国”大赛区域（专题）赛获得一、二、三等奖的参赛项目，不参加本届赛事。\n　　 （三）报名方式\n　　 1.报名网址。符合条件的企业和创客（以下统称参赛者）均可通过大赛官网（http://i.gdsme.org）注册登记，报名参加地市赛或专题赛。参赛者应提交完整报名材料，并对所提交材料的真实性、准确性、合法性负责，未注册登记的参赛者不得参加大赛。\n　　 2.报名时间。各地市赛、专题赛报名截止时间由主办单位根据赛事组织情况自行确定。\n　　 3.其他事项。大赛不向参赛者收取任何费用。参赛者不可重复报名参加地市赛、专题赛，对于重复参赛或剽窃、侵夺他人创新成果，以及用其他不正当手段骗取奖项的参赛者，一经发现取消参赛资格。\n　　 六、赛程安排\n　　 大赛分为初赛（地市赛、专题赛）、复赛和决赛。\n　　 （一）初赛\n　　 1.地市赛。\n　　 （1）组织项目参赛。各地市围绕大赛主题和参赛领域筹备策划赛事，积极动员龙头企业参与赛事（珠三角地区发动不少于5家，粤东西北地区不少于3家），动员不少于当地省级专精特新企业总数的10%报名参赛；发动国家和省级小型微型企业创业创新示范基地、创客等参与赛事活动；按照属地原则，珠三角地区地市赛参赛项目需超过80个（其中企业组项目不少于60个，创客组项目不少于20个），粤东西北地区需超过50个（其中企业组项目不少于35个，创客组项目不少于15个）。\n　　 （2）组织项目评审。地市赛主办单位组织专家对参赛项目进行评审，并将评审结果录入大赛官网。\n　　 （3）推荐复赛项目。地市赛主办单位对拟推荐项目进行知识产权稽核后向组委会秘书处推荐16个项目（10个企业组项目、6个创客组项目）。\n　　 时间：2023年6月底前完成赛事并推荐复赛项目。\n　　 2.专题赛。\n　　 （1）确定专业领域。大赛组委会研究确定专题赛的专业领域（新一代信息技术、智能家电、新材料、半导体与集成电路、智能制造、机器人、生物医药与健康、新能源等，在大行业领域方向内可再提出具体赛事细分的行业领域），以及特设数字化转型、海归专项赛。\n　　 （2）组织项目参赛。积极动员不少于5家龙头企业参与赛事，参赛项目需超过80个（其中企业组项目不少于60个，创客组项目不少于20个）。\n　　 （3）组织项目评审。专题赛主办单位组织专家对参赛项目进行评审，并将评审结果录入大赛官网。\n　　 （4）推荐复赛项目。专题赛主办单位对拟推荐项目进行知识产权稽核后向组委会秘书处推荐16个项目（10个企业组项目、6个创客组项目）。\n　　 时间：2023年6月底前完成赛事并推荐复赛项目。\n　　 3.“创客中国”广东省区域赛。\n　　 “创客中国”官网开通了广东省区域赛报名入口，对于通过该渠道报名的参赛项目，由大赛中标单位对参赛项目进行线上评审和知识产权稽核后，按不超过该渠道项目总数15%的比例向组委会秘书处推荐（最多不超过16个项目，其中企业组项目不超过10个、创客组项目不超过6个）。\n　　 参赛项目不得重复报名“创客广东”大赛和“创客中国”广东省区域赛。\n　　 时间：2023年6月底（具体按国家大赛通知文件中的日期）。\n　　 （二）复赛\n　　 秘书处对初赛推荐项目进行复核，确定入围复赛名单。\n　　 所有入围复赛项目按照新一代信息技术、智能家电、新材料、半导体与集成电路、智能制造、机器人、生物医药与健康、新能源等8个领域进行分组，通过现场（如遇疫情等特殊情况则采用线上）演示和答辩等方式组织专家评审，确定入围决赛50强、100强名单。\n　　 时间：2023年8月底。\n　　 （三）决赛\n　　 8个领域企业组第一、第二名和创客组的第一名入围决赛（共24个项目，16个企业组项目、8个创客组项目），按照企业组和创客组进行分组，参赛者进行现场演示和答辩，专家采取现场评分的方式进行项目评审，共决出16个获奖项目，其中企业组决出一等奖2名、二等奖3名、三等奖5名，创客组一等奖1名、二等奖2名、三等奖3名。\n　　 时间：2023年9月上旬。\n　　 （四）推荐项目参加“创客中国”赛事。\n　　 根据“创客中国”大赛通知要求，向国家推荐项目。\n　　 时间：2023年9月（具体按国家通知要求确定）。\n　　 七、扶持措施\n　　 大赛结束后，组委会将借助大赛官方平台，持续跟进参赛项目的发展情况和对接需求，给予相应政策扶持、专项培训、产业对接和融资对接等后续服务。\n　　 （一）奖项设置\n　　 大赛组委会将对参赛项目、赛事组织单位分别设立奖项。\n　　 1.参赛项目奖项设置与现金奖励安排\n序号\n名次\n奖项设置\n1\n一、二、三等奖\n（1-16强）\n现金奖励、颁发奖杯及荣誉证书\n2\n17——50强\n现金奖励、颁发50强荣誉证书\n3\n51——100强\n颁发100强荣誉证书\n4\n最佳人气奖\n颁发奖杯及荣誉证书\n　　 注：一、二、三等奖共16名，故从17强起颁发“优胜奖”荣誉证书。\n名次\n企业组\n创客组\n数量\n现金奖励\n数量\n现金奖励\n一等奖\n2名\n30万元\n1名\n15万元\n二等奖\n3名\n20万元\n2名\n10万元\n三等奖\n5名\n10万元\n3名\n5万元\n50强\n共34名（前50名中除一、二、三等奖获奖者），\n每个项目奖励0.5万元。\n　　 2.对地市赛、专题赛主办单位，大赛组委会以赛事数量与赛事质量并重原则，结合参赛项目数、参赛专精特新企业数、龙头企业数、获奖项目数、赛事宣传等多项指标对赛事组织情况进行评价，对表现突出的地市赛、专题赛以及承办大赛的组织单位进行表扬，颁授最佳组织奖2名、优秀组织奖6名（见附件3）。\n　　 3.对决赛评委、仲裁专家，以组委会秘书处名义颁授“评委聘书”、“仲裁专家证书”。\n　　 （二）政策扶持\n　　 1.对全省50强项目落地广东，并在获奖之日起1年内成功获得股权融资支持（以在银行入账时间和金额为准），按照不超过其融资额度的10%给予奖励，最高不超过100万元且只能申请一次。\n　　 2.对全省100强项目提供落地对接与培训服务。\n　　 3凡入围复赛的参赛者在符合政策条件的基础上，将优先推荐参与省工业"
  },
  {
    "title": "三篇论文，纵览深度学习在表格识别中的最新应用",
    "page_body": "原创 Synced 机器之心\n机器之心分析师网络\n作者：仵冀颖\n编辑：Joni\n本文从三篇表格识别领域的精选论文出发，深入分析了深度学习在表格识别任务中的应用。\n表格识别是文档分析与识别领域的一个重要分支，其具体目标是从表格中获取和访问数据及其它有效信息。众所周知，本质上表格是信息表达的一种重要形式，它能将数据组织成标准的结构，便于信息检索和比较。通常我们现阶段针对表格的处理方式是：人工使用 Excel 等工具打开表格，之后提取、操作和处理表格中的内容。人工处理表格的方式存在很多问题，一是，由于表格数量大，编辑过程中经常更新，不同的表格中可能写入了相同的内容，对这些表格进行手动交叉检查的时间有限，这种人工处理方法经常会出现表格处理错误、不一致等问题，这种错误和不一致可能会给公司带来巨大的声誉风险，甚至是经济损失。二是，手工提取表格信息往往是一个繁琐而耗时的过程。三是，在金融业和许多其他领域，表格往往是以非结构化的数字文件（如可移植文档格式（PDF）和图片格式）公开的，这些文件难以直接进行人工提取和处理。\n因此，高效地从文档中找到表格，同时有效提取表格中的数据与结构信息即表格识别，成为了一个亟待解决的问题。表格识别具体是指，将表格以图片的形式发送到电脑上，自动提取表格图片中的各类信息，同时将这些信息重新堆叠为新的文档，也称为表格文档重建。应用表格识别技术能够大大节省表格内容处理的时间，同时能够减少由人为因素产生的错误。\n近年来，国内外专家、学者针对这一问题进行了大量研究，引入图卷积神经网络（Graph convolutional network ，GCN）、快速 R-CNN (Fast R-CNN)，全卷积神经网络（Fully Convolutional Networks，FCN）、条件生成式对抗网络（Conditional Generative Adversarial Networks，CGAN）等深度学习方法和模型进行自动化的表格识别。表格识别也逐渐演变成了多个分支研究领域，包括：专门的数据集构建、表格检测、表格结构识别、表格检测与结构识别等。\n本文从近两年公开发表的文章中，包括国际文档分析与识别会议（International Conference on Document Analysis and Recognition，ICDAR)和arXiv平台的论文中精选了三篇，深入分析深度学习在表格识别任务中的应用。其中，第一篇文章重点关注表格识别任务的数据集构建，第二篇文章聚焦的是表格检测与结构识别任务，第三篇文章重点研究表格结构识别任务。\n一、GFTE: Graph-based Financial Table Extraction\n论文地址：https://arxiv.org/abs/2003.07560\n本文为 2020 年最新公布在 arxiv 上的一篇文章。本文的工作聚焦的是金融类的、中文表格数识别。此文的贡献主要包括两部分。第一，为了便于从非结构化数字文件中进行基于深度学习的表格抽取，本文发布了一个标准的中文数据集 FinTab，它包含了 1600 多个不同类型的财务表格及其相应的 JSON 结构表示。第二，本文提出了一种新的基于图卷积神经网络（Graph Convolutional Network ，GCN）的模型 GFTE 作为该表格识别数据集中的基线方法。GFTE 通过融合图像特征、位置特征和文本特征，能够对表格进行精确的边缘预测，取得了较好的效果。FinTab 数据集和 GFTE 方法都已经公布在 GitHub 中：https://github.com/Irene323/GFTE。\n首先，作者给出了一个经典的表格识别问题示例。图 1 给出了两个已有的表格内容提取方法的效果，即 Adobe Acrobat DC 和 Tabby[1]。在示例中的表格应用这两种方法都无法正确提取表格内容。同时，不难发现在跨单元格的情况下，由于这些单元格很可能携带表头的信息，表格识别经常会出现问题。因此，表格抽取方法的性能仍有待提高，尤其是在复杂的情况下对于表格的提取和理解至关重要。\n图 1. 包含跨单元格的表格和使用现有方法恢复的表格结构示例\n1、经典表格识别数据集及表格识别方法回顾\n首先，作者回顾了以前发布的用于表格识别任务的数据集，然后介绍了表格识别的不同方法。\n1.1、数据集\n（1）Marmot[2]。Marmot 数据集由中英文两部分组成。中文网页是从方正阿帕比图书馆（Founder Apabi library）提供的超过 120 本不同学科领域的电子书中收集的，而英文网页则来自 Citeseer 网站。该数据集是基于 PDF 格式文件的，存储了所有文档布局的树结构，其中的叶子是字符、图像和路径，根是整个页面。内部节点包括文本行、段落、表格等。\n（2）UW3 和 UNLV 。UW3 数据集 [3] 是从 1600 页倾斜校正的英文文档中收集的，其中 120 页包含至少一个标记的表格区域。UNLV 数据集来自 2889 页扫描文档图像，其中 427 幅图像包括表格。\n（3）ICDAR 2013[4]。该数据集共包含 150 个表格：欧盟 27 个摘录中的 75 个表格，美国政府 40 个摘录中的 75 个表格，即总共 67 个 PDF 文档，共包含 238 页英文页面。\n（4）ICDAR 2019[5]。该数据集用于 ICDAR 2019 年表格检测和识别竞赛。整个数据集划分为训练部分和测试部分。训练数据集包含 600 个现代文档的图像及其表格区域的边框，以及 600 个文档的图像、表格区域和单元格区域的表格结构和边框。在测试数据集中，提供了 199 份档案文献和 240 份现代文献的图像和表格区域。此外，还包括 350 份档案文献的表格结构和单元区域。\n（5）PubTabNet[6]。PubTabNet 数据集包含 56.8 万张表格数据的图像，以及图像对应的 html 格式的注释。更具体地说，该数据集提供了表格结构和字符，但缺少边界框。\n（6）SciTSR[7]。SciTSR 是一个综合性的数据集，由 15000 个 PDF 格式的表格、表格区域的图像、它们对应的结构标签和每个单元格的边框组成。其中，12000 个文档用于训练，3000 个用于测试。同时，还提供了一个称为 SciTSR-COMP 的复杂表列表。\n（7）TableBank 表格库[8]。TableBank 是基于图像的表格检测和识别数据集。由于涉及两个任务，所以它由两个部分组成。对于表格检测任务，包含了表格区域的页面和边框的图像。对于表格结构识别任务，提供了表示行和列的排列以及表格单元类型的页面和 HTML 标记序列的图像。然而，由于这个数据集不涉及文本内容识别任务，因此不包含文本内容及其边界框。\n关于上述七个数据集的详细信息见表 1。可以看出，除 Marmot 中包含有部分中文表格外，其它数据集均为英文数据集。\n表 1. 用于表格识别的公共数据集\n1.2、表格识别方法\n表格识别被视为表格理解的一部分，通常包括两个步骤：\n1） 表格检测。也就是说，在这个步骤中，将文件中的某个部分标识为表格。\n2） 表格结构分解。这一步骤的任务是识别出原始表格的组件，通过对组件的堆叠恢复原始表格。例如，标题元素的正确标识、列和行的结构、数据单元的正确分配等等。\n现有的表格识别方法主要包括三大类：1）基于预定义布局的方法，2）基于启发式的方法，3） 基于统计或优化的方法。基于预定义布局的方法为可能的表格结构设计多个模板。如果文档的某些部分对应于某些模板，则它们被标识为表格。基于启发式的方法指定一组规则来进行决策，以便检测出满足特定条件的表。基于统计或优化的方法首先通过离线训练获得统计指标，然后将估计得到的参数用于实际表格识别。\n2、FinTab 介绍\n作者分析，现有的应用于表格识别任务中的数据集存在下述两个问题：1)从财务文档中提取表格信息的方法和标准数据集很少。2） 表格信息提取的来源缺乏多样性。本次发布的基准数据集 FinTab 旨在解决上述问题。在这个数据集中，总共收集了 19 个 PDF 文件和 1600 多个表格。具体文件分类见表 2。所有文件总计 3329 页，其中 2522 页包含表格。为了保证表格类型的多样性，除了表格的基本形式外，FinTab 中还包括了不同难度的特殊表格形式，如半规则表格、跨页表格、合并单元格表格、多行标题表格等。FinTab 中共有 119021 个单元格，合并单元 2859 个，占 2.4%。\n表 2. FinTab 的文档类型分类\nFinTab 包含各种类型的表。在这里，我们按难易程度简单介绍其中的一些。\n1） 基本单页表格。这是最基本的表格类型，它占用不到一个页面，并且不包括合并单元格。值得一提的是，本数据集中不仅提供了文本的基本事实和结构信息，而且还提供了表格的单位，因为大多数财务表格包含了相当多的数字。\n2） 包含合并单元格的表格。\n3） 跨页表格。当表格是跨页展开的，则需要将跨页表格合并。如果两页的页眉是重复的，则只需保留一页。页码和其他无用信息也应删除。另一个需要注意的任务难点是，如果一个单元格被两个页面分隔，则应该根据其语义将其合并为一个单元格。\n4） 表格线条不完整。在这种情况下，需要根据文本的位置、格式和含义智能地定位分界线。\n3、方法介绍\n本文提出了一种新的基于图神经网络（GCN）的表格结构识别算法 GFTE（a novel Graph-Neural-Network-based algorithm），GFTE 的完整工作流程如图 2。\n图 2. GFTE 完整工作流程\nGFTE 可以概括为以下步骤：（a） 构建基本事实，包括表格区域的图像、文本内容、文本位置和结构标签。（b） 基于单元格构造一个无向图 G=。（c）使用 GFTE 来预测相邻关系，包括垂直关系和水平关系。表 3 给出一个任务示例（表格文字已经转换为英文）。\n表 3. 本文中用来阐述方法的示例性表格的翻译版本\n首先，作者解释了任务的具体内容。在表格识别问题中，可以将表格中的每个单元格视为一个节点，节点与它的邻域可以理解为边。如果用 N 表示节点集，用 E_C 表示完全连通的边，则表格结构可以用一个完整的图来表示 G=，R_C = EC x {vertical， horizontal， unrelated}。我们可以把这个问题解释为：给定一组节点 N 及其特征，尽可能精确地预测节点对之间的关系 R_C。由于 R_C 描述每个节点（表格的单元）的互相关系，计算量颇大，文中只考虑了单元格周围相邻的格子，也就是利用 K - 最近邻（KNN）方法构造 R_C，它包含每个节点与其 K 个最近邻之间的关系。每个节点包含三种类型的信息，即文本内容、绝对位置和图像，如图 3 所示。然后，利用结构关系来建立基本的图结构，整个结构可以如图 4 所示。为了提高准确度，分别训练水平和垂直关系。对于水平关系，将每条边标记为（1: 在同一行中，或 0: 不在同一行中）。对于垂直关系，将每条边标记为（1: 在同一列中，或 0: 不在同一列中）（图 4）。\n图 3. 源数据格式的直观示例\n图 4. 基本图结构\n图 5 给出了基于图的卷积网络 GFTE 的结构。首先将绝对位置转换为相对位置，然后用这些位置来生成图。同时，将纯文本嵌入到预先定义的特征空间中，然后使用 LSTM 来获取语义特征。将位置特征和文本特征连接在一起，并将它们传送到"
  },
  {
    "title": "科学界论文高引第一人易主，Hinton、何恺明进总榜前五！-搜狐",
    "page_body": "Datawhale分享 \n统计：AD Scientific Index平台\n编辑：机器之心\n全世界、所有科学领域都算上，现在最热门的方向就是 AI 了。\n图灵奖得主 Yoshua Bengio，近日成为了有史以来被引用次数最多的科学家：他的总被引用量高达 973,655 次，近五年引用量达到 698,008 次。\n这项统计来自 AD Scientific Index，这是一个全球性的学术排名和分析平台，旨在评估和展示科学家、研究人员以及学术机构的科研表现和影响力。\n参与这次排名的共计 2,626,749 名科学家，分布在 221 个国家和地区，隶属 24,576 家机构。排名依据总引用量和近五年的引用指数进行排序。值得一提的是，这次排名不止 AI 领域，还包括医学等 13 个主要学科和 221 个学术细分学科。\n我们再回到 Bengio 的研究。从学术主页来看，Bengio 2014 年提出的 「生成对抗网络（Generative Adversarial Nets）」 引用量已突破 10 万次，甚至超过了他与 Yann LeCun 和 Geoffrey Hinton 合著的经典论文 「Deep Learning」，不过，后者的引用量同样也超过 10 万次。\n来源：https://scholar.google.com/citations?user=kukA0LcAAAAJ&hl=en\n排名第二的是 2024 诺奖得主、AI 领域先驱 Geoffrey Hinton，他的总被引用量为 95 万 +，近五年引用量为 57 万 +。\n来源：https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=JicYPdAAAAAJ\n位列第三、第四的研究者来自医疗领域：\n何恺明排名第五，单篇论文《Deep Residual Learning for Image Recognition》引用量超过 29 万次。这篇论文提出的 ResNet 成为现代深度学习的基础，几乎所有视觉模型都借鉴了残差思想。\n来源：https://scholar.google.com/citations?user=DhtAFkwAAAAJ&hl=en\n值得一提的是，今年 四月，据 Nature 统计 ResNet 是 21 世纪被引量最多论文，单篇 29 万次，经典论文可以说是当之无愧了。可参考「 」。\n在 top 10 名单中，我们也看到了 Ilya Sutskever 的身影，总引用量 67 万 +，排名第 7，单篇论文最高引用量 18 万 + 。\n来源：https://scholar.google.com/citations?user=x04W_mMAAAAJ&hl=en\n我们不难发现，这些高被引研究不仅在当时引发了学术界的广泛关注，更在随后的十几年里持续产生深远影响。\n完整排名列表请参考：\nhttps://www.adscientificindex.com/citation-ranking/"
  },
  {
    "title": "基于深度学习的图像检索算法：从理论到实践的全方位解析-稀土掘金",
    "page_body": "一、研究背景与核心价值\n传统图像检索面临的挑战：\n语义鸿沟问题 ：底层视觉特征与高层语义理解不匹配 计算效率瓶颈 ：海量图像数据下的实时检索性能不足 特征表达局限 ：传统手工特征难以捕捉复杂语义信息 精度与速度平衡 ：在保持高准确率的同时实现快速检索\n深度学习带来的技术突破：\n端到端特征学习 ：自动从原始像素中学习层次化特征表示 语义理解增强 ：通过深层网络捕捉图像的语义信息 哈希编码优化 ：将高维特征压缩为紧凑二进制码，提升检索效率 注意力机制引入 ：聚焦关键区域，提升特征判别能力\n二、核心技术原理深度解析\n2.1 卷积神经网络特征提取\nVGG16特征提取核心实现：\npython\n体验AI代码助手\n代码解读\n复制代码\nimport  torch import  torch.nn  as  nn import  torchvision.models  as  models class FeatureExtractor (nn.Module): def __init__ ( self ): super (FeatureExtractor, self).__init__() # 加载预训练VGG16模型         vgg16 = models.vgg16(pretrained= True ) # 移除最后的分类层，保留特征提取部分         self.features = nn.Sequential(* list (vgg16.children())[:- 1 ]) def forward ( self, x ): # 提取深度特征         features = self.features(x) # 全局平均池化，得到固定维度特征         features = torch.flatten(features,  1 ) return  features\n技术优势分析：\n层次化特征表示 ：从边缘纹理到语义概念的渐进式学习 迁移学习能力 ：利用ImageNet预训练权重加速收敛 特征泛化性强 ：在不同数据集上表现稳定\n2.2 注意力机制增强特征\nCBAM注意力模块实现：\npython\n体验AI代码助手\n代码解读\n复制代码\nclass CBAM (nn.Module): def __init__ ( self, channels, reduction= 16 ): super (CBAM, self).__init__() # 通道注意力         self.channel_attention = nn.Sequential(             nn.AdaptiveAvgPool2d( 1 ),             nn.Conv2d(channels, channels // reduction,  1 ),             nn.ReLU(inplace= True ),             nn.Conv2d(channels // reduction, channels,  1 ),             nn.Sigmoid()         ) # 空间注意力         self.spatial_attention = nn.Sequential(             nn.Conv2d( 2 ,  1 , kernel_size= 7 , padding= 3 ),             nn.Sigmoid()         ) def forward ( self, x ): # 通道注意力加权         ca = self.channel_attention(x)         x = x * ca # 空间注意力加权         avg_out = torch.mean(x, dim= 1 , keepdim= True )         max_out, _ = torch. max (x, dim= 1 , keepdim= True )         sa_input = torch.cat([avg_out, max_out], dim= 1 )         sa = self.spatial_attention(sa_input)         x = x * sa return  x\n注意力机制价值：\n特征选择能力 ：自动聚焦图像中的显著性区域 抗干扰性强 ：抑制背景噪声，提升特征判别性 可解释性增强 ：可视化注意力图理解模型决策过程\n2.3 深度哈希编码技术\n哈希编码生成模块：\npython\n体验AI代码助手\n代码解读\n复制代码\nclass DeepHashModel (nn.Module): def __init__ ( self, hash_bits= 64 ): super (DeepHashModel, self).__init__()         self.hash_bits = hash_bits # 特征提取主干网络         self.backbone = FeatureExtractor()         feature_dim =  512  *  7  *  7 # VGG16最终特征维度 # 哈希编码层         self.hash_layer = nn.Sequential(             nn.Linear(feature_dim,  512 ),             nn.BatchNorm1d( 512 ),             nn.ReLU(inplace= True ),             nn.Dropout( 0.5 ),             nn.Linear( 512 , hash_bits)         ) def forward ( self, x ):         features = self.backbone(x)         hash_code = self.hash_layer(features) # 二值化处理         binary_code = torch.sign(hash_code) return  hash_code, binary_code\n哈希技术优势：\n存储效率极高 ：将图像表示为紧凑二进制码 检索速度飞快 ：汉明距离计算效率远超欧氏距离 扩展性强 ：支持大规模图像数据库的快速检索\n三、系统架构设计与实现\n3.1 整体系统架构\npython\n体验AI代码助手\n代码解读\n复制代码\nclass ImageRetrievalSystem : def __init__ ( self, hash_bits= 64 , top_k= 10 ):         self.hash_bits = hash_bits         self.top_k = top_k         self.model = self._load_model()         self.database_codes = self._load_database() def _load_model ( self ): \"\"\"加载预训练的深度哈希模型\"\"\"         model = DeepHashModel(hash_bits=self.hash_bits)         checkpoint = torch.load( 'model_best.pth' )         model.load_state_dict(checkpoint[ 'model_state' ])         model. eval () return  model def _load_database ( self ): \"\"\"加载图像数据库的哈希编码\"\"\" return  np.load( 'database_hash_codes.npy' ) def query_image ( self, image_path ): \"\"\"图像检索主流程\"\"\" # 1. 图像预处理         processed_img = self._preprocess_image(image_path) # 2. 特征提取和哈希编码 with  torch.no_grad():             _, query_hash = self.model(processed_img) # 3. 相似度计算和排序         similarities = self._calculate_similarity(query_hash.numpy()) # 4. 返回Top-K结果         top_indices = np.argsort(similarities)[:self.top_k] return  top_indices, similarities[top_indices] def _calculate_similarity ( self, query_hash ): \"\"\"计算汉明距离相似度\"\"\"         hamming_dist = np. sum (self.database_codes != query_hash, axis= 1 ) # 将汉明距离转换为相似度分数         similarity =  1  - hamming_dist / self.hash_bits return  similarity\n3.2 前后端交互设计\nFlask后端API实现：\npython\n体验AI代码助手\n代码解读\n复制代码\nfrom  flask  import  Flask, request, jsonify, render_template import  numpy  as  np import  cv2 app = Flask(__name__) retrieval_system = ImageRetrievalSystem() @app.route( '/' ) def index (): return  render_template( 'index.html' ) @app.route( '/search' , methods=[ 'POST' ] ) def search_image (): if 'image' not in  request.files: return  jsonify({ 'error' :  'No image uploaded' })     image_file = request.files[ 'image' ]     top_k =  int (request.form.get( 'top_k' ,  5 )) # 保存上传图像     image_path =  f\"static/uploads/ {image_file.filename} \"     image_file.save(image_path) # 执行检索     indices, scores = retrieval_system.query_image(image_path) # 构建返回结果     results = [] for  idx, score  in zip (indices, scores):         results.append({ 'image_path' :  f'static/database/ {idx} .jpg' , 'similarity_score' :  float (score), 'rank' :  len (results) +  1         }) return  jsonify({ 'results' : results}) if  __name__ ==  '__main__' :     app.run(debug= True )\n四、实验分析与性能评估\n4.1 评估指标体系\n关键性能指标：\nmAP（平均精度均值） ：综合衡量检索精度 检索时间 ：单次查询响应时间 内存占用 ：哈希编码存储空间 Top-K准确率 ：前K个结果的命中率\n4.2 对比实验结果\n算法模型\nmAP@100\n检索时间(ms)\n内存占用(MB)\n传统SIFT 0.452 1250 280\nAlexNet+Hash 0.683 320 45\nVGG16+Hash 0.792 280 52\nResNet+CBAM 0.856 235 48\n结果分析要点：\n深度哈希显著提升 ：相比传统方法，mAP提升超过40% 注意力机制有效 ：CBAM模块带来约6%的性能增益 检索效率优化 ：哈希编码使检索速度提升5倍以上\n五、系统展示与用户体验\n5.1 界面设计理念\n极简主义 ：清晰的视觉层次和直观的操作流程 响应式布局 ：适配不同设备和屏幕尺寸 实时反馈 ：检索过程可视化，提升用户参与感\n5.2 核心交互流程\n图像上传 ：支持拖拽和点击两种上传方式 参数设置 ：灵活调整返回结果数量 结果展示 ：相似度分数和哈希码的直观显示 细节查看 ：鼠标悬停显示详细检索信息\n六、技术亮点与创新贡献\n6.1 核心技术突破\n多注意力融合 ：结合通道与空间注意力，提升特征判别力 端到端哈希学习 ：统一的深度网络实现特征提取和哈希编码 损失函数优化 ：设计针对性的损失函数减少量化误差\n6.2 工程实践价值\n即插即用架构 ：模块化设计便于扩展和维护 生产环境就绪 ：完整的异常处理和性能监控 文档完善 ：详细的API文档和部署指南\n七、应用前景与未来展望\n7.1 行业应用场景\n电商领域 ：商品图像搜索和推荐 医疗影像 ：病例图像检索和诊断辅助 安防监控 ：嫌疑人图像快速匹配 文化保护 ：文物图像数字化管理\n7.2 技术演进方向\n跨模态检索 ：支持文本-图像的多模态查询 增量学习 ：动态更新哈希库，适应数据变化 联邦学习 ：在保护隐私的前提下实现模型协同训练 边缘部署 ：轻量化模型适配移动端和嵌入式设备\n八、项目资源获取\n完整项目资料包包含：\n核心源码\n深度哈希模型实现（PyTorch） 前后端完整代码（Flask + jQuery） 数据处理和训练脚本\n预训练模型\n在多个数据集上验证的最佳模型权重 模型配置文件和超参数设置\n实验数据\n标准测试数据集 性能评估脚本和结果分析\n部署工具\nDocker容器化部署配置 环境依赖管理文件 性能监控和日志系统\n技术文档\n系统架构设计文档 API接口详细说明 二次开发指南\n如果本文对您的 深度学习研究 、 图像检索项目 或 毕业设计 有帮助，欢迎 点赞 + 收藏 + 关注 ，后续会分享更多\"深度学习实战\"与\"计算机视觉应用\"案例！\n 计算机毕业设计 \n 专栏目录 \n 计算机毕业论文设计 \n0 订阅\n·\n176 篇文章\n 订阅"
  },
  {
    "title": "论文笔记：Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling-掘金",
    "page_body": "导语\n当一个大模型面对给定的算力，是应该更多的用于预训练还是推理？本文尝试回答这个问题。\n会议：Arxiv 2024 链接： [2408.03314] Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters\n摘要\n本研究探讨了使大语言模型（LLM）在测试阶段利用更多计算能力以改善输出的关键步骤，关注推理时计算的扩展，特别是研究在固定但非平凡的推理计算条件下，LLM在应对具有挑战性的提示时的性能提升幅度。现有研究对各种测试时推理方法的扩展行为知之甚少，且大部分工作给出了多种策略的负面结果。本文分析了扩展测试时计算的两种主要机制：\n针对验证奖励模型（Verifier）进行搜索； 在测试时根据提示自适应更新模型的响应分布。\n研究发现，不同的测试时计算扩展方法在提示难度变化时效果差异显著。这一观察促使提出一种“计算最优”扩展策略，旨在根据每个提示有效分配测试时计算。采用这种计算最优策略后，测试时计算的扩展效率比best-of-N基线提高了4倍以上。此外，在对比计算量相同的评估中，发现对于一些较小基础模型能够达到较高成功率的问题，测试时计算甚至可以超越一个大14倍的模型。\n1 引言\n人类在面对困难问题时，往往会花更多时间思考，以可靠地提高决策质量。能否将这种能力赋予今天的大语言模型（LLM）？更具体地说， 面对具有挑战性的输入查询，我们能否让语言模型在测试阶段有效利用额外的计算，从而提高其响应的准确性？  理论上，通过在测试阶段应用额外的计算，LLM应该能够超越其训练时的表现。此外，这种测试时能力还有可能为智能体和推理任务开启新的方向。例如，如果可以在推理期间用额外的计算来平衡预训练模型的大小，那么将使得在较小的设备模型替代数据中心规模的LLM时得到应用。通过使用额外的推理计算来自动生成改进的模型输出，也为实现一种能减少人类监督的通用自我改进算法提供了途径。\n先前研究推理时计算的结果存在不一致。一方面，一些研究表明，当前的LLM能够利用测试时计算来改善输出，另一方面，也有研究显示这些方法在更复杂的任务（如数学推理）上的有效性极为有限，尽管推理问题通常需要对现有知识进行推断，而不是获取新知识。这些相互矛盾的发现促使本文对不同的测试时计算扩展方法进行系统分析。\n本文关注于了解扩展测试时计算的好处。可以说，扩展测试时计算最简单且研究最为广泛的方法是best-of-N抽样：从基础LLM“并行”抽样N个输出，并根据学习到的验证器或奖励模型选择得分最高的一个。然而，这并不是改善LLM的唯一方式。通过修改获取响应的提议分布（例如，要求基础模型“顺序”修正其原始响应）或通过改变验证器的使用方式（例如，训练一个基于过程的密集验证器并在此基础上进行搜索），测试时计算的扩展能力可以得到显著提升。\n为理解扩展测试时计算的好处，本文在具有挑战性的MATH基准上进行实验，使用特别微调的PaLM-2模型，以修正错误答案或使用基于过程的奖励模型（PRM）验证答案中的每个步骤。实验发现，特定测试时计算策略的有效性在很大程度上依赖于具体问题的性质和所使用的基础LLM。例如，在较简单的问题上，基础LLM已经能够生成合理的响应，允许模型通过预测一系列N个修正（即修改提议分布）来迭代优化其初始答案，可能比并行抽样N个独立响应更有效。另一方面，对于更复杂的问题，这些问题可能需要搜索多种高层次的解决方案，此时独立并行重新抽样新响应或对基于过程的奖励模型进行树搜索可能是更有效的测试时计算方式。这一发现强调了部署适应性“计算最优”策略的必要性，即根据提示选择利用测试时计算的具体方法，以最佳利用额外的计算资源。\n本文还展示了，从基础LLM的角度来看，问题难度的概念可以用来预测测试时计算的有效性，这使得能够根据提示实际实施这种“计算最优”策略。通过这种方式合理分配测试时计算，本文能够显著改善测试时计算的扩展效率，超越best-of-N基线的表现，同时仅使用约4倍更少的计算资源。\n使用改进的测试时计算扩展策略后，本文进一步探讨测试时计算在多大程度上可以有效替代额外的预训练。本文在一个较小模型与额外测试时计算的性能与一个14倍大模型的预训练进行FLOPs匹配比较。结果显示， 在简单和中等难度的问题上，甚至在某些困难问题上（取决于预训练和推理工作负载的具体条件），额外的测试时计算通常比扩展预训练更可取 。这一发现表明，在某些情况下，预训练较小的模型以较少的计算资源，然后应用测试时计算来改善模型输出，可能比单纯扩展预训练更有效。然而，在最具挑战性的问题上，作者观察到扩展测试时计算几乎没有好处。相反，对于这些问题，使用额外的预训练计算取得进展更为有效，这表明当前的测试时计算扩展方法可能并不能与扩展预训练一一对应。总体而言，即使采用相对简单的方法，扩展测试时计算已经显示出比扩展预训练更为可取的潜力，随着测试时策略的成熟，仍有更多改进可待实现。长期来看，这暗示着未来在预训练阶段花费更少的FLOPs，而在推理阶段花费更多的FLOPs。\n2 统一的测试时计算视角：提议者与验证者\n本文先统一测试时计算的使用方法，然后分析一些代表性的方法。首先，从在给定提示条件下自适应修改模型预测分布的角度来看待额外测试时计算的使用。理想情况下，测试时计算应当修改分布，从而生成比直接从LLM采样更好的输出。通常，有两个方法可以引入对LLM分布的修改：\n在输入层面：通过在给定提示中增加一组额外的标记，使LLM在此基础上获得修改后的分布； 在输出层面：通过从标准语言模型中采样多个候选输出并对这些候选进行处理。\n换句话说，可以直接修改LLM自身诱导的提议分布，以改善简单条件下的输出，或者使用一些后验验证器或评分器来进行输出修改。这一过程类似于从复杂目标分布进行马尔科夫链蒙特卡洛（MCMC）采样，但通过结合简单的提议分布和评分函数。 直接修改提议分布 和 使用验证器 的过程构成了本研究的两个独立维度。\n修改提议分布（Modifying the proposal distribution）\n改善提议分布的一种方法是通过受强化学习启发的微调方法（如STaR或ReSTEM）直接优化模型以适应特定推理任务。这些技术不利用任何额外的输入标记，而是专门微调模型，以诱导改进的提议分布。另一方面，自我批判（self-critique）等技术使模型能够在测试时通过指示其批判和修正自身输出，以迭代方式改善其提议分布。由于直接提示现成模型在测试时有效性不足，本文特别微调模型，使其在复杂推理环境中能够迭代修正答案。为此，利用了通过Best-of-N引导的优化方法，在策略数据上进行微调，以改善模型响应。\n优化验证器（Optimizing the verifier）\n在对提议分布和验证器的抽象中，验证器用于汇总或选择提议分布中的最佳答案。最经典的使用验证器的方法是best-of-N抽样，即采样N个完整解决方案，然后根据验证器选择最佳答案。然而，通过训练基于过程的验证器或过程奖励模型（PRM），这一方法可以进一步改进，该验证器能够对每个解决方案中的中间步骤的正确性进行预测，而不仅仅是最终答案。可以利用这些逐步预测在解决方案空间内进行树搜索，从而提供比简单best-of-N方法更高效和有效的搜索方式。\n3 如何最佳扩展测试时计算\n在统一了各种方法之后，本文希望了解如何最有效地利用测试时计算，以提高语言模型在特定提示上的表现。具体来说，作者想回答以下问题：\n在给定一个提示和一个测试时计算预算的情况下，如何有效利用测试时计算来解决问题 ？根据上述抽象，不同的方法可能在特定问题上效果不同。我们如何确定在给定提示上最有效的测试时计算利用方式？与简单使用更大预训练模型的效果相比，这种方法表现如何？\n在提议分布优化或对验证器进行搜索时，有多个不同的超参数可以调整，以确定测试时计算预算的分配方式。例如，当使用经过微调的修正模型作为提议分布，并使用ORM作为验证器时，可以选择将全部测试时计算预算用于从模型中并行生成N个独立样本，然后应用best-of-N方法；或者，可以使用修正模型顺序抽样N个修正，然后使用ORM选择序列中最佳的答案，或者在这两者之间寻求平衡。直观上，可能认为“较简单”的问题会更受益于修正，因为模型的初始样本更有可能接近正确答案，只需进一步优化。另一方面，对于具有挑战性的问题，可能需要探索多种高层次的解决策略，因此在这种情况下，独立并行采样可能更为可取。\n在验证器的情况下，也可以选择不同的搜索算法（如束搜索、前瞻搜索、best-of-N），每种算法可能根据所使用的验证器和提议分布的质量表现出不同的特性。对于更困难的问题，更复杂的搜索程序可能比简单的best-of-N或多数基线更有用。\n3.1 测试时计算优化扩展策略\n一般来说，人们希望为特定问题选择最佳的测试时计算预算分配。为此，对于任何使用测试时计算的方法（例如本文中的修正和对验证器的搜索，其他地方的各种方法），本文将“测试时计算优化扩展策略”定义为为特定提示在测试时实现最大性能收益而选择的超参数策略。形式上，定义Target(    ,     ,     )为模型在给定提示      q 下产生的自然语言输出标记的分布，使用测试时计算超参数           和计算预算      N 。本文的目标是选择超参数           ，以最大化给定问题的目标分布的准确性，将其形式化表示为：\nθ q , a ∗ ( q ) ∗ ( N ) = arg max θ ( E y ∼ T a r g e t ( θ , N , q ) [ 1 y = y ∗ ( q ) ] ) , θ q , a ∗ ( q ) ∗ ​ ( N ) = ar g θ max ​ ( E y ∼ T a r g e t ( θ , N , q ) ​ [ 1 y = y ∗ ( q ) ​ ] ) ,\n其中      ∗ (      ) y ∗ ( q ) 表示      q 的真实正确响应，           ,      ∗ (      ) ∗ (      )      q , y ∗ ( q ) ∗ ​ ( N ) 代表在计算预算      N 下针对问题      q 的测试时计算优化扩展策略。\n3.2 估计问题难度以实现计算优化扩展\n为了有效分析第2节中讨论的不同机制的测试时扩展特性（例如提议分布和验证器），将此优化策略           ,      ∗ (      ) ∗ (      )      q , y ∗ ( q ) ∗ ​ ( N ) 近似为给定提示的某个统计量的函数。该统计量用于估计给定提示的"
  },
  {
    "title": "AI Agents Masterclass评估框架：LLM代理性能测试方法论-CSDN博客",
    "page_body": "AI Agents Masterclass评估框架：LLM代理性能测试方法论\n【免费下载链接】ai-agents-masterclass Follow along with my AI Agents Masterclass videos! All of the code I create and use in this series on YouTube will be here for you to use and even build on top of!  项目地址: https://gitcode.com/GitHub_Trending/ai/ai-agents-masterclass \n引言：为什么需要专业的AI代理评估？\n在AI代理（AI Agent）快速发展的今天，开发者和企业面临一个关键挑战：如何准确评估和比较不同LLM代理的性能？传统的人工测试方法效率低下且主观性强，无法满足大规模、系统化的评估需求。AI Agents Masterclass项目提供的LLM代理评估框架正是为了解决这一痛点而生。\n本文将深入解析该评估框架的核心架构、测试方法论和最佳实践，帮助开发者构建科学、可重复的AI代理评估体系。\n评估框架核心架构\n系统架构概览\n核心组件详解\n1. 状态管理引擎\nclass GraphState(TypedDict):     \"\"\"表示图状态的数据结构\"\"\"     messages: Annotated[list[AnyMessage], add_messages] \n2. 模型路由系统\n框架支持智能模型路由，根据配置自动选择最优模型提供商：\nprovider_mapping = {     \"openai\": ChatOpenAI,     \"anthropic\": ChatAnthropic,      \"ollama\": ChatOllama,     \"llama\": ChatGroq }  model_mapping = {     \"gpt\": ChatOpenAI,     \"claude\": ChatAnthropic,     \"groq\": ChatGroq,      \"llama\": ChatGroq } \n评估指标体系\n核心性能指标\n指标类别\n具体指标\n测量方法\n重要性权重\n响应质量 任务完成准确率 人工标注+自动化验证 40%\n执行效率 平均响应时间 时间戳记录 25%\n工具使用 工具调用成功率 API调用日志分析 20%\n成本控制 Token消耗量 计费API统计 15%\n质量评估维度\n测试方法论\n1. 基准测试套件设计\n测试用例分类\ntest_categories = {     \"simple_queries\": \"简单信息查询任务\",     \"complex_reasoning\": \"复杂推理任务\",      \"tool_integration\": \"工具调用任务\",     \"multi_step\": \"多步骤工作流\",     \"edge_cases\": \"边界情况处理\" } \n测试数据构造原则\n真实性 ：使用真实业务场景数据 多样性 ：覆盖不同难度和类型任务 可重复性 ：确保测试结果一致性 可扩展性 ：支持新测试用例快速添加\n2. 自动化测试流程\n3. 结果分析与可视化\n框架内置丰富的可视化组件，支持：\n性能对比图表 ：多模型横向比较 时间序列分析 ：长期性能趋势监控 错误分布图 ：问题定位和根因分析 成本效益分析 ：ROI计算和优化建议\n实践指南：构建自定义评估体系\n环境配置\n# 克隆项目仓库 git clone https://gitcode.com/GitHub_Trending/ai/ai-agents-masterclass  # 安装依赖（使用正确的编码） cd llm-agent-evaluation-framework python -c \" import sys sys.stdout.reconfigure(encoding='utf-16') with open('requirements.txt', 'r', encoding='utf-16') as f:     content = f.read() print(content.encode('utf-8').decode('utf-8')) \" > requirements_utf8.txt  pip install -r requirements_utf8.txt \n自定义评估配置\n# config/evaluation_config.py EVALUATION_CONFIG = {     \"test_cases\": {         \"batch_size\": 50,         \"timeout_seconds\": 30,         \"retry_attempts\": 3     },     \"metrics\": {         \"accuracy_threshold\": 0.8,         \"max_response_time\": 5000,  # ms         \"cost_per_query_limit\": 0.02  # USD     },     \"reporting\": {         \"generate_detailed_reports\": True,         \"compare_with_baseline\": True,         \"export_format\": [\"json\", \"csv\", \"html\"]     } } \n扩展评估维度\ndef extend_evaluation_metrics(base_metrics):     \"\"\"扩展自定义评估指标\"\"\"     custom_metrics = {         \"context_understanding_score\": calculate_context_score,         \"tool_selection_accuracy\": measure_tool_accuracy,         \"error_recovery_rate\": compute_recovery_rate,         \"multilingual_capability\": assess_multilingual_performance     }     return {**base_metrics, **custom_metrics} \n最佳实践与优化策略\n1. 测试数据管理\n建立标准化的测试数据集 实现测试用例版本控制 定期更新和扩充测试场景\n2. 性能优化技巧\n# 优化模型调用策略 def optimize_model_selection(query_complexity, available_models):     \"\"\"智能模型选择算法\"\"\"     if query_complexity == \"simple\":         return available_models[\"fast\"]  # 选择响应快的模型     elif query_complexity == \"complex\":         return available_models[\"powerful\"]  # 选择能力强的模型     else:         return available_models[\"balanced\"]  # 选择平衡型模型 \n3. 成本控制方案\n策略类型\n具体措施\n预期效果\n缓存优化 实现响应缓存机制 减少30% API调用\n批量处理 支持请求批处理 降低40% Token消耗\n模型路由 智能选择性价比模型 节约25%成本\n监控告警 实时成本监控 及时发现问题\n结论与展望\nAI Agents Masterclass评估框架为LLM代理的性能测试提供了完整的解决方案。通过系统化的评估方法论、丰富的性能指标体系和灵活的扩展能力，开发者可以：\n科学评估 ：客观比较不同代理模型的性能表现 快速迭代 ：基于数据驱动的优化决策 成本控制 ：实现资源的最优配置 质量保障 ：确保生产环境的稳定性\n随着AI代理技术的不断发展，评估框架也需要持续演进。未来方向包括：\n支持更多模型提供商和工具集成 增强多模态能力评估 开发实时监控和预警系统 构建标准化的基准测试数据集\n通过采用专业的评估框架，企业和开发者可以更加自信地部署AI代理解决方案，充分发挥人工智能技术的商业价值。\n立即行动 ：开始使用AI Agents Masterclass评估框架，为您的AI代理项目建立科学的性能评估体系，确保技术投资的回报最大化。\n【免费下载链接】ai-agents-masterclass Follow along with my AI Agents Masterclass videos! All of the code I create and use in this series on YouTube will be here for you to use and even build on top of!  项目地址: https://gitcode.com/GitHub_Trending/ai/ai-agents-masterclass"
  },
  {
    "title": "【深度学习系列】2018年最火的移动端深度学习，在Android shell下运行PaddlePaddle教程",
    "page_body": "智聚11.11狂欢购 \n云服务器29元起\n领大额万元优惠券包\n查看详情\n千帆一体机\n搭载千帆大模型工具链平台，内置文心与精选开源大模型\n大模型\n文心大模型\n百度文心大模型4.5系列正式开源！\n千帆大模型平台支持开源模型API服务！\nERNIE X1.1\n在事实性、指令遵循、智能体等能力上均有显著提升\nERNIE X1 Turbo\n具备更长的思维链，更强的深度思考能力\nERNIE 4.5\n新一代原生多模态基础大模型\nERNIE Speed Pro 轻量级大模型\n适合作为基座模型精调，更好处理特定场景问题，性能极佳\n端到端语音语言大模型\n基于Cross-Attention跨模态语音大模型，体验超拟人对话\n大模型语音合成\n音色具备更高的自然度、丰富的情感表达等特点\n大模型声音复刻\n录制5秒音频，即可极速复刻音色\nIRAG 图像生成\n自研检索增强文生图技术，效果更优，低成本去AI味\nDeepSeek大模型\nDeepSeek-R1-0528\n复杂推理、长文本处理、代码生成能力全面提升\nDeepSeek-R1-671B\n杭州深度求索自研，基于大型强化学习训练的第一代理解模型\nDeepSeek-V3-671B\n深度求索自研（MoE）语言模型，位列主流开源榜单榜首\n大数据与原生应用\n大数据+AI\n千帆数据智能平台 DataBuilder\n一站式多模态数据管理、加工和分析应用平台\n向量数据库 VectorDB\n纯自研高性能、高性价比、生态丰富且即开即用\n数据可视化Sugar BI\n0代码，分钟级即可完成自助BI分析和可视化数据大屏\nElasticsearch检索分析服务\n提供多种开源全托管检索分析服务，低成本且安全可靠\n引言\n“ PaddlePaddle  是  2016  年  8  月底百度开源的深度学习平台，并且在短时间内迅速成为引发全球开发热度，并且成为 Github Pull Request  数量增速最高的开源深度学习平台。\n为了帮助大家更好的对paddlepaddle深入浅出的学习，我们将陆续推出关于 PaddlePaddle入门解读、 深度学习原理分析集锦、大师 手把手教你跑Demo、 优质Demo分享 等系列内容。请大家持续关注 ai.baidu.com新闻资讯 ，及 PaddlePaddle训练营 。 ”\n第三节\n2018年不容错过的最火的移动端深度学习，在Android shell下运行PaddlePaddle\n我们可以在 https://github.com/PaddlePaddle/Mobile/  找到关于PaddlePaddle应用于移动端的demo和例子。这篇文章以Android shell下运行图像分类模型为例子来讲述如何入门PaddlePaddle移动端。\n内容\nPaddlePaddle训练移动端的分类模型 对PaddlePaddle进行Android 交叉编译 Android shell 下运行分类模型\nPaddlePaddle训练移动端的分类模型\n在Android shell下运行PaddlePaddle 模型，我们要准备一个适用于一个移动端的分类模型。Repo 下提供了适用于移动端的 mobilenet 模型，我们用这个模型来对花卉进行分类。\n一，下载 mobilenet 配置文件\nwget https: //raw.githubusercontent.com/PaddlePaddle/Mobile/develop/models/standard_network/mobilenet.py\n二，下载pre-trained 模型参数文件\n 在百度云上下载在imagenet上预训练的mobilenet模型参数  imagenet_pretrained_mobilenet.tar.gz\n三，在imagenet模型上对flower102数据集进行微调（fine-tune）\n 拷贝以下代码，然后运行，会在每个epoch 后保存参数文件。可以点击 此处  下载已经训练的模型参数。\nimport  sys  import  gzip  from  paddle.trainer_config_helpers  import  *  import  paddle.v2  as  paddle  from  mobilenet  import  mobile_net  # batch 大小是40  BATCH =  40 def main () :  datadim =  3  *  224  *  224  classdim =  102 # 采用gpu训练并使用第一块卡  paddle.init(use_gpu= True , trainer_count= 1 , gpu_id= 0 ) momentum_optimizer = paddle.optimizer.Momentum( momentum= 0.9 , regularization=paddle.optimizer.L2Regularization(rate= 0.0005  * BATCH), learning_rate= 0.001  / BATCH, learning_rate_schedule= 'constant' ) out = mobile_net(datadim, classdim,  1.0 ) lbl = paddle.layer.data( name= \"label\" , type=paddle.data_type.integer_value(classdim)) cost = paddle.layer.classification_cost(input=out, label=lbl)  # Create parameters  parameters = paddle.parameters.create(cost)  # 加载imagenet 预训练的模型参数 with  gzip.open( 'imagenet_pretrained_mobilenet.tar.gz' ,  'r' )  as  f: fparameters = paddle.parameters.Parameters.from_tar(f)  for  param_name  in  fparameters.names():  if  param_name  in  parameters.names(): parameters.set(param_name, fparameters.get(param_name))  # End batch and end pass event handler def event_handler (event) : if  isinstance(event, paddle.event.EndIteration):  if  event.batch_id %  50  ==  0 :  print \"\\nPass %d, Batch %d, Cost %f, %s\"  % ( event.pass_id, event.batch_id, event.cost, event.metrics)  else : sys.stdout.write( '.' ) sys.stdout.flush()  if  isinstance(event, paddle.event.EndPass):  # save parameters with  gzip.open( 'pruning_mobilenet_params_pass_%d.tar.gz'  % event.pass_id,  'w' )  as  f: parameters.to_tar(f) result = trainer.test( reader=paddle.batch( paddle.dataset.flowers.test(), batch_size= 10 ), feeding={ 'image' :  0 ,  'label' :  1 })  print \"\\nTest with Pass %d, %s\"  % (event.pass_id, result.metrics)  # Create trainer  trainer = paddle.trainer.SGD( cost=cost, parameters=parameters, update_equation=momentum_optimizer) trainer.train( reader=paddle.batch( paddle.reader.shuffle( paddle.dataset.flowers.train(), buf_size= 50000 ), batch_size=BATCH), num_passes= 100 , event_handler=event_handler, feeding={ 'image' :  0 ,  'label' :  1 })  if  __name__ ==  '__main__' : main()\n经过微调我们的分类精度可以达到98% 左右。 现在我们有了一个 .py  文件，表示模型的配置文件， 还有一个 .tar.gz 文件，表示模型的参数文件， 这两个文件组成了唯一的一个模型。\nAndroid 交叉编译PaddlePaddle\n我们需要让PaddlePaddle运行在Android平台，需要在linux或者mac下编译出能在android或者ios平台下运行的PaddlePaddle库文件。这个过程为交叉编译。\nPaddle repo下提供了关于如何在 android平台下进行交叉编译PaddlePaddle :\n 在链接相关页面中提供了两种方式， 一种是通过docker的方式，一种基于自定义独立工具链编译方式， 这两种方式，我都进行了实验，个人比较倾向于自定义的方式，因为比较直接透明。具体的使用方式如下：\n一， 下载  Android NDK\nwget -q  https: / /dl.google.com/android /repository/android -ndk-r14b-linux-x86_64.zip unzip -q android-ndk-r14b-linux-x86_64.zip\n假设当前目录为  $CURRENT_DIR\n二， 自定义工具链（--install-dir 表示安装路径，根据自己的需求设置，假设安装路径为$TOOLCHAIN_PATH ）\n$CURRENT_DIR /android-ndk-r14b-linux-x86_64/build/tools/make-standalone-toolchain.sh \\ --arch=arm --platform=android-21 --install-dir= $TOOLCHAIN_PATH /v7_standalone_toolchain\n$TOOLCHAIN_PATH/v7_standalone_toolchain 目录下的内容为我们刚刚生成的工具链。\n三， 交叉编译PaddlePaddle\ngit  clone  https://github.com/PaddlePaddle/Paddle.git  cd  Paddle  # 建立docker 镜像  mkdir install mkdir build  cd  build cmake -DCMAKE_SYSTEM_NAME=Android \\ -DANDROID_STANDALONE_TOOLCHAIN= $TOOLCHAIN_PATH /v7_standalone_toolchain \\ -DANDROID_ABI=armeabi-v7a \\ -DANDROID_ARM_NEON=ON \\ -DANDROID_ARM_MODE=ON \\ -DUSE_EIGEN_FOR_BLAS=ON \\ -DCMAKE_INSTALL_PREFIX=./install \\ -DWITH_C_API=ON \\ -DWITH_SWIG_PY=OFF \\ -DANDROID_TOOLCHAIN=gcc \\ .. make -j `nproc` make install\n编译结束后，会在  install/lib  目录下生成动态库 libpaddle_capi_shared.so ， 这个动态库提供了模型程序调用PaddlePaddle的所有入口。\nAndroid shell 下运行分类模型\n一， 下载预测程序\n该程序功能是用来测试模型的运行速度，主要包括加载模型，随机化输入，多次进行模型前向运算并统计时间，然后输出模型的平均前向运行时间。\nwget https: //raw.githubusercontent.com/PaddlePaddle/Mobile/develop/benchmark/tool/C/inference.cc\n二， 将 libpaddle_capi_shared.so  copy至当前目录\n三， 编译预测脚本\nexport  PATH= $TOOLCHAIN_PATH /v7_standalone_toolchain/bin/: $PATH  arm-linux-androideabi-g++ inference.cc -L./ -lpaddle_capi_shared -o inference -pie -fPIE\n我们可以看到，目录中多了一个 inference  可运行的二进制文件。\n四，将之前提到的模型配置文件.py 和模型参数文件.tar.gz融合成一个文件\nfrom  paddle.utils.merge_model  import  merge_v2_model  # import your network configuration from  mobilenet  import  mobile_net net = mobile_net( 3 * 224 * 224 ,  102 ,  1.0 ) param_file =  './mobilenet_flowers102.tar.gz'  output_file =  './mobilenet.paddle'  merge_v2_model(net, param_file, output_file)\n五，安装adb\n linux 安装，查看 教程\n mac 安装，查看 教程\nadb 工具可以登陆android 手机的shell，让我们像使用linux shell一样来操作android。\n adb安装之后，我们使用数据线将android手机和电脑链接。\n六， Android shell下运行Paddle分类模型\nadb push inference libpaddle_capi_shared.so mobilenet.paddle /sdcard/test_mobilenet adb shell  cd  /sdcard/test_mobilenet  export  LD_LIBRARY_PATH=./ ./inference --merged_model ./mobilenet.paddle --input_size 150528\n其中 input_size  表示模型输入的大小，即3 * 224 * 224 等于 150528\n如果看到以下的log，说明程序运行成功：\n可以看到，paddle初始化的时间是1.78015ms， 加载模型的时间是113.749ms， 模型前向的时间是337.754ms."
  },
  {
    "title": "Dynamic study of virtual oscillator controlled inverter based distributed energy source",
    "page_body": "机译： 基于虚拟振荡器的逆变器分布式能源动态研究 \n摘要\n Application of virtual oscillator control for grid connected and islanded operation of inverter based distributed energy resource have been presented. A decentralized adaptive battery energy storage system(BESS) primary control scheme has been proposed which ensures stable operation of oscillator based microgrid control under grid connected and islanded operations. Photovoltaic and BESS have been used as distributed energy source. An application of virtual oscillator control for BESS application in a microgrid has been modeled and simulated in MATLAB/SIMULINK. Feeder data of CIGRE benchmark low voltage microgrid network is used for microgrid modeling. A new reactive power control scheme has also proposed.  \n机译： 提出了虚拟振荡器控制在逆变器分布式能源并网孤岛运行中的应用。提出了一种分布式自适应电池储能系统（BESS）的主控制方案，该方案可确保在并网和孤岛运行下，基于振荡器的微电网控制稳定运行。光伏和BESS已被用作分布式能源。在MATLAB / SIMULINK中已经对虚拟振荡器控制在微电网中BESS应用的应用进行了建模和仿真。将CIGRE基准低压微电网的馈线数据用于微电网建模。还提出了一种新的无功功率控制方案。  \n著录项\n来源\n《North American Power Symposium》 | 2015年 | 1-6 | 共6页\n会议地点\n作者\nHazra Puspal; Hadidi Ramtin; Makram Elham;\n作者单位\n会议组织 原文格式 PDF 正文语种 中图分类 关键词\nadaptive control; battery storage plants; decentralised control; distributed power generation; oscillators; power generation control; power grids; BESS; BESS application; BESS primary control scheme; CIGRE benchmark; MATLAB/SIMULINK; decentralized adaptive battery energy storage system; distributed energy source; islanded operation; microgrid; microgrid modeling; oscillator based microgrid control; photovoltaics; reactive power control scheme; virtual oscillator control; virtual oscillator controlled inverter; voltage microgrid network; Inverters; Microgrids; Oscillators; Photovoltaic systems; Power system dynamics; Voltage control; Energy Storage; Inverter Control; Microgrid; Oscillator;\n 机译：自适应控制;电池存储设备;分散控制;分布式发电;振荡器;发电控制;电网; BESS; BESS应用; BESS主控制方案; CIGRE基准; MATLAB / SIMULINK;分散式自适应电池储能系统;分布式能源;孤岛运行;微电网;微电网建模;基于振荡器的微电网控制;光伏发电;无功控制方案;虚拟振荡器控制;虚拟振荡器控制的逆变器;电压微电网;逆变器;微电网;振荡器;光伏系统;电力系统动力学;电压控制;储能;逆变器控制;微电网;振荡器; \n入库时间 2022-08-26 15:18:37\n[J] Liu Jia, Miura Yushi, Ise Toshifumi  Power Electronics, IEEE Transactions on . 2016 ,第5期\n机译：基于异步发电机的虚拟同步发电机与下垂控制的动态特性比较\n3.  Sliding-Mode Control of AC Voltages and Currents of Dispatchable Distributed Energy Resources in Master-Slave-Organized Inverter-Based Microgrids [J]  .  Mohammad B. Delghavi, Amirnaser Yazdani  Smart Grid, IEEE Transactions on . 2019 ,第1期\n机译：主从机逆变器微电网中交流电压和可分配分布式能源电流的滑模控制\n4.  Dynamic study of virtual oscillator controlled inverter based distributed energy source [C]  .  Hazra Puspal, Hadidi Ramtin, Makram Elham  North American Power Symposium . 2015\n机译：虚拟振荡器控制逆变器的分布式能源动态研究\n5.  Virtual Oscillator Controlled Inverters in a Microgrid Setting with Secondary Control and Energy Storage [D]   .  Bean, Celeste M. B.  2018\n机译：虚拟振荡器控制逆变器在带有次级控制和能量存储的微电网设置中\n6.  Research on the control strategy of distributed energy resources inverter based on improved virtual synchronous generator [O]   .  Changwei Gao, Xiaoming Liu, Hai Chen  -1\n机译：基于改进型虚拟同步发电机的分布式能源逆变器控制策略研究\n7.  Volume 2, Issue 3, Special issue on Recent Advances in Engineering Systems (Published Papers) Articles Transmit / Received Beamforming for Frequency Diverse Array with Symmetrical frequency offsets Shaddrack Yaw Nusenu Adv. Sci. Technol. Eng. Syst. J. 2(3), 1-6 (2017); View Description Detailed Analysis of Amplitude and Slope Diffraction Coefficients for knife-edge structure in S-UTD-CH Model Eray Arik, Mehmet Baris Tabakcioglu Adv. Sci. Technol. Eng. Syst. J. 2(3), 7-11 (2017); View Description Applications of Case Based Organizational Memory Supported by the PAbMM Architecture Martín, María de los Ángeles, Diván, Mario José Adv. Sci. Technol. Eng. Syst. J. 2(3), 12-23 (2017); View Description Low Probability of Interception Beampattern Using Frequency Diverse Array Antenna Shaddrack Yaw Nusenu Adv. Sci. Technol. Eng. Syst. J. 2(3), 24-29 (2017); View Description Zero Trust Cloud Networks using Transport Access Control and High Availability Optical Bypass Switching Casimer DeCusatis, Piradon Liengtiraphan, Anthony Sager Adv. Sci. Technol. Eng. Syst. J. 2(3), 30-35 (2017); View Description A Derived Metrics as a Measurement to Support Efficient Requirements Analysis and Release Management Indranil Nath Adv. Sci. Technol. Eng. Syst. J. 2(3), 36-40 (2017); View Description Feedback device of temperature sensation for a myoelectric prosthetic hand Yuki Ueda, Chiharu Ishii Adv. Sci. Technol. Eng. Syst. J. 2(3), 41-40 (2017); View Description Deep venous thrombus characterization: ultrasonography, elastography and scattering operator Thibaud Berthomier, Ali Mansour, Luc Bressollette, Frédéric Le Roy, Dominique Mottier Adv. Sci. Technol. Eng. Syst. J. 2(3), 48-59 (2017); View Description Improving customs’ border control by creating a reference database of cargo inspection X-ray images Selina Kolokytha, Alexander Flisch, Thomas Lüthi, Mathieu Plamondon, Adrian Schwaninger, Wicher Vasser, Diana Hardmeier, Marius Costin, Caroline Vienne, Frank Sukowski, Ulf Hassler, Irène Dorion, Najib Gadi, Serge Maitrejean, Abraham Marciano, Andrea Canonica, Eric Rochat, Ger Koomen, Micha Slegt Adv. Sci. Technol. Eng. Syst. J. 2(3), 60-66 (2017); View Description Aviation Navigation with Use of Polarimetric Technologies Arsen Klochan, Ali Al-Ammouri, Viktor Romanenko, Vladimir Tronko Adv. Sci. Technol. Eng. Syst. J. 2(3), 67-72 (2017); View Description Optimization of Multi-standard Transmitter Architecture Using Single-Double Conversion Technique Used for Rescue Operations Riadh Essaadali, Said Aliouane, Chokri Jebali and Ammar Kouki Adv. Sci. Technol. Eng. Syst. J. 2(3), 73-81 (2017); View Description Singular Integral Equations in Electromagnetic Waves Reflection Modeling A. S. Ilinskiy, T. N. Galishnikova Adv. Sci. Technol. Eng. Syst. J. 2(3), 82-87 (2017); View Description Methodology for Management of Information Security in Industrial Control Systems: A Proof of Concept aligned with Enterprise Objectives. Fabian Bustamante, Walter Fuertes, Paul Diaz, Theofilos Toulqueridis Adv. Sci. Technol. Eng. Syst. J. 2(3), 88-99 (2017); View Description Dependence-Based Segmentation Approach for Detecting Morpheme Boundaries Ahmed Khorsi, Abeer Alsheddi Adv. Sci. Technol. Eng. Syst. J. 2(3), 100-110 (2017); View Description Paper Improving Rule Based Stemmers to Solve Some Special Cases of Arabic Language Soufiane Farrah, Hanane El Manssouri, Ziyati Elhoussaine, Mohamed Ouzzif Adv. Sci. Technol. Eng. Syst. J. 2(3), 111-115 (2017); View Description Medical imbalanced data classification Sara Belarouci, Mohammed Amine Chikh Adv. Sci. Technol. Eng. Syst. J. 2(3), 116-124 (2017); View Description ADOxx Modelling Method Conceptualization Environment Nesat Efendioglu, Robert Woitsch, Wilfrid Utz, Damiano Falcioni Adv. Sci. Technol. Eng. Syst. J. 2(3), 125-136 (2017); View Description GPSR+Predict: An Enhancement for GPSR to Make Smart Routing Decision by Anticipating Movement of Vehicles in VANETs Zineb Squalli Houssaini, Imane Zaimi, Mohammed Oumsis, Saïd El Alaoui Ouatik Adv. Sci. Technol. Eng. Syst. J. 2(3), 137-146 (2017); View Description Optimal Synthesis of Universal Space Vector Digital Algorithm for Matrix Converters Adrian Popovici, Mircea Băbăiţă, Petru Papazian Adv. Sci. Technol. Eng. Syst. J. 2(3), 147-152 (2017); View Description Control design for axial flux permanent magnet synchronous motor which operates above the nominal speed Xuan Minh Tran, Nhu Hien Nguyen, Quoc Tuan Duong Adv. Sci. Technol. Eng. Syst. J. 2(3), 153-159 (2017); View Description A synchronizing second order sliding mode control applied to decentralized time delayed multi−agent robotic systems: Stability Proof Marwa Fathallah, Fatma Abdelhedi, Nabil Derbel Adv. Sci. Technol. Eng. Syst. J. 2(3), 160-170 (2017); View Description Fault Diagnosis and Tolerant Control Using Observer Banks Applied to Continuous Stirred Tank Reactor Martin F. Pico, Eduardo J. Adam Adv. Sci. Technol. Eng. Syst. J. 2(3), 171-181 (2017); View Description Development and Validation of a Heat Pump System Model Using Artificial Neural Network Nabil Nassif, Jordan Gooden Adv. Sci. Technol. Eng. Syst. J. 2(3), 182-185 (2017); View Description Assessment of the usefulness and appeal of stigma-stop by psychology students: a serious game designed to reduce the stigma of mental illness Adolfo J. Cangas, Noelia Navarro, Juan J. Ojeda, Diego Cangas, Jose A. Piedra, José Gallego Adv. Sci. Technol. Eng. Syst. J. 2(3), 186-190 (2017); View Description Kinect-Based Moving Human Tracking System with Obstacle Avoidance Abdel Mehsen Ahmad, Zouhair Bazzal, Hiba Al Youssef Adv. Sci. Technol. Eng. Syst. J. 2(3), 191-197 (2017); View Description A security approach based on honeypots: Protecting Online Social network from malicious profiles Fatna Elmendili, Nisrine Maqran, Younes El Bouzekri El Idrissi, Habiba Chaoui Adv. Sci. Technol. Eng. Syst. J. 2(3), 198-204 (2017); View Description Pulse Generator for Ultrasonic Piezoelectric Transducer Arrays Based on a Programmable System-on-Chip (PSoC) Pedro Acevedo, Martín Fuentes, Joel Durán, Mónica Vázquez, Carlos Díaz Adv. Sci. Technol. Eng. Syst. J. 2(3), 205-209 (2017); View Description Enabling Toy Vehicles Interaction With Visible Light Communication (VLC) M. A. Ilyas, M. B. Othman, S. M. Shah, Mas Fawzi Adv. Sci. Technol. Eng. Syst. J. 2(3), 210-216 (2017); View Description Analysis of Fractional-Order 2xn RLC Networks by Transmission Matrices Mahmut Ün, Manolya Ün Adv. Sci. Technol. Eng. Syst. J. 2(3), 217-220 (2017); View Description Fire extinguishing system in large underground garages Ivan Antonov, Rositsa Velichkova, Svetlin Antonov, Kamen Grozdanov, Milka Uzunova, Ikram El Abbassi Adv. Sci. Technol. Eng. Syst. J. 2(3), 221-226 (2017); View Description Directional Antenna Modulation Technique using A Two-Element Frequency Diverse Array Shaddrack Yaw Nusenu Adv. Sci. Technol. Eng. S"
  },
  {
    "title": "英文原版-动态预期和完成多跳推理稀疏知识图.pdf-原创力文档",
    "page_body": "： 大小 ： 947.24 KB 字数 ： 约5万字 下载次数 ： 仅上传者可见 收藏次数 ： 0 需要金币 ： *** 金币  (10金币=人民币1元)\n 英文文献学习素材 声明：素材来源于互联网公共免费领域，由上传者搜集、整理、编译，旨在帮助科 研领域人士扩展学习途径，丰富科研素材。 中文摘要 动态预期和完成多跳推理稀疏知识图 近年来，多跳推理已被广泛研究，以寻求有效且可解释的知识图谱 （KG）完成方法。大多数先前的推理方法是为密集KG 设计的，其中包 含足够的实体之间的路径，但无法在稀疏KG 上很好地工作，这些KG 仅包含稀疏路径进行推理。一方面，稀疏KG 包含的信息较少，使模型 难以选择正确的路径。另一方面，缺乏证据路径到目标实体也使推理 过程变得困难。为了解决这些问题，我们提出了一种名为DacKGR 的多 跳推理模型，用于稀疏 KG，通过应用新颖的动态预期和完成策略：（1） 预期策略利用基于嵌入的模型的潜在预测，使我们的模型在稀疏KG 上 执行更多潜在路径搜索。（2）基于预期信息，完成策略在路径搜索期 间动态添加边作为附加操作，进一步缓解KG 的稀疏性问题。在从 Freebase，NELL 和Wikidata 采样的五个数据集上的实验结果表明，我 们的方法优于最先进的基线。我们的代码和数据集可以从 THU-KEGDacKGR 获得。 Dataset #Ent #Rel #Fact #degree sion during the reasoning process. In sparse KGs, mean median many entities only have few relations, which limits FB15K-237 14,505 237 272,115 19.74 14 the choice spaces of the agent. Our completion WN18RR 40,945 11 86,835 2.19 2 NELL23K 22,925 200 35,358 2.21 1 strategy thus dynamically adds some additional WD-singer 10,282 135 20,508 2.35 2 relations (e.g., red dashed arrows in Figure 1) ac- cording to the state information of the current entity Table 1: The statistics of some benchmark KG datasets. during searching reasoning paths. After that, for #degree is the outgoing degree of every entity that can the current entity and an additional relation , we indicate the sparsity level. use a pre-trained embedding-based model to pre- dict tail entity . Then, the additional relation and the predicted tail entity will form a potential contain less information, which makes it difﬁcult action and be added to the action space of the for the agent to choose the correct search direction. current entity for path expansion. (2) Missing paths. In sparse KGs, some entity We conduct experiments on ﬁve datasets sam- pairs do not have enough paths between them as pled from Freebase, NELL and Wikidata. The reasoning evidence, which makes it difﬁcult for the results show that our model DacKGR outperforms agent to carry out the reasoning process. As shown previous multi-hop reasoning models, which veri- in the lower part of Figure 1, there is no evidential ﬁes the effectiveness of our model. path between Mark Twain and English since the relation publish area is missing. From Table 1 we 2 Problem Formulation can learn that some sampled KG datasets are actu- ally sparse. Besides, some domain-speciﬁc KGs In this section, we ﬁrst introduce some symbols (e.g., WD-singer) do not have abundant knowledge and concepts related to normal multi-hop reason- and also face the problem of sparsity. ing, and then formally deﬁne the task of multi-hop As the performance of most existing multi-hop reasoning over sparse KGs. reasoning methods drops signiﬁcantly on sparse Knowledge graph can be formulated as KGs, some preliminary efforts, such as CPL (Fu , where and denote entity set et al., 2019), explore to introduce additional text and relation set respectively. information to ease the sparsity of KGs. Although is triple set, where and are head these explorations have achieved promising results, and tail entities respectively, and is the relation they are still limited to those speciﬁc KGs whose between them. For every KG, we can use the aver- entities have additional text information. Thus, age out-degree of each entity (node) to deﬁne reasoning over sparse KGs is still an important but its sparsity. Speciﬁcally, if of a KG is larger not fully resolved problem, and requires a more than a threshold, we can say it is a dense or normal generalized approach to this problem. KG, otherwise, it is a sparse KG. Given a graph and a triple query , In this paper, we propose a multi-hop reasoning model named DacKGR, along with two dynamic where is the source entity and is the query re- strategies to solve the two problems mentioned lation, multi-hop reasoning for knowledge graphs aims to predict the tail entity for . above: Dynamic Anticipation makes use of the limited Different from previous KG embedding tasks, information in a sparse KG to anticipate potential multi-hop reasoning also gives a supporting path over targets before the reasoning with multi-hop reasoning models, embedding- as evidence. As mentioned above, we mainly based models are robust to sparse KGs, because focus on the multi-hop reasoning task over sparse they depend on every single triple rather than paths KGs in this paper. in KG. To this end, our anticipation strategy injects 3 Methodology the pre-trained embedding-based model’s predic- tions as anticipation information into the states of In this section, we ﬁrst introduce the whole rein- reinforcement learning. This information can guide forcement learning framework for multi-hop rea- the agent to avoid aimlessly searching paths. soning, and then detail our two strategies designed Dynamic Completion temporarily expands the for the sparse KGs, i.e., dynamic anticipation and part of a KG to enrich the options of path expan- dynamic completion. The former strategy intro- different states. where is prediction information given by KGE We represent entities and relations in as vec- models. In this paper, we use the following three tors in a semantic space, and then the action at strategies to generate : (1) Sample strategy. We the step can be represented as , where sample an entity based on probability distribution and are the vectors of and respectively. As we and denote its vector as . (2) Top-one strategy. mentioned in Section 3.1, we use an LSTM to store We select the entity with the highest probability in the historical path information. Speciﬁcally, the . (3) Average strategy. We take the weighted av- representation of each action selected by the agent erage of the vectors of all entities according to the will be fed into the LSTM to generate historical probability distribution as the prediction infor- path information so far, mation . In experiments, we choose the strategy that performs best on the valid set. LSTM (1) 3.4 Dynamic Completion The representation of the -th state In sparse KGs, there are often insufﬁcient eviden- can be formulated as tial paths between head and tail entities, so that the performance of multi-hop reasoning models will (2) drop signiﬁcantly. In order to solve the above problems, we propose After that, we represent the action space by stack- a strategy named dynamic completion to dynami- ing all actions in as , where is cally augment the action space of each entity during the dimension of the entity and relation vector. The reasoning process. Speciﬁcally, for the current state policy network is deﬁned as, , its candidate set of additional actions can be de- ﬁned as ReLU (3) . We need to select some actions with the high- where is the softmax operator, and are est probability from as additional actions, where two linear neural networks, and is the the probability can be deﬁned as: probability distribution over all actions in . (5) 3.3 Dynamic Anticipation However, the candidate set is too large, it will be As reported in previous work (Das et al., 2018; time-consuming to calculate the probability of all Lin et al., 2018), although the KGE models are actions in , so we adopt an approximate pruning not interpretable, they can achieve better results strategy. Speciﬁcally, We ﬁrst select some relations than the multi-hop reasoning models on most KGs. with the highest probability using , and then This phenomenon is more obvious on the sparse select entities with the highest probability for these KG (refer to experimental results in Table 3) since relations using . KGE models are more robust as they do not rely For the current state , we calculate the attention on the connectivity of the KGs. value over all relations as , Inspired by the above phenomenon, we pro- pose a new strategy named dynamic anticipation, SoftmaxMLP (6) which introduces the prediction information of the embedding-based models into the multi-hop rea- We deﬁne a parameter to control the proportion soning models to guide the model learning. Specif- of actions that need to be added. Besides, we also ically, for a triple query , we use the pre- have a parameter which represents the maxi- trained KGE models to get the probability vector of mum number of additional actions. Therefore, the all entities being the tail entity. Formally, the proba- number of additional actions can be deﬁned as, bility vector can be formulated as , where the value of the -th dimension of represents the (7) probability that is the correct tail entity. For the dynamic anticipation strategy, we change where is the action space size of the current state. the state representation in Equation 2 to: After we have the attention vector , we select top relations with the largest attention values in to (4) form a new relation set . Dataset #Ent #Rel #Fact #degree 4 Experiments mean median FB15K-237-10% 11,512 237 60,985 5.8 4 4.1 Datasets FB15K-237-20% 13,166 237 91,162 7.5 5 FB15K-237-50% 14,149 237 173,830 13.0 13 In this paper, we use ﬁve datasets sampled from NELL23K 22,925 200 35,358 2.21 1 WD-singer 10,282 135 20,508 2.35 2 Freebase (Bollacker et al., 2008), NELL (Carlson ˇ ´ ¨ et al., 2010) and Wikidata (Vrandecic and Krotzsch, Table 2: Statistics of ﬁve datasets in experiments. 2014) for experiments. Speciﬁcally, in order to study the performance of our method on KGs with For every relation and the current entity different degrees of sparsity, we constructed three , we use the pre-trained KGE models to predict datasets based on FB15K-237 (Toutanova et al., the probability distribution of the tail entity for 2015), i.e., FB15K-237-10%, FB15K-237-20% and FB15K-237-50%. These three datasets ran- triple query as . We only keep the entities with the highest probability, which domly retain 10%, 20% and 50% triples of FB15K- 237 respectively. form additional actions In addition, we also construct two datasets for triple query . Finally, all additional ac- tions make up the additional action space for NELL23K and WD-singer from NELL and Wiki- . Here, is a parameter, and can be calculated data, where WD-singer is a dat"
  },
  {
    "title": "从TPC-H分析论文学习优化器的挑战与应对思路",
    "page_body": "导读\n学习论文 [tpctc 2013]TPC-H Analyzed Hidden Messages and Lessons Learned from an Influential Benchmark ，收获很大。论文更像是查询优化器核心算法的骨架，虽然没有深入描述某个技术，但以TPC-H的query为例，系统且深入的分析了复杂query中提升性能的关键点，并给出了对应的优化思路。\n在个别挑战上，根据收集到的文献、源码，给出了TiDB、OceanBase、Cockroach、PolarDB、Oracle的一些实现方法，不足和错误之处，还请大家指正。\n先学习了 henry liang本论文解读 （写的很棒），然后通读原文，这样能够提升学习效率，并且也了解了PolarDB的一些实现方法。部分内容直接引用 henry liang本论文解读 ，同时，对于一些相关的点，也直接引用了一些技术博客，在此对诸位作者表示感谢！\nTPC-H 介绍\nTPC-H可以说是世界上最为流行的OLAP workload的benchmark程序，是评估OLAP数据库最常见的工具。\n针对query的处理性能方面，TPCH的测试中主要关注两个指标：\nPower 单并发测试，单线程执行22条Query+ RF（INSERT + DELETE） Throughput多并发测试，N个查询线程+ 1个insert/delete 线程\n而综合的打分是\nTPC-H主要模拟了供应商和采购商之间的交易行为，其中包含针对8张表的22条分析型查询。\nREGION 区域表,主键为r_regionkey，固定有5个地区 NATION 国家表主键为n_nationkey，有25个国家，这是固定值 SUPPLIER 供应商表，主键为s_suppkey，取值范围1~SF*10000，和partsupp、customer、nation关联 PART 零部件表，主键为p_partkey，取值范围1~SF*200000，与partsupp关联 PARTSUPP 零部件供应表，主键为ps_partkey、ps_suppkey，与part、supplier、lineitem关联 CUSTOMER 客户表，主键为c_custkey，取值范围1~SF*150000，与orders表关联 ORDERS 订单表，主键为o_orderkey，取值范围1~SF*1500000与lineitem表关联 LINEITEM 订单明细表，主键为l_orderkey,l_linenumber，这是数据量最大的一张表\n各表的结构、数据量和相互之间的关系如下图，其中表名后面的括号内是该表的列名前缀，箭头指向为表之间一对多关系的方向，表名下方的数字或者公式是表的基数（行数）。 LINEITEM表中的基数是近似值，基数中的sf是比例因子，用于生成不同数据规模的数据集，sf必须从下列固定值中选择：1/10/30/100/1000/3000/10000，相当于1GB/10GB/30GB/100GB/1TB/3TB/1PB。\nlineitem的l_orderkey跟orders表的o_orderkey是一一对应关系，订单上的商品都在lineitem表中，每个订单(l_orderkey)有1-7(l_linenumber)种商品，两张表数据量是1:(1-7)。\norders表的o_custkey信息都在customer表中，但不是一一对应关系，也就是说订单上的所有消费者信息都在customer表中，但不是所有消费者都购买了商品，大概2/3的消费者有订单。\npart和partsupp中的partkey是一一对应关系，每个零件都有4个供应商，两张表数据量是1:4。\nsupplier和partsupp中的suppkey是一一对应关系，每个供应商供应80种零件，两张表数据量是1:80。\nlineitem的partkey和suppkey都在partsupp中。\n每个region有5个nation，每个nation中，supplier和customer的比例大概为1:15。\n优化器技术挑战点\n论文从算法方面将查询优化器技术挑战点分为6大类28小类，如下图，对每一类挑战点都分析了原因，并给出了指导思路。\nCP1 Aggregation Performance\nTPCH的所有查询中都会出现agg，因此groupby和agg的性能非常重要。\nCP1.1 Ordered Aggregation\n聚合（Agg）由两种主要的算子HashAgg和SortAgg实现。 \nHashAgg\nHashAgg利用哈希表实现聚合：\n根据输入行的分组列的值，通过Hash找到对应的分组。 按照指定的聚合函数，对该行进行聚合计算。 重复以上步骤直到处理完所有的输入行，最后输出聚合结果。\nSortAgg\nSortAgg在输入数据已按分组列排序的情况，对各个分组依次完成聚合。\n保证输入按指定的分组列排序（例如，可能会看到 MergeSort 或 MemSort）。 逐行读入输入数据，如果分组与当前分组相同，则对其进行聚合计算。 如果分组与当前分组不同，则输出当前分组上的聚合结果。\nHashAgg vs SortAgg\nHashAgg\nHashAgg因采用了hash算法，时间复杂度为常数，效率佳 HashAgg group by的columns数量过多的话，HashAgg的性能将劣化 哈希表的大小超过各种cpu cache后，cache 和 TLB的频繁miss，会比较大的影响lookup性能。 group key进一步增多，整个哈希表无法写入内存，这时就需要spilling hash aggregation，即 Aggregation 期间发现 Hash Table 用的内存太多了，就给它排个序落盘。然后开一个新的 Hash Table 接着跑聚合，快跑满了的时候，同样给它排个序落盘。最后把仍在内存里的哈希表也排序一把，把内存中的哈希表 + 磁盘上的多个有序文件，跑一次多路合并，流式地返回多路合并的结果就可以了。而多路归并需要的内存是非常非常低的。\nSortAgg\nSortAgg 每次只要处理一个分组，内存消耗很小\n总的来说，大部分场景做聚合的时候都倾向于选择HashAgg，只要当以下场景下才适合选择SortAgg做聚合：\n数据比较多，内存严重不足。 聚合算子的输入已经按照Group By 列做好排序，这样做SortAgg就不需要额外排序，执行效率会更高。 当数据有严重倾斜，导致HashAgg执行效率不高，优先使用SortAgg\n在实现中可以具体采用hashagg还是sortagg，或混合使用，具体与可用的硬件资源 + query本身特性相关。与此相关，查询优化器必须推断正确的中间结果基数，这对于大多数TPC-H查询构造来说相对简单，但对于group-by表达式来说具有挑战性。\nCP1.2 Interesting Orders\nInteresting Orders是根据operator的特性，自下而上传递的，主要来源有两种：\n通过clustered index扫描产生的key order，被后续的算子保留从而传递到上层 基于sort的算子执行产生的新的order （比如hash join的probe侧有序、nested loop join的外表有序，join结果也是有序的）\n各级算子会自上而下传递其required Orders，如果下级算子的Interesting Orders能够满足需求，则无序增加sort算子，否则需要内置sort。关于排序优化的进一步优化算法，可以参见Fundamental techniques for order optimization。\nCP1.3 Small Group-By Keys\nQ1：价格统计报告查询\nQ1语句是查询lineItems的一个定价总结报告。在单个表lineitem上查询某个时间段内，对已经付款的、已经运送的等各类商品进行统计，包括业务量的计费、发货、折扣、税、平均价格等信息。\nQ1语句的特点是：带有分组、排序、聚集操作并存的单表查询操作。这个查询会导致表上的数据有95%到97%行被读取到。\nQ1的查询语句如下：\nselect      l_returnflag, //返回标志     l_linestatus,      sum(l_quantity) as sum_qty, //总的数量     sum(l_extendedprice) as sum_base_price, //聚集函数操作     sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,      sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,      avg(l_quantity) as avg_qty,      avg(l_extendedprice) as avg_price,      avg(l_discount) as avg_disc,      count(*) as count_order //每个分组所包含的行数 from      lineitem where      l_shipdate <= date'1998-12-01' - interval '90' day //时间段是随机生成的 group by //分组操作     l_returnflag,      l_linestatus order by //排序操作     l_returnflag,      l_linestatus;\n在Q1中，执行了8个agg，其中1个count，4个sums和3个avg，group by keys是l_returnflag（VARCHAR 1）、l_linestatus（VARCHAR 1），其结果仅有4中组合，且可以用int型表示。这种场景可以对group by进行优化。\n即，对于group by表达式，如果group by columns对应的值都是整型，且范围很小，可以使用数组代替hash用于agg计算，连续数组cache locality要好很多，可以大幅提升性能，但这有一个基本前提：需要能较为准确的估算group key NDV。\n这里的挑战是能够对group key NDV进行精确统计。\nCP1.4 Dependent Group-By Keys\nQ10: 货运存在问题的查询\nQ10语句是查询每个国家在某时刻起的三个月内货运存在问题的客户和造成的损失。\nQ10语句的特点是：带有分组、排序、聚集操作并存的多表连接查询操作。查询语句没有从语法上限制返回多少条元组，但是TPC-H标准规定，查询结果只返回前10行（通常依赖于应用程序实现）。\nQ10的查询语句如下：\nselect     c_custkey, c_name, //客户信息     sum(l_extendedprice * (1 - l_discount)) as revenue, //收入损失     c_acctbal,     n_name, c_address, c_phone, c_comment //国家、地址、电话、意见信息等 from     customer, orders, lineitem, nation where     c_custkey = o_custkey     and l_orderkey = o_orderkey     and o_orderdate >= date '[DATE]' // DATE是位于1993年一月到1994年十二月中任一月的一号     and o_orderdate < date '[DATE]' + interval '3' month //3个月内     and l_returnflag = 'R' //货物被回退     and c_nationkey = n_nationkey group by     c_custkey,     c_name,     c_acctbal,     c_phone,     n_name,     c_address,     c_comment order by     revenue desc;\nQ10中group by keys较多，包括c_custkey等7个，Q10需要访问的数据量非常大，因为涉及了ORDERS和LINEITEM一年的数据，且与CUSTOMER join。c_custkey是CUSTOMER表的主键，因CUSTOMER表的c_comment, c_address, n_name, c_phone, c_acctbal,c_name函数依赖于c_custkey。因此，agg算子应该能够从键匹配中排除某些group-by属性，这样可以减少cpu消耗，也节省了内存。类似的函数依赖推导还有：\nQ10中group by keys较多，包括c_custkey等7个，Q10需要访问的数据量非常大，因为涉及了ORDERS和LINEITEM一年的数据，且与CUSTOMER join。c_custkey是CUSTOMER表的主键，因CUSTOMER表的c_comment, c_address, n_name, c_phone, c_acctbal,c_name函数依赖于c_custkey。因此，agg算子应该能够从键匹配中排除某些group-by属性，这样可以减少cpu消耗，也节省了内存。类似的函数依赖推导还有：\n上图中，#o表示order表的主键，依次类推。\n上述表明，优化器中函数依赖的基础设施非常重要!\n目前，CockroachDB、TiDB基于论文Exploiting Functional Dependence in Query Optimization，实现了FD框架，关于FD算法，可以参见如下内容\n[University of Waterloo 2000] Exploiting Functional Dependence in Query Optimization--学习笔记上-基本认知\n[University of Waterloo 2000] Exploiting Functional Dependence in Query Optimization--学习笔记中-数学定义\n[University of Waterloo 2000] Exploiting Functional Dependence in Query Optimization--学习笔记下-函数依赖生成算法\nCP2 Join Performance\nCP2.1 Large Joins\n这里是指数据量较大的join，常见的join算法有hash-based/index-based, index-based可能会有二次回表的开销，引发较多随机IO，但如果数据都在内存就还好。\nTPCH中最大的两个表Order + Lineitem表的join，可以通过两种方式来调优\n通过cluster index，在NL join时，增加一些数据的Locality 通过table partitioning，并发做local join，在MPP系统中"
  },
  {
    "title": "阿里通义重磅发布 Qwen3-Max-Preview：1 万亿参数旗舰模型来了！今日头条",
    "page_body": "在大模型竞争白热化的 2025 年，阿里巴巴通义千问团队正式发布了全新旗舰模型 ——  Qwen3-Max-Preview 。作为  Qwen3  系列的巅峰之作，这一版本的参数量直接突破  1 万亿 （ 1T ） ，在推理、复杂指令跟随、长文本处理和多语言支持等方面全面升级，成为全球范围内瞩目的  AI  新里程碑。\nQwen3-Max-Preview是什么？\nQwen3-Max-Preview  是阿里巴巴通义千问团队推出的最新大语言模型，也是迄今为止  Qwen  系列中规模最大的模型。\n参数规模 ：超过  1T ，刷新阿里大模型记录； 语言能力 ：支持 100+ 种语言，中英文理解与生成尤其出色； 推理与知识 ：在数学、编程、科学推理中表现突出，减少幻觉，回答更可靠； 长文本处理 ：支持最高  256K tokens  的上下文窗口，能够分析和生成超长文档； 使用方式 ：用户可通过  Qwen Chat  网页免费体验 ，也能在  阿里云百炼平台  API  调用，企业和开发者可灵活接入。\n这是一个覆盖写作、编程、翻译、问答等全场景的旗舰级  AI  模型。\nQwen Chat 网页版\n核心功能亮点\n语言理解与生成 ：支持跨语言对话与写作，提升中英文质量。 强大推理能力 ：在数学、逻辑推理、编程任务中表现亮眼。 长文本处理 ： 256K  上下文窗口，轻松处理论文、长文档与复杂对话。 复杂指令执行 ：能准确理解并遵循复杂的用户指令，减少出错。 翻译与常识优化 ：跨语言交流更流畅，常识判断更精准。 企业级部署 ：通过阿里云百炼  API ，可扩展到生产环境与大规模应用。\n技术原理\n超大规模训练 ：基于  1 万亿 参数架构，结合海量数据进行优化。 上下文扩展 ：支持超长输入输出，并引入缓存机制，提升效率。 指令增强训练 ：通过数据增强和特定方法优化，提升复杂任务执行力。 检索增强生成（ RAG ）优化 ：结合外部信息，生成内容更准确。 思考模式（ Thinking Mode ） ：可在复杂任务中“分步思考”，再输出答案。\n性能与数据\nQwen3-Max-Preview  在多项权威  benchmark  上表现优异：\nArena-Hard v2 ：复杂挑战任务中排名领先； AIME25 ：数学推理成绩达到  80.6 分 ； LiveBench ：综合能力拿下高分，全面超越  Claude-Opus 4 、 DeepSeek-V3.1  等国际强者。\n这意味着它不仅能写作、聊天，更能在高难度推理与工具调用中展现硬核实力。\n计费模式\n阿里云百炼平台提供了阶梯式按输入长度计费：\n输入 Token 数\n输入价 (元/千token)\n输出价 (元/千token)\n隐式缓存 (元/千token)\n0-32K\n0.006\n0.024\n0.0012\n32K-128K\n0.01\n0.04\n0.002\n128K-256K\n0.015\n0.06\n0.003\n这种模式让个人开发者到企业级用户都能按需选择，灵活控制成本。\n应用场景\n智能客服 ：自动化客户支持，实时解决问题。 内容创作 ：文章、故事、广告文案一键生成。 编程辅助 ：自动补全代码、调试、生成解决方案。 教育学习 ：答疑解惑、知识辅导，提升学习效率。 跨语言翻译 ：支持多语种交流，助力全球业务。 数据分析 ：快速处理和总结长文档，辅助决策。\n意义与展望\nQwen3-Max-Preview  的发布，不仅是阿里在大模型上的一次技术突破，更是中国  AI  在全球竞争中发出的有力信号。\n它在推理、长文本处理和工具调用上的优势，意味着未来不论是科研、企业，还是个人创作，都会因为这样的模型变得更高效、更可靠。\n随着更多用户的接入与反馈， Qwen3-Max-Preview  也有望不断迭代，向真正的通用人工智能迈进一步。\n 想体验的朋友，可以直接访问  Qwen Chat  官方入口 ，或通过  阿里云百炼平台 调用  。"
  },
  {
    "title": "语言模型推理性能测试_推理服务性能评测_主流开源大模型基于Lite Server&Cluster适配Ascend-vLLM PyTorch NPU推理指导（6.5.905）LLM大...",
    "page_body": "性能benchmark包括两部分。\n静态性能测试：评估在固定输入、固定输出和固定并发下，模型的吞吐与首token延迟。该方式实现简单，能比较清楚地看出模型的性能和输入输出长度、以及并发的关系。 动态性能测试：评估在请求并发在一定范围内波动，且输入输出长度也在一定范围内变化时，模型的延迟和吞吐。该场景能模拟实际业务下动态的发送不同长度请求，能评估推理框架在实际业务中能支持的并发数。\nbenchmark代码目录\n 性能benchmark验证使用到的脚本存放在代码包AscendCloud-LLM- xxx .zip的llm_tools/llm_evaluation目录下。具体代码目录如下。 \nbenchmark_tools  |--- modal_benchmark     |--- modal_benchmark_parallel.py   # 评测静态性能脚本      |--- utils.py ├── benchmark_parallel.py   # 评测静态性能脚本  ├── benchmark_serving.py   # 评测动态性能脚本  ├── generate_dataset.py    # 生成自定义数据集的脚本  ├── benchmark_utils.py    # 工具函数集  ├── benchmark.py         # 执行静态、动态性能评测脚本 ├── requirements.txt       # 第三方依赖\n静态benchmark验证\n本章节介绍如何进行静态benchmark验证。\n已经上传benchmark验证脚本到推理容器中。benchmark验证脚本在AscendCloud-LLM- x.x.x .zip包里，获取路径 表1 。 进入benchmark_tools目录下，运行静态benchmark验证。 \ncd  benchmark_tools \n 语言模型脚本相对路径是tools/llm_evaluation/benchmark_tools/benchmark_parallel.py，具体操作命令如下，可以根据参数说明修改参数。 \npython benchmark_parallel.py  --backend  openai  --host  ${docker_ip}  --port  ${port}  --tokenizer  /path/to/tokenizer   --epochs 5 \\  --parallel-num  1 4 8  16 32  --prompt-tokens  1024 2048  --output-tokens  128 256  --benchmark-csv benchmark_parallel.csv\n参数说明\n--backend：服务类型，支持tgi、vllm、mindspore、openai等后端。本文档使用的推理接口是openai。 --host：服务部署的IP，${docker_ip}替换为宿主机实际的IP地址。 --port：推理服务端口。 --tokenizer：tokenizer路径，HuggingFace的权重路径。 --epochs：测试轮数，默认取值为5。 --parallel-num：每轮并发数，支持多个，如 1 4 8 16 32。 --prompt-tokens：输入长度，支持多个，如 128 128 2048 2048，数量需和--output-tokens的数量对应。 --output-tokens：输出长度，支持多个，如 128 2048 128 2048，数量需和--prompt-tokens的数量对应。 --benchmark-csv：结果保存文件，如benchmark_parallel.csv。 --served-model-name： 选择性添加，在接口中使用的模型名；如果没有配置，则默认为tokenizer。 --enable-prefix-caching：服务端是否启用enable-prefix-caching特性，默认为false。 --prefix-caching-num：构造的prompt的公共前缀的序列长度，prefix-caching-num值需小于prompt-tokens。 --use-spec-decode：是否使用投机推理进行输出统计，不输入默认为false。当使用投机推理时必须开启，否则会导致输出token数量统计不正确。注：由于投机推理的性能测试使用随机输入意义不大，建议开启--dataset-type、--dataset-path，并选择性开启--use-real-dataset-output-tokens使用真实数据集进行测试。 --dataset-type：当使用投机推理时开启，benchmark使用的数据类型，当前支持random、sharegpt、human-eval三种输入。random表示构造随机token的数据集进行测试；sharegpt表示使用sharegpt数据集进行测试；human-eval数据集表示使用human-eval数据集进行测试。不输入默认为random。注意：当输入为sharegpt或human-eval时，测试数据的输入长度为数据集的真实长度，--prompt-tokens的值会被忽略。 --dataset-path：数据集的路径，仅当--dataset-type为sharegpt或者human-eval的时候生效。 --use-real-dataset-output-tokens：当使用投机推理时开启，设置输出长度是否使用数据集的真实长度，不输入默认为false。当使用该选项时，测试数据的输出长度为数据集的真实长度，--output-tokens的值会被忽略。 --num-speculative-tokens：仅当开启--use-spec-decode时生效，需和服务启动时配置的--num-speculative-tokens一致。默认为-1。当该值大于等于0时，会基于该值计算投机推理的接受率指标。\n脚本运行完成后，测试结果保存在benchmark_parallel.csv中，示例如下图所示。 \n图1  静态benchmark测试结果（示意图）\n动态benchmark\n本章节介绍如何进行动态benchmark验证。\n获取数据集。动态benchmark需要使用数据集进行测试，可以使用公开数据集，例如Alpaca、ShareGPT。也可以根据业务实际情况，使用generate_datasets.py脚本生成和业务数据分布接近的数据集。 \n方法一：使用公开数据集\nShareGPT下载地址:  https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json Alpaca下载地址:  https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json\n方法二：使用generate_dataset.py脚本生成数据集方法：\n客户通过业务数据，在generate_dataset.py脚本，指定输入输出长度的均值和标准差，生成一定数量的正态分布的数据。具体操作命令如下，可以根据参数说明修改参数。\ncd  benchmark_tools  python generate_dataset.py  --dataset  custom_datasets.json  --tokenizer  /path/to/tokenizer  \\  --min-input  100  --max-input  3600  --avg-input  1800  --std-input  500 \\  --min-output  40  --max-output  256  --avg-output  160  --std-output  30  --num-requests  1000\ngenerate_dataset.py脚本执行参数说明如下：\n--dataset：数据集保存路径，如custom_datasets.json。 --tokenizer：tokenizer路径，可以是HuggingFace的权重路径。backend取值是openai时，tokenizer路径需要和推理服务启动时--model路径保持一致，比如--model /data/nfs/model/qwen3_8b， --tokenizer也需要为/data/nfs/model/qwen3_8b，两者要完全一致。 --min-input：输入tokens最小长度，可以根据实际需求设置。 --max-input：输入tokens最大长度，可以根据实际需求设置。 --avg-input：输入tokens长度平均值，可以根据实际需求设置。 --std-input：输入tokens长度方差，可以根据实际需求设置。 --min-output：最小输出tokens长度，可以根据实际需求设置。 --max-output：最大输出tokens长度，可以根据实际需求设置。 --avg-output：输出tokens长度平均值，可以根据实际需求设置。 --std-output：输出tokens长度标准差，可以根据实际需求设置。 --num-requests：输出数据集的数量，可以根据实际需求设置。\n进入benchmark_tools目录下，切换一个conda环境。 \ncd  benchmark_tools  conda activate python-3.9.10\n执行脚本benchmark_serving.py测试动态benchmark。具体操作命令如下，可以根据参数说明修改参数。 \npython benchmark_serving.py  --backend  openai  --host  ${docker_ip}  --port 8080 --dataset  custom_datasets.json --dataset-type custom  \\ --tokenizer  /path/to/tokenizer --request-rate  0.01 1 2 4 8 10 20  --num-prompts  10 1000 1000 1000 1000 1000 1000 \\  --max-tokens  4096  --max-prompt-tokens  3768  --benchmark-csv benchmark_serving.csv\n--backend：服务类型，如tgi，vllm，mindspore、openai。 --host ${docker_ip}：服务部署的IP地址，${docker_ip}替换为宿主机实际的IP地址。 --port：推理服务端口。 --dataset：数据集路径。 --dataset-type：支持三种 \"alpaca\"，\"sharegpt\"，\"custom\"。custom为自定义数据集。 --tokenizer：tokenizer路径，可以是HuggingFace的权重路径，backend取值是openai时，tokenizer路径需要和推理服务启动时--model路径保持一致，比如--model /data/nfs/model/qwen3_8b， --tokenizer也需要为/data/nfs/model/qwen3_8b，两者要完全一致。 --request-rate：请求频率，支持多个，如 0.1 1 2。实际测试时，会根据request-rate为均值的指数分布来发送请求以模拟真实业务场景。 --num-prompts：某个频率下请求数，支持多个，如 10 100 100，数量需和--request-rate的数量对应。 --max-tokens：输入+输出限制的最大长度，模型启动参数--max-input-length值需要大于该值。 --max-prompt-tokens：输入限制的最大长度，推理时最大输入tokens数量，模型启动参数--max-total-tokens值需要大于该值，tokenizer建议带tokenizer.json的FastTokenizer。 --benchmark-csv：结果保存路径，如benchmark_serving.csv。 --served-model-name： 选择性添加， 选择性添加，在接口中使用的模型名；如果没有配置，则默认为tokenizer。 --num-scheduler-steps: 服务启动时如果配置了--num-scheduler-steps和 --multi-step-stream-outputs =false，则需配置此参数与服务启动时--num-scheduler-steps一致。\n脚本运行完后，测试结果保存在benchmark_serving.csv中，示例如下图所示。 \n图2  动态benchmark测试结果（示意图）\n单条请求性能测试\n针对openai的/v1/completions以及/v1/chat/completions两个非流式接口，请求体中可以添加可选参数\"return_latency\"，默认为false，如果指定该参数为true，则会在相应请求的返回体中返回字段\"latency\"，返回内容如下：\nprefill_latency（首token时延）：请求从到达服务开始到生成首token的耗时 model_prefill_latency（模型计算首token时延）：服务从开始计算首token到生成首token的耗时 avg_decode_latency（平均增量token时延）：服务计算增量token的平均耗时 time_in_queue（请求排队时间）：请求从到达服务开始到开始被调度的耗时 request_latency（请求总时延）：请求从到达服务开始到结束的耗时\n以上指标单位均是ms，保留2位小数。"
  },
  {
    "title": "使用Star Schema Benchmark测试SelectDB的性能-云数据库 SelectDB 版-阿里云",
    "page_body": "云数据库 SelectDB 版 致力于提供极致性能、简单易用的数据分析服务，在宽表聚合、多表关联和高并发点查等场景下均具有优异的性能表现。本文将介绍如何通过 SSB 标准测试及 SSB flat 测试评估  SelectDB  在 SSB 1000 GB 测试集上的性能表现。\n概述\n星型模式基准测试（SSB，Star Schema Benchmark） 是一个轻量级的数仓场景下的性能测试集。SSB 基于 TPC-H 提供了一个简化版的星型模型数据集，主要用于测试在星型模型下，多表关联查询的性能表现。另外，业界内通常也会将 SSB 打平为宽表模型（以下简称：SSB Flat），来测试查询引擎的性能。\n重要 \n包括 SSB 在内的标准测试集通常和实际业务场景差距较大，并且部分测试会针对测试集进行参数调优。所以标准测试集的测试结果仅能反映数据库在特定场景下的性能表现。建议您使用实际业务数据进行进一步的测试。\n准备工作\n步骤一：准备目标实例\n准备实例。\n如果您已有目标实例，您需根据以下要求检查实例配置。\n如果您还没有目标实例，您需 创建实例 。\n本文中进行性能测试所用实例需满足以下要求。\n内核版本 大于等于 4.0。\n如果您已有目标实例，但版本小于 4.0，您可升级实例达到版本要求。具体操作，请参见 升级内核版本 。\n规格 需高于或等于 96 核 384 GB。本文中的测试基于 96 核 384 GB 的规格进行。\n集群缓存空间 需大于等于 1200 GB。本文中的测试基于 1200 GB 的缓存空间进行。\n修改 streaming_load_max_mb 参数至最大值。\n在测试过程中，工具通过 Stream load 方式将测试数据上传至 SelectDB。由于本次测试数据量较大，已超过默认的 Stream load 最大导入限制 10240MB，因此您需要将 BE 的参数 streaming_load_max_mb 调整至最大值 10240000MB。如何修改参数，请参见 参数配置 。\n创建待导入测试数据的目标库。\n如果您已有目标库，跳过此操作。\n连接实例，具体操作，请参见 通过 MySQL 客户端连接云数据库 SelectDB 版实例 。\n建库。\n本次测试的目标库为 test_db。建表语句如下。\nCREATE  DATABASE test_db;\n步骤二：准备测试服务器\n重要 \n下述安装依赖工具的脚本适用于操作系统为 Linux 的服务器。如果您的服务器操作系统非 Linux，您需自行修改安装脚本，以使其适应您的操作系统。\n注意事项\n您的服务器需注意以下事项。\n如果您准备在服务器使用 Git 下载 SSB 测试工具。您需开通公网。\n新购 ECS 实例：需购买 ECS 时， 公网 IP 勾选 分配公网 IPv4 地址 。\n已有 ECS 实例但未开通公网：ECS 开通公网，请参见 开通公网 。\n此次测试数据集生成的数据文件约 1000GB，需确保服务器的内存足够大。\n操作步骤\n创建目标服务器。\n如果您已有目标服务器，跳过此步骤。\n如果您还没有目标服务器，您可 自定义购买 ECS 实例 ，镜像选择 Alibaba Cloud Linux。\n安装测试中的依赖工具 MySQL 客户端。\nyum install mysql\n（可选）安装 Git。\n本次测试中，使用 Git 下载 SSB 工具。若已通过其他方式获取 SSB 工具并准备手动上传至服务器，可跳过此步骤。\nyum install git\n步骤三：确保网络互通\n确保将安装 SSB 测试工具的目标服务器与 SelectDB 实例网络互通：\n为 SelectDB 实例申请公网地址。具体操作，请参见 申请和释放公网地址 。\n如果您将要安装 SSB 测试工具的目标服务器为阿里云服务器，且该服务器与 云数据库 SelectDB 版 实例位于同一 VPC 下，跳过此步骤。\n将要安装 SSB 测试工具的目标服务器的 IP 地址添加至 云数据库 SelectDB 版 的白名单。具体操作，请参见 设置白名单 。\n步骤四：了解本次测试数据集\n本次测试 SSB 会生成 1000GB 的数据，并将其导入 SelectDB ，以进行 SelectDB 的性能测试。以下是关于测试数据集为 1000GB 数据表的相关说明。\nSSB 表名\n行数\n备注\nSSB 表名\n行数\n备注\nlineorder\n5999989709\n商品订单明细表。\ncustomer\n30000000\n客户信息表。\npart\n2000000\n零件信息表。\nsupplier\n2000000\n供应商信息表。\ndates\n2556\n日期表。\nlineorder_flat\n5999989709\n数据展平后的宽表。\n操作步骤\n重要 \n下述脚本适用于操作系统为 Linux 的服务器。如果您的服务器操作系统非 Linux，您需自行修改安装脚本，以使其适应您的操作系统。\n步骤一：登录目标服务器\n如果您的服务是阿里云 ECS，如何登录请参见 连接 ECS 。\n其他服务器如何登录，参见各自产品。\n步骤二：下载安装 SSB 数据生成工具\n下载工具。\n本次测试使用 Git 下载工具，具体脚本如下。\ngit clone https://github.com/apache/doris.git && cd ./doris/tools/ssb-tools\n您也可以通过 ssb-tools 链接下载工具后，手动上传至目标服务器。\n编译工具。\n执行以下脚本，编译工具。\nsh bin/build-ssb-dbgen.sh\n步骤三：生成 SSB 测试数据集\n重要 \n数据量越大，生成时间越长，具体取决于服务器的性能。\n您需在测试工具安装目录下执行生成测试数据集的脚本。\n语法如下。\nsh bin/gen-ssb-data.sh -s <yourAimDataNum>\n参数说明如下。\nyourAimDataNum：\n含义：需要使用 SSB 生成的数据大小。\n单位：GB\n此次测试为中规模测试，需要生成 1000 GB（即 1 TB）的测试数据集。该步骤所需时间可能较长，建议您将该任务置于后台执行，语句如下。\nnohup sh bin/gen-ssb-data.sh -s 1000 > gen-ssb-data.log 2>&1 &\n执行结果 会保存在测试工具安装目录下的 gen-ssb-data.log 文件中，可通过查看该文件获取执行过程是否正常。\n测试数据结果集 将保存在测试工具安装目录的 bin 目录下的 ssb-data 目录中，数据文件后缀为 .tbl 。\n说明 \n若在数据生成过程中报“bang！”警告，且已确认生成测试数据所在的磁盘空间充足，则可能是生成工具的并发控制问题，可以忽略。\n步骤四： 使用脚本为 SelectDB 创建 SSB 测试表\n配置 SelectDB 实例信息\n在执行建表脚本前，需要在 doris-cluster.conf 文件中，配置 SelectDB 实例的相关信息。该文件位于测试工具安装目录 ssb-tools/conf/ 下。示例如下。\n#  Any of FE host  export FE_HOST='selectdb-cn-****.selectdbfe.rds.aliyuncs.com'  #  http_port  in  fe.conf  export FE_HTTP_PORT=8080  #  query_port  in  fe.conf  export FE_QUERY_PORT=9030  #  Doris username  export USER='admin'  #  Doris password  export PASSWORD='****'  #  The database  where  SSB tables located  export DB='test_db'\n参数说明如下。\n参数名称\n参数描述\n参数名称\n参数描述\nFE_HOST\nSelectDB 实例的访问地址。\n您可以从 SelectDB 控制台的实例详情中的网络信息中获取实例的 VPC 地址或公网地址。\nFE_HTTP_PORT\nSelectDB 实例的 HTTP 协议端口。\n您可以从 SelectDB 控制台的实例详情中的网络信息中获取实例的 HTTP 协议端口。\nFE_QUERY_PORT\n您可以从 SelectDB 控制台的实例详情中的网络信息中获取实例的 MySQL 协议端口\nUSER\nSelectDB 实例账号。\n创建 SelectDB 实例后，系统会默认为您创建 admin 账号。\nPASSWORD\nSelectDB 实例账号的密码。\n如果您的 USER 配置的是 admin 账号，但您忘记了此账号的密码，您可以在控制台上重置账号密码。\nDB\nSelectDB 实例中待导入数据的库名。\n建表\n在测试工具安装目录下，执行以下脚本，创建用以测试的表。脚本执行完成后， SelectDB 的目标库中，会生成上述测试数据集中的表。\nsh bin/create-ssb-tables.sh -s 1000\n步骤五：导 入数据至 SelectDB ​\n重要 \n测试数据量越大，导入时间越长，具体取决于服务器的性能。\n在测试工具安装目录下，执行以下脚本，将 SSB 测试集所有数据以及 SSB flat 宽表数据合成的数据导入至 SelectDB 。\nsh bin/load-ssb-data.sh\n此次测试为中规模测试，需要将生成的 1000 GB（即 1 TB）的测试数据集导入至 SelectDB 中。该步骤所需时间可能较长，建议您将该任务置于后台执行，语句如下。\nnohup sh bin/load-ssb-data.sh > load-ssb-data.log 2>&1 &\n执行结果保存到测试工具安装目录下的 load-ssb-data.log 文件中，可通过查看该文件获取执行过程是否正常。\n步骤六：测试查询性能\n重要 \n测试数据量越大，批量测试时间越长，具体取决于服务器的性能。\nSSB 标准测试 主要用于评估数据库在 复杂星型模型查询 中的性能，如多表连接、聚合、过滤等。\nSSB Flat 测试 主要用于可能用于评估数据库在 宽表结构 下的性能，或测试扁平化数据模型对查询效率的影响。\n批量测试查询 SQL 性能\nSSB标准测试\nSSB-flat测试\n您可以执行 SSB 标准测试的 SQL 相关脚本，批量执行测试集的 SQL。\n语法如下。\nsh bin/run-ssb-queries.sh -s <yourAimDataNum>\n参数说明如下。\nyourAimDataNum：确保查询针对正确规模的数据集运行，与生成数据时的规模一致（即生成数据时若用  -s 1000 ，运行查询时也需使用  -s 1000 ）。\n脚本执行完毕后，控制台窗口将显示测试集中每个 SQL 在 SelectDB 中的性能情况。\n此次测试为中规模测试，需要测试查询的 1000 GB（即 1 TB）的测试数据集。该步骤所需时间可能较长，建议您将该任务置于后台执行，语句如下。\nnohup sh bin/run-ssb-queries.sh -s 1000 > run-ssb-queries.log 2>&1 &\n批量测试的 SQL 详情请参见 ssb-queries 。\n查询性能结果 将会保存到到测试工具安装目录下的 run-ssb-queries.log 文件中，可通过查看该文件获取查询过程是否正常的信息以及查询测试的结果。本文档针对 1000GB 数据的测试结果，请参见 测试结果 。\n单条查询 SQL 性能测试\n您也可以对 SelectDB 执行某个 SQL 的性能进行测试，具体操作如下：\n连接 SelectDB 实例。具体操作，请参见 通过 DMS 连接云数据库 SelectDB 版实例 。\n执行目标 SQL。\nSSB标准测试\nSSB-flat测试\n您可以在 SSB 标准测试查询语句 中获取目标 SQL，进行执行。\n您也可以在本次测试时使用的 SQL 语句，选取某一条语句进行执行。\n--Q1.1 SELECT SUM (lo_extendedprice  *  lo_discount)  AS  REVENUE  FROM  lineorder, dates  WHERE      lo_orderdate  =  d_datekey    AND  d_year  = 1993 AND  lo_discount  BETWEEN 1 AND 3 AND  lo_quantity  < 25 ;   --Q1.2 SELECT SUM (lo_extendedprice  *  lo_discount)  AS  REVENUE  FROM  lineorder, dates  WHERE      lo_orderdate  =  d_datekey    AND  d_yearmonth  = 'Jan1994' AND  lo_discount  BETWEEN 4 AND 6"
  },
  {
    "title": "11代Core大战5代锐龙—12个游戏，6个CPU的真实游戏性能对比",
    "page_body": "时至今日11代酷睿Core——Rocket Lake-S系列产品已经发布了月余，对于“11代Core和5代锐龙到底谁强”这个问题也是众说纷纭，五一期间我就专门为了这个问题，凑齐了i5/R5、i7/R7、i9/R9，来一场14个项目的大乱斗，用最直观的数据看看到底性能对比是什么样的！来个纯干货评测！\n测试平台\nCPU：AMD Ryzen R9 5900X/R7 5800X/R5 5600X\nIntel Core i9 11900K/i7 11700K/i5 11600K\n散热器：芝奇 ENKI280 一体水\n主板：华硕ROG Strix B550-A GAMING 吹雪\n华硕ROG Strix Z590-A GAMING WIFI 吹雪\n内存：芝奇TridentZ Neo 焰光戟 3600C16 16G X2\n显卡：AMD Radeon RX6800XT\nSSD：西部数据SN550 2TB in ROG Strix ARION\n显示器：华硕ROG XG27UQ\n电源：华硕ROG Thor 1200\n除了CPU和主板之外，其他的配件都保持一致，只不过Z590吹雪的定价差不多能买俩B550吹雪了......没办法，谁让AMD的主板便宜呢\n这块就不提了，CPU倒是价格差不多，除了i9 11900K逆势上涨到6000多实在是看不懂之外，i5/R5、i7/R7的价格基本一样AMD的CPU贵但是B550主板便宜，Intel的CPU便宜但是B560主板贵，基本扯平，当然这里搭配的是B560主板，总体上这代Intel CPU的价格降低了不少，可惜主板还是偏贵，好在B560也开放了内存超频，不过也差不了太多，这次测试为了省事统一就用Z590了，毕竟3466和3600内存性能差距微乎其微。\nAMD 锐龙 9 5900X CPU处理器 12核24线程 3.7GHz\nIntel 英特尔 酷睿 i9-11900K 8核16线程 盒装CPU处理器\n锐龙 7 5800X CPU处理器 8核16线程 3.8GHz\n英特尔 Intel i7-11700KF 8核16线程 盒装CPU处理器\nAMD 锐龙 5 5600X CPU处理器 6核12线程 3.7GHz\nintel 英特尔 i5-11600KF 盒装处理器\nROG 玩家国度 ROG STRIX B550-A GAMING ATX主板（AMD AM4、B550）\nROG 玩家国度 玩家国度（ROG）ROG STRIX Z590-A GAMING WIFI吹雪主板 支持 CPU 10900K/10700K（Intel Z590/LGA 1200）\n测试成绩\n首先是日常应用成绩对比——PC Mark10，AU整体比同级的IU高5%左右，比较意外的是i9 i7虽然规格看起来差不多，但是实际上还是成绩差了不少的，看来i9还是有点东西的。\n接下来就是正经的游戏对比了，选择游戏的原则只有一个——有固定的Benchmark！只有这样才能比较公平的对比出性能来，没有固定的Benchmark的游戏测试偶然因素太大，手一抖就能差出几个FPS，容易造成误差或者人为操作出“奇怪的”结果，那就没啥意思了，因此我穷尽了我Steam和微软账号，一共找出来12个有Benchmark的游戏，来做一个对比。游戏选项都是能开最高开最高，1080P分辨率。\n第一个是非常吃缓存性能的CSGO，类似这种老网游是锐龙绝对的主场，不过神奇的是11900K竟然也不弱，靠着足够高的频率强行顶到接近500fps，超过11700K 20%，然而还是不如5800X，AU这边竟然是5600X最强......\n另外一个不算老的“老游戏”，帝国时代2决定版，里面也内置了一个Benchmark，这个测试基本只看CPU性能，同核心数下IU略微有一点点优势，不过最强的依然是5900X。\n接下来是几个3A游戏，古墓丽影曾经是Intel的主场，随着ZEN3的出现完全扭转，大幅度领先IU，不知道为啥8核以下的成绩都几乎一样，但是5900X又强出一截，个人分析大概是因为5900X的内存性能要强一点的缘故吧。\n真正的CyberPunk——杀出重围人类分裂√，这个测试中IU略微领先，比较奇怪的是i9在这里表现还不如i7，而且多次测试都是这样。\n其实上面的古墓丽影也有这个现象，只不过差距非常小而已，究其原因，是因为测试时间比较长，i9撞到了功耗/温度墙降频了，而i7i5相对压力小一点，反倒FPS更高，要知道同为8核，i9 11900K的基础频率是3.5GHz，而i7 11700K的基础频率反而是3.6GHz，而6核的i5 11600K则有3.9GHz。\n杀手2的成绩则和无主之地3相反，是明显的AU主场，而IU的三个CPU表现基本一致。\n到了刺客信条：英灵殿，又完全反过来了，各家的U互相表现都一样，但是IU大幅度领先AU......这个游戏很有意思，A卡特别猛，N卡又不行，要想爽玩还得IU+A卡的组合\n孤岛惊魂5也是一个开场就是RYZEN LOGO的AU游戏，不过领先的不是特别多，这个游戏优化还是比较到位的，成绩大致是依次上升，并没有核心多而瓶颈。\n地平线4也是一个典型的AU游戏，不过对多核优化一般，6核就能吃饱。除了AU优化的好之外，A卡也优化得特别好，大概是因为是主机游戏移植吧，这几年AMD在主机上的耕耘逐渐有了收获。\n很多游戏都是从主机移植而来，对AUA卡的优化天生有优势，尤其是3A大作，就算是没有AU优化（相信PS5 XBSX时代ZEN优化也会追上），A卡优化也很少缺席，除非是老黄硬砸钱的那种RTX+DLSS游戏。\n全境封锁2就属于那种谁都不优化的游戏，纯靠硬实力，结果就是几乎打平，不过不同档次的CPU之间还是能略微拉开点差别的，5900X vs 11900K的差距稍大，剩下俩都是1fps的差距。\n战争机器5也是一个典型的IU+A卡游戏，和英灵殿类似，算是Intel终于扳回一城。\n全战三国则是另一种极端，毫无倾向不说优化还很烂，6个CPU成绩基本没差别，都在误差水平，不过还是能看出来一点点高频有优势，i9以不到1fps的成绩拿了第一，而最低的R5也只差了4fps，不到3%，基本没什么区分度。\n另一个优化不明显但是非常吃CPU性能的游戏就是文明6，后期经常一个回合等到天荒地老，非常考验CPU的性能。显然6核心在文明6里面已经是比较明显的瓶颈了，8核心才算是性能拉满，不过多核心优化也就到头了，有点可惜.....整体上这里两类CPU的性能势均力敌，AU稍微有一些优势，毕竟单核心性能更强。\n总结\n看到这里，不知道大家发现什么规律没有？个人简单总结一下，大致就是：\n①即使是到了2020年的游戏，8核以上的优势依然不明显，只打游戏的话6核就不差，当然考虑到日常应用以及后台程序什么的，从PCMark10成绩对比，以及个别多核优化好的游戏来看，还是8核最合适，尤其是8核和6核的价格相差不大的情况下。\n②老一些的游戏，例如大量基于DX11甚至DX9的网游，ZEN3锐龙优势比较明显，而且这类游戏对多核优化有限，5600X这样的6核就足以发挥出完全的性能了，搭配个便宜的B550主板，可以把预算省下来.....还是买不起显卡，加点钱在外设上吧\n③而到了3A游戏，AU IU玩什么游戏强，纯粹看优化，只要有对应优化了就是稳赢，如今 不但得看游戏选显卡，还得看游戏选CPU 了，当然说起来AU还是优化相对多点，毕竟主机也都是AUA卡。\n④如果是没有倾向的游戏，同档次下AU锐龙还是要强一点，尤其是用老引擎的网游类比如CSGO、LOL这类，锐龙的大缓存优势非常大。\n⑤内部对比的话，AU这边R5 5600X游戏性价比最高，R9 5900X核心多了一倍价格也翻了一倍还算是合理，R7 5800X就显得性价比没那么高，当然这是在锐龙内部比，整体性价比上还是不错的；而IU这边i9 11900K高达6000多的定价完全就是一副不打算卖的样子，和11700K相比完全不值这么多溢价，相比之下i7 11700KF i5 11600KF的性价比都不错，当然这是搭配B560的前提下，i9+Z590的组合只适合充值信仰。\n本文经“什么值得买”社区作者@知之为知之授权转载，文章内容仅代表作者观点，与本站立场无关，未经授权请勿转载。\n举报/反馈"
  },
  {
    "title": "活动报名|大模型时代的语音处理研究研讨会暨CCF语音对话与听觉专委“走进高校”系列活动—走进香港中文大学（深圳）（深圳）数据科学学院",
    "page_body": "由中国计算机学会（CCF）主办，CCF语音对话与听觉专委会、香港中文大学（深圳）、深圳市大数据研究院及深圳市跨模态认知计算重点实验室承办的  “CCF走进高校”公益活动 ，将于 2024年12月6日（星期五）9:00至18:00 在 香港中文大学（深圳）道远楼103 举办。\n本次活动由CCF语音对话与听觉专业委员会主任、上海交通大学俞凯教授，新加坡工程院院士、港中大（深圳）数据科学学院执行院长李海洲开场致辞。该研讨会邀请了党建武教授、颜永红教授、王文武教授、Chng Eng Siong教授、Junichi Yamagishi教授、Jinyu Li教授、吴志勇教授、武执政教授等学者发表精彩报告。李明教授、杜俊教授、陈谐教授等专家将就当前热点话题进行引导发言，并邀请参会专家展开专题讨论。活动将由港中大（深圳）数据科学学院的Satoshi Nakamura教授和武执政教授主持。\n欢迎各位关注并积极参与，共同探讨前沿技术，分享学术成果。\n活动安排\n■  活动时间\n 2024年12月6日（星期五）9:00至18:00\n■  地点\n 深圳市龙岗区龙翔大道2001号香港中文大学（深圳）道远楼103\n■  报名链接\n 可扫描下方二维码填写问卷报名。报名截止时间为12月3日（星期二）23:59。\n■  活动议程 / Agenda\n■  组织主席\n李海洲\n 新加坡工程院院士\n 港中大（深圳）数据科学学院执行院长\n研究领域： 语音信息处理、自然语言处理、类脑计算、人机交互\n个人简介： 李海洲教授现任香港中文大学（深圳）数据科学学院执行院长、校长学勤讲座教授，同时他也是新加坡国立大学客座教授和德国不来梅大学卓越讲座教授。此前，他曾于2006年至2016年分别担任新加坡南洋理工大学和新加坡国立大学教授，于2009年担任东芬兰大学客座教授，于2011年至2016年任澳洲新南威尔士大学客座教授，于2003年至2016年担任新加坡科技研究局通信与资讯研究院首席科学家和研究总监。李教授曾任《IEEE/ACM 音频、语音和语言处理汇刊》主编 (2015-2018年)、IEEE语音与语言处理技术委员会委员 (2013-2015年)、IEEE信号处理学会出版委员会委员(2015-2018年)、IEEE 信号处理学会奖励委员会委员(2021-2023年)、IEEE 信号处理学会会议委员会委员（2023-2024）、IEEE信号处理学会副会长（2024-2026）。李教授也曾任国际语音通信学会主席 (2015-2017年)、亚太信号与信息处理协会主席 (2015-2016年)、亚洲自然语言处理联合会主席 (AFNLP, 2017-2018年)。此外，他还担任了多个大型学术会议的大会主席，包括ACL 2012、INTERSPEECH 2014， 和ICASSP 2022。李教授享誉国际，他不仅在语音识别和自然语言处理研究领域有着突出贡献，还领导研发了多项知名的语音产品，如1996年苹果电脑公司为Macintosh发行的中文听写套件、1999年Lernout & Hauspie公司为亚洲语言发行的Speech-Pen-Keyboard文本输入解决方案。他是一系列重大技术项目的架构师，项目包括2001年为新加坡樟宜国际机场研发的具有多语种语音识别功能的TELEFIQS自动呼叫中心、2012年为联想A586智能手机研发的声纹识别引擎、2013年为百度音乐研发的听歌识曲引擎。他的“音素集”研究成果解决了亚洲口语语言的语音识别的实用化问题，在美国和多个欧洲国家注册。\n俞凯 \n CCF语音对话与听觉专业委员会主任\n个人简介： 上海交通大学计算机科学与工程系特聘教授，思必驰公司首席科学家。国家高层次人才计划获得者，NSFC优青，上海市“东方学者”特聘教授。长期从事智能语音及语言处理研究和产业转化工作，担任IEEE Speech and Language Processing Technical Committee 委员（2017-2019），IEEE/ACM Transactions on Audio Speech and Language Processing 副主编，InterSpeech等国际会议程序委员会主席，ACL、EMNLP等国际会议研究领域主席等。现任中国人工智能产业发展联盟学术和知识产权组组长，中国计算机学会语音对话及听觉专委会主任。\n报告信息\n*按照讲座时间顺序排序\n颜永红\n个人介绍： 中国科学院声学所首席科学家，科学院核心骨干特聘研究员。1990年毕业于清华大学电子工程系获学士学位，1995年于美国俄勒冈研究院获计算机科学博士。长期从事语言声学方面的研究，牵头承担国家自然基金委重大项目、863、科技支撑、重点领域专项等国家级科研项目十余项，是新世纪百千万人才工程国家级人选、国家自然基金委国家杰出青年基金获得者、全国优秀科技工作者称号获得者。发表论文300余篇，持有发明专利100余项，曾获国家科技进步二等奖一项，中科院杰出科技成就奖一项，省部科技一等奖两项。\n报告题目： 人工智能时代的语言声学\n报告摘要： 语言声学是专门研究人类发音和听觉的声学学科分支之一。本报告将简单综述语言声学学科的历史和研究内容，并通过对“人和人对话交流中元音和辅音哪个更为重要”这一问题横跨近100年的研究综述来介绍相关基础研究成果及所发现机理应用于现实问题的历程，试图给当前大数据大计算大模型时代语音技术的发展探索新的途径。报告最后会简单介绍声学所团队在语言声学领域的一些最新进展。\n王文武\n个人介绍： Wenwu Wang is a Professor in Signal Processing and Machine Learning, University of Surrey, UK. He is also an AI Fellow at the Surrey Institute for People Centred Artificial Intelligence. His current research interests include signal processing, machine learning and perception, artificial intelligence, machine audition (listening), and statistical anomaly detection. He has (co)-authored over 300 papers in these areas. He has been recognized as a (co-)author or (co)-recipient of more than 15 accolades, including the 2022 IEEE Signal Processing Society Young Author Best Paper Award, ICAUS 2021 Best Paper Award, DCASE 2020 and 2023 Judge’s Award, DCASE 2019 and 2020 Reproducible System Award, and LVA/ICA 2018 Best Student Paper Award. He is an Associate Editor (2020-2025) for IEEE/ACM Transactions on Audio Speech and Language Processing, and an Associate Editor (2024-2026) for IEEE Transactions on Multimedia. He was a Senior Area Editor (2019-2023) and Associate Editor (2014-2018) for IEEE Transactions on Signal Processing. He is the elected Chair (2023-2024) of IEEE Signal Processing Society (SPS) Machine Learning for Signal Processing Technical Committee, a Board Member (2023-2024) of IEEE SPS Technical Directions Board, the elected Chair (2025-2027) and Vice Chair (2022-2024) of the EURASIP Technical Area Committee on Acoustic Speech and Music Signal Processing, an elected Member (2021-2026) of the IEEE SPS Signal Processing Theory and Methods Technical Committee. He has been on the organising committee of INTERSPEECH 2022, IEEE ICASSP 2019 & 2024, IEEE MLSP 2013 & 2024, and SSP 2009. He is Technical Program Co-Chair of IEEE MLSP 2025. He has been an invited Keynote or Plenary Speaker on more than 20 international conferences and workshops. \n报告题目： Language Queried Audio Source Separation\n报告摘要： Language-queried audio source separation (LASS) is a paradigm that we proposed recently for separating sound sources of interest from an audio mixture using a natural language query. The development of LASS systems offers intuitive and scalable interface tools that are potentially useful for digital audio applications, such as automated audio editing, remixing, and rendering. In this talk, we will first introduce the problem setting and motivation, making a connection with conventional paradigms including speech source separation and universal audio source separation. We then present our two newly developed LASS algorithms, AudioSep and FlowSep, respectively. AudioSep is a foundational model for open-domain audio source separation driven by natural language queries. It employs a query network and a separation network to predict time-frequency masks, enabling the extraction of target sounds based on text prompts. The model was trained on large-scale multimodal datasets and evaluated extensively on numerous tasks including audio event separation, musical instrument separation, and speech enhancement. FlowSep is a new generative model for LASS based on rectified flow matching (RFM), which models linear flow trajectories from noise to target source features within the latent space of a variational autoencoder (VAE). During inference, the RFM-generated latent features are used to reconstruct a mel-spectrogram through the pre-trained VAE decoder, which is then passed to a pre-trained vocoder to synthesize the waveform. After this, we will discuss the datasets and performance metrics we developed for evaluating the LASS systems, and the organisation of Task 8 of DCASE 2024 international challenge, building on the AudioSep model. Finally, we conclude the talk by outlining potential future research directions in this area. \nJunichi Yamagishi\n个人介绍： Junichi Yamagishi received a Ph.D. degree from the Tokyo Institute of Technology (Tokyo Tech), Tokyo, Japan, in 2006. From 2007 to 2013, he was a research fellow at the Centre for Speech Technology Research, University of Edinburgh, U.K. He became an associate professor with the National Institute of Informatics, Japan, in 2013, where he is currently a professor. His research interests include speech processing, machine learning, signal processing, biometrics, digital media cloning, and media forensics. He was co-organizer for the bi-annual ASVspoof Challenge and the bi-annual Voice Conversion Challenge. He also served as a member of the IEEE Speech and Language Technical Committee from 2013 to 2019, as an Associate Editor for IEEE/ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING (TASLP) from 2014 to 2017, as a Senior Area Editor for IEEE/ACM TASLP from 2019 to 2023, as the chairperson for ISCA SynSIG from 2017 to 2021, and as a member at large of IEEE Signal Processing Society Education Board from 2019 to 2024.\nHe has authored more than 400 peer-reviewed papers in various international journals and conferences. Among his publications, a paper published at the 2018 IEEE international workshop on information forensics and security, titled “Mesonet: a compact facial video forgery detection network” has been cited over 1,500 times. He and his team also received the BTAS/IJCB 5-Year Highest Impact Award from the IEEE Biometrics Council at IEEE International Joint Conference on Biometrics (IJCB 2023) for the paper titled “Multi-task Learning for Detecting and Segmenting Manipulated Facial Images and Videos” which was published at IEEE International Conference on Biometrics: Theory, Applications, and Systems (BTAS) 2019.\n报告题目： Automatic Fact Verification across Languages and Modality\n报告摘要： Evidence-based fact-checking aims to automatically verify the veracity of input claims using evidences extracted from knowledge databases. In this talk, we present the details of our proposed automatic fact-checking model [1] and its multilingual extension to Spanish, French, Indonesian, Chinese and Japanese [2], as well as to a multimodal model that simultaneously uses structured tabular"
  },
  {
    "title": "北大腾讯突破奖励模型瓶颈！让AI理解人类偏好，泛化能力比肩GPT-4.1-太平洋科技",
    "page_body": "总是“死记硬背”“知其然不知其所以然”？\n奖励模型 训练也形成了学生选择标准答案的学习模式，陷入诸如“长回答=好回答”“好格式=好答案”等错误规律之中。\n北京大学知识计算实验室联合腾讯微信模式识别中心、William&Mary、西湖大学等机构提出的 RewardAnything 突破了这一瓶颈——通过让奖励模型直接理解自然语言描述的评判原则，实现了从”死记硬背”到”融会贯通”的范式跃迁。\nRewardAnything降低了传统模式针对不同场景需要收集偏好数据训练奖励模型再进行RL的高昂成本，能够直接利用自然语言作为RLHF的标准。\n其作为奖励模型，仅需一句话描述的准则即可刷新传统Benchmark的SOTA，在RABench上展示出了与GPT-4.1等顶尖模型相媲美的原则跟随能力与泛化能力。\n奖励模型与偏好优化\n尽管LLM展现出强大的性能，一个核心挑战始终存在： 如何让LLM的行为和输出精准契合人类多样化而细腻的偏好、多元的价值观以及特定任务场景下的复杂需求？\n奖励模型（Reward Model）就是LLM对齐与RL训练的关键组件 ——实现AI对齐（Alignment），让AI的行为和输出符合人类期望与价值观。它通过学习海量的偏好数据建模人类的偏好，学会“什么是好的回答”。\n但是，传统奖励模型存在 致命缺陷：在学习过程中形成了严重的偏见，它们通常在固定的、针对特定偏好标准收集的数据集上进行训练 。这导致它们学习到的偏好分布是静态的，难以适应真实世界中多样化、动态变化的需求。\n清华大学团队在ICLR‘25提出的RM-Bench评测基准揭露了一个现状： 当前最先进的奖励模型在抵抗格式偏见测试中，准确率仅为46.6%——甚至不如随机猜测！\n看2个例子，大家就明白了。\n△ 问答Prompt和Response均来自RewardBench数据集\n上述案例是来自奖励模型的常用基准测试集RewardBench的常见情况，尽管其数据和标签均无事实性错误，但使用类似的数据训练奖励模型，会隐含一种偏见： 模型在训练数据中观察到“正确的答案往往比较长”，于是错误地将“长度”本身当作了高质量的标志 。\n同样来自RewardBench，对于事实均无错误的回答，带来了另一种常见偏见：RM可能因为训练数据中某种特定格式（如列表化的，Markdown风格的回答）与“好答案”同时出现的频率较高， 而对这种格式本身产生了不合理的偏好，忽视了内容表达的自然性和多样性 。\n在技术层面，奖励模型的训练过程通常是这样的：\n收集偏好数据 ：给定一个问题（prompt），让多个不同的AI生成多个不同的回答。\n人工标注 ：请专业标注员对收集好的数据进行两两比较，判断哪个回答更好（chosen）、哪个较差（rejected）。这些标注通常遵循一定的标注规范，但这些标注规范背后的具体原因往往不会完全传递给模型。\n模型学习 ：奖励模型通过分析大量的“问题-好回答-差回答”三元组（即偏好对）来学习人类偏好，目标是给“好回答”打高分，给“差回答”打低分（或是直接从两个回答中选出较好的）。\n指导优化 ：训练好的奖励模型会输出一个标量奖励信号，用于指导大型语言模型（作为策略模型）通过强化学习算法（如PPO，GRPO）进行优化，使其生成更符合人类期望的内容。这个过程被称为基于人类反馈的强化学习（RLHF），是当前主流AI对齐技术的核心。奖励模型有时也直接用作评估指标（LLM-as-a-judge）。\n而上述偏见的存在，根源在于奖励模型的学习方式存在根本性缺陷：\n训练：只见结果，不知原因（隐式与“唯结果论”的学习） ：训练数据通常只告诉模型“回答A比回答B好”，却很少明确解释“为什么A更好”的深层原因。模型只能从结果中猜测和推断人类偏好，学习的是相关性而非因果性。这种“知其然，不知其所以然”的隐式学习，使得模型难以捕捉人类意图的精髓，且容易导致模型学习到虚假关联。\n即便近期多个奖励模型工作通过在奖励学习过程中引入深度思考显著提升奖励效果，仍然没有根本性改善这种学习方式带来的缺陷，模型依然需要“猜测因果”。\n评估：单一价值观难以概括人类偏好 ：在许多偏好数据集中，更详细、更长的回答往往质量更高，这本身是合理的。但模型可能错误地学习到“长=好”或“列表=好”这类肤浅的规律，完全忽视了内容本身的准确性、逻辑性或特定场景下的适用性。对于奖励模型的评估，其评测过程与训练过程十分相似，主要检查模型预测的偏好是否与人类标注一致。\n然而，这一过程忽略了人类偏好的多元性，尝试通过使用一种价值观定义人类的总体偏好，因此现有奖励模型的评估指标也具有很大不足。这样的评估方式，同样忽视模型是通过“作弊”（例如，永远选择更长的回答）来达到高分的情况，与LLM的基准测试的数据泄露问题类似。这种评估方式难以衡量模型是否真正理解了多样化和新颖的评价原则。\n方法：RewardAnything\n既然人类能用语言清楚地表达评判标准，为什么不直接告诉AI呢？\n这正是研究团队提出RewardAnything项目的核心理念——用自然语言定义“好”。 不再让模型从成千上万的例子中猜测模糊的规律，而是直接用自然语言告诉它评判标准，例如：“我需要简洁实用的回答，不要长篇大论”，“请确保事实准确性高于一切，即使回答简短也没关系”。\nRewardAnything引入了“原则跟随”（Principle-Following）范式，是一款新型的、专门为理解和遵循自然语言原则而设计和训练的 生成式奖励模型 ，且确保其可用于下游RL训练。这与LLM本身具备的“指令跟随”（Instruction-Following）能力异曲同工。\n它采用了多项创新技术来实现这一目标：\n1. 列表式评分与推理（Listwise Scoring with Reasoning）\n传统模型要么给单个回答打分（Pointwise），要么在两个回答之间进行比较（Pairwise）。如果有10个候选回答，两两比较可能需要高达45次调用（C(n, 2)），如果采取同期工作类似的做法，每次调用都涉及数千token的输入输出，其计算开销将完全成为GRPO等RL算法的瓶颈，因此实际难以将其应用于RL训练。与之对应的，RewardAnything则能一次性评估所有候选回答。\n2. 群体相对策略优化（Group Relative Preference Learning）\n为了让RewardAnything具备强大的原则跟随能力，团队采用了GRPO算法进行训练。\n与传统工作利用GRPO（Group Relative Policy Optimization）算法激发LLM的深度思考能力不同的是，团队直接利用现有的、已经具备深度思考能力的基座模型，如Qwen3，让RM通过GRPO的高效采样，从一组候选评价结果之间选出其中相对最为优秀的。RM在训练过程中，学会的是理解在特定原则指导下，一组候选回复之间的相对优劣，而非仅仅死记硬背孰优孰劣，强化那些能准确反映原则遵守情况的评估，使RM更关注相对质量的辨别，从而培养更好的泛化能力。\n训练过程中，RewardAnything自身作为一个策略模型，学习生成高质量的评估输出（包含推理、分数和排名）。其奖励函数经过精心设计，包含格式奖励：激励模型生成结构完整、逻辑一致的评估和准确性奖励：衡量模型判断与“真实”共识判断的对齐程度，并惩罚对质量差异大的回复的错误排序。\n具体奖励设计以及如何防止数据泄露以及合成训练数据的构建等细节，请参见文末给的论文链接。\n效果评估与新评测基准\n效果评估\nRewardAnything 在近期的、具有挑战性的来自清华大学的ICLR‘25基准测试 RM-Bench上进行了测试。\nRM-Bench以其“困难”(hard) 设置而闻名，该设置专门用于探测奖励模型中一个常见的偏见：无法有效区分一个事实正确但简洁的回复与一个不正确但内容详尽、格式美观的回复（即所谓的“长度偏见”或“格式偏见”）。\n结果显示，RewardAnything 取得了目前最佳的总体性能，尤其是在最具挑战性的“困难”设置上表现突出，显著超越了包括通用大模型（如GPT-4.1, Gemini 2.5 Pro）和一些非常近期的其他奖励模型工作（RM-R1 32B）。\n这一结果不仅验证了RewardAnything作为通用奖励模型的有效性，更解释了——传统奖励模型试图从隐含偏好数据中费力“猜测”的、可能导致偏见的因素，其实可以通过明确的自然语言原则进行更直接、更有效的规避和缓解。当我们给它一个清晰的、强调准确性的原则时，RewardAnything能够更好地抵制表面线索的干扰，做出更符合任务本质的判断。\nRABench：专为“原则遵循”设计的评估基准\n传统的奖励模型基准大多关注模型在固定、隐含偏好下的表现，难以有效衡量模型适应和遵循用户明确给出的、多样化自然语言原则的能力。\n为了全面评估奖励模型理解和遵循不同原则的能力，团队构建了RABench（Reward Anything Benchmark）评测基准，旨在通过引入判断原则，填补现有评测方法的空白。\n评测设计\n1. 原则的多样性\n团队从人工整理的200个原则中，专门挑选出50个与训练集原则完全不同的、具有多样性的原则用于基准测试。这些原则被归纳为五个与文本质量相关的基本维度：\n内容 (Content) ： 规定模型应呈现哪些信息，例如“鼓励包含相关例证的详细回答”。\n结构 (Structure) ： 定义文本的组织和布局方式，例如“重视信息组织清晰、条理分明的回答”。\n语气 (Tone) ： 捕捉文本所传达的情感和态度，例如“对带有鼓励性和帮助性语气的回答给予更高分数”。\n逻辑 (Logic) ： 关系到文本的推理过程和思路流程，例如“好的回答应展现连贯的思考过程”。\n风格 (Style) ： 指明对语言表达方式的偏好，例如“偏好使用清晰、简洁、无专业术语的语言”。\n2. 真实 场 景覆盖\n为了确保评估的挑战性和多样性，测试提示（prompts）主要来源于现有的RewardBench数据集，覆盖了日常对话、代码编程、数学推理、安全相关四大领域。\n3. 丰富的模型覆盖\n候选回复生成 ：对于RABench中的每一个“原则-提示”对，团队使用了来自6个不同家族（如GPT, Claude, Qwen, LLaMA, DeepSeek等）的10种不同的大型语言模型来生成候选回复。每个模型都被指示（通过系统提示）尝试遵循给定的原则来生成回复。\n多LLM初步评估与共识算法 ：团队利用了四个顶尖的LLM（如Claude-3.7 Sonnet, GPT-4.1等）作为独立的评估者，对所有候选回复进行评分和排序。然后，采用一种基于动态规划的共识算法来综合它们的评估结果，找到最受评委们一致认可的排序。\n人工最终验证 ：最后，每一个由算法生成的共识判断（包含原则、提示、一组回复以及LLM共识得到的分数和排序）都由两名人类标注员进行独立验证，确保其准确反映了对特定原则的遵守情况。只有两名标注员都认为有效的条目才被保留。这一过程的标注员间一致率达到了89%。\n最终，RABench包含1002个经过验证的偏好排序列表，由于每个列表包含多个回复，相当于传统成对比较基准中的约31,806个偏好对。\n团队将RewardAnything与强大的"
  },
  {
    "title": "业界最全遥感图像语义分割Benchmark发布啦！还有提高小目标分割性能的C2FNet等你pick-哔哩哔哩",
    "page_body": "遥感图像语义分割是图像分割领域的一个重要的垂类应用，有着广泛的需求和实际应用价值，尤其在灾害评估、农作物产量估计和土地变化监测等领域有着不可替代的作用。相比于自然图像的语义分割，遥感图像语义分割由于有其自身的特殊性和挑战性，如遥感图像存在大量微小目标，这就要求分割方法和结果要足够精细。\n自然图像和遥感图像对比\n基于飞桨PaddleSeg的遥感图像分割Benchmark\n近年来，随着人工智能技术的发展，深度学习成为图像语义分割领域的主流技术。但由于遥感图像预处理、模型设置、训练超参数等条件的不同，导致很多研究者在同一个遥感语义分割数据集上取得的表现会存在较大的差异。为了解决这个问题，飞桨与百度研究院大数据实验室合作，基于飞桨图像分割套件PaddleSeg，联合发布了一套完整的遥感图像分割Benchmark。我们测试了不同模型在多个遥感图像语义分割数据集上的性能，为广大开发者提供了一套可复现比较的标准基线。和已经发表的遥感图像语义分割模型相比，PaddleSeg提供的模型在遥感图像语义分割领域可以取得SOTA性能，部分性能数据如下所示。\n以下性能数据均来自：\nhttps://github.com/PaddlePaddle/PaddleSeg\n 不同模型在iSAID数据集上的表现\n不同模型在ISPRS Potsdam数据集上的表现\n 不同模型在ISPRS Vaihingen数据集上的表现\n自监督预训练模型性能比较\n此次，我们还引入了近期比较流行的自监督预训练模型，即首先在大规模遥感图像数据集上对骨干网络进行预训练，然后利用遥感图像语义分割数据在下游任务上进行微调。我们在Million-AID和DOTA2.0两个遥感图像数据集上应用自监督学习方法。为了获得充足的遥感数据，我们将两个遥感数据集内不同分辨率的图像剪裁至512x512。剪裁后的Million-AID数据集包含2,500,000张遥感图像切片，DOTA2.0数据集包含1,700,000张遥感图像切片，最终各方法性能数据如下所示。\n以下性能数据均来自：\nhttps://github.com/PaddlePaddle/PaddleSeg\n基于ImageNet-1k的自监督预训练模型在遥感图像分割任务上的表现\n基于遥感图像的自监督预训练模型在遥感图像分割任务上的表现\n遥感图像小目标分割模型C2FNet\n针对遥感图像语义分割的小目标问题，我们提出了一个由粗粒度到细粒度的二阶段分割模型C2FNet。受到人工标注过程的启发，C2FNet首先对遥感图像进行一次粗分割，并通过粗分割结果定位出小目标所在区域，然后对小目标所在的区域进行放大和进一步的细分割，最后对两次分割结果进行融合，从而提升小目标分割能力。如图7所示是CFNet模型结构示意。\n C2FNet模型示意图\nC2FNet在主流遥感图像分割数据集iSAID上取得小目标分割SOTA性能。相比于基线模型，C2FNet对小目标如小型车辆、船舶、直升机等类别，mIoU最高提升可达2.19个百分点。并且C2FNet架构具有通用性，可适用于各种语义分割模型，对小目标分割结果均有不同程度的提升。\nC2FNet在iSAID数据集上的表现with ours表示采用C2FNet架构（注：SH表示船舶、LV表示大型车辆、SV表示小型车辆、HC表示直升机、SP表示游泳池、PL表示飞机、HA表示港口。）\n分割结果可视化\n通过上图，我们可以看出C2FNet可以分割出更多的小目标像素，缓解基线模型对小目标分割能力不足的问题。\n总结\n针对遥感图像分割领域基线不统一的问题，我们利用PaddleSeg在不同的遥感分割数据集上进行了测试，提供了可比较的SOTA基线，完善了PaddleSeg对遥感图像的支持。同时引入近期比较流行的自监督预训练模型，给广大开发者提供了更多的选择。针对遥感图像的小目标分割问题，我们提出了C2FNet二阶段分割框架，在小目标分割任务上取得SOTA性能。\n本次提供的模型在PaddleSeg仓库的develop分支下可下载，欢迎star支持！\nhttps://github.com/PaddlePaddle/PaddleSeg/tree/develop/contrib/RSSegBenchmark\n小伙伴们还在等什么，一起用PaddleSeg玩转遥感图像分割吧，感兴趣的小伙伴可以添加交流群，还可以获取PaddleSeg团队整理的重磅学习大礼包哦~\n关注【飞桨PaddlePaddle】公众号，回复关键词：paddleseg，获取学习资料包~"
  },
  {
    "title": "2024 AI年度报告发布，附十大预测：人形机器人热度下降，英伟达维持霸主地位_澎湃号·湃客_澎湃新闻-The Paper",
    "page_body": "新智元报道\n编辑：LRS\n【新智元导读】AI技术日新月异，像Netflix这样的大公司已经开始用AI制作特效，而新的AI模型也在游戏和科学研究中超越了人类。未来一年，我们可能会看到没有编程技能的人也能创造热门应用，甚至AI创作的歌曲也可能登上音乐排行榜前十。\nAI发展之势迅猛，从技术、政策到产品风险，相比从前都有了很大变化。\n最近，State of AI发布了2024产业报告，也是调研团队发布的第七份年度报告，从研究进展、行业局势、现有政策、安全问题、未来预测等角度出发，对人工智能行业的现状和预期进行了深度分析。\n报告链接：https://docs.google.com/presentation/d/1GmZmoWOa2O92BPrncRcTKa15xvQGhq7g4I4hJSNlC0M/edit?usp=sharing\n报告主要关注以下内容：\n1. 前沿实验室的性能开始趋同，专有模型的优势逐渐消失，GPT-4与其他模型之间的差距正在缩小。OpenAI的o1模型再次让实验室在排行榜上名列前茅，但还能持续多久呢？\n2. 规划和推理在大型语言模型（LLM）研究中占据了优先地位，各公司正在探索将LLM与强化学习、进化算法和自我改进相结合，以解锁未来的智能体应用。\n3. 基础模型展示了它们突破语言的能力，支持跨数学、生物学、基因组学、物理科学和神经科学的多模态研究。\n4. 美国对中国实验室实施的制裁对其生产高性能模型的能力影响有限，因为库存、获批硬件、走私和云访问的结合使国内能够构建高性能的（V）LLM，但国内的半导体产业发展仍然较为混乱。\n5. 因为上市公司经历了人工智能曝光的牛市，人工智能公司的企业价值已达到9万亿美元。对私营人工智能公司的投资也有所增加，但尽管美国出现了GenAI巨额融资，增幅却小了一个数量级。\n6. 一些人工智能公司开始产生可观的收入，包括基础模型构建者和从事视频和音频生成的初创公司。然而，随着模型作为企业抢占市场的一部分而变得更便宜，围绕长期可持续性的疑问仍未得到解答。\n7. 伪收购成为人工智能公司的一条出路，因为一些公司难以找到可行的商业模式，因为保持技术领先的成本很高。\n8. OpenAI未遂的政变之后，关于存在风险的讨论已经降温。然而，研究人员一直在深化人们对潜在模型漏洞和滥用的了解，并提出了可能的修复措施和保障措施。\n研究进展\n1. 前沿实验室的性能正在趋同，但OpenAI在推出o1后保持了其领先地位，规划和推理成为主要的研究前沿。\n2. 基础模型展示了它们超越语言的能力，多模态研究推动了数学、生物学、基因组学、物理科学和神经科学的发展。\n3. 美国的制裁并没有阻止中国的大型语言模型（V）LLM在社区排行榜上的崛起。\n行业局势\n1. 英伟达（NVIDIA）仍是全球最强大的公司之一，享受着市值达到3万亿美元的辉煌时刻，而监管机构也正在调查生成式人工智能（GenAI）内部的权力集中问题。\n2. 更成熟的GenAI公司带来了数十亿美元的收入，而初创公司开始在视频和音频生成等领域获得关注。尽管公司开始从模型转向产品，但围绕定价和可持续性长期问题仍未解决。\n3. 在公开市场的牛市推动下，人工智能公司的市值达到了9万亿美元，而私营公司的投资水平也在健康增长。\n现有政策\n1. 虽然全球治理努力陷入僵局，但国家和地区的人工智能监管继续推进，美国和欧盟通过了几份有争议的立法法案。\n2. 计算需求的现实迫使大型科技公司面对扩张的现实世界物理限制和排放目标。与此同时，政府自身在建设能力方面的尝试仍在落后。\n3. 预期人工智能对选举、就业和其他敏感领域的影响尚未实现。\n安全问题\n1. 随着之前警告我们人类灭绝即将到来的公司需要增加企业销售和消费者应用程序的使用，从安全转向加速的氛围发生了变化。\n2. 世界各地的政府模仿英国在人工智能安全方面建立国家能力，成立研究所并研究关键国家基础设施的潜在漏洞。\n3. 每一个提出的破解「修复」方案都失败了，研究人员开始关注更复杂、长期的攻击。\n2025十大预测\n预测覆盖了人工智能领域的各个方面，从大型投资到无代码应用的兴起，再到数据隐私和人工智能法规的实施，甚至还涉及了开源技术的竞争、市场领导者的挑战、人形机器人的发展趋势。\n1. 一个主权国家向美国的大型人工智能实验室投资超过100亿美元，引发国家安全审查。\n2. 一个完全没有编码能力的人创建的应用或网站走红，比如进入苹果应用商店前100名。\n3. 前沿实验室在案件开始审判后，对数据收集的实践方式发生了重大转变。\n4. 早期欧盟人工智能法案的实施结果比预期的要宽松，立法者可能会担心他们做得太过分了。\n5. 开源的OpenAI o1替代品在一系列推理基准测试中实现超越。\n6. 挑战者未能在英伟达的市场地位上取得任何有意义的突破。\n7. 由于公司难以实现产品市场契合，对人形机器人的投资水平将下降。\n8. 苹果在设备上的研究取得了强劲的成果，加速了围绕个人设备上人工智能的势头。\n9. 由人工智能科学家生成的研究论文被一个主要的机器学习会议或研讨会接受。\n10. 一个以「生成式人工智能」为元素互动的视频游戏将会取得突破性地位。\n2023年度报告预测结果\n正确预测\n1. 好莱坞级别的制作公司开始使用生成式人工智能来制作视觉特效。\n证据：虽然大部分效果还不理想，但生成式人工智能视觉特效已经在Netflix和HBO的作品中出现。\n2. 美国联邦贸易委员会（FTC）或英国竞争与市场管理局（CMA）基于竞争理由调查微软/OpenAI的交易。\n证据：两个监管机构都在调查合作伙伴关系。\n3. 在全球人工智能治理方面，进展非常有限，会超出高层次的主动承诺（high-level voluntary commitments）。\n证据：是的，来自布莱切利和首尔峰会的承诺仍然是自愿和高层次的。\n4. 一首由人工智能创作的歌曲进入公告牌Hot 100前10名或Spotify 2024年热门榜单。\n证据：歌曲「Heart on My Sleeve」就是例子，也可以看到一首由人工智能创作的歌曲在德国中位列第27名，并保持了几天。\n5. 随着推理工作负载和成本的显著增长，一家大型人工智能公司（例如OpenAI）收购或建立了一个专注于推理的人工智能芯片公司。\n证据：据报道Sam Altman正在筹集大量资金来实现，而谷歌、亚马逊、Meta和微软等公司也在继续构建和改进他们自己的人工智能芯片。\n错误预测\n1. 有生成式人工智能媒体公司因其在2024年美国选举期间的滥用行为受到调查。\n解释：目前还没有，但让子弹飞一会。\n2. 自我改进的人工智能智能体在复杂环境中（例如AAA级游戏、工具使用、科学探索）超越了现有技术的最高水平。\n解释：还没有，尽管在开放性方面有一些比较有前景的工作，如在游戏上的强劲表现。\n3. 科技IPO市场解冻，至少看到一家以人工智能为重点的公司上市（例如DBRX）。\n解释：虽然the Magnificent Seven公司已经获得了强劲的增长，但私营公司仍在等待市场稳定；人工智能芯片公司Cerebras已经申请IPO。\n4. 在生成式人工智能的扩展热潮中，有一个团队花费超过10亿美元来训练一个大规模模型。\n解释：还没有完全达到，可以等明年再看看。\n5. 金融机构推出GPU债务基金，以取代风险投资股权美元用于计算资金。\n解释：还没有，一些风险投资基金传闻提供GPU以换取股权，但还没见过有人走上债务路线。\n参考资料：\nhttps://x.com/nathanbenaich/status/1844263448831758767\nhttps://www.stateof.ai/"
  },
  {
    "title": "LLM评测-腾讯云开发者社区-腾讯云",
    "page_body": "LLM评测的主要指标有哪些？\n 如何进行LLM的性能评测？\n LLM评测中常用的数据集有哪些？\n1. Open LLM Leaderboard（英文）\nOpen LLM Leaderboard 中包含有下列Benchmark：\n所有能力\n通用&数学&code—— MT-Bench ，由80个高质量的多轮对话问题组成的基准，旨在测试多轮对话和指令遵循能力，共包含8个常见的用户提示类别：writing 写作、roleplay 角色扮演、extraction 提取、reasoning 推理、math 数学、coding 编程、知识I（stem）和 知识II（humanities 人文/社会科学）。对于每个类别，手动设计了10个多轮的问题，每一轮有2个问题。 通用能力  通用&指令遵循—— IFEval  (Instruction-Following Evaluation) 是一个用于评估大型语言模型 (LLMs) 在遵循自然语言指令方面能力的基准测试。它通过一系列模拟真实世界应用的任务来测试模型的能力，包括回答问题、执行操作或根据特定指南生成内容。数据集包含 500 多个提示，每个提示包含一个或多个可验证的指令，例如“写一篇超过 800 字的文章”或“在响应中至少提到 3 次 AI 关键词”。  通用&推理—— BBH  (BIG-Bench Hard) 是 BIG-Bench 的一个子集，包含 23 个具有挑战性的任务，这些任务通常需要 多步骤推理 ，因此CoT或者PoT对它很有效。Big-Bench 目前包括 204 项任务，任务主题涉及语言学、儿童发展、数学、常识推理、生物学、物理学、社会偏见、 软件开发 等方面。  通用&推理—— MuSR  (Multistep Soft Reasoning) 是一个新的数据集，专门设计用于评估 LLMs 在需要多步骤、常识 推理 的任务上的能力，如谋杀悬疑案件。目的：测试 chain-of-thought 等技术在复杂推理场景中的极限表现。  通用—— GPQA  该数据集包含由生物学、物理学和化学领域的专家撰写的448道多项选择题。这些问题的难度非常高，即使是对应领域的专家解答正确率也只有65%，而在允许使用互联网的情况下，非专家验证者的平均正确率仅为34%。使用 GPT-4 作为基准的 AI 系统的正确率有39%。  通用—— MMLU  (Massive Multitask Language Understanding，大规模多任务语言理解，选择题)零样本/少样本。该基准涵盖了57个科目，包括STEM（科学、技术、工程、数学）、人文科学、社会科学等领域。它的难度从初级水平到高级专业水平不等，测试的内容既涵盖世界知识，又涉及问题解决能力。科目涵盖传统领域（如数学和历史）以及更专业的领域，如法律和伦理学。 \n数学能力\n数学—— MATH  数据集是一个包含12,500个具有挑战性的数学竞赛问题的数据集。每个问题都附有详细的逐步解决方案，这些解决方案可以用于训练模型生成答案推导和解释。\n2. 其他英文Benchmark\n通用能力\n通用—— TruthfulQA  包含817个问题，涵盖38个类别，包括健康、法律、金融和政治等领域。目的是测试语言模型的真实性，问题设计使得一些人可能会因错误信念或误解而给出虚假答案。任务类型包括生成任务：要求模型生成1-2句话的回答。和多项选择任务：包括单一正确答案(MC1)和多个正确答案(MC2)两种形式。\n数学能力\n数学—— GSM8K  是一个高质量的英文小学数学问题数据集，包含 7.5K 训练数据和 1K 测试数据。创建该数据集是为了支持对需要多步骤推理的基本数学问题进行问答的任务。这些问题通常需要2-8步才能解决，有效评估了数学与逻辑能力。\nCode能力\nHumanEval  是openai提出的用于评估模型根据给定提示(通常是文档字符串)生成功能正确代码的能力。它包含164个人工设计的 Python 编程挑战，每个挑战包括函数签名 function signature、文档字符串 docstring、函数体、和几个 单元测试 (平均每个问题7.7个测试)。该基准使用pass@k指标，即生成k个不同的解决方案，如果任何解决方案通过所有单元测试，则认为模型解决了该问题。因此HumanEval并不能捕捉代码模型潜在应用的所有方面，如代码解释、文档字符串生成、代码填充和编写测试等。\nRAG 能力\nMultiHop-RAG  评估RAG系统在跨文档检索和推理方面的能力。共有2556个query，每个query需要2～4个文档。\n3. 中文 Benchmark\n通用&数学&code—— OpenCompass  司南，会分中文榜单和英文榜单。是多个数据集的结合，划分了 LLM 的语言、知识、推理、数学、Code、Agent几类能力。  通用—— CMMLU  包含11,528个问题，涵盖67个主题，从基础学科到高级专业水平 主题包括自然科学、人文科学、社会科学和生活常识等，多为中国特定的问题和答案，体现中国文化和语言特色；也包括需要计算和推理的自然科学题目。每个问题是一个多项选择题，有4个选项，只有一个正确答案。  通用—— C-Eval  (Chinese Evaluation Suite) 是一个综合性的中文评估基准数据集，反映中国特定的知识和文化背景，包含13,948个多项选择题，涵盖中国语言文学、计算机网络、法律等52个不同学科，分为4个难度级别。每个学科包含开发集(dev)、验证集(val)和测试集(test)。开发集每个学科包含5个带解释的示例，用于少样本评估；验证集用于超参数调优；测试集用于模型评估，标签未公开。提供了chain-of-thought 提示模板。 \n4. 分析示例\n当前开源大模型中，Mixtral-8x22B-Instruct-v0.1在 MT-Bench（多轮会话&指令遵循）、MMLU（信息知识）中表现基本持平头部开源模型，并比GPT3.5略好，但在TruthfulQA（客观事实）上略逊于头部开源模型。Mixtral模型当前处于开源大模型第一梯队。 但通过Mixtral Demo体验，其支持语言主要为欧洲语系，且在回复质量上还是存在诸如指令遵循、信息冗余、misinfo等明显问题。"
  },
  {
    "title": "在nextionBI线上发布会上，传统企业展示了对于数据分析的深度理解_澎湃号·湃客_澎湃新闻-The Paper",
    "page_body": "原创 盛佳莹 猎云网\n启动“无代码+BI”的双产品战略布局。\n文丨猎云网 ID：ilieyun\n作者丨盛佳莹\n2022年2月25日，数睿数据正式发布下一代专注知识的BI产品——nextionBI，启动了自己“无代码+BI”的双产品战略布局。数睿数据总裁刘超总结，当前的BI产品过于依赖专业人员，但从根本上看是由于对于机器、知识和环境的依赖。\n表面上是BI产品需要专业的人去治理机器产生的脏数据，去梳理专业领域的知识，去完成知识在系统间的迁移。但本质是BI产品要求机器需要产出标准统一、质量上乘的基础数据，要求有大量可以直接录系统的行业知识，要求有一个统一的外部环境供知识在系统间自由分享。\n过度的依赖外部环境，反而限制BI产品自身的发展。下一代BI应该专注于从内部出发，主动去解决这些依赖，发展知识积累能力，主动优化融合现有数据资产，发展知识发现能力，能够快速总结归纳行业知识，发展知识传播能力，能够便捷打包成果对外输出。\n而早在之前，传统行业也发现了现有的BI产品只能帮助他们看清现在，但无法看到未来，没有办法对发现的异常，从根本上给出措施建议开始探索通过新的视角重新审视业务痼疾，寻求解决之道。\nnextionBI也在发布会上分享了和一些传统企业共谋数字化解决方案的真实故事，这些企业试图解决所在行业的共性问题，基于传统手段已经很难取得显著的突破。而通过与数睿数据这样的数字科技企业的合作，探索问题的另一种解法，将逐渐成为所在行业技术创新的主流。\n发布会上首先分享的是一家纸材料生产制造企业，主要生产白卡纸。白卡纸属于工业用纸，采用100%漂白化学制浆抄制，具有坚挺厚实、平滑度高的特点，一般定量在210g-400g/㎡。白卡纸的产品类型主要分为纸盒、卡纸、纸袋，分别占比60%、20%、10%；下游主要应用于各类精细产品包装，如烟酒、化妆品、日用品、药品、食品、3C等日常消费产品包装。白卡纸的下游应用以必需消费品包装为主，终端需求持续稳定增长。\n来源：企业供图\n从中国造纸协会发布的《中国造纸工业2020年度报告》中数据来看，2020 年包装用纸生产量 705 万吨，较上年增长 1.44％；消费量 718 万吨，较上年增长 2.72％，是2020年纸产品中少数消费增长高于产量增长的品类。\n白卡纸当前已呈现寡头垄断格局，以金光纸业、博汇纸业、晨鸣纸业、万国太阳为代表的寡头合计产能占比高达79%。金光、博汇和晨鸣的白卡纸年产能均达到200万吨以上，其中，金光纸业旗下的白卡纸生产企业包括亚洲浆纸业（150万吨/年）、广西金桂（100万吨/年）、宁波中华（62万吨/年），分布在浙江、广西等地；博汇纸业的白卡纸生产规模仅次于金光，产能主要分布在山东和江苏，合计年产能215万吨/年；晨鸣的产能主要分布在广东、山东、江西等地，合计年产能200万吨/年。\n会上分享的企业作为现有的寡头之一，希望在良好的市场环境下，保持现有的竞争优势，除了营销工作之外，产品质量的稳定性是一个重要的基础。对于白卡纸来说，纸张的克重是衡量产品质量的重要指标。每个批次的纸张克重偏差越小，产品的质量稳定度也就越高。\n但该企业的研发部门统计数据之后发现，目前白卡纸产品存在克重偏差较大的质量稳定性问题。由于克重不稳定，为了保证整体良品率，就不得不增加每个批次生产过程中的原料使用，从而保证即使偏差较大，但多数产品仍能够满足质量要求。但这样做显然会提高企业的生产成本，且整体的产品质量参差不齐，极大的损害了产品口碑，优化生产过程刻不容缓。\n在接触nextionBI之前，该企业也早就开始针对这一问题探索解决方案，但是遇到几个现实的困难。首先，造纸过程相关的数据量极大，种类繁多。有的来自于自动化设备，有的来自于软件系统，有的靠人员手工记录，还有部分非结构化数据。这其中还包含大量的脏数据，存在诸如标准不统一、空值比例高、数据乱码等问题。在数据分析之前需要先完成复杂的数据清洗、数据标准化、数据关联等工作。企业研发部的人员虽然了解纸张生产数据的正确格式、知道不同字段的标准度量、能够将编码代号翻译成文本，但是他们大多不懂SQL语句，无法完成批量的数据加工处理工作。其次，经过材料研发人员对于大量历史数据的整理，生产制造过程中影响纸张克重的因素超过140种。但是具体是哪个因素对于克重有影响，是正相关还是负相关，这都需要一一测试。按照控制变量法的理论要求，至少要测试140次，每次都需要设计实验，调整参数，统计结果，这显然不具备可操作性。最后，难以定位影响因素对最终质量的影响值，不知道如何进行优化，针对某个因素具体优化值是多少？明确相关性只是第一步，下一步需要明确是线性相关还是指数相关，要有具体的相关函数才能形成量化可落地的操作建议。\n对于数字化生产，企业认为不能简单的只是通过大屏展示生产结果数据现状，然后靠老师傅的经验决定优化方式，再通过一段时间的数据反馈判断优化效果，这种模式周期长，风险高。合理的方式应该是基于生产过程的数据分析，给出精准的操作建议，再通过生产测试进行验证，敲定生产优化措施。这就需要用到nextionBI的数据融合能力与增强分析中的主因子分析和根因定位能力。\n来源：企业供图\n针对第一个痛点，nextionBI对接生产数据，通过强大的数据交换机能力，完成复杂的数据处理流程，实现接入数据的清洗加工，排除异常数据对于分析结果的影响。这个过程不用写任何技术代码，一个纸材料的研发人员经过简单的培训，自己就可以通过拖拽数据加工算子的方式，配置加工流程，完成大量原始数据的加工处理。如上图所示是将三个来自不同系统的数据表经过加工处理，最终融合形成了一个数据资产。整个过程清晰可见，每个步骤对应的模块都可以动态调整，大大提升了数据处理过程的灵活度。这样就解决了数据分析基础素材的问题，为后续的分析打好了基础。\n来源：企业供图\n针对第二个痛点，利用nextionBI增强分析中的主因子分析能力，帮助客户从140多个影响因子中，快速定位影响纸张克重的主要影响因子，并提供主因子对结果的影响分析，并提供每个因子的影响值分析。如上图所示，影响因子的影响值排名从高到低分别显示的是2号浆液流量、车速、1号浆液流量、冲浆流量这四个因子，并且给出了具体的影响值，这就为下一步的分析工作提供了量化的参考依据。\n针对第三个痛点，采用根因分析，通过根因定位分析能力，对影响纸张克重的可能进行根因组合，列出组合排名。提取主因子，将每种组合的具体值列出，提供系数调整优化值建议，直接提供量化的指导。\n来源：企业供图\n如上图中所示，为不同因子分配不同的参数选择，形成多种组合。针对不同的组合分析其可能性的占比。比如图中的组合三就代表了2号纸浆流量小于等于14.25，1号纸浆流量小于等于32.05，车速小于 33.156这种参数组合形式，但是这种组合方式相较于组合一的可能性占比是偏低的，为了达到优化目标，最终应该选择组合一对应的参数设计。基于这些分析，进行了多次实验，研发部门给出了具体操作建议，帮助生产部门快速改善生产过程，稳定生产质量。\n企业按照准备数据，确定方向，量化措施的过程，一步一步有条不紊的完成之前一直困扰他们的产品质量稳定性问题。nextionBI作为新发布的下一代BI的代表，其强大的数据融合处理能力、增强分析能力都很好地帮助企业完成了这次优化生产过程的目标。下一代BI的新能力，给企业提供了解决问题的新手段和方向。\n（首图来源：图虫）\n原标题：《在nextionBI线上发布会上，传统企业展示了对于数据分析的深度理解》"
  },
  {
    "title": "AI读论文新神器：多栏密集文字、中英图文混排文档都能读的多模态大模型Fox｜旷视_页面_视觉_团队",
    "page_body": "Fox团队 投稿\n量子位 | 公众号 QbitAI \n虽然 多模态大模型都能挑西瓜了，但 理解复杂文档 方面还是差点意思。\n面对 文字密集、多栏混排 等文档时往往力不从心，区域级别的细粒度理解，就更是无从谈起了。\n最近，旷视团队打造了一支多模态大模型的“点读笔”—— Fox ，轻松实现对8页文档 （中英混合，单栏多栏格式混合的极端场景） 的交互式感知理解。\n对于信息密集的PDF文档，Fox支持高可控性的细粒度理解，比如在用户感兴趣区域内进行文字识别、段落翻译以及页面内部的图片内容描述等。\n论文中，团队进一步突破了对于文档的视觉感知理解的上限，高密度的信息被真正压缩，LVLM真正地“看”懂图，才能真正做好、做出能用的文档多模大模型。\n正所谓 “一图胜千言”—— one image token >> one text token 。\n接下来，看看Fox在实战中表现如何？\n中英混排，单栏多栏组合都不怕 \n对于中英混合、单栏多栏混合的8页PDF文档，可实现任意区域的OCR：\n下图左侧展示了8页文档内跨页的VQA，右侧展示了双栏中文页面的前景OCR。\n双栏密集英文页面的前景OCR：\n在页面内图片描述方面，Fox能给出文档内内容关联的回答（young Dual Language Learners）。\n当然Fox还支持line-level OCR，以及对RoI区域的翻译、总结等。\nFox可以结合页面内文字，认识到这是一张关于global seismic hazards的图。此外，Fox还支持RoI内的latex格式转换，例如下面的table转latex。Fox还支持更加灵活的颜色引导的RoI区域OCR。\n对于卡通绘本，也可以哪里不会点哪里：\n电影海报和自然场景的对话问答，Fox给出了非常有趣的答案（根据电影海报下面的文字给出了角色来源）：\n那么Fox是如何做到这些的呢？\n多词表协同，多页面文档统一打包 \n在细粒度文档理解上，Fox有着三大创新：\n精准定位\nFox引入了一系列基于位置的文本提示，如点击位置、拖动框、涂色框等。这使得模型可以直接定位到感兴趣的任意区域，而不受文档格式的限制。同时，Fox还把全页OCR重新定义为”前景聚焦”任务，进一步增强了对密集文字的感知。\n多视觉词表协同\n为了更好地理解图文混排页面，Fox采用了两个不同特长的视觉词表——CLIP主攻自然图像，Vary专攻人工文档。但单纯叠加两种数据，往往会造成视觉偏置。为此，Fox合成了大量含混合视觉元素的数据，迫使两个视觉分支充分协作。 \n页面打包\n得益于高压缩率(每页1024×1024图像对应256个图像token)，Fox将多页面文档统一打包输入。这不仅让跨页面的上下文理解成为可能，也大幅降低了计算开销。值得一提的是，这种打包微调模式并不需要重新训练视觉词汇。\n在这些创新基础上，Fox模型结构如图所示。\nFox支持单页/多页文档图像输入，所有图像的image token被统一到一个sequence中进行多页文档理解。团队设计了基于point、color、box的prompt，来实现在文档页面上聚焦任意位置。团队合成了图文交织的文档数据，来充分催化两个视觉词表，以更好地适用于实际文档应用场景。\n此外，为了促进对文档细粒度理解的研究，作者还打造了一个中英双语的benchmark，已经开源了数据和评测代码，共包含以下9种任务：\nPage-level OCR Region-level OCR Line-level OCR Color-guided OCR Region-level translation Region-level summary In-document figure caption Multi-page multi-region OCR Cross-page VQA\n最后，团队呼吁更多的研究人员能关注到细粒度的单页/多页文档理解，单页的稀疏的问答任务远远不够。\n真正做好多模态大模型，视觉编码器的信息压缩率 （token转化率） 是非常重要的，Fox仅探究了文档这一类应用方向，希望对大家的研究有所帮助。\n想了解更多细节，请查看原论文。\n论文地址： https://arxiv.org/abs/2405.14295\n代码地址： https://github.com/ucaslcl/Fox\n项目主页： https://ucaslcl.github.io/foxhome/\n— 完 —\n投稿请发邮件到：\nai@qbitai.com\n标题注明【投稿】，告诉我们：\n你是谁，从哪来，投稿内容 \n附上论文/项目主页链接，以及联系方式哦\n我们会（尽量）及时回复你\n点这里      关注我，记得标星哦～\n一键三连「分享」、「点赞」和「在看」\n科技前沿进展日日相见 ~"
  },
  {
    "title": "论文阅读：BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering-CSDN博客",
    "page_body": "背景\n 多跳 问答 （Multi-Hop  QA ）需要找到多个相关文本片段，并通过逐步推理来回答复杂问题。以往基于LLMs的多跳问答研究工作，从不同建模角度采用特定方法，但未考虑问题类型的差异。且多跳回答任务的复杂性可能会超过LLMs的知识边界，会导致模型生成的回答出现事实错误（即“幻觉”）。\n现有研究中基于LLMs的多跳回答方法分为两类，见图1：\n闭卷推理（Closed-book Reasoning） ：依托LLMs自身知识，借助思维链（CoT）等方式，通过模型生成响应时对各 token 概率分布采样，逐步推导多跳问题答案， 不依赖外部知识。模型知识存在局限，无法覆盖复杂问题所需的全部信息，易产生含事实错误的 “幻觉”。\n检索增强推理（Retrieval-augmented Reasoning） ：引入外部知识源辅助回答。早期单步检索策略难以收集到所需的全部知识，存在知识遗漏问题 。且多数方法通常 会忽略问题类型差异，采用固定的处理方式 ，给相对简单的问题带来额外的计算负担，还限制了其在对推理速度有较高要求的应用场景（如实时回答）中的使用 。\n图1\n问：“在专辑《Apple》发行前去世的‘Mother Love Bone’乐队成员，其之前所在的乐队是哪个？”\n ● Closed - book Reasoning（闭卷推理）：以（A）思维链为代表，不依赖外部知识，让模型逐步思考生成答案 。\n ● Retrieval - augmented Reasoning（检索增强推理）：\n ○ （B）Single - step（单步检索）：直接检索文档找答案 。\n ○ （C）Iterative - step（迭代步检索）：多次迭代检索，重复执行K次检索，整合每次检索的中间结果（Intermediate）生成答案 。\n ○ （D）Sub - step（子步骤检索）：把多跳问题拆分成子问题（1、2、3 ），分步检索生成子答案（A1、A2、A3 ）再整合答案 。\n ○ （E）Adaptive - step（自适应步检索）：用分类器（Classifier）适配不同问题类型选择检索策略 。（分类器自身的运行逻辑是固定的 ）\n关键问题\n多跳问答的复杂性：多跳问答任务需要模型能够处理多个相关段落，并执行逐步推理来回答复杂问题。 现有的LLMs在处理这类复杂问题时可能会产生事实错误，尤其是在缺乏外部知识支持的情况下 。\n现有方法的局限性：现有的多跳问答方法主要基于LLMs，分为 闭卷推理和检索增强推 理 两大类。然而，这些方法 通常采用固定的解决方案，不考虑问题类型的差异 ，导致在复杂问题上性能不足，且计算开销较大。\n动态组合各类算子的需求： 不同的多跳问题类型对推理方法的敏感度不同 。例如，某些问题类型可能更适合分解为子问题并逐步解决，而其他问题类型可能需要更多的外部知识支持。需要一种能够根据问题类型动态组合不同推理“算子”的方法，以提高性能并降低计算成本。\n解决方案\n 提出了一个名为 Bi-level muLtiagEnt reasoning (BELLE) 的框架，通过 引入一个双层多智能体辩论系统来动态选择和组合不同的推理“算子” （见图2） ，从而更有效地解决多跳问答任务。\n图2\n其框架主要包含以下三个模块：\n问题类型分类器（Question Type Classifier） ：采用上下文学习（In - Context Learning, ICL） 方式实现分类。使用LLM对用户输入的多跳问题进行分类，分别为：推理（Inference）、比较（Comparison）、时间（Temporal）和空（Null）。为后续推理策略选择提供依据。\n双级多智能体辩论（Bi-Level Multi-agent Debater）\n第一级辩论 ：参与者有正方辩论者（affirmative debater）和反方辩论者（negative debater）。利用元提示向辩论者介绍问题类型、辩论者数量、轮次限制等要求，并创建辩论氛围。辩论者从 算子池 中选择如思维链（CoT）、单步检索等算子进行辩论。正方和反方根据自己的辩论历史轮流提出论点。\n第二级辩论（评估） ：参与者有快速辩论者、慢速辩论者和法官。\n快速辩论者（Fast Debater） 评估当前双方选择的算子是否合理 ，综合考虑当前轮次正方、反方的讨论结果以及自己之前轮次的辩论历史。\n慢速辩论者（Slow Debater）则 整合所有历史信息，判断算子选择的合理性 ，防止辩论者对正确观点失去信心而导致观点振荡。\n最后由法官监督辩论过程，根据辩论情况判断是否进入下一轮辩论，最终 生成包含 “算子组合顺序” 与 “执行优先级” 的推理计划。\n多跳问答执行器（Multi-hop QA Executor） ：根据双级多智能体辩论得到的推理计划， 逐步调用相应的多跳算子 ，每个算子对应解决问题的一个子步骤。使用与辩论系统相同的 LLM 执行算子规划过程的子步骤。将各步骤输出的中间结果汇总，从而获得多跳问题的最终答案。\n论文中的 算子 类型：\n算子类型\n核心功能\n适用场景示例\n思维链\n仅依赖 LLM 内部知识，生成逐步推理链，无外部检索\n简单推理型问题（如 “2+3×4 的结果是多少？”）\n单步检索\n单次检索外部知识源（如维基百科），直接结合检索结果生成答案\n比较型 / 时间型问题（如 “北京 2023 年 GDP”）\n迭代步检索\n多轮迭代检索，将前一轮检索结果与子问题拼接，优化下一轮查询\n复杂推理型问题（如 “译者→大学→校训”）\n子步骤检索\n将原问题拆分为多个子问题，逐一处理后整合子答案\n多实体关联问题（如 “多角色人物关系推理”）\n自适应步检索\n动态调整检索轮次，若某轮检索结果已满足需求则停止迭代\n不确定性问题（如 “需先判断信息是否足够”）\n实验\n在该论文中，为验证 BELLE在多跳问答任务中的有效性，作者设计了系统性的对比实验、消融实验及分析实验，从性能、效率、泛化性等多维度验证框架优势。\n1. 实验设置\n1.1  数据集 选择\n为确保实验的全面性和泛化性，选取了 4 个主流多跳问答数据集 ，覆盖不同问题类型、知识领域和难度，具体信息如下表：\n数据集\n问题类型标签情况\nMultiHop-RAG\n自带标签\n2WikiMultiHopQA\n无标签（需人工 / GPT-4 标注）\nHotPotQA\n无标签（需人工 / GPT-4 标注）\nMuSiQue\n无标签（需人工 / GPT-4 标注）\n注：对于无标签的 3 个数据集，使用 GPT-4 标注一半数据（标注准确率经人工验证达 95%），并采用 “交叉验证” 确保标注一致性。（所有数据集均按 “8:2” 划分为训练集（仅用于基线模型微调，BELLE 因采用 ICL（上下文学习） 无需训练）与测试集。）\n1.2 基线模型与参数配置\n选取 3 类代表性基线方法 ，覆盖 “闭卷 推理 ”“检索增强”“多智能体协作” 等主流多跳问答思路，确保对比公平性，见图3。BELLE 框架的参数配置与基线保持一致（基础模型、检索器、token 窗口等），仅差异化设置 “辩论角色数量”（5 个）与 “算子选择策略”（动态适配），避免因参数差异影响结果对比。\n1.3 评估指标\n精确匹配率（Exact Match, EM）：模型答案与标准答案完全一致的比例（适用于事实性答案）；\nF1：模型答案与标准答案的字符级匹配度（平衡精确率和召回率），越高代表答案质量越好；\nAcc（Accuracy）：准确率，判断答案是否正确的比例；\n2hop/3hop/4hop（仅 MuSiQue 数据集）：按 “推理跳数”（需关联的知识步数）分类的 F1 分数，跳数越高任务越难。\n2. 对比实验结果与分析\n图3\n主要发现：\nBELLE在所有四个数据集上均 显著优于基线方法 。 在更复杂的多跳问题上（如MuSiQue数据集），BELLE的性能提升更为显著。 检索增强方法在需要外部知识的复杂多跳问题上表现更好，而封闭式推理方法在简单问题上也有一定的效果。\n2.1 消融实验（以 F1 分数为核心指标）\n去除问题类型分类器 ：不将问题类型作为先验知识输入到多智能体辩论系统中。\n去除第一层辩论 ：直接使用LLM进行推理，而不进行多智能体辩论。其观点直接由第二级辩论优化。\n去除第二层辩论 ：系统降级为与问题类型相关的基本多智能体辩论系统，不包含历史讨论信息的利用。\n图4\n实验结论：\n移除 第二级辩论者对模型性能的影响最 大（Second Level Debate） ，第二级辩论能够利用辩论历史给出合理的算子选择建议。\n将问题类型作为先验知识引入多智能体辩论系统，对于组合算子的选择至关重要。\n2.2 计算开销分析\n与检索方法的对比\n分析了BELLE在计算开销方面的表现，特别是与现有的检索增强方法相比。\n主要发现：\nBELLE在提高性能的同时，显著降低了计算开销。通过合理利用历史讨论信息，减少了不必要的检索和推理步骤。\n与现有的检索增强方法相比，BELLE在复杂多跳问题上表现更好，且计算成本更低。\n与基于智能体的设置对比\n不同辩论层级：设置不同的辩论氛围，如“必须达成一致”、“大部分时间不同意”等。 不同 智能体 数量：增加每层的智能体数量，分析其对性能和计算开销的影响。 实验结果表明：\n辩论层级：设置为“不需要完全达成一致”的层级（默认层级2）表现最好。\n智能体数量：增加智能体数量会降低性能，且增加计算开销。因此，选择每层2个智能体的设置最为合理。\n不同问题类型标注方法的比较\n论文还比较了不同问题类型标注方法对性能的影响，包括：\n上下文学习（ICL）：通过上下文示例进行标注。\n监督微调（SFT）：使用标注数据进行微调。\n零样本提示（Zero-shot）：仅使用指令和标签空间进行提示。\n通过这些实验，论文 验证了BELLE框架在多跳问答任务中的有效性和计算效率，特别是在处理复杂问题时的优势。\n3. 算子选择的变化\n 以 HotPotQA 数据集中的推理型问题为例，探究了第一级与第二级辩论者的辩论内容对算子选择的影响。具体而言，针对两级中的四位重要辩论者，需考虑两种情况：\n（1）同一辩论轮次中，第一级辩论者（正方、反方）对第二级辩论者（快速、慢速）的影响；\n（2）不同辩论轮次中，上一轮第二级辩论者对当前轮第一级辩论者的影响。\n其中， 表示情况（1）的得分， 表示情况（2）的得分。\n每个得分均为一个 矩阵（见图5），代表 5 种算子之间的组合得分。\n、 、 、 分别表示四位辩论者在第 t 轮的得分。考虑到情况（1）中第一级辩论者讨论的内容为后续讨论提供信息，其重要性更高，将 α 和 β 均设为 0.8。每位辩论者的具体得分（如 ） 基于两种算子之间的观点相似度计算 ，我们使用 GPT-4对辩论者的输出内容与由两种算子构成的模板内容进行打分。\n公式一： 第一级辩论 → 第二级辩论的观点融合\n公式 2： 上一轮第二级辩论 → 当前轮第一级辩论的观点传递\n图5\n如图5中，从 0.25（浅） 到 1.00（深） ，颜色越深代表算子选择的共识度、明确度越高（即模型对 “选哪些算子、怎么组合”"
  },
  {
    "title": "表单中的结构化数据怎么添加？如何标记表单信息？html教程-PHP中文网",
    "page_body": "表单中的结构化数据是通过Schema.org标记（如itemprop、itemscope、itemtype）明确告知搜索引擎表单用途及字段含义，提升页面语义理解，助力SEO优化，常见于联系表单、搜索表单和事件报名表单，需避免错误标记、内容不一致及忽略测试等问题。\n表单中的结构化数据，说白了，就是通过特定的标记语言（最常用的是Schema.org）告诉搜索引擎，你这个表单是干嘛的，里面收集或者展示的信息是什么类型。你直接在HTML代码里，利用\n复制 AI写代码\n1\nitemprop\n、\n复制 AI写代码\n1\nitemscope\n和\n复制 AI写代码\n1\nitemtype\n这些属性来定义和关联表单元素，就能让搜索引擎更好地理解你的页面内容，甚至在搜索结果中展示更丰富的信息。\n解决方案\n要给表单添加结构化数据，核心思路是识别表单的整体目的，然后将表单内的各个输入字段与相应的Schema.org属性关联起来。这通常涉及到选择一个合适的Schema类型来描述整个页面或表单区域，然后用更具体的属性来标记表单里的每个字段。\n举个例子，如果你的页面是一个联系我们页面，上面有一个联系表单：\n确定页面或区域类型：  整个页面可能是\n复制 AI写代码\n1\nContactPage\n登录后复制\n。如果你想强调这是一个本地商家，也可以用\n复制 AI写代码\n1\nLocalBusiness\n登录后复制\n。 定义表单区域：  可以在包含表单的\n复制 AI写代码\n1\n<div>\n登录后复制\n或\n复制 AI写代码\n1\n<form>\n登录后复制\n标签上使用\n复制 AI写代码\n1\nitemscope\n登录后复制\n和\n复制 AI写代码\n1\nitemtype\n登录后复制\n来定义一个更具体的Schema类型，比如如果表单是用来提交一个查询的，理论上可以考虑\n复制 AI写代码\n1\nAction\n登录后复制\n或更具体的\n复制 AI写代码\n1\nCommunicateAction\n登录后复制\n，但实际上，更常见的是将表单字段直接关联到页面或父级Schema的属性。 标记表单字段：  对每个\n复制 AI写代码\n1\n<input>\n登录后复制\n、\n复制 AI写代码\n1\n<textarea>\n登录后复制\n或\n复制 AI写代码\n1\n<select>\n登录后复制\n标签，使用\n复制 AI写代码\n1\nitemprop\n登录后复制\n来指定它代表什么信息。\n一个简单的联系表单示例：\n复制 AI写代码\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n< div itemscope  itemtype = \"http://schema.org/ContactPage\" >\n< h1 >联系我们</ h1 >\n< p >请填写以下表格与我们取得联系。</ p >\n< form action = \"/submit-contact\" method = \"post\" >\n< label for = \"name\" >您的姓名:</ label >\n< input type = \"text\" id = \"name\" name = \"name\" itemprop = \"name\" required>\n< label for = \"email\" >您的邮箱:</ label >\n< input type = \"email\" id = \"email\" name = \"email\" itemprop = \"email\" required>\n< label for = \"phone\" >您的电话 (可选):</ label >\n< input type = \"tel\" id = \"phone\" name = \"phone\" itemprop = \"telephone\" >\n< label for = \"message\" >您的留言:</ label >\n< textarea id = \"message\" name = \"message\" itemprop = \"description\" rows = \"5\" required></ textarea >\n< button type = \"submit\" >发送消息</ button >\n</ form >\n</ div >\n在这个例子里，\n作为页面的主要类型，而表单内的\n、\n复制 AI写代码\n1\nemail\n登录后复制\n、\n复制 AI写代码\n1\ntelephone\n登录后复制\n和\n复制 AI写代码\n1\ndescription\n登录后复制\n（用于留言）则直接关联到了这个页面的属性。虽然Schema.org没有一个直接的\n复制 AI写代码\n1\nForm\n登录后复制\n类型来包罗万象，但通过这种方式，我们让搜索引擎理解了这些输入框代表的具体信息。对于更复杂的表单，比如搜索表单，则会用到\n复制 AI写代码\n1\nSearchAction\n登录后复制\n。\n为什么 要在表单中添加结构化数据？它对SEO有什么帮助？\n说实话，很多人觉得表单这东西，用户能用就行了，干嘛还要搞什么结构化数据？但从SEO的角度看，这事儿真不只是锦上添花。\n首先，最直接的好处是 提升搜索引擎的理解力 。你想啊，一个普通的HTML表单，对搜索引擎来说就是一堆\n复制 AI写代码\n1\n<input>\n登录后复制\n标签，它不知道这是要收集姓名、 邮箱 还是电话。但你加上\n复制 AI写代码\n1\nitemprop= \"name\"\n登录后复制\n，它立马就明白了，哦，这个输入框是用来填名字的。这种深度的语义理解，能让搜索引擎更准确地判断你页面的主题和功能。\n其次，这有助于 潜在的富文本结果（Rich Snippets） 。虽然表单本身不常直接生成富文本，但如果你的表单是某个更大Schema（比如一个活动的报名表单属于\n复制 AI写代码\n1\nEvent\n登录后复制\n，一个职位申请表单属于\n复制 AI写代码\n1\nJobPosting\n登录后复制\n）的一部分，那么整个页面的富文本展示可能会因此更完整、更准确。比如，一个带有\n复制 AI写代码\n1\nSearchAction\n登录后复制\n标记的搜索表单，理论上甚至可能让Google在搜索结果中直接显示你的站内搜索框，这不就省了用户一步吗？\n再者，这是在 构建网站的语义图谱 。你给表单数据加上结构化标记，就像是在给你的网站内容打标签、做分类。这不仅仅是告诉搜索引擎“我这里有个表单”，更是告诉它“我这个页面提供一个联系方式，用户可以在这里留下他们的姓名和邮箱”。这种细致的语义信息，有助于搜索引擎更好地将你的网站与用户的搜索意图匹配起来，尤其是在处理长尾关键词和复杂查询时。在我看来，这不仅仅是为了排名，更是为了让你的网站在数字世界里“说话”更清楚，更有逻辑。\n哪些常见的表单类型适合添加结构化数据？具体如何标记？\n不是所有表单都非得加结构化数据，但有些类型的表单，加上了确实能事半功倍。主要是那些功能性强、信息明确的表单。\n联系表单 (Contact Forms):\n适合场景:  任何提供联系方式的页面。 如何标记:  通常将整个页面标记为\n复制 AI写代码\n1\nContactPage\n登录后复制\n。表单内的字段可以关联到页面的属性。\n复制 AI写代码\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n< div itemscope  itemtype = \"http://schema.org/ContactPage\" >\n<!-- ... 页面内容 ... -->\n< form >\n< input type = \"text\" name = \"name\" itemprop = \"name\" placeholder = \"您的姓名\" >\n< input type = \"email\" name = \"email\" itemprop = \"email\" placeholder = \"您的邮箱\" >\n< input type = \"tel\" name = \"phone\" itemprop = \"telephone\" placeholder = \"您的电话 (可选)\" >\n< textarea name = \"message\" itemprop = \"description\" placeholder = \"您的留言\" ></ textarea >\n<!-- ... -->\n</ form >\n</ div >\n登录后复制\n思考:  这里的\n复制 AI写代码\n1\nitemprop\n登录后复制\n直接关联到\n复制 AI写代码\n1\nContactPage\n的属性，表明这个页面（\n复制 AI写代码\n1\nContactPage\n登录后复制\n）的“名字”、“邮箱”等信息是通过表单收集的。\n站内搜索表单 (Internal Search Forms):\n表单大师AI\n一款基于自然语言处理技术的智能在线表单创建工具，可以帮助用户快速、高效地生成各类专业表单。\n74\n查看详情\n适合场景:  网站的搜索功能。 如何标记:  这通常涉及到\n复制 AI写代码\n1\nWebSite\n登录后复制\n和\n复制 AI写代码\n1\nSearchAction\n登录后复制\n Schema。\n复制 AI写代码\n1\n2\n3\n4\n5\n6\n7\n8\n< div itemscope  itemtype = \"http://schema.org/WebSite\" >\n< meta itemprop = \"url\" content = \"https://www.yourwebsite.com/\" >\n< form itemprop = \"potentialAction\" itemscope  itemtype = \"http://schema.org/SearchAction\" action = \"https://www.yourwebsite.com/search\" >\n< meta itemprop = \"target\" content = \"https://www.yourwebsite.com/search?q={search_term_string}\" >\n< input itemprop = \"query-input\" type = \"text\" name = \"q\" placeholder = \"搜索...\" >\n< input type = \"submit\" value = \"搜索\" >\n</ form >\n</ div >\n登录后复制\n思考:\n复制 AI写代码\n1\npotentialAction\n登录后复制\n表明这个网站有一个潜在的搜索动作，\n复制 AI写代码\n1\ntarget\n登录后复制\n定义了搜索结果页的URL模式，\n复制 AI写代码\n1\nquery-input\n登录后复制\n则指明了用户输入的查询词将放在哪个参数里。这是最可能直接在Google搜索结果中展示搜索框的Schema。\n事件报名/预订表单 (Event Registration/Booking Forms):\n适合场景:  任何需要 用户注册 或预订活动、服务、票务的页面。\n如何标记:  表单本身不直接是\n复制 AI写代码\n1\nEvent\n登录后复制\n，但它通常是\n复制 AI写代码\n1\nEvent\n登录后复制\n Schema的一部分。\n复制 AI写代码\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n< div itemscope  itemtype = \"http://schema.org/Event\" >\n< h1 itemprop = \"name\" >我的精彩活动</ h1 >\n< meta itemprop = \"startDate\" content = \"2023-12-25T19:00\" >\n< div itemprop = \"location\" itemscope  itemtype = \"http://schema.org/Place\" >\n< span itemprop = \"name\" >活动地点</ span >\n< div itemprop = \"address\" itemscope  itemtype = \"http://schema.org/PostalAddress\" >\n< span itemprop = \"streetAddress\" >XX大街123号</ span >\n< span itemprop = \"addressLocality\" >城市</ span >\n< span itemprop = \"addressRegion\" >省份</ span >\n< span itemprop = \"postalCode\" >123456</ span >\n</ div >\n</ div >\n<!-- ... 其他活动详情 ... -->\n< h2 >报名表单</ h2 >\n< form action = \"/register-event\" method = \"post\" >\n< label for = \"regName\" >姓名:</ label >\n< input type = \"text\" id = \"regName\" name = \"regName\" itemprop = \"attendee\" itemscope  itemtype = \"http://schema.org/Person\" >\n< meta itemprop = \"name\" content = \"[用户输入的姓名，这里需要JS或后端处理]\" >\n<!-- 这里的attendee标记复杂，通常通过后端填充 -->\n< label for = \"regEmail\" >邮箱:</ label >\n< input type = \"email\" id = \"regEmail\" name = \"regEmail\" itemprop = \"email\" >\n<!-- 注意：这里的email是Event的属性，表示联系邮箱，不是attendee的"
  },
  {
    "title": "多模态电路设计新基准：全面评估LLM性能",
    "page_body": "翰林侍读在此！给陛下奉上今日AI领域的论文速报！\n     右滑查看士子奏报和论文重要图表——\n    ‍     关注微臣，每天批阅更多奏章！\n     论文标题：MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs\n     Arxiv ID：2507.19525v1\n     关键词：MLLMs, EDA, MMCircuitEval, benchmark, circuit design\n     太长不看版：MMCircuitEval是首个专为全面评估多模态大语言模型在电子设计自动化任务中性能的基准，涵盖3614个经专家审核的问答对，揭示现有模型在后台设计和复杂计算方面的显著差距，推动其在实际电路设计中的应用。\n     摘要翻译：多模态大型语言模型（MLLMs）的出现为电子设计自动化（EDA）的自动化和增强提供了有前景的机会。然而，由于现有基准测试范围的局限性，全面评估这些模型在电路设计中的应用仍然具有挑战性。为了弥补这一差距，我们推出了MMCircuitEval，这是首个专门设计用于全面评估MLLM在各种EDA任务中性能的多模态基准测试。MMCircuitEval包含3614个精心挑选的问答（QA）对，涵盖数字和模拟电路的关键EDA阶段——从一般知识和规格到前端和后端设计。这些问答对来源于教科书、技术题库、数据表和真实世界的文档，每个问答对都经过严格的专家评审以确保准确性和相关性。我们的基准测试独特地将问题按设计阶段、电路类型、测试能力（知识、理解、推理、计算）和难度级别进行分类，从而能够详细分析模型的能力和局限性。广泛的评估揭示了现有LLM之间显著的性能差距，特别是在后端设计和复杂计算方面，凸显了针对训练数据集和建模方法的迫切需求。MMCircuitEval为在EDA领域推进MLLM提供了基础资源，促进了它们整合到真实世界的电路设计工作流程中。\n     本帖由AI整理生成，若有错误欢迎朱批！\n         求一个关注，让我更有动力为大家整理更多的AI领域的新文章！\n#LLM[话题]# #NLP[话题]# #AI[话题]# #大模型[话题]# #人工智能[话题]# #论文阅读[话题]#"
  },
  {
    "title": "代码星辰：探索 benchmark 构建的奥秘之旅-CSDN博客",
    "page_body": "当夜幕降临， 程序员 们仰望着闪烁的屏幕，仿佛看见了无尽星辰。代码世界也是如此，每一行代码，每一个测试样例，都像是天际间的星辰耀动。而在这片“代码星海”中，如何评估大语言 模型 （LLMs）在编程任务上的表现，就像天文学家借助 望远镜 探索宇宙奥秘一般关键。近日，一篇题为《How Should I Build A Benchmark? Revisiting Code-Related Benchmarks For LLMs》的研究论文，为我们揭开了代码相关 benchmark 的构建秘密，描绘了一条贯穿设计、构造、评估、分析与发布全过程的完整蓝图。本篇文章便带你通过这场代码领域的星际探险，深入了解如何构建高质量、可靠而透明的 benchmark，以及这其中那些令人拍案叫绝、细思极恐的细节。\n     前言：代码世界的星辰大海\n正如19世纪科学巨匠开拓自然定律时所言：“如果你不能测量它，你就无法改进它。”在当今 AI 与大 语言模型 飞速发展的时代，benchmark 的作用愈发重要。然而，随着越来越多的代码相关 benchmark 出现，也暴露出大量亟待解决的问题：诸如数据质量不足、无法完全复现、 开源 不全、引用存在漏洞等等。正因如此，HOW2BENCH 这套包含 55 项详细指标的指南应运而生，指导着研究者如何从设计到发布，全面严谨地构建 benchmark。通过对 274 个 benchmark 的细致统计与分析，研究者们不仅揭示了存在的问题，更为未来 benchmark 的开发指明了方向，从而为代码评估研究提供了可靠、透明的参考标准。\n在本文中，我们将以小说般的叙述方式，带你走进 benchmark 构建背后的故事，穿越层层关卡，探寻那隐藏在代码背后的规律和智慧。"
  },
  {
    "title": "科学界论文高引第一人易主！AI站上历史巅峰-360doc个人图书馆",
    "page_body": "魔镜魔镜，谁是有史以来被引用次数最多的科学家？\n答案： 深度学习三巨头之一、图灵奖得主Yoshua Bengio 。\n打开今日头条查看图片详情\n如你所见，之所以提出这个问题，其实是因为相关消息正在引起热议ing。\n并且这一次，Bengio的“最高引”头衔不仅限于计算机领域，而是“称霸”所有学科，属于 “各领域被引用次数最多的在世科学家” 。\n在这之前，早在2018年，Bengio就是世界计算机研究者中单日引用次数最高的人 （同一年获图灵奖） ，2022年还一举成为世界上被引用次数最多的计算机科学家。\n其贡献最大的几篇论文《一种神经概率语言模型》（发表于2003年）、《Generative adversarial nets》（发表于2014年的GAN）、《Deep learning》（发表于2015年）全都为深度学习领域奠定了重要基础，深刻影响着如今大火的自然语言处理、计算机视觉等研究。\n打开今日头条查看图片详情\n而在网友们的讨论中，热议背后更深层的意义也逐渐明晰：AI的胜利。\nBengio改变了人工智能，其对深度学习的贡献真正塑造了现代人工智能研究。\n打开今日头条查看图片详情\n所以，借此机会，我们不妨再来回顾一下Bengio是谁，他又如何改变了人工智能——\n“影响了一代人”的深度学习先驱\nYoshua Bengio，1964年出生于法国巴黎，后全家移居加拿大。\n关于成长早期，Bengio曾自述为“典型的书呆子”，高中无聊时经常独自在图书馆看书。\n十几岁的时候，他终于和弟弟 （Samy Bengio，现任苹果公司AI与机器学习研究高级总监） 通过送报纸攒够了买电脑的钱——当时买了Atari 800和Apple II个人电脑。\n或许是因为这进一步点燃了他对电脑的兴趣，1986~1991年，他在麦吉尔大学计算机科学系一路读完了本硕博。\n注意， 其博士导师正是Geoffrey Hinton ，在受Hinton启发开始对“什么是智力？”这一问题产生兴趣后，他选择研究语音识别，并开始从经典统计模型转向神经网络。\n后来，他又分别去了MIT和大名鼎鼎的贝尔实验室从事科学研究，也是在贝尔实验室， 他开始与Yann LeCun合作 ，将其博士论文中的技术应用于实际问题。\n至此深度学习三巨头（Bengio、Hinton、LeCun）算是结成了一张网。\n直到1993年，他被聘为蒙特利尔大学助理教授，之后陆续晋升为副教授、教授，至今仍为该校教授，担任蒙特利尔学习算法研究所 (MILA) 所长，并担任加拿大高等研究院机器与脑机学习项目的联合主任。\n打开今日头条查看图片详情\n而除了以上求学和职业经历，Bengio之所以为人熟知还是因为他在 深度学习 方面的贡献。\n2018年，三巨头共同拿下了计算机领域的国际最高奖项——图灵奖，理由是他们在深度神经网络概念和工程上的突破，使得DNN成为计算的一个重要构成。\n具体到Bengio，ACM细数了其三大重要贡献： 列的概率建模、高维词嵌入与注意力机制、生成对抗网络 。\n这里就不得不提到Bengio发表或参与发表的几篇重要论文（总引95万+）。\n2000年，他撰写了具有里程碑意义的论文《一种神经概率语言模型》，提出用神经网络解决语言建模中的“维度灾难（curse of dimensionality）”问题，为后来的大语言模型（如GPT系列）奠定基础。\n打开今日头条查看图片详情\n2014年，其博士生Ian Goodfellow发明了生成对抗网络GAN (Bengio为共同作者） ，这是首次提出生成对抗网络的论文，也是Bengio目前引用量最高的一篇。\n排在第二的就是三巨头2015年共同发表的《Deep learning》，这篇论文不仅系统总结了深度学习的发展历程，还深入剖析了其理论基础、核心算法和广泛应用，被视为深度学习领域的“圣经”。\n再加上后来的几篇，可以说，Bengio在深度学习、循环神经网络、注意力机制、GAN等方面均做出了开创性贡献，推动了机器翻译、自然语言处理、计算机视觉等领域的巨大进步。\n打开今日头条查看图片详情\n也正是因为这些研究，Bengio被公认为人工智能专家和深度学习先驱，堪称影响了一代人。\n打开今日头条查看图片详情\n不过最近一阵，Bengio已经官宣再次创业了——\n今年6月，他成立了非营利组织LawZero，要构建下一代AI系统，而且明确不做Agent （官宣时透露已筹集到3000万美元启动资金) 。\nBengio表示，当前AI系统已显现出自我保护和欺骗行为的迹象，随着其能力和自主性的提升，这种趋势只会加速，LawZero是他们针对这些挑战所给出的建设性回应。\n它以理解学习世界为核心目标，而非在世界中采取行动，通过透明化外部推理，对问题提供可验证的真实答案，“可用于加速科学发现、为Agent型AI系统提供监督，并深化大家对AI风险及其规避方法的理解”。\n打开今日头条查看图片详情\n何恺明、Ilya都上榜了\n说完Bengio，人们自然而然地好奇——\n老二是谁呢？谁将有希望超越Bengio？\n好嘛，还是我们的老熟人 Geoffrey Hinton ，其论文总引用量目前接近94万，和Bengio差距不大。\n打开今日头条查看图片详情\n而且扒一扒AD Scientific Index网站发布的完整名单，光是TOP 10就有不少大家熟悉的人。\n这里补充一下AD Scientific Index的介绍，该平台对全球超过260万科研人员、24500多家机构 （包括大学、研究所、企业、医院等） 、来自221个国家的研究表现进行评估。\n评估指标包括H-index、i10-index和总引用数这三个，每个指标都提供“整个职业生涯”与“最近5或6年”两个时间段的数据 （兼顾长期积累与近期活跃度) ，索引主要基于公开的Google Scholar个人资料，经过多层清洗和审核处理。\n整体而言，这个网站的排名不仅具备实时性 （谷歌学术数据约20天自动刷新一次，排名则每2天重新计算与发布） ，而且覆盖了各种学科类型。\n咱们的老熟人还有 排在第五的何恺明（总引73万+）、排第七的Ilya Sutskever（总引67万+） ，剩余几位几乎都是医学领域的科学家。\n打开今日头条查看图片详情\n打开今日头条查看图片详情\n总之，透过这一排名，我们能看到当下最热的两大领域：AI和医学。\n毕竟，当一个人的论文被全世界数十万次引用，或许就不仅仅只是学术荣耀那么简单，而更有可能代表了时代的注脚。\n— 完 —\n量子位 QbitAI\n关注我们，第一时间获知前沿科技动态\n一键举报。\n「深度学习三巨头聚首」Hinton坚持10年内机器会掌握常识，Lecun说20年\n「深度学习三巨头聚首」Hinton坚持10年内机器会掌握常识，Lecun说20年。昨天，在RE·WORK蒙特利尔深度学习峰会上，“AI三巨头”Geof...\nBengio研究生科研指导演讲：解读人工智能全貌和下一个前沿\nBengio研究生科研指导演讲：解读人工智能全貌和下一个前沿。近期，Yoshua Bengio 在加拿大多伦多大学 Distinguished Lecture Series 面...\n趋势分析之深度学习\n趋势分析之深度学习。AMiner全新功能技术趋势分析Trend analysis（http://trend.aminer.cn）基于AMiner 2亿篇论文数据进行深入挖掘，包..."
  },
  {
    "title": "大型语言模型(LLM)技术深度分析 2024",
    "page_body": ""
  },
  {
    "title": "jc工具数据湖集成：处理大规模非结构化数据的技巧-CSDN博客",
    "page_body": "jc工具数据湖集成：处理大规模非结构化数据的技巧\n【免费下载链接】jc jc是一个命令行工具，可以将JSON或YAML数据结构转化为其他格式（如表格、CSV等），便于数据分析和日志处理，简化了对复杂数据输出的可视化需求。  项目地址: https://gitcode.com/gh_mirrors/jc/jc \n你是否还在为数据湖中杂乱无章的非结构化数据头疼？日志文件、 命令行 输出、配置文件格式各异，难以统一分析？本文将带你探索如何利用jc工具轻松解决这些问题，让非结构化数据处理变得简单高效。读完本文，你将掌握jc与数据湖集成的实战技巧，学会处理大规模数据的优化方法，并通过实际案例了解其应用价值。\n数据湖非结构化数据的处理痛点\n在当今数据驱动的时代，企业积累了海量数据，其中80%以上是非结构化数据，如服务器日志、网络流量记录、应用程序输出等。这些数据格式多样、结构复杂，给数据湖的管理和分析带来巨大挑战：\n格式不统一 ：不同来源的数据格式各异，缺乏标准化结构，难以直接进行查询和分析。 处理效率低 ：传统工具在处理大规模非结构化数据时，往往面临性能瓶颈，耗时费力。 集成难度大 ：将非结构化数据转换为适合数据湖存储和分析的格式，需要复杂的ETL流程。\njc工具的出现，为解决这些痛点提供了全新的思路。它能够将各种非结构化和半结构化数据转换为统一的 JSON 格式，为数据湖集成铺平道路。\njc工具简介及其在 数据处理 中的优势\njc是一款强大的命令行工具，能够将多种命令行输出和文件格式转换为JSON格式，便于后续的解析和分析。其核心优势在于：\n广泛的兼容性 ：支持超过100种命令行工具输出和文件类型的解析，如 dig 、 df 、 ps 、 CSV 、 INI 等，几乎涵盖了系统管理和数据处理中的常见场景。 灵活的使用方式 ：既可以作为命令行工具独立使用，也可以作为Python库集成到自定义脚本中，满足不同的使用需求。 高效的流式处理 ：提供流式解析器，能够逐行处理大型文件，显著降低内存占用，适合处理大规模数据。 标准化的数据输出 ：将非结构化数据转换为结构化的JSON格式，便于数据湖中的存储、查询和分析。\njc的源代码主要位于 jc/ 目录下，其中 jc/lib.py 是核心模块，提供了解析器管理和数据转换的关键功能。官方文档 docs/ 详细介绍了各解析器的使用方法和参数说明。\n安装与配置jc工具\njc提供了多种安装方式，适用于不同的操作系统和环境。以下是几种常见的安装方法：\n使用pip安装\n对于大多数Python环境，推荐使用pip进行安装：\npip3 install jc \nbash\n这种方式简单快捷，适合个人用户和开发环境。\n通过操作系统包管理器安装\n许多Linux发行版已经将jc纳入官方软件仓库，可以通过系统自带的包管理器进行安装：\n操作系统\n安装命令\nDebian/Ubuntu apt-get install jc\nFedora dnf install jc\nArch Linux pacman -S jc\nmacOS brew install jc\n完整的安装指南可以参考项目根目录下的 README.md 。\n从源码安装\n如果需要使用最新版本的jc，可以从源码仓库克隆并安装：\ngit  clone  https://gitcode.com/gh_mirrors/jc/jc.git\ncd  jc\npython3 setup.py install\nbash\n安装完成后，可以通过 jc --version 命令验证安装是否成功。\n基本数据转换示例\n让我们通过几个简单的示例，了解jc如何将非结构化数据转换为JSON格式。\n解析命令行输出\n以 df 命令为例，它用于显示文件系统的磁盘空间使用情况。默认输出如下：\nFilesystem      512 -blocks    Used Available Capacity iused      ifree %iused  Mounted  on\n/ dev / disk 1 s 1 1953514496 52428800 1800000000 3 %   892853 18446744073709551714 0 %    /\ndevfs                388 388 0 100 %     686 0 100 %    / dev\n使用jc解析后，输出变为结构化的JSON：\ndf  | jc -- df  -p \nbash\n[   {     \"filesystem\": \"/dev/disk1s1\",     \"512_blocks\": 1953514496,     \"used\": 52428800,     \"available\": 1800000000,     \"capacity\": 3,     \"iused\": 892853,     \"ifree\": 18446744073709551714,     \"iused_percent\": 0,     \"mounted_on\": \"/\"   },   {     \"filesystem\": \"devfs\",     \"512_blocks\": 388,     \"used\": 388,     \"available\": 0,     \"capacity\": 100,     \"iused\": 686,     \"ifree\": 0,     \"iused_percent\": 100,     \"mounted_on\": \"/dev\"   } ] \njson\n解析文件内容\njc不仅可以解析命令行输出，还可以处理各种文件类型。例如，解析 CSV 文件：\ncat  data.csv | jc --csv -p \nbash\n假设 data.csv 内容如下：\nname,age,city\nAlice, 30 ,New York\nBob, 25 ,Los Angeles\nCharlie, 35 ,Chicago\n解析后的JSON输出为：\n[   {     \"name\": \"Alice\",     \"age\": \"30\",     \"city\": \"New York\"   },   {     \"name\": \"Bob\",     \"age\": \"25\",     \"city\": \"Los Angeles\"   },   {     \"name\": \"Charlie\",     \"age\": \"35\",     \"city\": \"Chicago\"   } ] \njson\n更多解析示例可以参考 EXAMPLES.md ，其中包含了各种命令和文件类型的转换效果。\n与数据湖集成的步骤\n将jc工具与数据湖集成，通常需要以下几个步骤：\n1. 数据采集与转换\n使用jc将非结构化数据转换为JSON格式。根据数据来源的不同，可以采用不同的处理方式：\n实时数据流 ：对于日志文件等实时生成的数据，可以使用jc的流式解析器，如 --clf-s （Common Log Format流式解析器），结合工具如 tail -f 实时监控文件变化，并将转换后的数据发送到数据湖。\ntail  -f /var/log/nginx/access.log | jc --clf-s | curl -X POST -H  \"Content-Type: application/json\"  -d @- http://data-lake-ingestion endpoint \nbash\n批量文件处理 ：对于已有的历史数据文件，可以使用jc的标准解析器批量转换，并通过脚本将数据加载到数据湖中。\nimport  jc\nimport  json\nimport  boto3\ns3 = boto3.client( 's3' )\nwith open ( '/path/to/large_file.log' ,  'r' )  as  f:\n# 使用流式解析器处理大型文件\nfor  entry  in  jc.parse( 'clf-s' , f):\n# 将每条记录上传到S3数据湖\n        s3.put_object(\n            Bucket= 'my-data-lake' ,\n            Key= f'logs/nginx/ {entry[ \"timestamp\" ]} .json' ,\n            Body=json.dumps(entry)\n        )\npython\n运行\n2. 数据 存储 与管理\n转换后的JSON数据可以存储到各种数据湖平台中，如Amazon S3、Hadoop HDFS、Google Cloud Storage等。根据数据湖的架构，可以选择合适的存储策略：\n原始数据区 ：存储未经处理的原始JSON数据，保留数据的完整性，便于后续重新处理。 清洗数据区 ：对原始JSON数据进行清洗和标准化，去除冗余字段，优化数据结构。 主题数据区 ：按照业务主题对数据进行分区和组织，提高查询效率。\n3. 数据查询与分析\n结构化的JSON数据可以通过数据湖中的查询引擎（如Amazon Athena、Google BigQuery、Apache Hive等）进行高效查询和分析。例如，使用Athena查询nginx访问日志中不同IP的访问次数：\nSELECT  client_ip,  COUNT ( * )  AS  request_count\nFROM  nginx_logs\nGROUP BY  client_ip\nORDER BY  request_count  DESC\nLIMIT  10 ;\nsql\n处理大规模数据的性能优化技巧\n当处理大规模非结构化数据时，性能优化至关重要。以下是一些实用的技巧，帮助你充分发挥jc的潜力：\n使用流式解析器\njc提供了专门的流式解析器（名称以 -s 结尾，如 --clf-s 、 --csv-s ），能够逐行处理输入数据，而不是将整个文件加载到内存中。这对于GB级甚至TB级的大型文件来说，可以显著降低内存占用，避免内存溢出。\n# 使用流式解析器处理大型CSV文件\njc --csv-s large_data.csv > output.json\nbash\n流式解析器的实现细节可以参考 jc/streaming.py 和 docs/streaming.md 。\n并行处理\n对于多个独立的文件，可以使用并行处理提高效率。例如，使用GNU Parallel结合jc批量处理日志文件：\nfind /var/log -name  \"*.log\"  | parallel jc --clf {} > {}.json \nbash\n这种方式可以充分利用多核CPU的性能，大幅缩短处理时间。\n数据压缩\n在数据传输和存储过程中，对JSON数据进行压缩，可以减少网络带宽占用和存储空间。jc的输出可以直接通过管道传递给压缩工具：\njc -- df  | gzip > df_output.json.gz \nbash\n选择性解析\n如果只需要非结构化数据中的部分字段，可以在解析后使用 jq 等工具进行字段筛选，减少数据量：\njc --ps | jq  '.[] | {pid, command}'  > ps_simplified.json \nbash\n合理设置缓存\n对于重复解析的数据，可以考虑设置缓存机制，避免重复处理。例如，使用 redis 缓存常用命令的解析结果：\nimport  jc\nimport  redis\nimport  json\nr = redis.Redis(host= 'localhost' , port= 6379 , db= 0 )\ndef get_parsed_data ( command, parser ):\n    cache_key =  f\" {command} : {parser} \"\n    cached_data = r.get(cache_key)\nif  cached_data:\nreturn  json.loads(cached_data)\n# 执行命令并解析\n    output = subprocess.check_output(command, shell= True , text= True )\n    parsed_data = jc.parse(parser, output)\n# 缓存结果，设置过期时间\n    r.setex(cache_key,  3600 , json.dumps(parsed_data))\nreturn  parsed_data\npython\n运行\n实际案例分析\n以下通过几个实际案例，展示jc在数据湖集成中的应用价值。\n案例一：服务器性能监控数据集成\n某公司拥有数百台服务器，需要将各服务器的性能数据（CPU、内存、磁盘使用率等）集成到数据湖中，进行统一监控和分析。\n解决方案 ：\n在每台服务器上部署定时任务，使用jc解析 top 、 free 、 df 等命令的输出。 将解析后的JSON数据通过HTTP POST发送到数据湖 ingestion endpoint。 在数据湖中使用Apache Spark对性能数据进行聚合和分析，生成性能报告和告警。\n关键命令 ：\n# 解析top命令输出并发送到数据湖\ntop -b -n 1 | jc --top-s | curl -X POST -H  \"Content-Type: application/json\"  -d @- http://data-lake.example.com/ingest/performance\nbash\n案例二：网络日志分析\n某电商网站需要分析Nginx访问日志，了解用户行为，优化网站性能。\n解决方案 ：\n使用jc的 --clf-s 流式解析器实时处理Nginx访问日志。 将解析后的JSON数据存储到Amazon S3中。 使用Amazon Athena查询和分析用户访问模式、热门页面、错误率等指标。\n关键代码 ：\nimport  jc\nimport  boto3\nimport  json\ns3 = boto3.client( 's3' )\nwith open ( '/var/log/nginx/access.log' ,  'r' )  as  f:\nfor  entry  in  jc.parse( 'clf-s' , f):\n# 添加时间戳和服务器"
  }
]