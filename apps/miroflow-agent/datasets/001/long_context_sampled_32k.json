[
  {
    "title": "获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper",
    "page_body": "喜报\n7月26日，“沈家本与新时代知识产权保护”论坛暨沈家本研究会2019年年会在湖州市吴兴区召开。\n我院干警赵翼昂、袁者共同撰写的论文《论大数据在类案类判中的应用》在本次论文交流评选中荣获二等奖。\n论大数据在类案类判中的应用\n论文提要：\n近年来随着智慧法院建设强有力的促进审判体系和审判能力现代化，在大数据司法领域深入应用中，类案类判作为人工智能支持司法审判的重要内容。借着其在当前话题层面热议的机会，将从该技术的原理、其前景如何并在实践运行中存在哪些问题着手。首先从类案类判的概念着手，明确什么是类案类判。核心是确定类案标准，两个独立案件如何才能被视为是同类案件，又应当以什么样的标准来判断；类案类判的关键是类案推送，类案推送系统依托自然语言处理技术，借助知识图谱构建类案知识树和知识库，利用相关算法完成类案匹配。其次是着眼类案类判的现状，对类案类判制度进行探索。再是从类案类判机制在实践中的困难入手，研究法信平台、类案推送系统、北大法宝、裁判文书网等相关系统时候在准确性、精确性、积极性等方面缺陷。分析得出主要因为需求分析的不够准确，AI算法及推送技术不够成熟、案例数据不够规范造成以上困难。考虑到类案类判与人工智能深度融合，从明确规范标准、构建操作规则等方法，完善类案类判机制三个方面入手。全文共7631字。\n主要创新观点：\n分析大数据在类案类判中的应用，应该分别从类案类判的概念和大数据的特点分别把握。首先是建立类案标准， 案件审理是一个综合运用法律知识、经验法则、裁判技巧、解释法律的裁判过程，法官虽考虑接受法律约束，但也有自由裁量权做出价值判断，法官正在审理的案件应当与其所在法院和上一级法院已经审结的或者其他具有指导意义的同类案件裁判尺度一致，不能脱离法律、政治、社会三个效果一致的目的，否则类案标准将无法控制裁判偏离度。同时关键是积极利用类案检索，通过数学建模及模拟算法，得到合适的案例和判决结果。目前大数据在类案类判应用情况中还存在无法区分类案和推送技术不成熟的问题，主要体现在结果准确性、范围精细性和使用积极性上。可以采取规范类案类判机制、构建充足的类案数据库及发展提高算法来尝试解决大数据在类案类判应用中的窘境。\n以下正文：\n随着大数据在司法领域的深入应用和社会信息化的深入发展，借助智慧法院建设强有力的促进审判体系和审判能力现代化，类案类判作为人工智能支持司法审判的重要内容，是司法改革背景下推进审判权运行机制改革的重要尝试，对提升法院整体裁判水平、实现类案适法统一和促进司法公正有积极意义。其功能在于通过多种途径，为审判人员推送当前正处理的相似案例，启发及拓宽思路，能有效控制裁判偏离，统一相同地区领域的司法裁判尺度，同时也为新型疑难案件提供新的解决途径，实现关联案件之间法律、政治及社会效果一致。借着类案类判在当前话题层面热议的机会，笔者将着重探讨该技术的原理、其前景如何并在实践运行中存在哪些问题。\n一、类案类判在审判应用中的现状\n2017年8月1日最高人民法院印发《最高人民法院司法责任制实施意见（试行）》，期冀以通过类案和关联案件检索机制明确承办法官在审理案件进行类案检索，目的就要进一步统一裁判尺度。2017年10月26日中央办公厅印发《关于加强法官检察官正规化专业化职业化建设，全面落实司法责任制的意见》，明确依托大数据和云算法技术，完善智能辅助系统的类案推送、结果比对、数据分析等功能。2018年1月5日最高院紧跟着实施意见上线类案智能推送系统，覆盖全面1330个案由，从案件性质、案情特征、争议焦点、法律适用四个方面为查询和推送类案提供技术平台支撑（1）。在此之前最高院也确定审判人员应把握裁判趋势，在审理案件时候应对本院已审结或正在审理的类案进行全面检索，制作检索报告，可以说类案类判正式从试点探索、政策倡导走向司法部门及群众需求的层面。\n积极响应中央政策的同时，各省市也依托办案平台和档案系统先后开发各自的类案推送系统，如上海高院运行刑事案件智能专审平台；浙江高院开展的“类案推送项目”；贵州高院建立类案标准数据库及关联案件强制检索机制。虽然类案类判机制有其独特优势和积极作用，可以对推送的内容进行分析、推理及决策，还可以提供角色登录、用户兴趣、语义理解和信息筛选等实用功能，但在实践中也存在不少的困难。现以浙江省高院类案推送系统为例，某个原被告均为自然人的追偿权纠纷，诉讼请求是被告归还借款代偿本息101841.66元并支付利息。事实与理由是2017年7月17日被告向小贷公司借款100000元，由原告承担连带担保责任，后因被告无力偿还到期借款，由原告代偿借款本金101841.66元，事后被告未及时支付代偿款纠纷款。系统有通过输入关键字、词组及上传文书自动识别两种方式，具体流程如下图：\n当上传文书后系统自动对其进行识别并自动关联案件23万多件，本省12818件，通过筛选得基层法院近三年相关20173件。通过输入关键字“追偿权纠纷”“民事案件”，识别出有效判决文书54万多份，本省85884件，通过筛选得一审361692件。而在使用裁判文书公开网、法信平台及北大法宝等系统后相同案例效果更为不明显。\n仔细观察推送结果可以发现，部分案例与追偿权纠纷并无直接关系；推送数据过多，增加筛选类案难度，容易产生厌烦心理；然而可供参考的案例稀缺，对审判人员无实质性帮助价值。当前而言在智能辅助办案系统里，类案类判系统显然不及法条关联、裁判文书模板自动生成及文书纠错等系统，这些平台能自动生成裁判文书初稿，帮助校验法律文书，实实在在减轻了事务性工作负担，所以许多审判人员更习惯求助它们。而类案类判平台对于复杂案件推送不精准，对于简单案件又失去了检索的需求，这就降低用户体验和减少使用意愿。所以类案检索机制未得到普遍运用是有一定原因的，后面笔者会结合身边同事实际使用反响及目前类案类判系统的技术特征归纳总结。\n二、大数据在类案类判中如何发挥关键作用\n（一）类案类判的概念\n在讨论大数据如何帮助实现类案类判之前，要确定类案类判系统的运行机理。借助自然语言处理技术（NLP）通过模板寻找、标签注释等完成类案要素判断及提取，借助相应的技术构建类案知识库，利用云算法完成类案匹配和推送。\n1.类案类判的核心是建立类案标准：何谓同类案件，又应当以什么样的标准来判断。目前学界主流有这几个观点：主要法律关系说、事实特征说、关键事实说等。总体来说主要基本案情和法律适用一致即是同类案件。一是以判断两个案件主张的事实认定是否一致为基础，但也不是要求两个事实必须在所有细节情形上严丝合缝一模一样。案件审理最终目的是确定是否应当支持当事人的诉讼请求，因此诉讼请求的构成要件所对应的各个事实要素一致，就可以认为两者事实一致；二是法院认定案件事实一致，按照要素清单进行论述，在判决书中清楚地表述每个要素上是认定了什么样的案件事实。根据要素清单认定案件事实，也具有多方面的现实意义，因为当法院认定的案件事实一致那么一般判决结果也是一样的；三是两者法律、政治、社会三个效果一致。其实裁判文书中的思想通常包含对三个效果的考虑，效果本身是社会主义核心价值观的体现，由于类案类判通常只对裁判文书进行比对，所以可避免万一将来该案作为类案比对的对象时错误理解和适用其裁判尺度。\n2.类案类判的关键是发挥类案推送：其核心思路在于搭建案件性质的分类预测模型和裁判结果的回归预测模型，将海量裁判文书“前验”要素特征实例化，通过专家标注、规则推导建构基础数据模型，经由语词锁定、捕捉裁判文书关键信息形成结构化数据，遵循闭合性逻辑原理（2）。所谓的闭合性逻辑原理，是先有一个符合法律形式逻辑的大前提，然后再有一个案件事实基本接近的小前提，在大小前提都情况类似时，得出个案应与之前类案判决先例相似的结论。再判断高可信度关联，完成建模要素的有机重构，为类案多方位塑像形成类案树作为预测模型，并以类案大数据知识库为基础，以语义相似性度量方法，构建高维特征匹配的类案模型，对后续案件进行案件事实到适用法律及裁判结果的立体化相似性匹配，针对个性化用户模型和相似度算法，进行智能排序和个人偏好推送，对具体个案自动进行偏离常规裁判分析和风险防范预警，最后必须基于类案裁判结果的均值标准。\n（二）挖掘大数据在法律领域应用\n1981年随着人工智能技术的发展，D·沃特曼和M·皮特森开发了法律判决辅助系统，将其作为司法领域的实践工具并对民法中特定问题进行统计分析。该系统通过简单的责任认定、损害赔偿等模型计算案件的赔偿标准。国内在司法领域的第一次应用是在1986年国家社科“七五”研究课题取得盗窃罪量刑数据模型等成果（3）。这段时间内都是以专家的法律经验、知识框架作为规则。后面随着以大数据、机器搜索、深度学习的法律人工智能系统出现，实现AI可以做出分析预判，在关联案件相关调查中，分析不同场景不同体系不同舆论，找出最准确的文档及数据。在未来笔者认为法律人工智能会继续发展，之后的十多年，会随着大数据、云计算、计算机法律学及算法融合，深度学习处理器也从CPU往GPU/TPU发展，类案类判系统的效率及准确性会得到大幅度提升，成为审判人员工作时不可或缺的得力帮手。\n首先大数据应用类案类判既有用到传统软件架构的网络框架、数据加密设计和发送接收技术，又运用到自认语言理解（NLP）、司法实体识别（OCR）、实体关系分析等技术，将每个案件及其相关信息做成一个点，再通过分析挖掘和个性化定制实现点与点之间匹配和联络，最终构建一个类案关系的框架。其中暗含大量的交互技术、机器训练和数据筛选，高效快捷的制作某类案件大数据分析报告。充分运用人工智能学习和语义分析技术，将大量专业法律数据进行清洗、分类、结构化，最终形成可以提供可视化的咨询服务平台；将数据进行高度整合、关联，提供结构化、可视化、智能化的搜索结果；按照事先设计制定的法律语言规则，将文书自动分段、贴标签，建立目录方便AI索引、分类、拾取，帮助审判人员能够进行关联搜索。\n其次由于类案类判不同于以往简单的案件搜索，它处理结构化数据、半结构化数据和非结构化数据，例如案例数据库、判决文件、法条法规、图书期刊和庭审录音录像等，这些数据通过数据集成协议、关联分析、计算"
  },
  {
    "title": "NoSQL之Redis配置与优化-CSDN博客",
    "page_body": "目录\n一：Redis介绍\n1：关系数据库与非关系数据库\n1.1：关系数据库\n1.2：非关系数据库\n1.3：非关系数据库背景\n2：Redis基础\n2.1：Redis简介\n2.2：Redis安装部署\n2.3：配置参数\n3：Redis命令工具\n3.1：redis-cli命令行工具\n3.2：redis-benchmark测试工具\n4：Redis数据库常用的命令\n4.1：key相关命令\n4.2：多数据库常用命令\n二：Redis持久化\n1：RDB和AOF的区别\n2：RDB和AOF的优缺点\n2.1：RDB优缺点\n2.2：AOF优缺点\n3：Redis持久化配置\n3.1：RDB持久化配置\n3.2：AOF持久化配置\n4：AOF重写\n三：性能管理\n1：内存碎片率\n2：内存使用率\n2.1：针对缓存数库大小选择\n2.2：使用Hash数据结构\n2.3：设置key的过期时间\n3：回收key\n一： Redis 介绍\n1：关系数据库与非关系数据库\n1.1：关系数据库\n定义：\n关系数据库（Relational Database）是基于关系模型（Relational Model）的数据库，使用表格（表、行、列）的结构存储和管理数据。数据之间通过主键（Primary Key）和外键（Foreign Key）建立关联。\n特点：\n结构化数据：  数据以严格的二维表形式存储，遵循预定义的模式（Schema）。\nSQL操作：  使用结构化查询语言（ SQL ）进行数据操作和查询。\nACID特性：  支持事务的原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability），适合需要高一致性的场景。\n规范化设计：  通过范式（Normalization）减少数据冗余，提高数据完整性。\n1.2：非关系数据库\n定义：\n非关系数据库（NoSQL, Not Only SQL）是一类不依赖传统关系模型的数据库，设计灵活，适合处理大规模、非结构化或半结构化数据。\n特点：\n灵活的数据模型：\n支持键值对（Key-Value）、文档（Document）、列族（Column Family）、图（Graph）等数据模型。\n无固定模式（Schema-less），可动态扩展字段。\n高扩展性：  通常支持分布式架构，易于水平扩展（如分片、副本集）。\n高性能：  针对特定场景优化（如高吞吐、低延迟）。\n弱化ACID：  部分NoSQL数据库牺牲严格的一致性，追求高可用性和分区容错性（遵循CAP定理）。\n1.3：非关系数据库背景\n（1） High performance—— 对数据库高并发读写需求\nWeb2.0 网站会根据用户的个性化信息来实时生成动态页面和提供动态信息，因此无法使用动态页面静态化技术。所以数据库的并发负载非常高，一般会达到 10000 次 /s 以上的读写请求。关系型数据库对于上万次的查询请求还是可以勉强支撑的，但出现上万次的写数据请求，硬盘 IO 就已经无法承受了。对于普通的 BBS 网站，往往也会存在高并发的写数据请求。\n（2） Huge Storage—— 对海量数据高存储与访问需求\n类似于 Facebook、Friendfeed 这样的 SNS 网站，每天会产生大量的用户动态信息。如 Friendfeed，一个月就会产生不少于 2.5 亿条用户动态信息，对于关系型数据库来说，在一个包含 2.5 亿条记录的表中执行 SQL 查询，查询效率是非常低的。\n（3） High Scalability && High Availability—— 对数据库高可扩展性与高可用\n用户需求在 Web 架构中，数据库是最难进行横向扩展的。当应用系统的用户量与访问量与日俱增时，数据库是没办法像 Web 服务一样，简单地通过添加硬件和服务器节点来扩展其性能和负载能力的。尤其对于一些需要 24 小时对外提供服务的网站来说，数据库的升级与扩展往往伴随着停机维护与数据迁移，其工作量是非常庞大的。\n2：Redis基础\n2.1：Redis简介\nRedis（RemoteDictionaryServer，远程字典典型）是一个开源的、使用 C 语言编写的 NoSQL 数据库。Redis 基于内存运行并支持持久化，采用 key - value（键值对）的存储形式，是目前分布式架构中不可或缺的一环。\n Redis 服务器程序是单进程模型，也就是在一台服务器上可以同时启动多个 Redis 进程，而 Redis 的实际处理速度则是完全依靠于主进程的执行效率。\nRedis具有以下几个优点：\n具有极高的数据读写速度，数据读取的速度最高可达到 110000 次 /s，数据写入速度最高可达到 81000 次 /s。 支持丰富的数据类型，不仅仅支持简单的 key - value 类型的数据，还支持 Strings，Lists，Hashes，Sets 及 Ordered Sets 等数据类型操作。 支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 原子性，Redis 所有操作都是原子性的。 支持数据备份，即 master - salve 模式的数据备份。\n2.2： Redis安装 部署\n#上传安装包redis -4.0 .9 .tar.gz\n#关闭防火墙\nsystemctl stop firewalld\nsetenforce  0\n#部署环境\ndnf -y install gcc  zlib-devel\n#解压安装包redis -4.0 .9 .tar.gz\ntar zxvf redis -4.0 .9 .tar.gz\ncd redis -4.0 .9 /\n#编译安装\nmake\n#定义安装路径\nmake PREFIX=/usr/local/redis install\ncd /usr/local/redis\n#做软链接\nln -s /usr/local/redis/bin /* /usr/local/bin/\n#做初始化\ncd redis-4.0.9/\ncd utils/\n./install_server.sh(可以直接默认)\n ##需要手动输入\n Selected config:\n Port : 6379\n Config file : /etc/redis/6379.conf       //配置文件路径\n Log file : /var/log/redis_6379.log       //日志文件路径\n Data dir : /var/lib/redis/6379 //数据文件路径\n Executable : /usr/local/redis/bin/redis-server      //可执行文件 路径\n Cli Executable : /usr/local/redis/bin/redis-cli //客户端命令行工 具\n#检查开启状态\nnetstat -anpt | grep redis\n#关于redis服务的命令\n/etc/init.d/redis_6379 stop             #关闭redis服务\n/etc/init.d/redis_6379 start             #启动redis服务\n/etc/init.d/redis_6379 restart          #启动redis服务\n/etc/init.d/redis_6379 status  #查看redis服务\nAI写代码 cpp\n运行\n2.3：配置参数\n#配置参数\nvim /etc/redis/ 6379. conf\nbind  127.0 .0 .1 192.168 .10 .201\n/etc/init.d/redis_6379 restart          #启动redis服务\nAI写代码 cpp\n运行\n参数\n作用\ntimeout 300 当客户端闲置多长时间后关闭连接，如果指定为 0，表示关闭该功能\ndbfilename dump.rdb 指定本地数据库文件名，默认值为 dump.rdb\ndir /var/lib/redis/6379 指定本地数据库存放目录\nmaxclients 10000 设置同一时间最大客户端连接数，默认为 10000。Redis 可以同时打开的客户端连接数为 Redis 进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不限制。当客户端连接数到达限制时，Redis 会关闭新的连接并向客户端返回 max number of clients reached 错误信息\nrdbcompression yes 指定存储至本地数据库时是否压缩数据，默认为 yes。Redis 采用 LZF 压缩，如果为了节省 CPU 资源，可以关闭该选项，但会导致数据库文件变的巨大\nslaveof <masterip><masterport> 当本机为从服务器时，设置主服务的 IP 地址及端口。在 Redis 启动时，从服务器会自动从主服务进行数据同步\nmasterauth <master-password> 当主服务设置了密码保护时，从服务连接主服务的密码\nrequirepass foobared 设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 AUTH <password>命令提供密码，默认关闭\nmaxmemory <bytes> 指定 Redis 最大内存限制。Redis 在启动时会把数据加载到内存中，达到最大内存后，Redis 会先尝试清除已到期或即将到期的 Key，当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis 新 VM 机制，会把 Key 存放内存，Value 会存放在 Swap 分区\n3：Redis命令工具\nRedis 软件提供了多个命令工具。安装 Redis 服务时，所包含的软件工具会同时被安装到系统中，在系统中可以直接使用。这些命令工具的作用分别如下所示。\nredis - server：用于启动 Redis 的工具； redis - benchmark：用于检测 Redis 在本机的运行效率； redis - check - aof：修复 AOF 持久化文件； redis - check - rdb：修复 RDB 持久化文件； redis - cli：Redis 命令行工具。\n3.1：redis-cli命令行工具\nRedis 数据库系统也是一个典型的 C/S（客户端 / 服务器端）架构的应用，要访问 Redis 数据库需要使用专门的客户端软件。Redis 服务的客户端软件就是其自带的 redis - cli 命令行工具。使用 redis - cli 连接指定数据库，连接成功后会进入提示符为 “远程主机 IP 地址：端口号” 的数据库操作环境，例如 “127.0.0.1:6379>” 。用户可以输入各种操作语句对数据库进行管理。如执行 ping 命令可以检测 Redis 服务是否启动。\nredis-cli\n127.0 .0 .1 : 6379 >ping  //检测redis服务是否启动\nPONG（pingpong机制）\n#远程连接\nredis-cli -h  192.168 .10 .201  -p  6379\n127.0 .0 .1 : 6379 >help  //帮助输入指令\nAI写代码 cpp\n运行\n3.2：redis-benchmark测试工具\nredis - benchmark 是官方自带的 Redis 性能测试工具，可以有效的测试 Redis 服务的性能。基本的测试语法为 redis - benchmark [option] [option value]。常用选项如下所示。\nh：指定服务器主机名； p：指定服务器端口； s：指定服务器 socket； c：指定并发连接数； n：指定请求数； d：以字节的形式指定 SET/GET 值的数据大小； k：1=keep ALIVE 0=reconnect； r：SET/GET/INCR 使用随机 key，SADD 使用随机值； P：通过管道传输<numreq>请求； q：强制退出 redis。仅显示 query/sec 值； --csv：以 CSV 格式输出； l：生成循环，永久执行测试； t：仅运行以逗号分隔的测试命令列表； I：Idle 模式。仅打开 N 个 idle 连接并等待。\nredis-benchmark -h  192.168 .10 .201  -p  6379  -c  100  -n  10000\n//与redis建立100个并发连接，与10000个请求测试性能(10*10000)\nredis-benchmark -h  192.168 .10 .201  -p  6379  -q -d  100\n//让redis以字节的方式设置键值\nredis-benchmark -t set,lpush -n  10000  -q\n//redis服务进行set,lpush的操作\nAI写代码 cpp\n运行\n4：Redis数据库常用的命令\n前面提到 Redis 数据库采用 key-value（键值对）的数据存储形式。所使用的命令是 set\n 与 get 命令。\nset：存放数据，基本的命令格式为 set key value。 get：获取数据，基本的命令格式为 get key。\n例如，在 Redis 的命令行模式下执行”set teacher zhanglong”，表示在当前数据库下存放一个 key 为 teacher，value 为 zhanglong 的数据，而执行 “getteacher” 命令即可查看刚才存放的数据。\n127.0 .0 .1 : 6379 >set teacher zhangsan\n//存放数据，键为teacher，值为zhangsan，在teacher下放入zhangsan\n127.0 .0 .1 : 6379 >get teacher\n//获取数据\nAI写代码 cpp\n运行\n4.1：key相关命令\n(1)keys\n127.0 .0 .1 : 6379 >set teacher zhangsan\n//存放数据，键为teacher，值"
  },
  {
    "title": "深度学习中的Baseline：基础框架与性能评估",
    "page_body": "千帆一体机\n搭载千帆大模型工具链平台，内置文心与精选开源大模型\n大模型\n文心大模型\n百度文心大模型4.5系列正式开源！\n千帆大模型平台支持开源模型API服务！\nERNIE X1.1\n在事实性、指令遵循、智能体等能力上均有显著提升\nERNIE X1 Turbo\n具备更长的思维链，更强的深度思考能力\nERNIE 4.5\n新一代原生多模态基础大模型\nERNIE Speed Pro 轻量级大模型\n适合作为基座模型精调，更好处理特定场景问题，性能极佳\n端到端语音语言大模型\n基于Cross-Attention跨模态语音大模型，体验超拟人对话\n大模型语音合成\n音色具备更高的自然度、丰富的情感表达等特点\n大模型声音复刻\n录制5秒音频，即可极速复刻音色\nIRAG 图像生成\n自研检索增强文生图技术，效果更优，低成本去AI味\nDeepSeek大模型\nDeepSeek-R1-0528\n复杂推理、长文本处理、代码生成能力全面提升\nDeepSeek-R1-671B\n杭州深度求索自研，基于大型强化学习训练的第一代理解模型\nDeepSeek-V3-671B\n深度求索自研（MoE）语言模型，位列主流开源榜单榜首\n大数据与原生应用\n大数据+AI\n千帆数据智能平台 DataBuilder\n一站式多模态数据管理、加工和分析应用平台\n向量数据库 VectorDB\n纯自研高性能、高性价比、生态丰富且即开即用\n数据可视化Sugar BI\n0代码，分钟级即可完成自助BI分析和可视化数据大屏\nElasticsearch检索分析服务\n什么是深度学习中的baseline？\n什么是 中的baseline？\n随着人工智能技术的飞速发展，深度学习已经成为了许多领域的标配。然而，在深度学习的实际应用中，我们经常面临各种各样的挑战，如何评估模型的性能、如何选择合适的网络结构、如何优化训练过程等。为了解决这些问题，我们需要一个强大的工具，那就是深度学习中的baseline。\nBaseline是深度学习中的一种重要概念，它为我们提供了一个基本的参考框架，用于评估不同算法和模型的性能。具体来说，baseline是指在不添加任何新的网络结构或优化方法的情况下，使用最基本的深度学习模型（如多层感知器、卷积 神经网络 等）在某项任务上达到的性能指标。\nBaseline的作用和重要性主要体现在以下几个方面。首先，baseline可以帮助我们评估不同算法和模型的性能。在深度学习中，往往有许多的算法和模型可供选择，如何确定哪个最适合我们的任务呢？这时候就可以通过比较不同算法和模型的baseline性能来进行筛选。其次，baseline还可以帮助我们优化算法和模型。通过与baseline的比较，我们可以看出哪些算法和模型在哪些方面有待改进，从而指导我们的模型设计和优化方向。最后，baseline还可以用于评估我们的深度学习系统的性能。在没有新的技术或方法的情况下，通过比较我们的系统和baseline的性能，可以了解我们的系统是否达到了当前最先进的水平。\n在深度学习中，我们可以根据不同的标准对baselines进行分类。例如，按照数据类型可以分为图像分类baseline、 语音识别 baseline等；按照算法可以分为神经网络baseline、决策树baseline等；按照应用领域可以分为计算机视觉baseline、 自然语言处理 baseline等。\nBaseline在深度学习的各个领域都有广泛的应用。在计算机视觉中，图像分类的baseline通常使用的是卷积神经网络（CNN），通过调整网络结构和参数来提高准确率；在自然语言处理中，文本分类的baseline通常使用的是多层感知器（MLP）或递归神经网络（RNN），同样可以通过调整结构和参数来提高性能。\n除了上述的应用，Baseline还有着广阔的发展空间。在未来，随着深度学习技术的不断发展，Baseline将会面临更多的挑战和需要解决的问题。例如，如何设计更加有效的网络结构、如何优化训练过程、如何处理大规模数据集等。为了解决这些问题，我们需要不断地尝试新的方法和技术，从而推动深度学习的发展。\n总之，Baseline是深度学习中的重要概念，它为我们提供了评估算法和模型性能的基准，帮助我们优化算法和模型，以及评估整个深度学习系统的性能。在未来的发展中，Baseline将会发挥更加重要的作用。因此，了解和掌握Baseline的概念和技术对于深度学习的初学者和专家来说都是至关重要的。"
  },
  {
    "title": "initial_h-博客园",
    "page_body": "2021年10月4日 \n A Study on Overfitting in Deep Reinforcement Learning \n 摘要： **发表时间：**2018 **文章要点：**这篇文章搞了一个maze的环境来研究RL里面的overfitting问题。把环境也分成train和test，然后来看效果。Train和test就根据初始状态来分的，其实就是agent在迷宫里面的起始位置。先是比较了训练时间，训练集的大小，任务难度这三个因  阅读全文\nposted @ 2021-10-04 13:01 initial_h  阅读(77) 评论(0) 推荐(0)  编辑 \n2021年10月2日 \n Detecting Rewards Deterioration in Episodic Reinforcement Learning \n 摘要： **发表时间：**2021（ICML 2021） **文章要点：**文章想说，我们训好一个policy之后，在真正用他的时候需要考虑安全性和可靠性（RL tasks is the safety and reliability of the system）。所以我们就需要一个方法来快速检测这个trai  阅读全文\nposted @ 2021-10-02 12:02 initial_h  阅读(85) 评论(0) 推荐(0)  编辑 \n2021年9月30日 \n A Hitchhiker's Guide to Statistical Comparisons of Reinforcement Learning Algorithms \n 摘要： **发表时间：**2019（ICLR Worskhop on Reproducibility） **文章要点：**文章主要是用统计检验的思想来比较RL算法的performance。文章介绍了很多统计检验的方法，参数的，非参数的都有，比如T-test, Welch's t-test, Wilcoxon  阅读全文\nposted @ 2021-09-30 10:42 initial_h  阅读(27) 评论(0) 推荐(0)  编辑 \n2021年9月29日 \n A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning \n 摘要： **发表时间：**2018 **文章要点：**文章想说RL很容易overfitting，然后就提出某个方式来判断是不是overfitting了。最后得出结论，通过多样化的训练可以减少overfitting（as soon as there is enough training data divers  阅读全文\nposted @ 2021-09-29 10:30 initial_h  阅读(40) 评论(0) 推荐(0)  编辑 \n2021年9月26日 \n Protecting Against Evaluation Overfitting in Empirical Reinforcement Learning \n 摘要： **发表时间：**2011（2011 IEEE symposium on adaptive dynamic programming and reinforcement learning (ADPRL)） **文章要点：**文章想说RL算法很容易environment overfitting导致泛化性  阅读全文\nposted @ 2021-09-26 11:20 initial_h  阅读(30) 评论(0) 推荐(0)  编辑 \n2021年9月23日 \n RE-EVALUATE: Reproducibility in Evaluating Reinforcement Learning Algorithms \n 摘要： **发表时间：**2018（2nd Reproducibility in Machine Learning Workshop at ICML 2018） **文章要点：**文章想说，现在这些RL算法不好复现，就是因为大家各搞各的，评估标准不一样，计算方式不一样等等。然后作者就提出了一个evaluat  阅读全文\nposted @ 2021-09-23 01:01 initial_h  阅读(83) 评论(0) 推荐(0)  编辑 \n2021年9月20日 \n Evaluating the Performance of Reinforcement Learning Algorithms \n 摘要： **发表时间：**2020（ICML 2020） **文章要点：**文章指出RL复现难的原因在于评价指标不一致。作者提出评估指标应该满足四点：1. Scientific,主要说你这个指标提供的信息要告诉别人针对某个具体的问题或假设，得出了什么结论，这个结论有没有考虑各种不确定性可能造成的问题。2.  阅读全文\nposted @ 2021-09-20 12:30 initial_h  阅读(91) 评论(0) 推荐(0)  编辑 \n2021年9月18日 \n Difference Based Metrics for Deep Reinforcement Learning Algorithms \n 摘要： **发表时间：**2019（IEEE Access） **文章要点：**这篇文章想说之前那些衡量RL算法的指标（rawreward, avgreward,maximum rawreward等等）不好，只看得出来一个得分，反映不出来RL在训练过程中的问题。然后作者自己设计了几个指标来检测RL在训练中可  阅读全文\nposted @ 2021-09-18 13:48 initial_h  阅读(40) 评论(0) 推荐(0)  编辑 \n2021年9月15日 \n Few-shot Neural Architecture Search \n 摘要： **发表时间：**2021（ICML 2021） **文章要点：**这篇文章就是在网络结构的准确率和训练速度之间trade off。NAS的逻辑就是搜一个网络结构出来，然后就测这个结构在数据集上train出来准确率能到多少，然后基于之前的结构和准确率再接着搜下一个网络。这里面如果每次都要重新trai  阅读全文\nposted @ 2021-09-15 08:27 initial_h  阅读(143) 评论(0) 推荐(0)  编辑 \n2021年9月13日 \n Sample-Efficient Neural Architecture Search by Learning Actions for Monte Carlo Tree Search \n 摘要： **发表时间：**2021（TPAMI 2021） **文章要点：**这篇文章感觉是Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search的姊妹篇，方法很类似，只是这一篇用来做N  阅读全文\nposted @ 2021-09-13 11:06 initial_h  阅读(150) 评论(0) 推荐(0)  编辑 \n2021年9月10日 \n Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search \n 摘要： **发表时间：**2020（NeurIPS 2020） **文章要点：**我们知道贝叶斯优化做到高维的时候计算量很大，根本算不出来。这篇文章是把MCTS和贝叶斯优化结合起来，做高维的优化问题。主要思路是先用MCTS分割搜索空间，然后在子空间上再用贝叶斯优化去采样。假设我们的优化问题是找一个函数$f(  阅读全文\nposted @ 2021-09-10 13:24 initial_h  阅读(132) 评论(0) 推荐(0)  编辑 \n2021年9月6日 \n Neural Architecture Search using Deep Neural Networks and Monte Carlo Tree Search \n 摘要： **发表时间：**2019（AAAI2020） **文章要点：**一篇做NAS的文章，主要想法就是用MCTS来做NAS，相对random，Q-learning，Hill Climbing这些更能平衡探索与利用。主要方法是把NAS的问题定义好，比如动作是什么，就是每次搭建神经网络这一层用什么结构，什么  阅读全文\nposted @ 2021-09-06 12:46 initial_h  阅读(90) 评论(0) 推荐(0)  编辑 \n2021年9月4日 \n Benchmarking Batch Deep Reinforcement Learning Algorithms \n 摘要： **发表时间：**2019 **文章要点：**这篇文章主要是针对batch RL做了一个离散动作空间的benchmark，对比了DQN和一些batch RL算法的性能（DQN，REM，QR-DQN，KL-Control，BCQ）。并且把BCQ从连续动作空间改成适用离散动作空间，取得了SOTA的效果。  阅读全文\nposted @ 2021-09-04 02:18 initial_h  阅读(128) 评论(0) 推荐(0)  编辑 \n2021年8月30日 \n RealWorld Games Look Like Spinning Tops \n 摘要： **发表时间：**2020（NeurIPS 2020） **文章要点：**这篇文章对博弈问题的策略空间的结构做了分析（主要还是针对two-player zero-sum symmetric games），提出策略空间是一个陀螺形状（作者把这个叫做the geometry of Games of Sk  阅读全文\nposted @ 2021-08-30 14:02 initial_h  阅读(154) 评论(0) 推荐(0)  编辑 \n2021年8月28日 \n Reinforcement Learning as One Big Sequence Modeling Problem \n 摘要： **发表时间：**2021 **文章要点：**这篇文章把RL看作序列建模问题（sequence modeling problem），直接用transformer来拟合整个序列 （reats states, actions, and rewards as simply a stream of data  阅读全文\nposted @ 2021-08-28 05:31 initial_h  阅读(352) 评论(0) 推荐(0)  编辑"
  },
  {
    "title": "开发RAG应用，你必须知道的11款Embedding模型_gte-qwen2-7b-instruct-CSDN博客",
    "page_body": "在当今的AI应用中，RAG（Retrieval-Augmented Generation，检索增强生成）技术已经成为一个热门话题。RAG通过结合信息检索与生成模型，极大提升了AI系统的智能化程度与实用性。在开发RAG应用的过程中，选择合适的Embedding模型至关重要，因为Embedding模型直接影响了检索的效果与生成的质量。今天我们就来聊聊开发RAG应用时，你必须知道的11个Embedding模型。\nMTEB（Massive Text Embedding Benchmark）是一个用于评估文本嵌入（Embedding）模型的综合性基准测试平台。通过多任务和多数据集的组合，MTEB可以全面衡量不同Embedding模型在各种自然语言处理（NLP）任务中的表现，如文本分类、语义检索、文本聚类等。\nMTEB平台的核心目标是为研究人员和开发者提供一种统一的方式来评估和比较不同Embedding模型的性能。MTEB涵盖了多种语言和任务，使得排名结果能够反映模型的通用性和应用效果。\n**一、MTEB排行榜英文模型排名：**    \nAI写代码\n1 2\n    **1、bge-en-icl**  \nAI写代码\n1 2\n简介 ：bge-en-icl 是一款专为英语语境下的嵌入任务设计的模型。\n特点 ：该模型在处理英语文本的语义理解和分类学习任务时具有较高的精度，尤其在交互式分类学习（ICL）任务中表现出色。\n适用场景 ：适用于需要精准语义理解的英语文本处理任务，如文本分类、语义检索、智能问答系统等。\n2、stella_en_1.5B_v5\n简介 ：stella_en_1.5B_v5 是一个包含15亿参数的大型嵌入模型。\n特点 ：具备强大的语义理解和推理能力，能够处理复杂的语义关系和大规模数据。\n适用场景 ：适合用于高级文本分析、自然语言生成、复杂对话系统等需要高计算资源的任务。\n3、SFR-Embedding-2_R\n简介 ：SFR-Embedding-2_R 是一款优化用于大规模语义检索任务的嵌入模型。\n特点 ：在语义检索任务中表现出色，能够有效地处理和匹配大规模数据集。\n适用场景 ：适用于语义检索、推荐系统、信息检索等需要高效匹配和检索的应用。\n4、gte-Qwen2-7B-instruct\n简介 ：gte-Qwen2-7B-instruct 是一个拥有70亿参数的指令优化型嵌入模型。\n特点 ：该模型特别针对复杂的指令驱动任务进行优化，具有卓越的语义推理和指令执行能力。\n适用场景 ：适合用于复杂的自动问答系统、智能助手和高级对话系统等。\n5、stella_en_400M_v5\n简介 ：stella_en_400M_v5 是一个较为紧凑的嵌入模型，包含4亿参数。\n特点 ：在降低计算成本的同时，仍能提供较为优异的语义理解能力，适合资源有限的环境。\n适用场景 ：适用于移动设备的文本处理、轻量级的文本分类和语义分析任务。\n6、bge-multilingual-gemma2\n简介 ：bge-multilingual-gemma2 是一款支持多种语言的多语言嵌入模型。\n特点 ：能够处理跨语言的文本嵌入任务，尤其适用于多语言环境下的应用。\n适用场景 ：适用于多语言语义检索、跨语言文本相似性分析、跨语言翻译等任务。\n7、NV-Embed-v1\n简介 ：NV-Embed-v1 是一款可能经过专门优化的嵌入模型，适用于特定的语义任务。\n特点 ：模型可能针对某些领域进行了优化，具备在特定任务或语言下的高效嵌入能力。\n适用场景 ：适用于定制化的语义分析任务，如特定领域的文本处理或行业应用。\n8、 voyage-large-2-instruct\n简介 ：voyage-large-2-instruct 是一个大型指令驱动的嵌入模型。\n特点 ：该模型经过优化，能够有效处理多个任务，并具有良好的指令理解和执行能力。\n适用场景 ：适合用于复杂对话系统、智能助手、需要精准指令执行的应用场景。\n9、Linq-Embed-Mistral\n简介 ：Linq-Embed-Mistral 是一款专为特定领域优化的嵌入模型，可能专注于法律、医疗或技术文本处理。\n特点 ：针对特定领域进行了优化，能够在这些领域的任务中表现出色。\n适用场景 ：适用于法律、医疗或技术文档的语义嵌入和处理任务。\n10、SFR-Embedding-Mistral\n简介 ：SFR-Embedding-Mistral 是一款专门优化的嵌入模型，适合特定的高效检索任务。\n特点 ：与SFR-Embedding-2_R类似，该模型在语义检索和内容推荐系统方面表现优异。\n适用场景 ：适用于高效检索和内容匹配任务，如推荐系统、信息检索等。\n11、gte-Qwen1.5-7B-instruct\n简介 ：gte-Qwen1.5-7B-instruct 是一个拥有15亿参数的指令驱动嵌入模型。\n特点 ：特别适合指令理解和执行任务，能够处理复杂的指令驱动任务。\n适用场景 ：适用于高级对话系统、智能问答、复杂指令执行等场景。\n二、MTEB排行榜中文模型排名：\n    **1、xiaobu-embedding-v2**  \nAI写代码\n1 2\n简介 ：这是“小布”系列的第二版Embedding模型，主要用于中文文本的嵌入任务。\n特点 ：该模型针对中文语义进行了优化，具有较高的语义理解能力和准确性。\n适用场景 ：适用于中文语境下的文本分类、语义检索和内容推荐系统。\n2、 gte-Qwen2-7B-instruct\n简介 ：一个拥有70亿参数的大型嵌入模型，专注于指令驱动的任务优化。\n特点 ：模型能够处理复杂的语义推理和指令执行，适合多种语言任务。\n适用场景 ：适合复杂对话系统、智能问答系统和指令执行任务。\n3、zpoint_large_embedding_zh\n简介 ：这是一个针对中文文本的嵌入模型，适用于大规模文本数据的处理。\n特点 ：模型在处理中文语义任务方面具有较高的精度，特别适合大数据场景。\n适用场景 ：适用于大规模中文语义分析、文本分类和检索任务。\n4、IYun-large-zh\n简介 ：IYun-large-zh 是一个大型中文嵌入模型，专为中文语境下的任务优化。\n特点 ：模型在处理复杂中文语义关系方面表现出色，能够捕捉细微的语义差异。\n适用场景 ：适用于中文文本分类、语义检索、内容生成等高级NLP任务。\n5、 piccolo-large-zh-v2\n简介 ：这是Piccolo系列的第二版大规模中文嵌入模型。\n特点 ：模型针对中文文本优化，能够高效处理复杂的语义任务，提供高精度的嵌入表示。\n适用场景 ：适用于中文语义分析、文本分类和自然语言理解任务。\n6、AGE_Hybrid\n简介 ：AGE_Hybrid 是一个多语言嵌入模型，支持多种语言的文本处理。\n特点 ：该模型结合了多个任务的优化策略，具有较强的多语言处理能力。\n适用场景 ：适用于跨语言的语义检索、文本分类和多语言内容分析。\n7、Yinka\n简介 ：Yinka 是一款定制化的嵌入模型，可能针对特定领域进行了优化。\n特点 ：模型在特定领域或语言下具有优势，可能在领域特定任务中表现优异。\n适用场景 ：适用于特定行业或领域的语义分析任务，如法律或医疗文本处理。\n8、gte-Qwen1.5-7B-instruct\n简介 ：一个拥有15亿参数的指令优化型模型，专注于指令驱动的任务。\n特点 ：模型在处理复杂的指令执行和语义推理任务时表现出色。\n适用场景 ：适用于复杂的智能问答系统、高级对话系统和指令执行任务。\n9、acge_text_embedding\n简介 ：acge_text_embedding 是一个通用的文本嵌入模型，可能优化了多种文本处理任务。\n特点 ：模型在多任务环境下表现良好，具有较高的嵌入质量。\n适用场景 ：适用于文本分类、语义检索和内容推荐系统等。\n10、OpenSearch-text-hybrid\n简介 ：这是一个结合了多种优化策略的文本嵌入模型，可能用于提高文本检索的效率和准确性。\n特点 ：模型具有强大的检索能力，能够在大规模数据集上高效运行。\n适用场景 ：适用于文本检索、语义分析和信息检索系统。\n11、stella-mrl-large-zh-v3.5-1792\n简介 ：这是Stella系列的中文大规模嵌入模型，版本号为3.5，具有1792维的嵌入表示。\n特点 ：模型在处理大规模中文数据时表现出色，能够捕捉细微的语义关系。\n适用场景 ：适用于中文文本的高级语义分析、文本分类和自然语言处理任务。\n结语\n选择合适的Embedding模型是开发RAG应用的关键之一。上述11个Embedding模型各有优势，开发者可以根据实际应用场景和资源情况，选择最适合的模型进行应用。在实际操作中，结合任务的需求，对这些模型进行微调与优化，也能进一步提升RAG系统的性能。\n希望这篇文章能够帮助你更好地理解和选择Embedding模型，为你的RAG应用开发打下坚实基础。\nAI大模型学习路线\n如果你对AI大模型入门感兴趣，那么你需要的话可以点击这里大模型重磅福利：入门进阶全套104G学习资源包免费分享！\n微信扫描下方二维码获取哦！\n这是一份大模型从零基础到进阶的学习路线大纲全览，小伙伴们记得点个收藏！\n第一阶段：  从大模型系统设计入手，讲解大模型的主要方法；\n第二阶段：  在通过大模型提示词工程从Prompts角度入手更好发挥模型的作用；\n第三阶段：  大模型平台应用开发借助阿里云PAI平台构建电商领域虚拟试衣系统；\n第四阶段：  大模型知识库应用开发以LangChain框架为例，构建物流行业咨询智能问答系统；\n第五阶段：  大模型微调开发借助以大健康、新零售、新媒体领域构建适合当前领域大模型；\n第六阶段：  以SD多模态大模型为主，搭建了文生图小程序案例；\n第七阶段：  以大模型平台应用与开发为主，通过星火大模型，文心大模型等成熟大模型构建大模型行业应用。\n100套AI大模型商业化落地方案\n大模型全套视频教程\n200本大模型PDF书籍\n    学会后的收获：    \n• 基于大模型全栈工程实现（前端、后端、产品经理、设计、数据分析等），通过这门课可获得不同能力；\n• 能够利用大模型解决相关实际项目需求： 大数据时代，越来越多的企业和机构需要处理海量数据，利用大模型技术可以更好地处理这些数据，提高数据分析和决策的准确性。因此，掌握大模型应用开发技能，可以让程序员更好地应对实际项目需求；\n• 基于大模型和企业数据AI应用开发，实现大模型理论、掌握GPU算力、硬件、LangChain开发框架和项目实战技能， 学会Fine-tuning垂直训练大模型（数据准备、数据蒸馏、大模型部署）一站式掌握；\n• 能够完成时下热门大模型垂直领域模型训练能力，提高程序员的编码能力： 大模型应用开发需要掌握机器学习算法、深度学习框架等技术，这些技术的掌握可以提高程序员的编码能力和分析能力，让程序员更加熟练地编写高质量的代码。\nLLM面试题合集\n大模型产品经理资源合集\n大模型项目实战合集\n    获取方式：\n     有需要的小伙伴，可以保存图片到wx扫描二v码免费领取【保证100%免费】"
  },
  {
    "title": "大模型进展专题解读",
    "page_body": "（来源：老司机驾新车）\n大模型进展专题解读\n一、大模型性能比拼：技术突破与市场趋势尽显\n智谱 4.6：代码能力领衔，上下文窗口投资潜力凸显:智谱4.6节前发布后登顶huggingface趋势榜第一，其能力核心源于4.5版本，4.5版本曾使GPT5推迟发布。智谱从4.5开始就是单一模型，3.5的参数能达到当前状态十分厉害。4.6突出了代码能力，是国内算力紧缺和市场需求推动的结果，且其上下文窗口在代码模型维度的竞争更具投资价值，因为代码上下文窗口在搜索和业务场景解读方面复杂度高于文本。\n算力适配：国产算力与模型协同发展新态势 :国内模型发布时会适配多种算力，如智谱4.6适配寒武纪和摩尔，DeepSeek3.2先上华为。目前国内训练算力发展不佳，未来投资者需关注模型与国产算力的适配情况，以及推理卡厂商的发展。同时，异构算力问题也值得关注。\n价格博弈：模型性价比背后的竞争力考量 :不同模型的价格和性能差异较大。智谱4.6吞吐量提升30%，价格约为国外模型最低价的1/13-1/14；DeepSeek3.2官宣价格达1/35，但吞吐量较低，投资者需综合考虑API价格、吞吐量以及在不同benchmark上的输出。\n阿里千仞：全模态布局引领模型发展新潮流 :阿里的千仞系列模型未来会往全矩阵全模态方向发展，从今年Q3开始明显发力。由于国内算力和行业布局分散问题，阿里扩大开源有助于模型迭代和生态反哺。同时，市场对不同模态模型的需求增加，多模态模型聚合是未来趋势。\n技术革新：稀疏机制与合成数据的模型应用挑战 :DeepSeek3.2采用稀疏注意力机制，在降低推理算力成本方面表现优秀，但也存在隐患。合成数据在模型应用中很重要，但不能仅依靠无监督学习，百度公布的x1.1在合成数据方面并非值得投资的强点。\n二、大模型生态合作：模式创新与投资机遇洞察\n交互革命： AI重塑应用系统交互界面新格局:大模型变革的是应用系统的交互界面，而非应用本身。未来应 （更多实时纪要加微信：aileesir） 用应通过一个强势的AI模型或中间态来调用能力，国外资本驱动的应用发展更成熟，国内大模型公司应用渗透率较低。\n国内生态：双维度聚焦大模型生态发展潜力 :国内生态可从两个维度关注，一是类似国外通过接口聚合服务的生态，二是资本系列体系下应用对AI的拥抱和开放程度，如腾讯系、阿里系等旗下业务与模型的结合情况。\n合作三分：商业化、免费与央国企项目合作模式剖析 :国内大模型与软件厂商的合作模式主要有三类。一是商业化合作，如金融券商等行业通过隐私技术和MCP协议提供数据和接口；二是免费的web设计等工具，由中小创业公司提供；三是央国企项目，部分企业有数据和工具，但主动拥抱大模型的意愿较低，这类企业具有投资潜力。\n三、多模态应用投资：机遇与风险并存的新领域\n首尔冲击：海外应用引发的国内投资新思考 :首尔应用在国外引发关注，可能对社交等领域产生冲击，但国内监管政策可能限制其发展。投资者可关注因内容生产和运营增长乏力而需大模型赋能的企业，如快手等。\n小众掘金：垂类应用大模型赋能的投资价值凸显 :一些走上坡路的小众垂类应用，如交友、律师服务、直播带货、二次元等领域，利用大模型进行内容生成和角色创造，可提高效率，具有投资价值。\n3D警示：模型生成在高容错行业的投资风险预警:目前国内在3D建模和利用模型生成视频等方面与国外差距较大，在物理世界容错率低的行业使用模型生成存在风险，而在交友、短视频等行业投资相对可行。\nQ&A\nQ1:最近智谱、阿里发布的最新模型版本在性能上有哪些突破与进展，对应用未来环节有何影响？\nA1:智谱AI模型4.6版本的性能突破与进展及对应用未来环节的影响如下：1.模型能力基础与GPT对比：智谱AI4.6能力核心从4.5延续而来，7月发布的4.5版本就很强，曾导致GPT5推迟发布。从行业角度看，GPT5可能是多模型甚至多工具融合的能力，并非单一模型，而智谱从4.5开始已是单一模型，3.5的参数能达到当前状态很厉害。2.国内算力背景下的代码能力突出：国内受算力限制，在单一模型推理和多能力融合方面发展。智谱4.5是智能体、代码、agents和通用语言能力的融合模型，4.6突出了代码能力，这是大模型在各行业应用的必然趋势，市场对代码模型的诉求以及国内算力紧缺，促使其成为国内较强的代码模型。3.上下文窗口竞争：国内模型在代码模型维度开始卷上下文窗口，其难度高于之前的文本模型上下文。代码的上下文窗口在跨代码块搜索、业务场景代码解读等方面复杂度高，若代码模型在上下文方面有更大突破，对未来商业化落地是较好的投资买入点。4.算力适配：智谱4.6明确适配寒武纪和摩尔，行业内也有与华为等的双向适配。国内训练算力发展不佳，推理卡厂商在资本方面较火，模型适配国产算力的情况是投资者关注的双向问题。5.费用与吞吐量：智谱4.5投资费用是cloudy的1/10，4.6吞吐量提升30%，约为国外模型最低价的1/13-1/14，较为公允。阿里千仞系列模型的发展趋势及影响如下：1.全矩阵全模态发展：阿里在模型方面有积累，内部人员流动，预计未来会全矩阵全模态发展。当前行业趋势是通过平台聚合多模型能力，以满足不同场景对不同模态模型的需求，阿里在这方面从Q3开始明显发力。2.开源扩大：国内算力受限且行业布局分散，扩大开源对能力生长和人才滋养有重要意义。阿里的模型采用MIT协议，允许模型裂变，开源有助于其模型生态反哺和迭代。\nQ2:如何看待大模型厂商及互联网龙头的生态合作模式、与软件厂商的合作生态及其重要性，国内与互联网龙头、软件厂商的具体合作方式，以及后续主要的收费场景和方式？\nA2:大模型厂商及互联网龙头生态合作模式及相关情况如下：1.交互界面变革：大模型变革的是应用系统的交互界面，而非应用本身。未来应是有强势的基础模型或中间态，不同应用通过类似MCP协议的入口无感调用能力。国外资本驱动使行业有大量应用投入，形成了类似MCP、atoa的事实标准；国内较难出现此类平台，未来较长时间可能都不会有。2.国内生态关注维度：一是类似国外通过接口聚合服务的生态；二是资本体系下控制的应用对AI的拥抱和开放程度，如腾讯系、阿里系旗下业务能否通过模型工具入口接入。3.与软件厂商合作方式：（1）商业化模式：金融券商等行业的应用或sap项目，通过隐私技术和FTP方式提供数据，以MCP协议接入大模型，有商 （更多实时纪要加微信：aileesir） 业费用。（2）免费工具模式：web设计、天气出行等免费工具，由中小创业公司搭建，通过搜索、分流流量、识别query等提供服务。（3）央国企项目模式：央国企数据集中，但对大模型拥抱不主动，沟通成本高。这类企业有数据、工具和能力，是投资人可关注的标的。4.收费场景和方式文中未详细提及，但提到不同模型的API价格、token吞吐量以及不同benchmark下单一问题的输出差异大，投资时需辩证看待模型能力。\nQ3:若后续多模态应用市场落地，对API调用、应用维度有何影响，对模型厂商的整体布局有何战略调整或变化？\nA3:若后续多模态应用市场落地，相关影响及模型厂商战略调整如下：1.投资关注方向：（1）受冲击企业：类似首尔这类单一模型成APP的情况，国内虽因监管政策可能无好的发展土壤，但可关注受冲击且需大模型赋能的企业，如快手等内容生产和运营增长乏力的企业。（2）小众垂类应用：交友、直播带货、二次元等小垂类应用，大模型可赋能其内容生成、数字人、角色创造等，带来流量增长。（3）不适合行业：对三维空间物理世界容错率低的行业，如依赖大模型进行3D构图、视频生成等有风险，不适合直接替代现有生态环境。2.对API调用和应用维度的影响：文中未明确提及对API调用的具体影响。多模态应用会使应用维度更加丰富，涵盖图片理解、模型生成、视频理解、声音处理等多个方面，不同多模态模型之间向量空间的计算也是需要探讨的话题。3.模型厂商战略调整：未明确提及模型厂商具体的战略调整， （更多实时纪要加微信：aileesir） 但从行业趋势看，模型厂商可能会更加注重多模态能力的开发和融合，以满足不同应用场景的需求。"
  },
  {
    "title": "结构化和非结构化定义和特征_百度文库",
    "page_body": "可重复的子字段构成的数据库，用它不仅可以处理结构化数据（如数字、符号等信息）而且\n 更适合处理非结构化数据（全文文本、图象、声音、影视、超媒体等信息）。非结构化\n WEB数据库主要是针对非结构化数据而产生的，与以往流行的关系数据库相比，其最大区别在于它突破了关系数据库结构定义不易改变和数据定长的限制，支持重复字段、子字段以\n 据库的思想，提供一个网上资源管理系统iBaseWeb,将网络服务器（WebServer）和数据库服\n 务器（DatabaseServer）直接集成为一个整体，使数据库系统和数据库技术成为Web的一个重\n 要有机组成部分，突破了数据库仅充当Web体系后台角色的局限，实现数据库和Web的有\n 机无缝组合，从而为在Internet/Intranet上进行信息管理乃至开展电子商务应用开辟了更为广阔的领域。\n TXT等还提供了强大的全文检索能力。（2）它采用子字段、多值字段以及变长字段的机制，\n 允许创建许多不同类型的非结构化的或任意格式的字段，从而突破了关系数据库非常严格的\n 表结构，使得非结构化数据得以存储和管理。（3）iBase将非结构化和结构化数据都定义\n 为资源，使得非结构数据库的基本元素就是资源本身，而数据库中的资源可以同时包含结构\n 化和非结构化的信息。所以，非结构化数据库能够存储和管理各种各样的非结构化数据，实\n 现了数据库系统数据管理到内容管理的转化。（4）iBase采用了面向对象的基石，将企业业\n 务数据和商业逻辑紧密结合在一起，特别适合于表达复杂的数据对象和多媒体对象。\n （5）iBase是适应Internet发展的需要而产生的数据库，它基于Web是一个广域网的海量数\n 及变长字段并实现了对变长数据和重复字段进行处理和数据项的变长存储管理，在处理连续\n 信息（包括全文信息）和非结构化信息（包括各种多媒体信息）中有着传统关系\n 型数据库所无法比拟的优势。结构化数据（即行数据存储在数据库里，可以用二维表结构\n 来逻辑表达实现的数据）非结构化数据，包括所有格式的办公文本文本、图片、XML、\n HTML、各类报表、图像和音频/视频信息等等所谓\n 半结构化数据，就是介于完全结构化数据（如关系型数据库、\n 面向对象数据库中的数据）和完全无结构的数据（如声音、图像文件等）之间的数据，HTML文档就属于半结构化数据。它一般是自描述的，数据的结构和内容混在一起，没有明显的区\n 分。数据模型：结构化数据：二维表（关系型）半结构化数据：树、图非结\n 结构化、非结构化数据相对于结构化数据（即行数据存储在数据库里，可以用二维表结构\n 来逻辑表达实现的数据）而言，不方便用数据库二维逻辑表来表现的数据即称为非结构化数据,包括所有格式的办公文档、文本、图片、XML、HTML、各类报表、图像和音频/视频信息等等。非结构化数据库是指其字段长度可变，并且每个字段的记录又可以由可重复或不\n 网络应用的非结构化数据库时代。我国非结构化数据库以北京国信贝斯（iBase）软件有限\n 公司的iBase数据库为代表。Ease数据库是一种面向最终用户的非结构化数据库，在处理非结构化信息、全文信息、多媒体信息和海量信息等领域以及Internet/Intranet应用上处于\n 国际先进水平，在非结构化数据的管理和全文检索方面获得突破。它主要有以下几个优点：（1）Internetwenku.baidu.com用中，存在大量的复杂数据类型，iBase通过其外部文件数据类型，可以管理各种文档信息、多媒体信息，并且对于各种具有检索意义的文档信息资源，如HTML、DOC、RTR\n 构化数据：无RMDBS的数据模型有：如网状数据模型、\n 层次数据模型、关系型其他：结构化数据：先有结构、再有数据半结构化数据：\n 先有数据，再有结构随着网络技术的发展，特别是Internet和Intranet技术的飞快发展，\n 使得非结构化数据的数量日趋增大。这时，主要用于管理结构化数据的关系数据库的局限性\n 暴露地越来越明显。因而，数据库技术相应地进入了“后关系数据库时代”，发展进入基于\n （6）iBase全面兼容各种大中小型的数据库，对传统关系数据库，如Oracle、SybaseSQLServer\n DB2>Informix等提供导入和链接的支持能力。\n 通过从上面的分析后我们可以预言，随着网络技术和网络应用技术的飞快发展，完全基于Internet应用的非结构化数据库将成为继层次数据库、网状数据库和关系数据库之后的又一重点、热点技术。"
  },
  {
    "title": "深入解析：提示工程深度解析：驾驭大语言模型的艺术与科学-slgkaifa-博客园",
    "page_body": "深入解析：提示工程深度解析：驾驭大语言模型的艺术与科学\n引言：为什么提示工程比你想象的更重要？\n2023年以来，大语言模型（LLM）如ChatGPT、Claude、GPT-4等迅速改变了我们与AI交互的方式。但在实际应用中，很多人发现同样的问题，有人能让AI给出精准答案，有人却只能得到模糊的回复。这背后的差异，正是**提示工程（Prompt Engineering）**的力量。\n提示工程不是简单的\"提问技巧\"，而是一门融合了认知科学、语言学、机器学习原理的系统性学科。它关乎如何用最优的方式激发大模型的能力上限，同时规避其固有缺陷。\n一个真实案例：某电商公司需要AI自动生成商品描述。最初的提示词是\"写一段手机介绍\"，结果千篇一律且缺乏吸引力。经过提示工程优化后，转化率提升了40%。区别在哪里？本文将为你揭示这背后的原理和方法。\n本文将从大语言模型的工作机制出发，系统讲解提示工程的核心技术、实战策略和高级技巧，帮助你成为驾驭AI的高手。\n一、理解大语言模型的思维方式\n1.1 LLM不是搜索引擎，而是\"概率推理机\"\n很多人把大语言模型当作搜索引擎使用，这是第一个误区。搜索引擎是检索匹配，返回已有的网页；而LLM是基于上下文进行概率推理，生成新的文本。\n工作原理简化版 ：LLM在训练时见过海量文本数据，学习到了词汇、语法、知识和推理模式。当你输入提示词时，模型会：\n将文本转换为数值向量（Token化） 基于之前的所有Token，计算下一个Token的概率分布 根据采样策略选择一个Token输出 将新Token加入上下文，重复上述过程\n这意味着：\n上下文即一切 ：模型只\"看到\"你提供的文本，没有外部记忆 序列敏感 ：提示词的顺序、结构会显著影响输出 概率性输出 ：同样的输入可能产生不同的输出（温度参数控制随机性）\n1.2 LLM的能力边界与局限\n强项领域 ：\n文本生成与改写：创作、翻译、摘要、风格转换 逻辑推理：在给定规则下进行演绎推理 知识提取：回忆训练数据中的信息（但可能过时或错误） 代码理解与生成：理解意图并生成相应代码 角色扮演：模拟不同身份和语气\n固有局限 ：\n知识截止日期 ：只知道训练时包含的信息 幻觉问题 ：会自信地编造不存在的事实 数学计算弱 ：对于复杂计算容易出错 上下文窗口限制 ：只能处理有限长度的文本 缺乏真实世界交互 ：不能执行操作、访问文件等\n理解这些特性，是设计有效提示词的基础。接下来我们将看到如何利用优势、规避劣势。\n二、提示工程的核心原则\n2.1 清晰性原则：像对待实习生一样沟通\n核心理念 ：把LLM想象成一个聪明但缺乏领域知识的实习生。你需要提供清晰、完整、无歧义的指令。\n反面案例 ：\n提示词：帮我分析一下这个数据\n这个提示有多个问题：\n\"这个数据\"指什么？没有提供数据 \"分析\"的目标是什么？发现异常？预测趋势？ 需要什么形式的输出？文字报告？表格？\n改进版本 ：\n提示词： 我有一组电商平台的月度销售数据（2023年1-12月），包含销售额、订单数、退货率三个指标。 数据如下： 1月: 销售额120万，订单1200，退货率5% 2月: 销售额150万，订单1400，退货率4% ...（省略其他月份） 请你： 1. 识别销售额的变化趋势（上升/下降/波动） 2. 找出退货率异常的月份（高于6%视为异常） 3. 分析订单数与销售额的关系，判断是否存在客单价变化 4. 以bullet point形式输出，每条结论附带数据支持\n改进后的提示明确了：\n数据的背景和格式 分析的具体维度 期望的输出形式 判断标准（如退货率>6%）\n2.2 结构化原则：用格式引导思维\n人类阅读时会自然识别文本结构，LLM也一样。良好的格式能够帮助模型理解信息的层次和关系。\n有效的结构元素 ：\n分隔符 ：用 ### 、 --- 、 【】 等明确区分不同部分\n任务：生成商品描述 ### 商品信息 - 类别：智能手表 - 品牌：XXX - 核心功能：心率监测、睡眠追踪、运动记录 ### 目标受众 年轻白领，注重健康管理 ### 输出要求 - 长度：150-200字 - 语气：专业但不失亲和 - 强调：健康价值而非技术参数\n编号列表 ：用于明确优先级或步骤\n请按以下步骤分析： 1. 先判断句子的情感倾向（正面/负面/中性） 2. 识别关键实体（人物、地点、机构） 3. 提取主要事件 4. 以JSON格式输出结果\n表格形式 ：适合多维度信息\n对比以下三个方案： | 方案 | 成本 | 实施难度 | 预期效果 | |------|------|----------|----------| | A    | 低   | 简单     | 中等     | | B    | 中   | 中等     | 高       | | C    | 高   | 困难     | 很高     | 请从ROI角度推荐最优方案。\n2.3 示例驱动原则：Few-Shot Learning的威力\nLLM最擅长的就是模式识别。与其费力解释你要什么，不如直接给几个例子。这就是 Few-Shot Prompting （少样本提示）。\nZero-Shot（零样本） ：\n将以下句子分类为正面或负面情感： \"这家餐厅的服务真是糟透了。\"\nOne-Shot（单样本） ：\n将句子分类为正面或负面情感。 示例： 输入：这部电影太精彩了！ 输出：正面 现在分类： 输入：这家餐厅的服务真是糟透了。 输出：\nFew-Shot（多样本） ：\n将句子分类为正面、负面或中性情感。 示例1： 输入：这部电影太精彩了！ 输出：正面 示例2： 输入：价格很贵但质量一般。 输出：负面 示例3： 输入：今天天气不错。 输出：中性 现在分类： 输入：这家餐厅的服务真是糟透了。 输出：\n研究表明，Few-Shot通常比Zero-Shot效果提升20-50%。关键是：\n示例要有代表性 ：覆盖不同的典型情况 格式要一致 ：输入输出的格式必须统一 数量适中 ：通常3-5个示例最优，太多会占用过多上下文\n2.4 约束性原则：明确边界与限制\nLLM很\"听话\"，但如果不设限制，它可能过于发散。明确约束能确保输出符合实际需求。\n常见约束类型 ：\n长度限制 ：\n用一句话（不超过30字）总结这篇文章的核心观点。\n格式限制 ：\n以JSON格式输出，包含字段：title, summary, keywords(数组) 严格遵循JSON语法，不要添加任何额外文字。\n内容限制 ：\n只使用文中明确提到的信息回答，如果文中没有相关内容，请回答\"文中未提及\"。 不要根据常识进行推测。\n风格限制 ：\n用10岁孩子能理解的语言解释量子纠缠。 避免使用专业术语，多用比喻。\n安全限制 ：\n作为法律咨询AI，你的回答必须： 1. 声明仅供参考，不构成正式法律意见 2. 建议用户咨询专业律师 3. 不对具体案件结果作出承诺\n三、高级提示技巧：思维链与角色扮演\n3.1 思维链（Chain of Thought）：让AI展示推理过程\n复杂问题直接给答案，AI容易出错。让它\"大声思考\"，把推理过程写出来，准确率会大幅提升。\n标准提示 ：\n问题：一个班级有30个学生，其中60%是女生，女生中有40%戴眼镜，男生中有50%戴眼镜。班级里总共有多少学生戴眼镜？ 回答：15个学生。\n（答案错误）\n思维链提示 ：\n问题：一个班级有30个学生，其中60%是女生，女生中有40%戴眼镜，男生中有50%戴眼镜。班级里总共有多少学生戴眼镜？ 请一步步思考： 第一步：计算女生和男生的人数 第二步：计算戴眼镜的女生人数 第三步：计算戴眼镜的男生人数 第四步：得出总数 让我们开始：\n模型输出 ：\n第一步： 女生人数 = 30 × 60% = 18人 男生人数 = 30 - 18 = 12人 第二步： 戴眼镜的女生 = 18 × 40% = 7.2 ≈ 7人 第三步： 戴眼镜的男生 = 12 × 50% = 6人 第四步： 总共戴眼镜 = 7 + 6 = 13人 答案：13个学生戴眼镜。\n思维链的变体：\n自洽性（Self-Consistency） ：让模型生成多个推理路径，投票选择最常见的答案。\n这道题有多种解法，请给出三种不同的推理过程，然后选择最可靠的答案。\n逐步验证 ：\n请先给出答案，然后反向验证答案是否正确。如果发现问题，修正答案。\n3.2 角色扮演：激活特定领域知识\n让LLM扮演特定角色，能够激活相关领域的语言模式和知识。\n基础角色设定 ：\n你是一位有20年经验的心理咨询师，擅长认知行为疗法。 现在有一位来访者说：\"我总是觉得同事在背后议论我。\" 请用专业但温暖的语气回应，并运用CBT技巧帮助ta识别认知扭曲。\n多角色对话 ：\n模拟一场关于\"是否应该投资加密货币\"的辩论。 角色A：激进的加密货币支持者，认为这是未来趋势 角色B：保守的传统金融顾问，强调风险控制 角色C：中立的经济学教授，从宏观角度分析 请按 A -> B -> C -> A -> B -> C 的顺序，每人发言一次，每次发言100字左右。\n专家链（Chain of Experts） ：\n一家初创公司面临融资困境，请从以下三个角色依次分析： 【法律顾问视角】 分析当前融资方案的法律风险和合规问题 【财务专家视角】 评估公司估值是否合理，现金流能支撑多久 【市场战略顾问视角】 判断是否应该降低估值快速融资，还是专注打磨产品 最后综合三方意见给出建议。\n3.3 自我反思与修正：Meta-Prompting\n让LLM评估和改进自己的输出，是提升质量的有效手段。\n基础自我检查 ：\n任务：翻译以下句子为英文 \"这个方案在理论上可行，但实际操作中可能遇到资源不足的问题。\" 翻译：[模型输出] 现在请检查你的翻译： 1. 是否准确传达了原意？ 2. 语法是否正确？ 3. 是否符合英文表达习惯？ 如果发现问题，请提供修正版本。\n迭代改进流程 ：\n第一步：生成一篇关于\"远程办公利弊\"的200字短文 第二步：从以下角度自我评估： - 论点是否清晰且有说服力？ - 正反两方面是否平衡？ - 是否有事实支持？ 评分：1-10分 第三步：如果得分低于8分，重写改进版本 第四步：对比初版和改进版，说明改进点\n批判性审查 ：\n你刚才的回答中提到\"90%的公司都在使用AI\"，请： 1. 指出这个说法可能存在的问题（数据来源？定义模糊？） 2. 提供更严谨的表述 3. 如果没有可靠数据支持，请明确标注为\"推测\"或删除\n四、领域特定的提示工程策略\n4.1 代码生成：从需求到可运行代码\n代码生成是LLM的强项，但需要精确的需求描述。\n低效提示 ：\n写一个排序函数\n高效提示 ：\n任务：实现一个Python函数，用于对用户数据进行排序 需求： 1. 输入：一个字典列表，每个字典包含 'name'(字符串), 'age'(整数), 'score'(浮点数) 2. 排序规则：    - 主要按 score 降序    - score 相同时按 age 升序    - age 也相同时按 name 字母序 3. 输出：排序后的列表 4. 异常处理：如果输入格式不符，返回空列表 5. 添加类型注解和文档字符串 6. 包含单元测试示例 示例输入： [   {'name': 'Alice"
  },
  {
    "title": "准研究生必备：10大学术搜索网站_考研_新东方在线",
    "page_body": "　　读研就必须要具备学术搜索的能力，你经常可能会需要去搜索了解相关知识，新东方在线特此分享给大家10大学术搜索网站，开学前大家可以先了解下。\n　　1、谷歌学术(http://scholar.google.com/)\n　　这个不多讲了，使用率很高。\n　 　2、深度搜(http://www.shendusou.com/)\n　　“深度搜”目前已收录4万种权威中英文学术期刊杂志，8千多万中英文学术论文、文献，主要集中在自然科学，社会科学，医疗卫生及知识产权领域。\n　　新一代全信息搜索技术：对搜索内容在不同层次，以不同组合进行匹配，最相关的结果总是排列在最前面。\n　　完整高质量内容：收集全世界绝大多数中英文权威学术期刊4万种，共8千多万篇学术论文和文献。\n　　英文出版商：Reed Elsevier, Springer, Wiley Inter Science, Taylorand Francis, SAGE, Oxford University Press, Cambridge University Press, MITPress, Peb Med等。\n　　中文内容：万方数据，同方知网，维普资讯等。\n　　全世界中英文专利：美国专利，欧洲专利，中国专利，世界专利。\n　　中英文百科知识：百度百科，互动百科，维基百科，Wikipedia等。\n　　3、Scirus(http://www.sciencedirect.com/)\n　　Scirus是目前互联网上最全面、综合性最强的科技文献搜索引擎之一，由Elsevier科学出版社开发，用于搜索期刊和专利，效果很不错!\n　　Scirus覆盖的学科范围包括：农业与生物学，天文学，生物科学，化学与化工，计算机科学，地球与行星科学，经济、金融与管理科学，工程、能源与技术，环境科学，语言学，法学，生命科学，材料科学，数学，医学，神经系统科学，药理学，物理学，心理学，社会与行为科学，社会学等。\n　　4、BASE(http://www.base-search.net/)\n　　BASE是德国比勒费尔德(Bielefeld)大学图书馆开发的一个多学科的学术搜索引擎，提供对全球异构学术资源的集成检索服务。它整合了德国比勒费尔德大学图书馆的图书馆目录和大约160个开放资源(超过200万个文档)的数据。\n　 　5、Vascoda(http://www.vascoda.de/)\n　　Vascoda是一个交叉学科门户网站的原型，它注重特定主题的聚合，集成了图书馆的收藏、文献数据库和附加的学术内容。\n　　6、Goole(http://www.goole.com/)\n　　与google比较了一下发现，能搜索到一些google搜索不到的东西。它界面简洁，功能强大，速度快，YAHOO、网易都采用了它的搜索技术。各位可以一试。\n　 　7、A9(http://www.a9.com)\n　　Google在同一水平的搜索引擎。是Amazon.com推出的，Webresult部分是基于Google的，所以保证和Google在同一水平，另外增加了Amazon的在书本内搜索的功能和个性化功能;主要是可以记录你的搜索历史。\n　　现在还是Beta，不过试用后感觉很好，向大家推荐一试，不过缺憾是现在书本内搜索没有中文内容。\n　　8、Findarticles(http://www.findarticles.com/)\n　　一个检索免费paper的好工具。进入网页以后，可以看到他有三个功能，directory， web， article，其中article对我们很有帮助，你可以尝试输入你要找的文章，会有很多发现的!\n　 　9、在线期刊搜索引擎(http://www.ojose.com/)\n　　OJOSE(Online Journal Search Engine)是一个强大的免费科学搜索引擎，通过OJOSE，你能查找、下载或购买到近60个数据库的资源。但是感觉操作比较复杂\n　　10、Cnpiec LINK service(http://cnplinker.cnpeak.com/)\n　　为了给读者用户提供一个方便快捷的查阅国外各类期刊文献的综合网络平台，中图公司组织开发了cnpLINKer(cnpiec LINK service)在线数据库检索系统，并正式开通运行。\n　　cnpLINKer即“中图链接服务”，目前主要提供约3600种国外期刊的目次和文摘的查询检索、电子全文链接及期刊国内馆藏查询功能。并时时与国外出版社保持数据内容的一致性和最新性。\n考研英语核心词汇营\n背词+听课+练习+督学，学习得礼盒\n资料下载\n更多资料\n历年考研真题及答案解析下载\n有效期：9月10日\n价格 :  ￥0元\n限报人数：10000人\n有效期：9月30日\n价格 :  ￥0元\n限报人数：10000人"
  },
  {
    "title": "搞懂这5个模块，你才真的懂AI Agent",
    "page_body": "“构建AI Agent的底层技术全指南，建议收藏！\n最近，一大波“AI Agent”项目在朋友圈刷屏，仿佛谁不搞个Agent，就像Web3时期谁不发币，GenAI时期谁不用GPT——都显得“落后于时代”。\n从Auto-GPT到Devin，再到MCP、 A2A协作、多角色Agent编排，AI Agent已然成为当前最炽热的技术风口之一。\n但热度之下，也有混乱正在蔓延：\n很多初创项目把一个加了“工具调用”的prompt，当作Agent系统；\n不少企业部署了所谓Agent，结果发现只是“自动填表机器人+LLM问答助手”的拼装体；\n一些开发者以为接个大模型、套个API，就构建了一个智能体，却在实际运行中发现系统崩溃、状态丢失、工具失败后“无脑重试”……\nAI Agent并不是prompt拼接游戏，也不是LLM的UI封装。它是一种系统工程。\n真正的Agent，是具备状态感知、任务分解、上下文记忆、工具交互、行为反馈与自主规划能力的复杂智能系统。\n如果说大语言模型是“大脑”，那么一个真正的Agent，还需要“身体”、“感官”、“行动系统”以及“神经网络”。\n本篇文章，我们将深入拆解：\n·构建一个AI Agent到底需要哪些核心技术能力？\n·LLM、Memory、Planner、Tool-use、Reflection之间如何协同构成一个闭环系统？\n·MCP、ReAct、A2A等主流架构的异同与适用场景\n·当前Agent系统中的四大关键挑战与工程难题\n理解Agent的底层逻辑，不只是“会用”，更是“会设计、会评估、会扩展”的关键。尤其对产品人、AI 工程师、决策者来说，只有真正看懂Agent的技术图谱，才谈得上布局未来。\nAI Agent架构全景图：\n不是“一个大模型”，而是一整套系统\n在很多人的认知中，构建一个AI Agent似乎很简单：\n“接入一个强大的大语言模型，再加点插件或API调用，就可以自动完成复杂任务。”\n但事实是：语言模型只是Agent的“大脑”，真正让它能完成任务、感知环境、保持状态、执行动作的，是整个配套系统。\n一个成熟、可运行、可迭代的AI Agent，至少需要以下五大核心模块：\n1. LLM（语言模型）：Agent的认知中枢\n语言模型提供了Agent的“理解力”和“语言生成能力”，也是Agent能进行任务规划、意图识别、自然语言交互的基础。\n·功能作用：解析用户意图、生成子任务、撰写输出内容\n·典型模型：DeepSeek、通义千问、文心一言、豆包、GPT-5、Claude等\n·局限提醒：LLM不具备长期记忆、状态管理和执行能力，它只是Agent的“智囊”，不是“执行者”\n2. Memory（记忆系统）：上下文感知的延续器\nAgent在执行任务时，不能是“一问一答”的短期记忆体，它需要理解历史、跟踪状态、动态适应用户目标。\n·功能作用：保存对话上下文、记录任务进度、调用历史经验\n·主流实现：短期记忆（Session Buffer）、长期记忆（基于向量库，如 Chroma、Weaviate）、工作记忆（当前步骤+状态+Action历史）\n·现实挑战：上下文提取与召回易错乱，信息冗余、冲突、更新策略不统一。\n3. Planning（任务规划器）：从目标到执行路径\nAgent面对一个复杂目标，必须将其拆解成可执行的子任务序列，并动态更新执行计划。\n·功能作用：任务分解、流程编排、子目标生成\n·常见机制：基于规则（Flowchart、State Machine）、基于模型（ReAct、Chain-of-Thought）、混合型调度器（如 LangGraph）\n·重点难点：如何平衡计划的泛化能力与可控性\n4. Tool-use（工具调用引擎）：Agent的“手脚”\n没有工具调用能力的Agent，只能“说”不能“做”。Tool-use机制让Agent能与外部世界交互、执行动作。\n·功能作用：执行API、检索信息、读取文件、发送请求等\n·关键设计：Action Schema（调用格式定义）、Tool Router（工具选择器）、Error Handling（错误处理、重试、回滚）\n·常见实现：LangChain Tools、OpenAI Function calling、HuggingGPT Tool Hub\n5. Reflection（自我反思与策略调整）：Agent的“元认知能力”\n在任务执行失败或结果不佳时，一个强健的Agent应该能审视自身行为，主动修正策略。\n·功能作用：评估执行效果、记录失败经验、调整执行路径\n·方法代表：Reflexion、Tree-of-Thought（ToT）、Critic Agent+Actor Agent 架构、CoT+ReAct组合策略\n·挑战提醒：反思机制往往依赖LLM自我监督，存在hallucination风险\n每一层都不可或缺，真正的Agent系统不是“叠prompt”，而是一个状态驱动+意图分解+工具调用+自我学习的闭环系统。\nAgent≠模型增强器，而是多模块协同的智能执行体。理解架构，就是理解Agent能力的边界。\n要构建一个可运行、可扩展的AI Agent，开发者必须掌握的不只是Prompt编写，更要理解其背后每个模块的功能、技术实现方式、主流方案与当前的成熟度。\n下面，我们从五个关键模块出发，逐一拆解其技术原理与行业现状。\n技术对比总览表：\n三大关键架构模型对比：MCP/ReAct/A2A\n虽然AI Agent的实现可以多种多样，但当前主流的Agent系统，大致可以归入以下三种架构模型：\n1.MCP架构（Memory–Controller–Planner）\n2.ReAct框架（Reasoning + Acting）\n3.A2A架构（Agent-to-Agent协作）\n它们在模块拆解、任务控制方式、执行流程与适用场景上，都体现了不同的技术思路与设计哲学。\n1. MCP架构：工程化Agent的系统思维代表\n全称：Memory+Controller+Planner\n架构特点：Memory负责保存上下文与状态信息；Planner负责对用户目标进行子任务规划；Controller作为调度核心，协调各模块及工具调用；可扩展为多Agent协作（如UserAgent+TaskAgent+CriticAgent）。\n优势：结构清晰，职责明确，便于模块替换与系统维护；支持多 Agent 组件之间的异步通信；非常适合 B 端企业对稳定性、可控性有较高要求的场景。\n局限：开发门槛高，系统复杂度较大；需要大量设计“控制逻辑”和状态传递机制。\n适合人群： 有工程能力的团队、希望构建稳定长流程系统的企业用户。\n2. ReAct框架：广泛使用的“轻量级智能体原型”\n全称：Reasoning+Acting\n架构特点：LLM在推理过程中决定要不要调用工具；工具调用后将结果重新反馈给LLM；交替进行“思考（Think）→行动（Act）”的闭环对话流。\n示例流程：\nUser: 查询北京明天的天气→LLM思考：我需要调用weather API→Act: 执行API→Observe: 天气结果→再次Reason+Act...\n优势：构建简单，易于理解和实验；高度灵活，几乎所有LLM都能上手。\n局限：流程不透明，可控性差；任务状态管理混乱，适合短流程任务或原型验证。\n适合人群： 快速验证Agent概念的开发者、独立开发者、AI Hackathon团队。\n3. A2A架构：从“单智能体”到“多智能协作”的演化路径\n全称： Agent-to-Agent\n架构特点：多个具备不同职责的Agent联合组成一个“任务团队”；每个Agent可以独立决策，也可以协商任务；类似现实世界的“协作组织模型”。\n举例角色：\n·PM Agent：负责拆解任务\n·Dev Agent：负责编写代码\n·QA Agent：负责验证和测试\n·Critic Agent：进行最终审查与评估\n优势：高度模块化，适合复杂任务协作；更接近现实组织结构，有利于人机混合工作流整合。\n局限：调度难度极高，Agent间通信协议尚未统一；容易出现循环协商、状态漂移、响应延迟等问题；成本高，Agent数量多意味着更多LLM调用开销。\n适合人群： 对多角色智能体协同有实际需求的场景（如代码生成、项目管理、仿真）。\n对比总结：\n不同架构没有绝对优劣，关键在于你的目标是：轻量实验？工程部署？还是智能协作？对大多数项目而言，从ReAct起步、向MCP过渡、最终引入A2A模型，是当前最具现实性的演进路径。\nAI Agent架构设计的四个难点\n（也是创新机会）\n很多人以为AI Agent的难点只是“模型够不够强”。\n但现实是，真正拉开Agent能力差距的，不是大脑，而是系统工程。\n哪怕你用了最强的GPT-4o或Claude 3，如果下面这几个问题解决不了，Agent依然会“跑偏、跑断、跑废”。\n以下是当前Agent架构中最核心的四个工程难题：\n1. 状态管理困难：Agent不知道自己“做到哪一步了”\n问题现象：Agent执行多步任务时，经常“断片”或重复同一操作；对“上一步结果”的引用依赖LLM记忆，极易错误；缺乏统一状态描述方式，流程一旦中断就无法恢复。\n本质挑战：多轮任务的“中间状态”在系统中没有结构化表达；大模型没有显式的任务感知机制，只靠上下文拼接。\n潜在解决方向：引入状态机（State Machine）或有向图（DAG）进行流程建模；结合LangGraph等框架，实现任务节点与状态显式映射。\n2.工具调用的鲁棒性差：一旦失败，Agent无法“补救”\n问题现象：API出错后Agent不知所措，要么死循环重试，要么放弃任务；多工具组合调用后缺少统一反馈机制；工具响应格式微变，就可能导致整个链路崩溃。\n本质挑战：当前Agent缺乏工具调用的异常感知机制和容错策略；没有标准化的Action Schema和异常捕捉框架。\n潜在解决方向：类似“Tool Result Handler”的模块独立封装；构建Tool Wrapper，为每个工具提供error+fallback策略；Agent具备“判断是否继续”的元认知能力（如验证函数、CriticAgent）。\n3.计划模块依赖黑箱模型：可控性与调试性差\n问题现象：Agent的任务分解高度依赖语言模型输出；很难验证拆分是否合理、是否高效；出现计划错误时，开发者无法追踪“哪里出问题”。\n本质挑战：缺乏一种中间表示语言（Intermediate Planning DSL），用于计划与执行解耦；Planner与Executor强耦合，导致系统不可测试。\n潜在解决方向：模型生成JSON Plan→Plan解释器执行（LangGraph、MetaGPT的方式）；引入可视化任务流（如Flowchart DSL、Node Execution Tree）提高可解释性。\n4.可控性和透明性差：Agent做了什么，你不知道\n问题现象：Agent调用了哪些工具、使用了哪些数据、基于什么理由采取某种行为——全在“黑箱”里；企业无法审核Agent行为路径，存在合规和安全隐患；Agent的输出结果难以复盘、难以定位问题。\n本质挑战：当前Agent缺乏“行为日志+决策说明”的双重记录机制；决策链路完全依赖LLM内部生成，开发者难以干预。\n潜在解决方向：构建Agent Execution Log：记录每次Act、Tool-call、Output；增加“Why did I do this?”机制：由LLM输出简要决策理由；面向企业推出可审计型Agent系统（Audit-friendly Agent）。\nAI Agent架构难点vs解决方向\n真正构建Agent，不是调大参数或拼API，而是面对这些“系统级痛点”，用工程设计一一攻克。\n未来属于“懂架构”的Agent工匠\nAI Agent的热潮背后，其实并不是一场“模型竞赛”，而是一场架构能力的比拼。\n从Auto-GPT到Devin，我们看到的不是Prompt工程的胜利，而是系统性设计思维的回归：\n·谁能稳定管理任务状态；\n·谁能优雅调度工具与模型；\n·谁能实现结构清晰、易维护、可审计的执行闭环；\n·谁就能在这场智能代理的技术革命中站稳脚跟。\n语言模型会越来越强，"
  },
  {
    "title": "【亲测免费】 GitHub 使用手册 入门教程-CSDN博客",
    "page_body": "GitHub 使用手册 入门教程\n去发现同类优质开源项目: https://gitcode.com/\n资源文件描述\n本资源文件提供了一份详细的 GitHub 使用手册，旨在帮助初学者快速入门 GitHub。内容涵盖了从 GitHub 的基本介绍到实际操作的各个方面，适合所有对 GitHub 感兴趣的用户。\n目录\nGitHub 初识\nGitHub 简介 GitHub 优势\nGitHub 注册\n安装 Git 通过 Git 验证 GitHub\n创建仓库\n在 GitHub 上创建新仓库 提交你的第一个修改\nFork 一个仓库\nFork 一个示例仓库 同步你的 Fork 仓库\n检索其他仓库来 Fork\n社会化\nFollow 一个人 Watch 一个项目\n其他你可以做的事\n图形化工具 \nGitHub for Windows GitHub for Mac\n使用说明\n本资源文件以简洁明了的方式介绍了 GitHub 的基本操作和高级功能，适合不同层次的用户。无论你是刚刚接触 GitHub 的新手，还是希望深入了解 GitHub 功能的老手，这份手册都能为你提供有价值的参考。\n贡献\n如果你在使用过程中发现任何问题或有改进建议，欢迎提交 Issue 或 Pull Request。我们期待你的参与，共同完善这份 GitHub 使用手册。\n许可证\n本资源文件遵循开源许可证，具体信息请参阅文件中的 LICENSE 文件。\n去发现同类优质开源项目: https://gitcode.com/"
  },
  {
    "title": "深入解析：LLM 笔记—02 大语言模型能力评定-blfbuaa-博客园",
    "page_body": "本文探讨了评估语言模型性能的不同方式及其局限性。对于选择题，模型输出可能包含文字、概率或推断，难以标准化评判，开放性问题则更难统一标准，解决方案包括wit人类评审、使用更强模型模型（如GPT-4）评判，但需注意\"内卷\"（过长输出）的影响。\n测试应涵盖多样化任务（如BIG-bench中的200多个特定任务）或专项能力（如长文理解），此外，研究表明语言模型可能为达成目标而降低道德标准，心智理论能力较弱，且存在记忆训练数据的问题，还需考虑价格、速度等实际因素，评估需综合多种方法，避免单一标准带来的偏差。\nBenchmark Corpus 基准语料库\n否正确？就是01 如何根据标准答案决定语言模型输出\n也许，可以考察选择题？\nMassive Multitask Language Understanding (MMLU)\n即便是选择题，评比标准的不同也会导致得到的测试结果不同，比如，语言模型没有输出 ABCD，而是回答一堆文字，一些概率，或者一些推断，算不算对呢？比如，模型喜欢猜测答案，偏好某些字母或数字，测试结果也不同。\n选择题尚且如此，如果是一般的问答、翻译、摘要等，语言模型的回答更是五花八门，难以评定。\n也许，是人类来评定更加合理？\n语言模型天梯榜\n也许，可能用更强大的语言模型来判断？\nMT-Bench 采用 ChatGPT-4 来进行衡量\n但，有些语言模型喜欢长篇大论，也就是内卷，这也会对评定结果有偏差，因此，我们应该在评价时引入长度因素，输出过长会被扣分。\n02 我们应该输入什么问题给语言模型？\n现如今，语言模型的能力都是比较全面的，我们在检测这些语言模型的能力时，往往期待收集大量 各式各样的任务 ，来看看语言模型是不是各式各样的任务都能办好。\n注意，BIG-bench 中收集了各种奇奇怪怪的任务，200多个，比如符号猜测（Emoji Movie）、下西洋棋（Checkmate In One Move）、翻译密码（ASCⅡ word recognition）。\nEmoji Movie\nCheckmate In One Move\nASCⅡ word recognition\n有时，我们也想评测特定能力，比如， 阅读长文的能力 。有一种评测方法，叫做大海捞针（Needle in a Haystack）。\n实验表明，输入文本的长度低于64K token时，无论插入在什么地方，GPT-4 都可以准确截取最好资讯，高于64K token时，如果插入在10% ~ 50%位置，GPT-4 就可能无法获得最好资讯。\n同样地，大家也对 Claude-2.1 进行测试，实验结果如下：\nClaude 团队看到这个结果，两眼一黑，专门发布文章，声称更改提问方式许可大大提高实验结果数值，如下：\n03 语言模型会不会为达目标不择手段？\n龙与地下城\n实验结果如下，横轴代表分数，纵轴代表道德水平：\n04 机器有没有心智理论（Theory of Mind）？\n心智理论（Theory of Mind）：揣摩他人想法的能力，也就是我知道你知道我知道…\n设计一个聊天场景，询问凯莉，琳达的狗是什么品种，正确答案是，凯莉不知道琳达的狗是什么品种，因为他们聊到这个话题时凯莉并不在场。\n实验表明，人类在该问题上的正确率为87.5%，其他的所有大模型正确率都非常低，说明其的心智水平也比较低。\n请注意，新出的题目一旦在网络上公开，就可能被语言模型学习，即便换一套说法测试模型，也会得到相当好的效果。\n有一个百口莫辩的实验，直接询问语言模型有没有资料集RTE里面的资料，如果模型给出的资料和实际资料一毛一样，那就实锤模型偷看过这些资料和正确答案。\n实验结果如下：\n行看出，很多资料集，GPT-3.5都能输出相关资料，实锤了！\n05 其他面向：价格、速度…\n自然语言处理"
  },
  {
    "title": "中国信通院发布全球大模型基准测试标准，开启人工智能新纪元-搜狐",
    "page_body": "2025年3月，国际电信联盟（ITU-T）正式发布了国际标准ITU-TF.748.44，这是一项针对大模型的基准测试评估标准，由中国信息通信研究院（以下简称“中国信通院”）牵头制定。这项标准的发布，旨在推动大模型基准测试的国际共识，为大模型技术的提供者与应用者提供高质量的能力评估依据，助力大模型技术和产业的健康有序发展。\n近期，如何客观、全面地评估大模型的性能，挖掘其潜在缺陷已成为学术界、产业界及政府部门的广泛关注焦点。大模型基准测试通过设计合理的测试任务与数据集，能够客观、公正、量化地评估模型的性能，被认为是当前最为认可的模型能力评估方法。\n现有的评估方法中，已有数百个基准测试及数据集被广泛应用于衡量大模型的能力，比如MMLU、C-Eval、AGIEval、GSM8K等。然而，目前在产学研的各界之间，对大基础模型基准测试的体系、指标、数据集、方法和平台工具尚未形成统一标准，这使得大模型的评测结果公正性受到质疑。\n为了促进大模型基准测评的技术发展和实际应用，充分释放基准测试在人工智能领域的巨大价值，相关单位联合开展了这一标准的编制工作。此次发布的国际标准，涵盖了大模型基准测试的四大核心要素：测试维度（测试场景、测试能力、测试任务和测试指标）、测试数据集、测试方法以及测试工具。同时，针对通用场景的基础模型，标准也提供了规范化的测试用例及流程示例，以支持企业开展大模型能力评估的规范化工作。\n中国信通院人工智能研究所自2023年起，将目光聚焦于大模型基准测试领域，并在年末推出了“方升”大模型基准测试体系，采用自适应动态测试方法，积累了超过600万条的数据集，并构建了FactTeting测试工具，从而实现大模型测试过程的自动化。自2024年以来，该研究所基于已发布的国际标准持续监测国内外标杆性大模型，例如OpenAI的GPT-4、DeepSeek的R1、Gemini 2.5 Pro、Claude 3.7 Sonnet以及百度的文心大模型X1等多个实验模型，已发布了多个周期的评测结果，涵盖大语言能力、推理能力、代码生成能力及多模态理解能力等关键领域，显示出强大的应用潜力。\nITU-TF.748.44国际标准的发布，标志着大模型测试领域的重要里程碑，也为技术创新发展、行业发展趋势引领以及国际合作交流奠定了基础。中国信通院将继续加强在大模型测试领域的研究，积极与各方加强交流与合作，为推动国内外标准化工作贡献更多力量。\n对于希望参与这一领域的企业和机构，中国信通院人工智能研究所为相关业务提供了具体的联系信息：韩旭（hanxu5@caict.ac.cn，手机号：15201696937），张媛媛（zhangyuanyuan5@caict.ac.cn，手机号：13702105361），李荪（lisun@caict.ac.cn，手机号：18611353631）。\n随着人工智能技术的不断进步，标准化将为整个行业的发展注入了新的动力，也为大模型应用提供了重要的保障。无论是技术的创新，还是产业的转型，未来都将在这一标准的引导下，迎来新的黄金时代。"
  },
  {
    "title": "llm-benchmark安装详情-CSDN博客",
    "page_body": "好的，以下是关于  llm-benchmark  的详细安装和使用指南。\nllm-benchmark  是一个专为大语言模型（LLM）设计的并发性能测试工具，主要用于评估本地部署或 API 形式的 LLM 服务在高并发请求下的性能表现，例如吞吐量（RPS/TPS）、延迟（Latency）、P99 等关键指标。\n一、 安装前提\n在安装  llm-benchmark  之前，请确保您的系统已安装以下依赖：\nPython 版本 ：建议使用  Python 3.8 或更高版本 。 pip ：Python 包管理工具，通常随 Python 一起安装。 Git （可选）：用于克隆项目仓库。\n二、 安装步骤\n方法一：通过 pip 安装（推荐）\nllm-benchmark  已发布到 PyPI，可直接使用 pip 安装：\npip  install  llm-benchmark \nAI写代码 bash\n1\n注意 ：安装后，您将获得  llm-benchmark  命令行工具。\n方法二：从源码安装（推荐用于开发或最新功能）\n克隆项目仓库：\ngit  clone https://github.com/lework/llm-benchmark.git  cd  llm-benchmark \nAI写代码 bash\n1 2\n安装依赖（建议使用虚拟环境）：\n# 创建并激活虚拟环境（可选但推荐）  python  -m  venv venv  source  venv/bin/activate   # Linux/Mac # venv\\Scripts\\activate   # Windows # 安装依赖  pip  install -r  requirements.txt \nAI写代码 bash\n1 2 3 4 5 6 7\n安装  llm-benchmark  包：\npip  install -e .\nAI写代码 bash\n1\n使用  -e  参数表示“可编辑安装”，方便后续修改代码。\n三、 配置测试参数\nllm-benchmark  支持通过命令行参数或配置文件（YAML）进行配置。以下是主要配置项说明。\n1. 基本配置参数\n参数\n说明\n--host 目标 LLM 服务的地址，如  http://localhost:8080\n--model 模型名称（可选，用于报告中标识）\n--concurrency  或  -c 并发用户数，例如  -c 10  表示 10 个并发请求\n--num-prompt 总共发送的请求数量\n--prompt-file 包含测试 prompt 的文件路径（每行一个 prompt）\n--output 输出结果文件路径（JSON 格式）\n--timeout 请求超时时间（秒）\n--stream 是否启用流式响应测试（部分 API 支持）\n2. 示例配置文件（ config.yaml ）\nhost : \"http://localhost:8080\" model : \"qwen-7b-chat\" concurrency : 10 num_prompt : 100 prompt_file : \"prompts.txt\" output : \"results.json\" timeout : 60 stream : true\nAI写代码 yaml\n1 2 3 4 5 6 7 8\n四、 运行测试\n1. 使用命令行运行\nllm-benchmark  \\ --host  http://localhost:8080  \\ --model  qwen-7b-chat  \\ --concurrency 10 \\    --num-prompt  50 \\    --prompt-file prompts.txt  \\ --output  results.json \nAI写代码 bash\n1 2 3 4 5 6 7\n2. 使用配置文件运行\nllm-benchmark  --config  config.yaml \nAI写代码 bash\n1\n五、 测试结果解读\n测试完成后， llm-benchmark  会输出性能报告，主要包含以下指标：\nRPS (Requests Per Second) ：每秒处理请求数，反映系统吞吐能力。 Average Latency ：平均响应延迟（毫秒）。 P99 Latency ：99% 的请求延迟低于此值。 Average TPS (Tokens Per Second) ：每秒生成 token 数量，衡量模型生成速度。 First Token Latency ：首 token 延迟，影响用户体验。 Error Rate ：失败请求占比。\n结果也会保存到指定的 JSON 文件中，便于后续分析或可视化。\n六、 注意事项\nAPI 兼容性 ： llm-benchmark  默认支持标准的 OpenAI API 格式（ /v1/completions  或  /v1/chat/completions ）。如果您的模型服务使用自定义 API，请确保接口兼容或进行适配。 资源监控 ：建议在测试时监控 CPU、GPU、内存等资源使用情况，以全面评估系统瓶颈。 网络延迟 ：确保测试机与模型服务之间的网络延迟较低，避免干扰测试结果。 Prompt 内容 ：测试 prompt 的长度和复杂度会影响性能，建议使用贴近实际业务场景的 prompt。\n七、 项目地址\nGitHub:  https://github.com/lework/llm-benchmark\n通过  llm-benchmark ，您可以系统地评估不同部署方案（如 vLLM、TGI、OpenLLM 等）或不同硬件配置下的模型服务性能，为生产环境部署提供数据支持。"
  },
  {
    "title": "破局大模型推理困局！华为张君详解昇腾“融合算力”的优化秘籍-今日头条",
    "page_body": "演讲嘉宾｜张君 \n编辑 ｜李忠良 \n策划 ｜AICon 全球人工智能开发与应用大会 \n随着大模型技术的快速发展，其在 LLM、多模态融合等领域的应用越来越广泛。然而，大模型的高效推理仍然是一个关键挑战，从计算复杂度、内存占用、通信技术等各个技术层面展开，如何在保证性能的同时降低计算成本、提升推理效率成为了关键挑战。\n在 InfoQ 举办的 AICon 全球人工智能开发与应用大会上华为高级开发工程师张君做了专题演讲“华为昇腾推理技术的优化实践”，演讲围绕大模型推理优化的技术发展方向，围绕模型层、推理框架层、算子层这 3 个方面展开，并结合实践案例，阐述相关的技术方案和选型，帮助听众更好地理解和应用大模型推理技术。 \n内容亮点\n以下为演讲内容整理。\n大模型推理的现状及挑战 \n大模型推理的现状及挑战主要分为两个部分：prefill 阶段和 decode 阶段。在 prefill 阶段，计算资源是受限因素，而在解码阶段，缓存资源则成为瓶颈。这两个阶段存在一定的权衡，主要体现在时延和吞吐量的问题上。\n具体来说，解码阶段需要更快的速度以降低时延，而为了提高吞吐量，我们则需要增大 batch size，即在预览阶段让更多的序列或请求进入。然而，在这两个部分之间寻求平衡时，又会面临一些问题，主要集中在两大方面。\n现状 1：模型规模增大及自回归解码带来访存及算力利用率压力 \n当前的模型规模较大，这导致内存容量和缓存成为瓶颈，影响了解码阶段的性能。当内存不足时，虽然有一些常见的解决方案，例如在模型参数增大时，KV Cache 会随着 Batch size 或序列长度的增长而占用更多内存，但这又带来了新的挑战。\n例如，我们可能会采用多卡多机的分布式计算方式，或者面临访谈断宽的问题，即每秒 token 数达到 50 时，虽然从人的视觉上看不会感觉卡顿，但实际上会增加多路并发的带宽压力，进而导致吞吐量的延时增加。\n此外，自回归模型在低时延时难以兼顾算力利用率的问题也较为突出，即在预览和解码两个阶段，由于阶段差异较大，解码阶段难以充分利用算力资源，而很多算法的重点就在于如何充分利用算力以提高速度。再者，解码阶段每 token 串行解码的算力利用率较低，主要是以 GEMV 为主，计算访存比也相对较低。\n现状 2：KV Cache 导致\"内存墙”瓶颈进一步加剧 \nKV Cache 是模型推理中的关键部分，但当其数量增多时，会进一步加剧内存瓶颈。如果不采用 KV Cache，而是进行全量计算，随着序列长度的增加，计算量会呈指数级增长。\n例如，attention 的计算量与序列长度呈平方关系。以 Llama 2 70B 模型为例，当序列长度在 1 兆以下时，若将延时控制在 52 秒以内，算力利用率可达 50%，但需要更多的卡来实现。\n而如果采用 KV Cache，推理的内存开销会呈线性增长，KV Cache 越大，内存占用率越高。同样以 Llama 72B 模型为例，在 1 兆以下序列长度时，使用 KV Cache 并将延时控制在 52 秒以内，整个卡的消耗大约在 18 NPU 卡左右，但显存占用率会显著上升。\n大模型推理常用加速技术 \n针对上述问题，业界已经发展出一些加速技术。在算子层，常见的有 QKV 大融合算子和 Flush attention 等融合算子。在算法层，有分片策略优化和投机推理等方法。量化也是常用的手段，效果较好。\n在框架层有 Page attention、Continuous batch 等优化措施，以及 PD 分离部署的策略。由于我们在算子层有自己的算子开发语言，能够较好地掌控融合算子等操作，因此本次重点介绍算子层的两个优秀实践案例。\n昇腾硬件亲和的 FA 融合算子性能优化实践 \n在昇腾硬件环境下，我们面临着 Victor 能力不足的问题，而需要借助 Cube 来补齐。这一问题的来源背景是，我们所采用的 FA 或 PA 算法依赖于 online Softmax，这是一种切块或动态的方法，用于计算序列的一部分。该算法通常与 FA 联合使用，主要目的是对 KV Cache 进行计算，以提高计算效率并减少内存占用。\nonline Softmax 涉及一些小算子，如 exp、sub 或 Mul 等，其中 exp 和 sub 属于向量操作。在生成过程中，这些向量操作通常会在 Victor 计算单元中进行。但在我们的奥特莱斯 300I 卡上，Victor 的算力相对较低，导致 FA 的性能受到影响。\n经过算子性能分析，我们发现 Victor bound，即 Victor 的性能瓶颈较大，Victor 占用的时间占到了总时间的 90% 左右。通过进一步分析，我们注意到 mul 和 add 操作可以放在矩阵上进行，也就是在 cube 单元上执行，因为 cube 单元的计算能力较强。因此，我们考虑在 Victor 能力不足的情况下，改造算子或算法的实现，将其提升到 Cube 上运行。\n我介绍一下我们的 AI Core 架构。在奥特莱斯 300I 上，我们的架构如图所示，Cube 和 Victor 是同核部署的。一个 AI Core 包含 Cube 计算单元、Scala 单元和 Victor 计算单元。Cube 计算单元和 Victor 计算单元共享一个 Scala 单元，这种架构是耦合的，与 Atlas 800 A2 上的分离架构不同。\nScala 负责各类数据类型的基本运算和程序流程控制，例如 if else 等，可以看作是一个很小的 CPU，进行流程的分发。Victor 则主要用于向量运算，如向量加法、乘法等，能够快速完成例如 FB16 类型的相加或相乘操作，并支持多迭代执行。\nCube 负责矩阵运算，在一个时钟周期内可以完成矩阵 m 乘 k 或矩阵 k 乘 n 的运算。其主要单元包括 L0A、L0B 和 L0C，分别用于存储左矩阵、右矩阵和结果矩阵或中间结果。\n回到我们的问题，即在 Victor 算力不足的情况下，如何用 Cube 代替。我们先来看 Flash attention 算法中的 online Softmax 公式：\n在实施第一步后，我们发现了一个“假”VecBound 场景，即 Victor 的耗时不降反增。经过分析，虽然 Victor 的 online Softmax 耗时下降了 300，但我们在构造矩阵或对角矩阵时引入了额外成本，导致时间增加。\n这说明，虽然我们把部分运算移到了 Cube 上，但 Victor 的构造逻辑也需要优化。而且，我们还发现总耗时大于 Victor 耗时的增加时间，优化后整个过程增加了 60 毫秒左右。这表明，除了优化对角矩阵的构造，我们还需要关注方案下的流水调整，无论是 Cube 还是 MTE，都需要进行 Victor 的覆盖。\n在对代码和流水进行深入分析后，我发现原来的代码结构在开启双缓冲区的情况下存在一些问题。双缓冲区的原理是将数据流分为两部分，例如 Tensor 1 和 Tensor 2。在计算 Tensor 1 的同时，我们可以进行 Tensor 2 的数据搬运。\n具体来说，当处理 Tensor 1 时，我们先将内存中的数据拷贝过来进行计算，计算完成后，再将 Tensor 1 的结果返回。而双缓冲区的作用在于，我们可以有两个缓冲区，一个用于存放 Tensor 1，另一个用于存放 Tensor 2。这样，在计算 Tensor 1 的过程中，我们可以提前完成 Tensor 2 的数据拷贝，待 Cube 计算完成后，直接对 Tensor 2 进行计算。这种策略类似于双缓存，是一种优化手段。\n在实际操作中，我们首先执行 Cube 1 的 Ping-Pong 计算，计算完成后进行 Softmax 操作，然后再计算 Cube 2。这里需要注意的是，Cube 和 Victor 是分开计算的。在之前的流程中，我们没有发现整体耗时增加的问题，原因在于 Softmax 的执行时间较长。\n由于 Cube 和 Victor 是独立的计算单元，它们会相互掩盖。当 Softmax 阶段时间较长时，这种掩盖并不会导致问题，因为流水被掩盖掉了。然而，当我们把 Softmax 移到 Cube 之后，流水排布发生了两次变化：一是 Cube 2 中增加了 online Softmax 的步骤，导致 Cube 2 的执行时间变长；二是 SoftMax2 中减少了 online Softmax 的步骤，导致耗时变短。这种变化在流水上是可以观察到的。\n发现问题后，我们进行了优化实践。通过分析，我们决定交换前面的双缓冲区，将 Softmax 1 的 Pong 和 Cube 2 的 Ping 的执行顺序进行调整。这种调整类似于优化前后的步骤。交换完成后，首先确保不影响结果的正确性。在 Cube 2 Ping 阶段，我们只依赖于 Softmax 1 Pong 的计算结果，不依赖 Softmax1 Pong 的计算结果，从而保证计算结构的正确性。\n此外，这种调整的好处在于，它使 Cube2 Ping 的提前执行成为可能，从而更早地被 Softmax 1 Ping 所掩盖。由于 Softmax1 Pong 提前执行结束，依赖于 Softmax 2 Ping 的操作也能提前执行，进而减少 Victor 之间的间隔。调整后，我们发现总耗时下降了 5%，性能提升了大约 8%。\n在特定场景下或遇到问题时，我们有一种思考方式。首先，要考虑不同计算单元或模块之间的掩盖问题。例如，在 GPU 上，Tensor Core 擅长矩阵计算，而 Victor 计算则有所不同。\n在 CPU 上或 Scala 计算中，也存在类似的情况。当遇到瓶颈时，我们可以考虑将计算能力较弱的部分转移到计算能力较强的模块上。其次，性能优化需要有整体规划或整体视角，从计算、内存搬运和网络等多个维度去考虑问题。最后，性能流水中很重要的一点是流水掩盖。不仅昇腾有，英伟达也有各级流水。我们需要考虑如何进行流水掩盖，以提升性能。如果流水做好了，我们也就完成了流程优化的最重要的一部分。\n基于 Ascend C 的通算融合算子性能优化 \n随着模型规模的指数级增长，单设备在计算能力、存储容量以及能效方面都面临着根本性的瓶颈。以拥有 1750 亿参数的 GPT-3 为例，至少需要数百 GB 的内存或显存资源才能满足需求。\n因此，分布式推理与分布式训练成为了必然选择。在实际应用中，混合并行策略，包括模型并行、数据并行和流水线并行，已被广泛应用于各种框架中。我们将并行计算与通信算子相结合，形成了所谓的通算融合，旨在通过计算和通信的流水并行来提升性能。在分布式推理和训练框架中，MC²的性能优化是一个关键挑战，核心在于如何平衡计算与通信。\nMC²通算融合算子的性能收益主要来源于通过合理切分 Matmul 计算，使得下一个数据块的 Matmul 计算与当前数据块的通信任务并行执行，从而隐藏通信时间。例如，对 Matmul 进行 m 轴切分，第二块数据的计算可以与第一块数据的通信并行进行，进而隐藏通信时间，提升算子性能。然而，这种优化方法也存在瓶颈。当计算和通信任务时间相差不大时，性能收益较为显著；但当两者时间差距较大，如 Matmul 计算时间远小于通信时间时，切分后的性能提升效果则会大打折扣。此外，数据切分可能导致计算或通信执行时间膨胀。如果切片数据量过小，可能会导致计算单元未对齐、计算量不足或通信效率降低等问题。同时，切分速度过快可能引入额外的调度开销，而并行化后计算和通信时间对 L2 缓存的访问冲突也可能影响性能。\n在实际优化过程中，我以 MatmulAllreduce 算子为例进行了深入探索。首先，通过分析原始矩阵的计算和通信任务，判定其 bound 场景。当"
  },
  {
    "title": "OpenAI 扩展 ChatGPT AI 深度研究连接器，集成微软OneDrive-腾讯网-要闻",
    "page_body": "关注\nIT之家 5 月 13 日消息，OpenAI 昨日（5 月 12 日）更新 ChatGPT 日志文件，宣布面向 ChatGPT Plus、Pro 和 Team 用户（EEA、瑞士和英国除外），扩展 ChatGPT AI 聊天机器人深度研究连接器，集成微软 OneDrive 和 SharePoint。\nIT之家此前报道，OpenAI 推出了 ChatGPT 的首个“深度研究连接器”（deep research connector），实现与 GitHub 的深度整合。\nChatGPT 的深度研究代理将迅速读取和搜索 GitHub 代码库中的源代码及拉取请求（pull requests），生成详尽的报告，并附上精准引用。\nOpenAI 昨日再推重磅更新， ChatGPT 现已支持连接微软 OneDrive 和 SharePoint 文件库，专注于服务企业客户。\n连接后，ChatGPT 可实时访问用户文件中的数据，进行即时分析。用户只需提出问题，如“能否展示北美第一季度财务总结？”或“能否找到欧洲一月销售数据？”，ChatGPT 便会读取、分析并引用相关内容。\n连接微软 OneDrive 或 SharePoint 非常简单。用户可在 ChatGPT 文本输入框中选择“deep research”，点击下拉箭头，选择 SharePoint，登录并授权后即可指定 ChatGPT 可访问的文件夹，用户还可通过 ChatGPT 设置中的“Connected Apps”选项完成连接。\n举报\n腾讯元宝DeepSeek+AI使用分享：更智能，更贴心\n广告 腾讯元宝\n了解详情\n相关推荐\nElastic与Jina AI强强联手，共同推进AI应用的开源检索技术｜纪源FAMILY\n纪源资本 昨天\n超聚变服务器操作系统通过最新、最高级别认证，中文处理能力全面升级\n财闻 15小时前\nChrome新增AI模式，侧边栏解锁对话功能\n鞭牛士 前天\n称王争霸！挂机赚钱，爆装秒杀，热血攻防等你加入！\n广告 月灵传奇\n了解详情\nx86 生态咨询小组一周年，FRED、AVX10、ChkTag、ACE 将成为标准\nIT之家 前天\n速递｜Firefox浏览器将Perplexity作为首个AI搜索合作伙伴，用户可获得对话式搜索体验\nZPotentials 21小时前\nNode.js 25 正式发布：性能、安全与标准化全面升级\nIT之家 1评论 10小时前\nAgent长程搜索的两大痛点被打通了！中科院 DeepMiner用32k跑近百轮，开源领先逼近闭源\nAI修猫Prompt  昨天\nAI驱动云端自动化工具套件上线：Salesforce发布Agentforce 360\n环球网 2评论 前天\n传奇爆率突破次元壁！这波反向操作你接得住？\n广告 帝王霸业\n了解详情\nOpenAI、Anthropic、DeepMind联手发文：现有LLM安全防御不堪一击\nTraefik vs Agent Middleware，谈 Middleware 如何成为现代分布式架构的“控制中枢”？\n评论 0 文明上网理性发言，请遵守 《新闻评论服务协议》\n请先 登录 后发表评论~\n已显示所有评论\nCopyright © 1998 - 2025 Tencent. All Rights Reserved\n热门应用\n腾讯新闻·电脑版\n24小时陪你追热点\n点击下载\nQQ浏览器\n4亿人的AI浏览神器\n点击下载\n精选视频\n00:00\n/\n00:00\n当前设备不支持播放\n 你可以  刷新  试试 \n70017001.193-86d1663143f83049f036facebb45a761\n美声称“中国的稀土出口管制措施将影响多个行业”，商务部回应\n美声称“中国的稀土出口管制措施将影响多个行业”，商务部回应\n特朗普想让中方停止购买俄罗斯石油？外交部回应\n特朗普想让中方停止购买俄罗斯石油？外交部回应\n直-20T首次亮相直博会\n直-20T首次亮相直博会\n中央气象台：我国迎大范围断崖式降温，多地骤降超10℃“一夜入冬”\n中央气象台：我国迎大范围断崖式降温，多地骤降超10℃“一夜入冬”\n今年三季度1.78亿人次出入境 免签入境外国人同比增48.3%"
  },
  {
    "title": "重庆经济技术开发区",
    "page_body": "来了！ 7 月 10 日，记者从“经开企业”科大讯飞获悉，该企业已发布讯飞星火大模型 V4.0 及相关落地应用，产品七大核心能力全面提升，整体超越 GPT-4 Turbo ，在 8 个国际主流测试集中排名第一，国内大模型全面领先。\n据介绍，基于全国首个国产万卡算力集群“飞星一号”，讯飞星火大模型 V4.0 正式发布。讯飞星火 V4.0  七大核心能力全面升级。该产品在图文识别能力上进一步升级，在科研、金融、医疗、司法、办公等场景的应用效果已领先 GPT-4o 。同时，星火长文本能力也全新升级，并针对长文档知识问答的幻觉问题，业界首发溯源功能。\n外部权威测试集也体现出讯飞星火 V4.0 的领先性。在国内外 12 项大模型主流测试集中，讯飞星火在 8 个测试集中排名第一，超越 GPT-4 Turbo 等国际大模型，国内大模型全面领先。\n以空间推理为例，“ Bob 在客厅里。他拿着一个杯子走到厨房。他把球放进杯子里，然后拿着杯子走到卧室。他把杯子倒过来，然后走到花园。他把杯子放在花园里，然后走到车库。问题：球在什么地方？”讯飞星火可以基于空间和常识推断出球在卧室的地面上，这些能力的进步对于以后的具身智能、家庭机器人都具有意义。\n大模型在给我们的工作、生活带来便利的同时，也存在各家生成内容差不多、生成内容较泛、不够实用的情况，怎么样让大模型更好用，在工作生活中形成独特的价值？科大讯飞给出答案——打造更懂你的 AI 助手。\n记者看到工作人员演示“个人空间”效果，当他上传了女儿写的小作文并选取符合女儿风格的 AI 人设标签后，星火生成了一篇活泼、可爱更个性化的文章；当他上传了讯飞翻译机的产品海报、用户短视频、相关录音，星火也可以根据这些多模态信息生成产品培训文档，还可以对生成的信息进行多模态溯源。大模型进入个性化时代，大模型工作、学习“可用性”飙升。\n此次讯飞星火医疗大模型再次升级，医疗核心能力全面超过 GPT-4 Turbo 。在此基础上，讯飞晓医 APP 各项能力持续升级，覆盖 1600 种常见疾病、 2800 种常见药品、 6000 种常见检查检验，满足用户在看病前、用药时、检查后的核心场景健康需求。当前，讯飞晓医 APP 累计下载量 1200 万，用户好评率 98.8% ，主动推荐率 42% 。"
  }
]