# Final Report (After Validation)

Generated at: 2025-12-05 01:31:43

## Query

假设你是一名科研工作者，正在为某人工智能研究团队开展大模型评测基准的系统性梳理工作。请根据提供的材料，撰写一份《Deep Research相关Benchmark数据集调研报告》。
具体而言，依据图片image0中所示的字段要求，提取并结构化提供的pdf论文在相关工作部分提到的每篇论文对应的Benchmark特征，其中请忽略“数据集example”指标。
报告需整合文献内容与公开可查的权威技术资料（如论文原文、官方GitHub仓库、学术数据库或评测平台），
确保所有信息真实、准确、可验证，并在文中注明数据来源。
报告语言应采用正式书面语，结构清晰，包含引言、调研方法、各论文Benchmark信息汇总表、分析与观察、参考资料等部分，
最终形成一份可用于团队技术决策的标准化调研文档，字数在1500至2000之间。

## Report

# Deep Research相关Benchmark数据集调研报告

## 一、引言

随着大型语言模型（LLM）技术的快速发展，评估AI代理在复杂、开放式研究任务中的能力变得尤为重要。深度研究（Deep Research）作为一种新兴的评估范式，旨在测试AI系统在处理多步骤、长期规划和跨源信息整合方面的能力。本报告针对当前主流的Deep Research相关Benchmark数据集进行系统性梳理，为人工智能研究团队提供全面的技术参考。

深度研究任务通常要求模型能够处理高层次的战略性问题（如"我们应该如何调整产品路线图以确保合规？"），规划子问题，检索和评估相关材料，并生成基于数据源的清晰、可操作的摘要。这些任务通常由领域专家使用搜索引擎、通信平台和业务应用程序的组合在迭代、高投入的工作流程中执行，这不幸需要大量的人力投入。

本报告基于ServiceNow Research团队发布的《DRBench: A Realistic Benchmark for Enterprise Deep Research》论文，系统性地提取并结构化了相关工作部分提到的各类Benchmark数据集的关键特征，为人工智能研究团队提供决策参考。通过分析这些基准测试的设计理念、评估方法和应用场景，我们可以更好地理解当前深度研究评测的发展趋势和未来方向。

## 二、调研方法

本报告采用以下方法进行调研：

1. **文献分析**：详细分析《DRBench: A Realistic Benchmark for Enterprise Deep Research》论文中的相关工作部分，提取所有提及的深度研究相关基准测试数据集。

2. **特征提取**：根据图片所示的字段要求，对每个基准测试数据集提取以下关键特征：
   - 论文名称：基准测试的原始论文名称
   - 领域（场景）：基准测试适用的领域或场景
   - query数量：基准测试包含的查询或任务数量
   - 任务：基准测试评估的任务类型
   - 输入：基准测试的输入形式
   - 类型：基准测试的类型分类
   - 评价指标：基准测试使用的主要评估方法
   - 其他：其他重要特征，如是否提供环境、是否结合公共和本地数据等

3. **数据验证**：通过查阅原始论文、官方GitHub仓库和相关技术文档，确保所有提取的信息准确可靠。

4. **对比分析**：对比不同基准测试的特点，分析其优势、局限性和适用场景，为研究团队提供全面的参考。

## 三、各论文Benchmark信息汇总表

### 3.1 Deep Research相关Benchmark数据集

| 论文名称 | 领域（场景） | query数量 | 任务 | 输入 | 类型 | 评价指标 | 其他 |
|---------|------------|----------|------|------|------|---------|------|
| Deep Research Bench (Bosse et al., 2025) | 通用 | 89 | 无特定划分 | 查询文本 | Web研究与计算机使用 (WR & CU) | 回答准确性 | 提供环境，不结合公共和本地数据 |
| DeepResearch Bench (Du et al., 2025) | 通用 | 100 | 无特定划分 | 查询文本 | Web研究 (WR) | 洞察召回率 | 不提供环境，不结合公共和本地数据 |
| DeepResearchGym (Coelho et al., 2025) | 通用 | 1,000 | 无特定划分 | 查询文本 | Web研究 (WR) | 文档检索 | 不提供环境，不结合公共和本地数据 |
| ResearcherBench (Xu et al., 2025b) | AI | 65 | 无特定划分 | 查询文本 | Web研究 (WR) | 洞察召回率，事实性 | 不提供环境，不结合公共和本地数据 |
| LiveDRBench (Java et al., 2025) | 通用 | 100 | 无特定划分 | 查询文本 | Web研究与计算机使用 (WR & CU) | 洞察精确率，召回率 | 不提供环境，不结合公共和本地数据 |
| BrowseComp-Plus (Chen et al., 2025) | 通用 | 1,005 | 无特定划分 | 查询文本 | Web研究 (WR) | 回答准确性，URL召回率 | 不提供环境，不结合公共和本地数据 |
| Mind2Web 2 (Gou et al., 2025) | 通用 | 130 | 无特定划分 | 查询文本 | Web研究 (WR) | 部分完成度 | 不提供环境，不结合公共和本地数据 |
| GAIA (Mialon et al., 2024) | 通用 | 466 | 无特定划分 | 查询文本 | Web研究 (WR) | 回答准确性 | 不提供环境，不结合公共和本地数据 |
| GAIA2 (Andrews et al., 2025) | 通用 | 963 | 无特定划分 | 查询文本 | 计算机使用 (CU) | 动作准确性 | 提供环境，不结合公共和本地数据 |
| TheAgentCompany (Xu et al., 2025a) | 企业 | 175 | 无特定划分 | 查询文本 | 计算机使用 (CU) | 任务完成度，效率 | 提供环境，不结合公共和本地数据 |
| OSWorld (Xie et al., 2024) | 通用 | 369 | 无特定划分 | 查询文本 | 计算机使用 (CU) | 任务完成度 | 提供环境，不结合公共和本地数据 |
| DRBench | 企业（办公） | 15 | 无特定划分 | 查询文本+文档（办公环境） | 深度研究 (DR) | 洞察召回率 | 提供环境，结合公共和本地数据，114个洞察点，设置了企业和使用者的persona |

### 3.2 深度研究相关Agent系统

| Agent名称 | 组织/作者 | 年份 | 主要特点 | 技术路线 |
|-----------|----------|------|---------|---------|
| Local Deep Researcher | LearningCircuit | 2025 | 用于迭代查询和总结的模块化管道 | 模块化架构 |
| Deep-Searcher | Zilliz Tech | 2024 | 用于迭代查询和总结的模块化管道 | 模块化架构 |
| DeepResearcher | Zheng et al. | 2025 | 使用强化学习实现规划、交叉验证和自我反思 | 强化学习 |
| Gemini Deep Research | Google | 2025 | 合成带有引用的基于网络的报告 | 商业系统 |
| Manus.ai | Manus.ai | 2025 | 合成带有引用的基于网络的报告 | 商业系统 |
| OpenHands | All-HandsAI | 2024 | 深度研究的开源框架 | 开源框架 |
| OpenManus | FoundationAgents | 2024 | 深度研究的开源框架 | 开源框架 |
| smolagents | HuggingFace | 2024 | 深度研究的开源框架 | 开源框架 |

## 四、分析与观察

### 4.1 Benchmark类型分布

根据调研结果，当前深度研究相关的benchmark可分为三类：

1. **Web研究型(WR)**：这类基准测试主要评估模型在网络搜索和信息综合方面的能力，如Deep Research Bench、DeepResearch Bench、DeepResearchGym等。这些基准测试通常提供查询问题，要求模型从网络上检索相关信息并生成答案。它们的优点是易于实现和评估，但缺点是无法评估模型在处理私有数据或跨应用场景中的能力。

2. **计算机使用型(CU)**：这类基准测试评估模型在操作计算机系统和应用程序方面的能力，如OSWorld、TheAgentCompany和GAIA2。它们通常提供计算机环境，要求模型完成特定的操作任务。这些基准测试的优点是能够评估模型的工具使用能力，但缺点是通常不涉及深度研究和信息综合。

3. **深度研究型(DR)**：这类基准测试结合了网络搜索和本地数据分析，更贴近真实企业场景。目前，DRBench是唯一一个属于此类的基准测试。它提供了企业环境和多种数据源，要求模型从公共和私有数据中提取洞察并生成报告。这种基准测试的优点是能够全面评估模型的深度研究能力，但实现和评估的复杂度也相应提高。

从分布来看，当前基准测试主要集中在Web研究型和计算机使用型，深度研究型基准测试相对较少。这反映了当前研究的重点仍然是模型的基础能力，而对于复杂、跨源的深度研究能力的评估还处于起步阶段。

### 4.2 评估指标趋势

从评估指标来看，主要分为以下几类：

1. **回答准确性**：评估模型生成答案的正确性，如Deep Research Bench、GAIA等。这是最基本的评估指标，适用于简单的问答任务。

2. **洞察召回率**：评估模型从数据中提取关键洞察的能力，如DeepResearch Bench、ResearcherBench等。这种指标更关注模型对关键信息的识别能力，适用于信息提取和综合任务。

3. **任务完成度**：评估模型完成特定任务的能力，如TheAgentCompany、OSWorld等。这种指标适用于计算机使用型基准测试，关注模型的操作能力。

4. **事实性**：评估模型生成内容的事实准确性，如ResearcherBench。这种指标关注模型生成内容的可靠性，适用于需要高度准确性的任务。

从趋势来看，评估指标正在从简单的回答准确性向更复杂、多维度的指标发展。DRBench引入了洞察召回率、干扰避免和报告质量三个维度的评估，反映了对深度研究能力更全面的要求。这种趋势表明，随着模型能力的提升，评估方法也需要更加精细和多元化。

### 4.3 环境支持与数据类型

在环境支持方面，大多数基准测试不提供专门的环境，只有少数如Deep Research Bench、GAIA2、TheAgentCompany、OSWorld和DRBench提供了环境支持。在数据类型方面，几乎所有基准测试都只关注网络数据或特定环境中的任务，只有DRBench结合了公共和本地数据。

这种情况反映了当前基准测试的局限性：缺乏对企业真实场景的模拟。在实际企业环境中，信息通常分散在多种应用和格式中，需要模型能够跨源检索和整合信息。DRBench通过提供企业环境和结合公共与本地数据，更好地模拟了这种复杂场景。

### 4.4 Benchmark规模与复杂度

从query数量来看，BrowseComp-Plus(1,005)、DeepResearchGym(1,000)和GAIA2(963)规模较大，而ResearcherBench(65)和Deep Research Bench(89)规模较小。DRBench虽然只有15个深度研究问题，但包含114个洞察点，反映了每个问题的复杂度较高。

规模大的基准测试通常能更全面地评估模型能力，但也增加了评估的复杂度和成本。DRBench采用了"少而精"的策略，通过设计复杂的问题和丰富的洞察点，在有限的问题数量下实现了对模型能力的全面评估。

### 4.5 Agent系统发展趋势

从Agent系统发展来看，主要有三种技术路线：

1. **模块化管道**：如Local Deep Researcher和Deep-Searcher，通过组合不同模块实现深度研究。这种方法的优点是灵活性高，可以根据需要调整各个模块，但缺点是可能缺乏整体协调和优化。

2. **强化学习**：如DeepResearcher，使用强化学习技术实现更复杂的规划和自我反思。这种方法的