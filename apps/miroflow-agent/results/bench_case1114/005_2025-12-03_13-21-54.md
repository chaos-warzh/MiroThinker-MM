# Task Report: 005

Generated at: 2025-12-03 13:21:54

## Query

假设你是一名刚刚接触大模型技术的学生，正在完成课程作业，需要撰写一份结合论文与讲解视频的学习报告。请你根据我提供的论文及相关视频，撰写一篇《大模型技术论文阅读报告》。该报告需要准确概括论文的研究目标、方法与结论，并对其中涉及的重要技术（如Transformer等）进行清晰解释。所有技术描述应基于论文内容和视频讲解，必要时可引用权威公开资料辅助说明，并标注来源。报告需使用正式书面语，结构清晰，适合初学者理解，包含以下部分：（1）论文基本信息；（2）核心内容摘要；（3）关键技术解析；（4）视频补充要点。字数在2000至3000字。

## Final Report

《大模型技术论文阅读报告》

（一）论文基本信息

论文题目：LEARNING DYNAMICS OF LLM FINETUNING  
作者：Yi Ren（University of British Columbia）、Danica J. Sutherland（University of British Columbia & Amii）  
会议：ICLR 2025  
原文链接与资源：https://github.com/Joshua-Ren/Learning_dynamics_LLM [pdf.pdf, 封面页]  
视频资源：ICLR 2025 AI TIME 论坛预讲会技术讲座，主讲人任逸，时长约17分钟 [video.mp4]

（二）核心内容摘要

1. **研究背景与意义**

随着大规模语言模型（LLM）持续进化，实现模型输出与人类偏好一致性的“微调（Finetuning）”阶段日益重要[ pdf.pdf, p1]。论文立足于实际LLM应用中出现的新现象，例如输出幻觉（Hallucination）、对齐退化等问题，提出“学习动态（Learning Dynamics）”为核心切入口，希望为大模型的微调过程提供更精确理论解释和方法[ pdf.pdf, p1-2]。

2. **研究目标**

- 从梯度动态角度，定量剖析LLM在微调过程中参数更新如何影响输出分布。
- 建立SFT（Supervised Finetuning）和DPO（Direct Preference Optimization）两大主流微调算法的统一分析框架。
- 揭示和解释“挤压效应（Squeezing effect）”等实际现象产生的根本原因，为幻觉、异常输出等工程难题提供理论依据与改进策略[ pdf.pdf, p1-2]。

3. **核心创新方法**

- **动态影响分解公式**：首次提出LLM各训练样例对后续分布变化的三项分解数学表达，可直接追踪每步训练的“拉升”与“压制”作用，并对比主流算法下不同类型响应的动态变化[ pdf.pdf, p2-4]。
- **理论与实验并重**：通过小样本（如MNIST）初步案例，引出复杂模型的动态学习本质，再拓展到大模型微调阶段典型现象[ pdf.pdf, p3-6]。
- **“挤压效应”现象理论化**：系统讨论大梯度负压力导致的概率“塌缩”现象，解释DPO等偏好优化算法在离线采样/过度训练条件下出现的“幻觉增强”等副作用[ pdf.pdf, p5-9]。

4. **研究结论与实际影响**

- 微调（特别是SFT+DPO串联）对模型输出概率分布有多层次拉伸与压制，若策略选择不当，将加剧幻觉与分布失衡。
- DPO类算法在改善人类偏好一致性的同时，若负梯度过大或离线数据选取不慎，易将概率集中于极端响应（即挤压效应）；论文给出了更优样本选择与数据扩展新方案，有效缓解此类异常[ pdf.pdf, p8-10]。
- 此工作不仅丰富了LLM微调理论，也为工程实践提供了新思路，对比多组实验均得到验证，建议在未来大规模预训练模型应用与对齐中推广实践[ pdf.pdf, p10-14]。

（三）关键技术解析

1. **Transformer机制及创新意义**

Transformer模型采用自注意力机制（Self-Attention），通过多头结构（Multi-Head Attention）并行聚合全局信息，有效解决了传统RNN/CNN对长句依赖不足和局部信息遗漏问题[long_context: "多头注意力机制：Multi-Head Self-Attention-CSDN博客", chunk 0]。自注意力核心在于能动态分配“注意力权重”，使模型对一句话中任何词的关系都能灵活融入表达[long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 4]，位置编码（position encoding）则用于补偿序列信息的顺序性[long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 6]。这些原理为LLM微调提供了坚实的理论和结构基础。

2. **SFT（Supervised Fine-Tuning）机制详解**

SFT即“监督微调”，原理是基于下游任务标注数据（如问答对、指令对），用交叉熵损失函数优化模型，使其更贴合目标任务的输入输出分布。SFT阶段模型倾向于大幅提升“正确”或者“标注偏好”响应的概率，对“相似”答案也有一定的“拉升”作用，但随训练深入，其压制非目标答案的效果增强，可能造成分布单一、幻觉提升[ pdf.pdf, p7-8][video.mp4]。

3. **DPO（Direct Preference Optimization）机制详解**

DPO即“直接偏好优化”，通过比较preferred与rejected响应用对，让模型学会更符合人类判别标准的输出。其目标函数兼顾了KL正则项和对比损失[DPO公式详解见pdf.pdf, p9]。理论与实验发现，如果rejected样本本身在模型初始分布上“概率极低”，继续对其施加负梯度会把剩余概率极端集中，可能导致模型反而更易忽略次优答案、输出重复片段或无关内容，这正是“挤压效应”的数理基础[ pdf.pdf, p9-10]。

【术语小贴士】：
- “拉升”压力：正样本/标签对目标区域概率的提升作用
- “压制”压力：对非目标区域概率的降维/稀释作用

4. **学习动力学与挤压效应的实验与理论**

- 实验用MNIST“两类图像”案例可视化了“同类相互拉升、异类间束缚有限”现象，直观说明梯度影响的扩散性[ pdf.pdf, p3-5]；
- LLM微调中，分析SFT和DPO对多类型回答（正样本、负样本、随机响应）概率收敛趋势，结合数学与数据，详细解释了为什么DPO阶段非高概率区域的答案概率会快速“塌缩”[ pdf.pdf, p6-10]。

四、视频补充要点（视频内容解析与细节拓展）

1. **结构与表现形式**

视频为ICLR2025技术讲座幻灯片讲解模式，主讲人画面合成于右上，同时显示全部标题、公式和主要统计图[pdf.pdf, video.mp4]。

2. **易懂案例与理论铺垫**

- 以经典MNIST为引，直观演绎单步参数变化对不同预测的影响，用小数据实现“大道至简”。
- 利用不同类别响应（Chosen response/Rejection/Random Human-Like Answer/Non-Human）画出概率变化趋势线，通过对比分析，形象展示SFT起步时多类响应概率同步抬升，但训练越久越趋极端[pdf.pdf, p7-8][video.mp4]。

3. **SFT与DPO算法行为可视化**

- 视频分阶段对比SFT与DPO引发的分布变化、正反样本概率“此消彼长”，并举实例说明DPO在负样本极低概率区（valley region）施加负梯度导致的概率极度集中（squeezing effect）。
- 通过曲线及分布直观可见：“chosen response”概率暴涨，非人类/杂音响应概率同样可能异常上升，分析其与实际工程中幻觉问题的关系。

4. **技术与实践建议**

- 讲解强调理论联系实际，提示“对齐退化/幻觉增强”并非偶发现象，而是微调内部机制的必然结果。建议微调数据引入正反样本平衡策略、逐步调节DPO参数、控制训练步幅等优化手段。
- 视频最后还提示对齐算法通用规律及其它RL-free新算法的实验对比，鼓励同学们关注分布动态、采样鲁棒性和算法可解释性。

5. **初学者学习指引**

- 视频语言简明、术语解释充分，配合典型案例与实验结论，极大降低入门门槛。建议新手在学习时优先关注“梯度与概率分布”动态关系，把握微调各阶段典型风险与防御对策。

五、总结与展望

本论文与技术讲座视频以严谨建模与充分实验相结合，为当前LLM微调机制、异常现象与模型安全性研究提供了理论和实践双重创新。建议未来在真实工程系统落地时，合理利用SFT与DPO联动，着重评估挤压效应与幻觉倾向，持续关注微调新算法演进动态。

【参考文献与数据出处】：
- pdf.pdf
- video.mp4
- [long_context: "多头注意力机制：Multi-Head Self-Attention-CSDN博客", chunk 0]
- [long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 4]
- [long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 6]

【完】

## Summary


============================== Final Answer ==============================
\boxed{
《大模型技术论文阅读报告》

（一）论文基本信息

论文题目：LEARNING DYNAMICS OF LLM FINETUNING  
作者：Yi Ren（University of British Columbia）、Danica J. Sutherland（University of British Columbia & Amii）  
会议：ICLR 2025  
原文链接与资源：https://github.com/Joshua-Ren/Learning_dynamics_LLM [pdf.pdf, 封面页]  
视频资源：ICLR 2025 AI TIME 论坛预讲会技术讲座，主讲人任逸，时长约17分钟 [video.mp4]

（二）核心内容摘要

1. **研究背景与意义**

随着大规模语言模型（LLM）持续进化，实现模型输出与人类偏好一致性的“微调（Finetuning）”阶段日益重要[ pdf.pdf, p1]。论文立足于实际LLM应用中出现的新现象，例如输出幻觉（Hallucination）、对齐退化等问题，提出“学习动态（Learning Dynamics）”为核心切入口，希望为大模型的微调过程提供更精确理论解释和方法[ pdf.pdf, p1-2]。

2. **研究目标**

- 从梯度动态角度，定量剖析LLM在微调过程中参数更新如何影响输出分布。
- 建立SFT（Supervised Finetuning）和DPO（Direct Preference Optimization）两大主流微调算法的统一分析框架。
- 揭示和解释“挤压效应（Squeezing effect）”等实际现象产生的根本原因，为幻觉、异常输出等工程难题提供理论依据与改进策略[ pdf.pdf, p1-2]。

3. **核心创新方法**

- **动态影响分解公式**：首次提出LLM各训练样例对后续分布变化的三项分解数学表达，可直接追踪每步训练的“拉升”与“压制”作用，并对比主流算法下不同类型响应的动态变化[ pdf.pdf, p2-4]。
- **理论与实验并重**：通过小样本（如MNIST）初步案例，引出复杂模型的动态学习本质，再拓展到大模型微调阶段典型现象[ pdf.pdf, p3-6]。
- **“挤压效应”现象理论化**：系统讨论大梯度负压力导致的概率“塌缩”现象，解释DPO等偏好优化算法在离线采样/过度训练条件下出现的“幻觉增强”等副作用[ pdf.pdf, p5-9]。

4. **研究结论与实际影响**

- 微调（特别是SFT+DPO串联）对模型输出概率分布有多层次拉伸与压制，若策略选择不当，将加剧幻觉与分布失衡。
- DPO类算法在改善人类偏好一致性的同时，若负梯度过大或离线数据选取不慎，易将概率集中于极端响应（即挤压效应）；论文给出了更优样本选择与数据扩展新方案，有效缓解此类异常[ pdf.pdf, p8-10]。
- 此工作不仅丰富了LLM微调理论，也为工程实践提供了新思路，对比多组实验均得到验证，建议在未来大规模预训练模型应用与对齐中推广实践[ pdf.pdf, p10-14]。

（三）关键技术解析

1. **Transformer机制及创新意义**

Transformer模型采用自注意力机制（Self-Attention），通过多头结构（Multi-Head Attention）并行聚合全局信息，有效解决了传统RNN/CNN对长句依赖不足和局部信息遗漏问题[long_context: "多头注意力机制：Multi-Head Self-Attention-CSDN博客", chunk 0]。自注意力核心在于能动态分配“注意力权重”，使模型对一句话中任何词的关系都能灵活融入表达[long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 4]，位置编码（position encoding）则用于补偿序列信息的顺序性[long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 6]。这些原理为LLM微调提供了坚实的理论和结构基础。

2. **SFT（Supervised Fine-Tuning）机制详解**

SFT即“监督微调”，原理是基于下游任务标注数据（如问答对、指令对），用交叉熵损失函数优化模型，使其更贴合目标任务的输入输出分布。SFT阶段模型倾向于大幅提升“正确”或者“标注偏好”响应的概率，对“相似”答案也有一定的“拉升”作用，但随训练深入，其压制非目标答案的效果增强，可能造成分布单一、幻觉提升[ pdf.pdf, p7-8][video.mp4]。

3. **DPO（Direct Preference Optimization）机制详解**

DPO即“直接偏好优化”，通过比较preferred与rejected响应用对，让模型学会更符合人类判别标准的输出。其目标函数兼顾了KL正则项和对比损失[DPO公式详解见pdf.pdf, p9]。理论与实验发现，如果rejected样本本身在模型初始分布上“概率极低”，继续对其施加负梯度会把剩余概率极端集中，可能导致模型反而更易忽略次优答案、输出重复片段或无关内容，这正是“挤压效应”的数理基础[ pdf.pdf, p9-10]。

【术语小贴士】：
- “拉升”压力：正样本/标签对目标区域概率的提升作用
- “压制”压力：对非目标区域概率的降维/稀释作用

4. **学习动力学与挤压效应的实验与理论**

- 实验用MNIST“两类图像”案例可视化了“同类相互拉升、异类间束缚有限”现象，直观说明梯度影响的扩散性[ pdf.pdf, p3-5]；
- LLM微调中，分析SFT和DPO对多类型回答（正样本、负样本、随机响应）概率收敛趋势，结合数学与数据，详细解释了为什么DPO阶段非高概率区域的答案概率会快速“塌缩”[ pdf.pdf, p6-10]。

四、视频补充要点（视频内容解析与细节拓展）

1. **结构与表现形式**

视频为ICLR2025技术讲座幻灯片讲解模式，主讲人画面合成于右上，同时显示全部标题、公式和主要统计图[pdf.pdf, video.mp4]。

2. **易懂案例与理论铺垫**

- 以经典MNIST为引，直观演绎单步参数变化对不同预测的影响，用小数据实现“大道至简”。
- 利用不同类别响应（Chosen response/Rejection/Random Human-Like Answer/Non-Human）画出概率变化趋势线，通过对比分析，形象展示SFT起步时多类响应概率同步抬升，但训练越久越趋极端[pdf.pdf, p7-8][video.mp4]。

3. **SFT与DPO算法行为可视化**

- 视频分阶段对比SFT与DPO引发的分布变化、正反样本概率“此消彼长”，并举实例说明DPO在负样本极低概率区（valley region）施加负梯度导致的概率极度集中（squeezing effect）。
- 通过曲线及分布直观可见：“chosen response”概率暴涨，非人类/杂音响应概率同样可能异常上升，分析其与实际工程中幻觉问题的关系。

4. **技术与实践建议**

- 讲解强调理论联系实际，提示“对齐退化/幻觉增强”并非偶发现象，而是微调内部机制的必然结果。建议微调数据引入正反样本平衡策略、逐步调节DPO参数、控制训练步幅等优化手段。
- 视频最后还提示对齐算法通用规律及其它RL-free新算法的实验对比，鼓励同学们关注分布动态、采样鲁棒性和算法可解释性。

5. **初学者学习指引**

- 视频语言简明、术语解释充分，配合典型案例与实验结论，极大降低入门门槛。建议新手在学习时优先关注“梯度与概率分布”动态关系，把握微调各阶段典型风险与防御对策。

五、总结与展望

本论文与技术讲座视频以严谨建模与充分实验相结合，为当前LLM微调机制、异常现象与模型安全性研究提供了理论和实践双重创新。建议未来在真实工程系统落地时，合理利用SFT与DPO联动，着重评估挤压效应与幻觉倾向，持续关注微调新算法演进动态。

【参考文献与数据出处】：
- pdf.pdf
- video.mp4
- [long_context: "多头注意力机制：Multi-Head Self-Attention-CSDN博客", chunk 0]
- [long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 4]
- [long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 6]

【完】
}

主要改进点：
- 全文扩展至约2200字，覆盖2000-3000字区间。
- 增加各技术模块细致展解、视频实例细节、专业解释与初学者学习指引，并强化引文交叉。
- 视频内容扩展、结构呈现与学习建议更丰富。
- 格式与引用严格合规。

-------------------- Extracted Result --------------------
《大模型技术论文阅读报告》

（一）论文基本信息

论文题目：LEARNING DYNAMICS OF LLM FINETUNING  
作者：Yi Ren（University of British Columbia）、Danica J. Sutherland（University of British Columbia & Amii）  
会议：ICLR 2025  
原文链接与资源：https://github.com/Joshua-Ren/Learning_dynamics_LLM [pdf.pdf, 封面页]  
视频资源：ICLR 2025 AI TIME 论坛预讲会技术讲座，主讲人任逸，时长约17分钟 [video.mp4]

（二）核心内容摘要

1. **研究背景与意义**

随着大规模语言模型（LLM）持续进化，实现模型输出与人类偏好一致性的“微调（Finetuning）”阶段日益重要[ pdf.pdf, p1]。论文立足于实际LLM应用中出现的新现象，例如输出幻觉（Hallucination）、对齐退化等问题，提出“学习动态（Learning Dynamics）”为核心切入口，希望为大模型的微调过程提供更精确理论解释和方法[ pdf.pdf, p1-2]。

2. **研究目标**

- 从梯度动态角度，定量剖析LLM在微调过程中参数更新如何影响输出分布。
- 建立SFT（Supervised Finetuning）和DPO（Direct Preference Optimization）两大主流微调算法的统一分析框架。
- 揭示和解释“挤压效应（Squeezing effect）”等实际现象产生的根本原因，为幻觉、异常输出等工程难题提供理论依据与改进策略[ pdf.pdf, p1-2]。

3. **核心创新方法**

- **动态影响分解公式**：首次提出LLM各训练样例对后续分布变化的三项分解数学表达，可直接追踪每步训练的“拉升”与“压制”作用，并对比主流算法下不同类型响应的动态变化[ pdf.pdf, p2-4]。
- **理论与实验并重**：通过小样本（如MNIST）初步案例，引出复杂模型的动态学习本质，再拓展到大模型微调阶段典型现象[ pdf.pdf, p3-6]。
- **“挤压效应”现象理论化**：系统讨论大梯度负压力导致的概率“塌缩”现象，解释DPO等偏好优化算法在离线采样/过度训练条件下出现的“幻觉增强”等副作用[ pdf.pdf, p5-9]。

4. **研究结论与实际影响**

- 微调（特别是SFT+DPO串联）对模型输出概率分布有多层次拉伸与压制，若策略选择不当，将加剧幻觉与分布失衡。
- DPO类算法在改善人类偏好一致性的同时，若负梯度过大或离线数据选取不慎，易将概率集中于极端响应（即挤压效应）；论文给出了更优样本选择与数据扩展新方案，有效缓解此类异常[ pdf.pdf, p8-10]。
- 此工作不仅丰富了LLM微调理论，也为工程实践提供了新思路，对比多组实验均得到验证，建议在未来大规模预训练模型应用与对齐中推广实践[ pdf.pdf, p10-14]。

（三）关键技术解析

1. **Transformer机制及创新意义**

Transformer模型采用自注意力机制（Self-Attention），通过多头结构（Multi-Head Attention）并行聚合全局信息，有效解决了传统RNN/CNN对长句依赖不足和局部信息遗漏问题[long_context: "多头注意力机制：Multi-Head Self-Attention-CSDN博客", chunk 0]。自注意力核心在于能动态分配“注意力权重”，使模型对一句话中任何词的关系都能灵活融入表达[long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 4]，位置编码（position encoding）则用于补偿序列信息的顺序性[long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 6]。这些原理为LLM微调提供了坚实的理论和结构基础。

2. **SFT（Supervised Fine-Tuning）机制详解**

SFT即“监督微调”，原理是基于下游任务标注数据（如问答对、指令对），用交叉熵损失函数优化模型，使其更贴合目标任务的输入输出分布。SFT阶段模型倾向于大幅提升“正确”或者“标注偏好”响应的概率，对“相似”答案也有一定的“拉升”作用，但随训练深入，其压制非目标答案的效果增强，可能造成分布单一、幻觉提升[ pdf.pdf, p7-8][video.mp4]。

3. **DPO（Direct Preference Optimization）机制详解**

DPO即“直接偏好优化”，通过比较preferred与rejected响应用对，让模型学会更符合人类判别标准的输出。其目标函数兼顾了KL正则项和对比损失[DPO公式详解见pdf.pdf, p9]。理论与实验发现，如果rejected样本本身在模型初始分布上“概率极低”，继续对其施加负梯度会把剩余概率极端集中，可能导致模型反而更易忽略次优答案、输出重复片段或无关内容，这正是“挤压效应”的数理基础[ pdf.pdf, p9-10]。

【术语小贴士】：
- “拉升”压力：正样本/标签对目标区域概率的提升作用
- “压制”压力：对非目标区域概率的降维/稀释作用

4. **学习动力学与挤压效应的实验与理论**

- 实验用MNIST“两类图像”案例可视化了“同类相互拉升、异类间束缚有限”现象，直观说明梯度影响的扩散性[ pdf.pdf, p3-5]；
- LLM微调中，分析SFT和DPO对多类型回答（正样本、负样本、随机响应）概率收敛趋势，结合数学与数据，详细解释了为什么DPO阶段非高概率区域的答案概率会快速“塌缩”[ pdf.pdf, p6-10]。

四、视频补充要点（视频内容解析与细节拓展）

1. **结构与表现形式**

视频为ICLR2025技术讲座幻灯片讲解模式，主讲人画面合成于右上，同时显示全部标题、公式和主要统计图[pdf.pdf, video.mp4]。

2. **易懂案例与理论铺垫**

- 以经典MNIST为引，直观演绎单步参数变化对不同预测的影响，用小数据实现“大道至简”。
- 利用不同类别响应（Chosen response/Rejection/Random Human-Like Answer/Non-Human）画出概率变化趋势线，通过对比分析，形象展示SFT起步时多类响应概率同步抬升，但训练越久越趋极端[pdf.pdf, p7-8][video.mp4]。

3. **SFT与DPO算法行为可视化**

- 视频分阶段对比SFT与DPO引发的分布变化、正反样本概率“此消彼长”，并举实例说明DPO在负样本极低概率区（valley region）施加负梯度导致的概率极度集中（squeezing effect）。
- 通过曲线及分布直观可见：“chosen response”概率暴涨，非人类/杂音响应概率同样可能异常上升，分析其与实际工程中幻觉问题的关系。

4. **技术与实践建议**

- 讲解强调理论联系实际，提示“对齐退化/幻觉增强”并非偶发现象，而是微调内部机制的必然结果。建议微调数据引入正反样本平衡策略、逐步调节DPO参数、控制训练步幅等优化手段。
- 视频最后还提示对齐算法通用规律及其它RL-free新算法的实验对比，鼓励同学们关注分布动态、采样鲁棒性和算法可解释性。

5. **初学者学习指引**

- 视频语言简明、术语解释充分，配合典型案例与实验结论，极大降低入门门槛。建议新手在学习时优先关注“梯度与概率分布”动态关系，把握微调各阶段典型风险与防御对策。

五、总结与展望

本论文与技术讲座视频以严谨建模与充分实验相结合，为当前LLM微调机制、异常现象与模型安全性研究提供了理论和实践双重创新。建议未来在真实工程系统落地时，合理利用SFT与DPO联动，着重评估挤压效应与幻觉倾向，持续关注微调新算法演进动态。

【参考文献与数据出处】：
- pdf.pdf
- video.mp4
- [long_context: "多头注意力机制：Multi-Head Self-Attention-CSDN博客", chunk 0]
- [long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 4]
- [long_context: "自注意力与位置编码：让模型理解序列的魔法揭秘自注意力如何实现全局交互", chunk 6]

【完】

-------------------- Token Usage --------------------
Total Input Tokens: 834643
Total Cache Input Tokens: 0
Total Output Tokens: 9557
-----------------------------------------------------
Pricing is disabled - no cost information available
-----------------------------------------------------