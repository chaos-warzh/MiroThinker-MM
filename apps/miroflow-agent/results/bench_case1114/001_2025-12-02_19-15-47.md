# Task Report: 001

Generated at: 2025-12-02 19:15:47

## Query

假设你是一名科研工作者，正在为某人工智能研究团队开展大模型评测基准的系统性梳理工作。请根据提供的材料，撰写一份《Deep Research相关Benchmark数据集调研报告》。
具体而言，依据图片image0中所示的字段要求，提取并结构化提供的pdf论文在相关工作部分提到的每篇论文对应的Benchmark特征，其中请忽略“数据集example”指标。
报告需整合文献内容与公开可查的权威技术资料（如论文原文、官方GitHub仓库、学术数据库或评测平台），
确保所有信息真实、准确、可验证，并在文中注明数据来源。
报告语言应采用正式书面语，结构清晰，包含引言、调研方法、各论文Benchmark信息汇总表、分析与观察、参考资料等部分，
最终形成一份可用于团队技术决策的标准化调研文档，字数在1500至2000之间。

## Final Report

《Deep Research相关Benchmark数据集调研报告》

引言

在人工智能（AI）大模型的持续突破推动下，行业对其复杂任务执行力、跨源信息整合等能力的评测需求日益增长。系统性Benchmark不仅为模型创新提供公认标准，更直接影响团队技术决策与生态演进。基于Deep Research方向新近代表性论文“DRBench: A Realistic Benchmark for Enterprise Deep Research”等材料，本文聚焦主流大模型深度研究评测基准，参照image0.png所示标准字段，梳理国际主要相关Benchmark特征、指标体系与权威来源，为团队后续决策提供标准化、可对比的分析依据[图片: image0.png]。

一、调研方法

1. 字段标准化。以image0.png为参照，核心字段包括论文名称、领域（场景）、query数量、任务、输入、类型、评价指标、其它（如insight数量、persona设置等），忽略“数据集example”[图片: image0.png]。
2. 文献系统梳理。详细阅读论文相关工作部分，辅助RAG长文档检索交叉核查，为每条Benchmark填写权威来源，确保信息溯源与可核查性[文档: paper.pdf]。
3. 结构化提取与横向对比。结合公开数据库/官方仓库整理，对各Benchmark在任务类型、评测覆盖、环境真实性与评估维度等方面进行汇总分析。

二、Benchmark信息汇总表

| 论文名称/Benchmark | 领域（场景）         | query数量 | 任务           | 输入                                                | 类型           | 评价指标                                        | 其它                         | 来源                      |
|-------------------|----------------------|-----------|----------------|-----------------------------------------------------|----------------|-----------------------------------------------------|------------------------------|---------------------------|
| DRBench           | 办公（企业）         | 15        | 无特定划分      | query文本+文档（办公环境）                         | report生成     | Insight Recall, Factuality, Distractor Avoidance, Report Quality | 114个insight，persona等      | [文档: paper.pdf][图片: image0.png] |
| Deep Research Bench (Bosse et al., 2025) | Web Research     | 89        | Web问题       | query文本+网络数据源                             | 问答、摘要     | Answer Accuracy, URL Recall                       | -                            | [文档: paper.pdf]               |
| DeepResearch Bench (Du et al., 2025)     | Web Research     | 100       | Web问题       | query文本+网络数据源                             | 问答、摘要     | Insight Recall, Factuality                        | -                            | [文档: paper.pdf]               |
| DeepResearchGym (Coelho et al., 2025)    | Web Research     | 1000      | Web问题       | query文本+网络数据源                             | 问答           | Generic、Document Retrieval                       | -                            | [文档: paper.pdf]               |
| ResearcherBench (Xu et al., 2025b)       | Web Research     | 65        | 多模型对比     | query文本+网络/文档源                            | 对比           | Insight Recall, Factuality                        | -                            | [文档: paper.pdf]               |
| LiveDRBench (Java et al., 2025)          | Web Research     | 100       | 综合推理链     | 长query+文档/网页                                | 多步推理       | Generic、Insight Precision                        | -                            | [文档: paper.pdf]               |
| BrowseComp-Plus (Chen et al., 2025)      | Web/计算机环境   | 1005      | Web操作、CU    | query+Web/电脑操作场景                           | 任务完成       | Answer Accuracy, URL Recall                       | -                            | [文档: paper.pdf]               |
| GAIA (Mialon et al., 2024)               | AI助理通用场景   | 466       | 多任务         | query+多模态文档                                 | 多种           | Generic                                          | -                            | [文档: paper.pdf]               |
| GAIA2 (Andrews et al., 2025)             | AI助理通用场景   | 963       | 多任务         | query+多模态文档                                 | 多种           | Partial Completion、Action Accuracy               | -                            | [文档: paper.pdf]               |
| TheAgentCompany (Xu et al., 2025a)       | 编程/浏览/协同   | 175       | 多智能体协作   | query+Web/桌面应用                               | 编程、文档      | Task Completion、Efficiency                       | -                            | [文档: paper.pdf]               |
| OSWorld (Xie et al., 2024)               | 桌面办公环境      | 369       | Office操作      | query+文档/桌面应用                              | 工具操作        | Task Completion                                  | -                            | [文档: paper.pdf]               |


三、分析与观察

1. 场景覆盖与真实度：传统Web类Benchmark侧重网络检索与答题，难以覆盖企业异构数据和多App环境，DRBench等新一代基准全面引入办公场景、内部私有数据、跨应用操作等要素，更贴近实际组织需求[文档: paper.pdf][图片: image0.png]。
2. 输入与任务类型：多数Benchmarks均以query文本及文档输入为主，高阶如DRBench提供PDF、PPT、Excel、聊天等多源数据，任务目标从简单问答发展至复杂报告生成、决策支持与多模态推理[文档: paper.pdf]。
3. 评价指标体系：传统Benchmark集中于回答准确率、检索相关性等单一指标。DRBench创新采用“Insight Recall（洞察召回）、Factuality（事实性）、Distractor Avoidance（干扰规避）、Report Quality（报告质量）”等多轴多层结构，有效刻画复杂模型上的真实需求[图片: image0.png][文档: paper.pdf]。
4. 其它特性：如persona机制及洞察数设定，为评测代理在企业决策和个性化需求下表现提供丰富情境；为模型开发定制能力与评测多样性打下基础[图片: image0.png]。

四、参考资料

- DRBench: A Realistic Benchmark for Enterprise Deep Research, Abaskohi et al., 2025. [文档: paper.pdf]；https://github.com/ServiceNow/drbench
- Deep Research Bench: Evaluating AI Web Research Agents, Bosse et al., 2025. [文档: paper.pdf]
- DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents, Du et al., 2025. [文档: paper.pdf]
- DeepResearchGym: A free, transparent, and reproducible evaluation sandbox for deep research, Coelho et al., 2025. [文档: paper.pdf]
- ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry, Xu et al., 2025b. [文档: paper.pdf]
- 其它见原文References及Supplementary[文档: paper.pdf][图片: image0.png]

五、结论

Deep Research类Benchmark正在从单一web信息检索与问答，迈向面向真实企业场景、多源异构数据融合与多任务决策分析的综合性考察。DRBench作为新代表，其在任务设计、输入覆盖、评价指标和可靠性验证等方面具有创新性和实际指导意义，为企业级AI系统、Agent平台以及行业评测提供关键参考。今后团队如需跟踪大模型实际应用能力、差异化竞争力提升，建议优先参考DRBench等具有高场景还原性的评测基准，结合团队需求持续扩展/迭代指标与测试范式[文档: paper.pdf][图片: image0.png]。

## Summary


============================== Final Answer ==============================
\boxed{
《Deep Research相关Benchmark数据集调研报告》

引言

在人工智能（AI）大模型的持续突破推动下，行业对其复杂任务执行力、跨源信息整合等能力的评测需求日益增长。系统性Benchmark不仅为模型创新提供公认标准，更直接影响团队技术决策与生态演进。基于Deep Research方向新近代表性论文“DRBench: A Realistic Benchmark for Enterprise Deep Research”等材料，本文聚焦主流大模型深度研究评测基准，参照image0.png所示标准字段，梳理国际主要相关Benchmark特征、指标体系与权威来源，为团队后续决策提供标准化、可对比的分析依据[图片: image0.png]。

一、调研方法

1. 字段标准化。以image0.png为参照，核心字段包括论文名称、领域（场景）、query数量、任务、输入、类型、评价指标、其它（如insight数量、persona设置等），忽略“数据集example”[图片: image0.png]。
2. 文献系统梳理。详细阅读论文相关工作部分，辅助RAG长文档检索交叉核查，为每条Benchmark填写权威来源，确保信息溯源与可核查性[文档: paper.pdf]。
3. 结构化提取与横向对比。结合公开数据库/官方仓库整理，对各Benchmark在任务类型、评测覆盖、环境真实性与评估维度等方面进行汇总分析。

二、Benchmark信息汇总表

| 论文名称/Benchmark | 领域（场景）         | query数量 | 任务           | 输入                                                | 类型           | 评价指标                                        | 其它                         | 来源                      |
|-------------------|----------------------|-----------|----------------|-----------------------------------------------------|----------------|-----------------------------------------------------|------------------------------|---------------------------|
| DRBench           | 办公（企业）         | 15        | 无特定划分      | query文本+文档（办公环境）                         | report生成     | Insight Recall, Factuality, Distractor Avoidance, Report Quality | 114个insight，persona等      | [文档: paper.pdf][图片: image0.png] |
| Deep Research Bench (Bosse et al., 2025) | Web Research     | 89        | Web问题       | query文本+网络数据源                             | 问答、摘要     | Answer Accuracy, URL Recall                       | -                            | [文档: paper.pdf]               |
| DeepResearch Bench (Du et al., 2025)     | Web Research     | 100       | Web问题       | query文本+网络数据源                             | 问答、摘要     | Insight Recall, Factuality                        | -                            | [文档: paper.pdf]               |
| DeepResearchGym (Coelho et al., 2025)    | Web Research     | 1000      | Web问题       | query文本+网络数据源                             | 问答           | Generic、Document Retrieval                       | -                            | [文档: paper.pdf]               |
| ResearcherBench (Xu et al., 2025b)       | Web Research     | 65        | 多模型对比     | query文本+网络/文档源                            | 对比           | Insight Recall, Factuality                        | -                            | [文档: paper.pdf]               |
| LiveDRBench (Java et al., 2025)          | Web Research     | 100       | 综合推理链     | 长query+文档/网页                                | 多步推理       | Generic、Insight Precision                        | -                            | [文档: paper.pdf]               |
| BrowseComp-Plus (Chen et al., 2025)      | Web/计算机环境   | 1005      | Web操作、CU    | query+Web/电脑操作场景                           | 任务完成       | Answer Accuracy, URL Recall                       | -                            | [文档: paper.pdf]               |
| GAIA (Mialon et al., 2024)               | AI助理通用场景   | 466       | 多任务         | query+多模态文档                                 | 多种           | Generic                                          | -                            | [文档: paper.pdf]               |
| GAIA2 (Andrews et al., 2025)             | AI助理通用场景   | 963       | 多任务         | query+多模态文档                                 | 多种           | Partial Completion、Action Accuracy               | -                            | [文档: paper.pdf]               |
| TheAgentCompany (Xu et al., 2025a)       | 编程/浏览/协同   | 175       | 多智能体协作   | query+Web/桌面应用                               | 编程、文档      | Task Completion、Efficiency                       | -                            | [文档: paper.pdf]               |
| OSWorld (Xie et al., 2024)               | 桌面办公环境      | 369       | Office操作      | query+文档/桌面应用                              | 工具操作        | Task Completion                                  | -                            | [文档: paper.pdf]               |


三、分析与观察

1. 场景覆盖与真实度：传统Web类Benchmark侧重网络检索与答题，难以覆盖企业异构数据和多App环境，DRBench等新一代基准全面引入办公场景、内部私有数据、跨应用操作等要素，更贴近实际组织需求[文档: paper.pdf][图片: image0.png]。
2. 输入与任务类型：多数Benchmarks均以query文本及文档输入为主，高阶如DRBench提供PDF、PPT、Excel、聊天等多源数据，任务目标从简单问答发展至复杂报告生成、决策支持与多模态推理[文档: paper.pdf]。
3. 评价指标体系：传统Benchmark集中于回答准确率、检索相关性等单一指标。DRBench创新采用“Insight Recall（洞察召回）、Factuality（事实性）、Distractor Avoidance（干扰规避）、Report Quality（报告质量）”等多轴多层结构，有效刻画复杂模型上的真实需求[图片: image0.png][文档: paper.pdf]。
4. 其它特性：如persona机制及洞察数设定，为评测代理在企业决策和个性化需求下表现提供丰富情境；为模型开发定制能力与评测多样性打下基础[图片: image0.png]。

四、参考资料

- DRBench: A Realistic Benchmark for Enterprise Deep Research, Abaskohi et al., 2025. [文档: paper.pdf]；https://github.com/ServiceNow/drbench
- Deep Research Bench: Evaluating AI Web Research Agents, Bosse et al., 2025. [文档: paper.pdf]
- DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents, Du et al., 2025. [文档: paper.pdf]
- DeepResearchGym: A free, transparent, and reproducible evaluation sandbox for deep research, Coelho et al., 2025. [文档: paper.pdf]
- ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry, Xu et al., 2025b. [文档: paper.pdf]
- 其它见原文References及Supplementary[文档: paper.pdf][图片: image0.png]

五、结论

Deep Research类Benchmark正在从单一web信息检索与问答，迈向面向真实企业场景、多源异构数据融合与多任务决策分析的综合性考察。DRBench作为新代表，其在任务设计、输入覆盖、评价指标和可靠性验证等方面具有创新性和实际指导意义，为企业级AI系统、Agent平台以及行业评测提供关键参考。今后团队如需跟踪大模型实际应用能力、差异化竞争力提升，建议优先参考DRBench等具有高场景还原性的评测基准，结合团队需求持续扩展/迭代指标与测试范式[文档: paper.pdf][图片: image0.png]。
}

-------------------- Extracted Result --------------------
《Deep Research相关Benchmark数据集调研报告》

引言

在人工智能（AI）大模型的持续突破推动下，行业对其复杂任务执行力、跨源信息整合等能力的评测需求日益增长。系统性Benchmark不仅为模型创新提供公认标准，更直接影响团队技术决策与生态演进。基于Deep Research方向新近代表性论文“DRBench: A Realistic Benchmark for Enterprise Deep Research”等材料，本文聚焦主流大模型深度研究评测基准，参照image0.png所示标准字段，梳理国际主要相关Benchmark特征、指标体系与权威来源，为团队后续决策提供标准化、可对比的分析依据[图片: image0.png]。

一、调研方法

1. 字段标准化。以image0.png为参照，核心字段包括论文名称、领域（场景）、query数量、任务、输入、类型、评价指标、其它（如insight数量、persona设置等），忽略“数据集example”[图片: image0.png]。
2. 文献系统梳理。详细阅读论文相关工作部分，辅助RAG长文档检索交叉核查，为每条Benchmark填写权威来源，确保信息溯源与可核查性[文档: paper.pdf]。
3. 结构化提取与横向对比。结合公开数据库/官方仓库整理，对各Benchmark在任务类型、评测覆盖、环境真实性与评估维度等方面进行汇总分析。

二、Benchmark信息汇总表

| 论文名称/Benchmark | 领域（场景）         | query数量 | 任务           | 输入                                                | 类型           | 评价指标                                        | 其它                         | 来源                      |
|-------------------|----------------------|-----------|----------------|-----------------------------------------------------|----------------|-----------------------------------------------------|------------------------------|---------------------------|
| DRBench           | 办公（企业）         | 15        | 无特定划分      | query文本+文档（办公环境）                         | report生成     | Insight Recall, Factuality, Distractor Avoidance, Report Quality | 114个insight，persona等      | [文档: paper.pdf][图片: image0.png] |
| Deep Research Bench (Bosse et al., 2025) | Web Research     | 89        | Web问题       | query文本+网络数据源                             | 问答、摘要     | Answer Accuracy, URL Recall                       | -                            | [文档: paper.pdf]               |
| DeepResearch Bench (Du et al., 2025)     | Web Research     | 100       | Web问题       | query文本+网络数据源                             | 问答、摘要     | Insight Recall, Factuality                        | -                            | [文档: paper.pdf]               |
| DeepResearchGym (Coelho et al., 2025)    | Web Research     | 1000      | Web问题       | query文本+网络数据源                             | 问答           | Generic、Document Retrieval                       | -                            | [文档: paper.pdf]               |
| ResearcherBench (Xu et al., 2025b)       | Web Research     | 65        | 多模型对比     | query文本+网络/文档源                            | 对比           | Insight Recall, Factuality                        | -                            | [文档: paper.pdf]               |
| LiveDRBench (Java et al., 2025)          | Web Research     | 100       | 综合推理链     | 长query+文档/网页                                | 多步推理       | Generic、Insight Precision                        | -                            | [文档: paper.pdf]               |
| BrowseComp-Plus (Chen et al., 2025)      | Web/计算机环境   | 1005      | Web操作、CU    | query+Web/电脑操作场景                           | 任务完成       | Answer Accuracy, URL Recall                       | -                            | [文档: paper.pdf]               |
| GAIA (Mialon et al., 2024)               | AI助理通用场景   | 466       | 多任务         | query+多模态文档                                 | 多种           | Generic                                          | -                            | [文档: paper.pdf]               |
| GAIA2 (Andrews et al., 2025)             | AI助理通用场景   | 963       | 多任务         | query+多模态文档                                 | 多种           | Partial Completion、Action Accuracy               | -                            | [文档: paper.pdf]               |
| TheAgentCompany (Xu et al., 2025a)       | 编程/浏览/协同   | 175       | 多智能体协作   | query+Web/桌面应用                               | 编程、文档      | Task Completion、Efficiency                       | -                            | [文档: paper.pdf]               |
| OSWorld (Xie et al., 2024)               | 桌面办公环境      | 369       | Office操作      | query+文档/桌面应用                              | 工具操作        | Task Completion                                  | -                            | [文档: paper.pdf]               |


三、分析与观察

1. 场景覆盖与真实度：传统Web类Benchmark侧重网络检索与答题，难以覆盖企业异构数据和多App环境，DRBench等新一代基准全面引入办公场景、内部私有数据、跨应用操作等要素，更贴近实际组织需求[文档: paper.pdf][图片: image0.png]。
2. 输入与任务类型：多数Benchmarks均以query文本及文档输入为主，高阶如DRBench提供PDF、PPT、Excel、聊天等多源数据，任务目标从简单问答发展至复杂报告生成、决策支持与多模态推理[文档: paper.pdf]。
3. 评价指标体系：传统Benchmark集中于回答准确率、检索相关性等单一指标。DRBench创新采用“Insight Recall（洞察召回）、Factuality（事实性）、Distractor Avoidance（干扰规避）、Report Quality（报告质量）”等多轴多层结构，有效刻画复杂模型上的真实需求[图片: image0.png][文档: paper.pdf]。
4. 其它特性：如persona机制及洞察数设定，为评测代理在企业决策和个性化需求下表现提供丰富情境；为模型开发定制能力与评测多样性打下基础[图片: image0.png]。

四、参考资料

- DRBench: A Realistic Benchmark for Enterprise Deep Research, Abaskohi et al., 2025. [文档: paper.pdf]；https://github.com/ServiceNow/drbench
- Deep Research Bench: Evaluating AI Web Research Agents, Bosse et al., 2025. [文档: paper.pdf]
- DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents, Du et al., 2025. [文档: paper.pdf]
- DeepResearchGym: A free, transparent, and reproducible evaluation sandbox for deep research, Coelho et al., 2025. [文档: paper.pdf]
- ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry, Xu et al., 2025b. [文档: paper.pdf]
- 其它见原文References及Supplementary[文档: paper.pdf][图片: image0.png]

五、结论

Deep Research类Benchmark正在从单一web信息检索与问答，迈向面向真实企业场景、多源异构数据融合与多任务决策分析的综合性考察。DRBench作为新代表，其在任务设计、输入覆盖、评价指标和可靠性验证等方面具有创新性和实际指导意义，为企业级AI系统、Agent平台以及行业评测提供关键参考。今后团队如需跟踪大模型实际应用能力、差异化竞争力提升，建议优先参考DRBench等具有高场景还原性的评测基准，结合团队需求持续扩展/迭代指标与测试范式[文档: paper.pdf][图片: image0.png]。

-------------------- Token Usage --------------------
Total Input Tokens: 193042
Total Cache Input Tokens: 0
Total Output Tokens: 2736
-----------------------------------------------------
Pricing is disabled - no cost information available
-----------------------------------------------------