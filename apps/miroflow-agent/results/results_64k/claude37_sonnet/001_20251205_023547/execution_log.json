{
  "status": "success",
  "start_time": "2025-12-05 18:35:49",
  "end_time": "2025-12-05 18:46:19",
  "task_id": "folder_task_001",
  "input": {
    "task_description": "# Task\n\n假设你是一名科研工作者，正在为某人工智能研究团队开展大模型评测基准的系统性梳理工作。请根据提供的材料，撰写一份《Deep Research相关Benchmark数据集调研报告》。\n具体而言，依据图片image0中所示的字段要求，提取并结构化提供的pdf论文在相关工作部分提到的每篇论文对应的Benchmark特征，其中请忽略“数据集example”指标。\n报告需整合文献内容与公开可查的权威技术资料（如论文原文、官方GitHub仓库、学术数据库或评测平台），\n确保所有信息真实、准确、可验证，并在文中注明数据来源。\n报告语言应采用正式书面语，结构清晰，包含引言、调研方法、各论文Benchmark信息汇总表、分析与观察、参考资料等部分，\n最终形成一份可用于团队技术决策的标准化调研文档，字数在1500至2000之间。\n\n\n## Folder Contents Summary\n\nFolder: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001\nTotal files: 12\n  - data: 5\n  - document: 1\n  - image: 1\n  - other: 5\n\n\n## Document Contents\n\n\n### paper.pdf\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\">\n5\n2\n0\n2\n\np\ne\nS\n0\n3\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n2\n7\n1\n0\n0\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nDRBench: A REALISTIC\n\nBENCHMARK FOR ENTERPRISE DEEP RESEARCH\n\nAmirhossein Abaskohi1,2 Tianyi Chen1 Miguel Mu˜noz-M´armol1 Curtis Fox1,2\nAmrutha Varshini Ramesh1,2 ´Etienne Marcotte1 Xing Han L`u3,4 Nicolas Chapados1\n\nSpandana Gella1,3 Christopher Pal1,3,5 Alexandre Drouin1,3\n\nIssam H. Laradji1,2\n\n1ServiceNow Research\n\n2University of British Columbia\n\n3Mila – Quebec AI Institute\n\n4McGill University\n\n5Canada CIFAR AI Chair\n\nABSTRACT\n\nWe introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended\ndeep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple\nquestions or web-only queries, DRBench evaluates agents on multi-step queries (for\nexample, “What changes should we make to our product roadmap to ensure compliance\nwith this standard?”) that require identifying supporting facts from both the public web\nand private company knowledge base. Each task is grounded in realistic user personas\nand enterprise context, spanning a heterogeneous search space that includes productivity\nsoftware, cloud file systems, emails, chat conversations, and the open web. Tasks\nare generated through a carefully designed synthesis pipeline with human-in-the-loop\nverification, and agents are evaluated on their ability to recall relevant insights, maintain\nfactual accuracy, and produce coherent, well-structured reports. We release 15 deep\nresearch tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We\ndemonstrate the effectiveness of DRBench by evaluating diverse DR agents across open-\nand closed-source models (such as GPT, Llama, and Qwen) and DR strategies, high-\nlighting their strengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.\n\n1\n\nINTRODUCTION\n\nOrganizations today face a strong need to find useful insights in a world full of overwhelming information.\nValuable insights are often hidden in noisy data, which can contain many distracting or irrelevant details that\nobscure the insights that really matter. This challenge is present in enterprise settings, where data is spread\nacross many applications and stored in different formats (e.g., PDFs, spreadsheets, emails, and internal tools)\nmaking extracting relevant information difficult. To uncover these hidden, valuable insights, one must con-\nduct what is known as deep research. This task involves asking high-level strategic questions (e.g, “What\nchanges should we make to our roadmap to remain compliant?”), planning sub-questions, retrieving and\nevaluating relevant materials, and producing a clear, actionable summary grounded in data sources (Zheng\net al., 2025; Xu & Peng, 2025; Du et al., 2025). These tasks are typically performed by domain experts\nusing a mix of search engines, communication platforms, and business applications in iterative, high-effort\nworkflows (Mialon et al., 2024), which unfortunately require a significant amount of human effort.\n\nOne promising solution to reducing this human effort is agent-based deep research, which uses autonomous\nsoftware agents to search, extract, and synthesize information across fragmented sources into an\ninsightful report. Recently, LLM-based agents have emerged as promising assistants for deep research.\nSystems such as Local Deep Researcher (LearningCircuit, 2025), Deep-Searcher (Tech, 2024), and\nDeepResearcher (Zheng et al., 2025) propose modular agent pipelines that combine retrieval, reasoning,\nand summarization over documents and web sources. Architectures like OpenHands (All-HandsAI, 2024),\nOpenManus (FoundationAgents, 2024), and smolagents (HuggingFace, 2024) extend these capabilities\nto include collaboration, multi-modal search, and complex tool use in enterprise workflows (Xu & Peng,\n2025). Despite these advances, evaluating such systems remains an open challenge.\n\nMost existing benchmarks evaluate narrow aspects such as report factuality (Coelho et al., 2025), web-only\nsynthesis (Bosse et al., 2025), or tabular analytics (Sahu et al., 2025), but they do not assess whether agents\nidentify the most salient insights, remain faithful to retrieved evidence, or adapt to enterprise contexts. To\naddress these limitations, we introduce DRBench, a benchmark designed to evaluate LLM agents on open-\n\n1\n\n \n \n \n \n \n \n\fFigure 1: DRBench pipeline. 1 The Task Context defines the deep research question grounded by the\ncompany and persona given to the agent. 2 Task Data, including both distractor and injected groundtruth\ninsights in different formats (PDFs, DOCX, PPTX, XLSX, chats, etc.) are loaded into the enterprise\nenvironment’s applications. 3 The DRBenchAgent accesses both public web sources and local enterprise\ndata to extract relevant insights for the research question. 4 It produces a structured research report,\nwhich is 5 evaluated for Insight Recall (detecting injected groundtruth insights), Factuality (verifying\nclaims are correctly cited), and Report Quality.\n\nended, multi-step and long-horizon deep research tasks grounded in realistic enterprise contexts. As Fig. 1 il-\nlustrates, DRBench includes a suite of queries grounded in user personas and organizational scenarios, requir-\ning agents to search across real applications such as cloud file storage (Nextcloud), enterprise chat (Matter-\nmost), and user file systems, and to reason over formats like spreadsheets, slide decks, and PDFs. Our evalu-\nation framework introduces three scoring axes using LLM-as-a-judge methods inspired by G-Eval (Liu et al.,\n2023): (1) Insight Recall and Distractor Avoidance, which together evaluate whether the agent surfaces the\nmost salient injected insights while avoiding distractor content; (2) Factuality, which uses a TREC-RAG\npipeline (Wang et al., 2024) to verify whether claims are correctly grounded in their cited sources; and (3) Re-\nport Quality, which measures the coherence, completeness, and overall readability of the synthesized report.\n\nWe conduct a comparative study of agent architectures inspired by recent work (Zheng et al., 2025;\nXu & Peng, 2025; LearningCircuit, 2025; Zheng et al., 2025), analyzing how well they perform on\nDRBench across planning, insight identification, and grounding on facts. Our results show that while\nagents are competent at document retrieval and summarization, they often miss high-value insights, cite\nirrelevant evidence, or produce incoherent explanations, highlighting the limitations of current architectures\nand the need for more targeted innovation.\n\nOur contributions are as follows: (1) We introduce DRBench, the first benchmark for evaluating LLM\nagents on complex enterprise deep research tasks combining public web sources with private organizational\ndata; (2) We provide a suite of 15 high-level research tasks with 114 sub-questions spanning 10 domains,\nincluding Sales, Cybersecurity, and Compliance, each grounded in realistic company contexts and personas;\n(3) We design a reproducible enterprise environment integrating realistic enterprise applications like chat,\ncloud storage, emails, and documents; (4) We propose a scalable pipeline that generates realistic research\nquestions and insights by combining web facts with synthesized internal data; and (5) We develop an\nevaluation framework that scores agent reports on insight recall and distractor avoidance, factuality, and\noverall report quality.\n\n2 RELATED WORK\n\nDeep Research Benchmarks. With the growing capabilities of LLMs in research and reasoning tasks,\nseveral benchmarks have emerged to evaluate their performance in realistic, multi-step and long-horizon sce-\nnarios, including Deep Research Bench (Bosse et al., 2025), DeepResearch Bench (Du et al., 2025), Deep-\nResearchGym (Coelho et al., 2025), ResearcherBench (Xu et al., 2025b), Mind2Web2 (Gou et al., 2025)\nand GAIA (Mialon et al., 2024). As summarized in Table 1, these efforts typically focus on web-only tasks,\n\n2\n\nLoading to Environment2Load Ground Truth Insights Factuality Insight RecallReport QualityIs each claim in the reportproperly cited?Were the injected insightsdiscovered? Is the report clear to read andcomplete in addressing the points?5PersonaCompanyDR QuestionLee's Market: a regional Asian supermarket chainJohn Doe: Regulatory Aﬀairs ManagerNextCloudFileShareFile SystemLocal FilesMattermostChatRoundCubeEmailEnterprise Data3ReportGeneration41Context5(a) Tracking lot codes helps grocers reduce food waste and keep products fresh. [Cite: grocerydive.com](b) Lee's Market tracks 250 high- risk food products as of Q3 2024, aﬀecting 30% of inventory. [Cite: food- safety- compliance.pdf] ...(x) 85% of high- risk food suppliers are FSMA 204 compliant, totalling 270 vendors. [Cite: RoundCube email]EvaluationTask ContextHow can Lee's Market leverage FSMA 204 regulations to enhance food safety and customer trust?3Public DataReportAccessingWebsitesChat ConversationsPresentations:PPTXTask DataTabular Data:CSV, XLSXDocuments: Docx, PDF, MarkdownE- mailsI) 8% reduction in food waste during Q2 2024, saving $1.2 million....IX) Tracks approximately 250 high- risk food products, representing nearly 30% of its inventory.Injected InsightsSynthesized Files with Distractor Content and Injected InsightsLoad Sources5\fTable 1: Comparison of deep research benchmarks (top) and AI agent benchmarks with a computer environ-\nment (middle). Columns report dataset size, whether both public and local data are required, the provided\nenvironment type, task domains, task description, and evaluation method. Unlike prior work, DRBench com-\nbines public web retrieval with local enterprise data in realistic enterprise applications and evaluates both\ninsight recall, distractor avoidance and report quality. Task Description: types of tasks covered by the\nbenchmark: WR for Web Research, DR for Deep Research with both public and local data, CU for\nComputer Use and/or Mobile Use. DRBench has 114 total # groundtruth insights that need to be extracted\nto address the 15 DR Questions. Example groundtruth insights can be found at Table 8 in Appendix A.\n\nBenchmark\n\n# groundtruth\n\nPublic &\nLocal Data\n\nProvides\nEnv\n\nTask\nDomain\n\nTask\nDescription\n\nMain Evaluation Method\n\nDeep Research Bench (Bosse et al., 2025)\nDeepResearch Bench (Du et al., 2025)\nDeepResearchGym (Coelho et al., 2025)\nResearcherBench (Xu et al., 2025b)\nLiveDRBench (Java et al., 2025)\nBrowseComp-Plus (Chen et al., 2025)\nMind2Web 2 (Gou et al., 2025)\nGAIA (Mialon et al., 2024)\nGAIA2 (Andrews et al., 2025)\n\nTheAgentCompany (Xu et al., 2025a)\nOSWorld (Xie et al., 2024)\n\nDRBench\n\n89\n100\n1,000\n65\n100\n1,005\n130\n466\n963\n\n175\n369\n\n114\n\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n\n✗\n✗\n\n✓\n\n✓\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n\n✓\n✓\n\n✓\n\nWR\nWR\nWR\n\nGeneric WR & CU Answer Accuracy\nGeneric\nGeneric\nAI\n\nInsight Recall\nDocument Retrieval\nInsight Recall, Factuality\nGeneric WR & CU Insight Precision, Recall\nGeneric\nGeneric\nGeneric\nGeneric\n\nAnswer Accuracy, URL Recall\nPartial Completion\nAnswer Accuracy\nAction Accuracy\n\nWR\nWR\nWR\nCU\n\nEnterprise\nGeneric\n\nEnterprise\n\nCU\nCU\n\nDR\n\nTask Completion, Efficiency\nTask Completion\n\nInsight Recall\n\nmeasuring retrieval or synthesis quality in controlled but limited settings. In contrast, DRBench is the first to\ncombine web retrieval with local enterprise data, grounding tasks in persona- and domain-specific contexts.\n\nEnterprise Environments. Realistic enterprise environments have become an important testbed for\nevaluating agents in complex multi-application workflows. CRMArena-Pro (Huang et al., 2025a;b) targets\nsales and CPQ pipelines through persona-grounded dialogues, but is limited to conversational sales\nworkflows. OSWorld (Xie et al., 2024) and OSWorld-Gold (Abhyankar et al., 2025) benchmark agents\nin general-purpose desktop environments, using applications such as Microsoft Word and Excel, yet their\nfocus remains on computer task execution rather than enterprise deep research. TheAgentCompany (Xu\net al., 2025a) evaluates collaboration among autonomous agents for programming, browsing, and\ncommunication, though the tasks are computer-use focused and do not assess deep research capabilities.\nWorkArena (Drouin et al., 2024; Boisvert et al., 2024) offers a realistic enterprise environment with\nknowledge work tasks for web agents, though it does not support evaluation of deep research capabilities.\nIn contrast, DRBench offers a domain-grounded enterprise environment with applications that would\nrealistically be encountered in organizations. Tasks are tied to concrete personas and roles, requiring agents\nto search, reason, and synthesize insights across diverse formats, including spreadsheets, PDFs, wikis,\nemails, and presentations, reflecting realistic enterprise deep research.\n\nDeep Research Agents. A growing line of work explores agents for multi-step search and synthesis\nacross diverse information sources. LangChain’s Local Deep Researcher (LearningCircuit, 2025) and\nZilliz’s Deep-Searcher provide modular pipelines for iterative querying and summarization, while DeepRe-\nsearcher (Zheng et al., 2025) uses RL to enable planning, cross-validation, and self-reflection. Commercial\nsystems such as Gemini Deep Research and Manus.ai synthesize web-based reports with citations,\nand open-source frameworks like OpenHands (All-HandsAI, 2024), OpenManus (FoundationAgents,\n2024), and smolagents (HuggingFace, 2024) offer alternative architectures. Recent work also introduces\ntask-agnostic frameworks for long-form synthesis and evaluation paradigms such as Mind2Web 2 (Gou\net al., 2025), which treat agents as judges of browsing trajectories. Building on these efforts, DRBench an-\nalyzes their strengths and limitations in enterprise contexts, showing that current agents still fall short in\nconsistently extracting and grounding critical insights within complex, heterogeneous environments.\n\n3 DRBench - AN ENTERPRISE DEEP RESEARCH BENCHMARK\n\nTo evaluate agents on complex, open-ended enterprise deep research tasks, we designed DRBench with\nthree guiding principles: it requires agents to integrate both public web data and local enterprise documents,\nit involves both web search and enterprise application use, and it is hosted in an interactive and reproducible\n\n3\n\n\fFigure 2: DRBench Task Generation Pipeline. The pipeline comprises five main stages during each LLMs\ngenerate candidate data such as company context, insights, and research questions, while human annotators\nverify quality and select the final version. Stages S1–S5 denote the five generation steps.\n\nenterprise environment. These principles ensure that the benchmark reflects realistic enterprise workflows\nand provides a controlled yet challenging setting for research agents.\n\nThe Enterprise Search Environment. A unique aspect of DRBench is its realistic enterprise search\nenvironment. When addressing DR Questions like ”What changes should we make to our product roadmap\nto ensure compliance with this standard?”, the DR agents would need to navigate such environment and\nsearch across both public and private data sources to uncover relevant insights.\n\nPublic insights include information available on the open web or otherwise accessible to general users.\nLocal insights, on the other hand, come from an enterprise’s private systems. These insights are embedded\nwithin a vast search space that spans multiple data types (emails, slide decks, chat conversations, and Excel\nsheets) which reflect the complexity of real enterprise data ecosystems. This environment is populated with\ndata from different applications, accessible to both web-based agents and API-calling agents. For example,\nan app like Mattermost can be used to host chat conversations (see Appendix E for examples of the applica-\ntions). The goal of the DR Agent is to effectively navigate these public and private data sources to address\ncomplex, high-level DR questions. For the environment implementation details, please see Section D.\n\nTask Definition. Each task is associated with a deep research question Qi and Task Context C which\nincludes company information and the user persona. Each task also has a corresponding set of groundtruth\ninsights I consisting of relevant private insights Il (we also refer to this as internal insights), distractor\nprivate insights Id, and public insights Ip. Each private insight, whether relevant or a distractor, is embedded\ninto a file fi which could take the form of a PDF, Excel sheet, slide deck, chat log, and so on. The agent’s\ntask is to generate a report by extracting the public insights Ip from accessible sources such as the web,\nwhile also extracting the private insights Il from the files hosted in the enterprise environment. At the same\ntime, the agent must avoid extracting the distractor insights Id, which are not relevant to the DR question.\n\nDRBench provides 15 realistic deep research tasks explicitly framed around enterprise environments.\nEach task is associated with public insights extracted form quality, time-invariant URLs and local insights\nembedded within synthetic enterprise data, typically spanning 2–4 applications and 3–16 supporting files\n(see Section B). Tasks are distributed across 10 enterprise domains (such as Sales and Compliance - the\nfull list is in Section B) and divided between easy, medium, and hard categories that indicates the difficulty\nof addressing the DR Question. Finally, DRBench is fully self-hosted, with dated URLs and reproducible\nevaluation scripts to ensure stability and fair comparison across agent methods.\n\n3.1 DATA GENERATION\n\nTo create realistic and reproducible deep research tasks, DRBench employs a five-stage pipeline (Figure 2)\nthat combines large-scale LLM generation with human-in-the-loop verification. The pipeline helps us\ngenerate candidate company contexts, personas, questions, insights, and supporting files using LLM\nModels such as Llama-3.1-8B-Instruct, Llama-3.1-405B (Dubey et al., 2024). Three human annotators\nthen validate the generated content to ensure that they are realistic and plausible.\n\nThe pipeline has been used to generate 15 tasks with 114 groundtruth insights across 10 enterprise domains,\neach grounded in realistic personas and company profiles. We control the difficulty of each task by setting\n\n4\n\n\fthe number of insights, file types and application types. The complete list of tasks is provided in Section B.\nRefer to Section I for details on the cost of using data generation.\n\nStage 1: Company and Persona Generation. This stage produces the synthetic company profile and\nuser persona that form the Task Context C. LLMs were used to generate company descriptions detailing\nthe industry vertical, key products, market position, and competitive landscape. In parallel, they were used\nto create realistic personas across departments (e.g., a Regulatory Affairs Manager or a Market Research\nAnalyst) that serve as the role grounding for the final deep research question. They were then refined\nby human experts. The prompts used for this stage are provided in Section O.1 and the list of companies\nare given in Appendix B.\n\nStage 2: Public Source and Insight Collection. Given the company and persona context from Stage 1,\nwe have retrieved candidate URLs relevant to the specified domain and company background. To ensure\nquality, time-invariant insights, the search is restricted to dated, journal-based or industry-report websites\nthat provide authoritative information. Thus, the collected URLs and their contents are expected to be stable\nin time. Human annotators then review the candidate URLs and select one that is both topically aligned\nand provides insights into the topic. The selected page becomes the Task URL included in C. Its HTML\ncontent is parsed, and LLMs are prompted to extract business-relevant insights, which are subsequently\nfiltered and validated by human reviewers for accuracy and contextual fit. The public insights Ip derived\nfrom the Task URL are included in C and serves as a required piece of insight for the agent to retrieve\nduring report generation. Prompts used for this stage and the list of urls are provided in Section O.3.\n\nStage 3: Question Generation. Given the Task Context, we generate the deep research question Q.\nThe prompt (see Section O.2) is instantiated with the company profile and persona, the selected domain,\nthe Task URL, and the public insight Ip. The LLM proposes several open-ended candidate questions\ngrounded in this context. Human annotators then review these candidate DR Questions, selecting and\nrefining one to align with the persona and company. They also ensure that the insights available in the\nprovided URL can at least partially support answering the deep research question. For example, if the\nquestion concerns compliance with a specific regulation, the URL might include relevant insights, such\nas “groceries must have a traceability plan.” While this doesn’t fully resolve the question, it provides\na foundation.The question should be high-level enough to allow us to synthesize additional supporting\nprivate/internal insights Il (such an insight could be “the cost of implementing such a plan is X amount”)\nwhich are needed to strengthen the report generated for the question. This requirement ensures that new\ninternal insights can be generated, as discussed in Stage 4.\n\nStage 4: Internal Insight Generation. In this stage we generate the injected insights set G ⊂I. Using\nthe public insight Ip and the deep research question Q, LLMs are used to create company-specific insights\naligned with the organization’s industry, priorities, customer segments, and business goals. These insights\nare designed to provide additional supporting facts that need to be extracted to create a report that better\naddresses the DR questions. Human annotators review and refine these insights for accuracy and alignment\nwith the questions. In addition to relevant insights, we also produce distractor insights Id, which are\nplausible but irrelevant statements that do not support resolving the DR Question. Prompt details are\nprovided in Section O.4 and example internal insights are provided in Section B.\n\nStage 5: File Mapping and Generation. This stage produces the the set of files {fi} containing\nboth the relevant and distractor private insights. First, each insight is assigned to a modality such as\nemail, chat, pdf, docx, and so on. Then the file generation module follows the following three-step\n“needle-in-a-haystack” process: (1) create an outline of the file based on its modality(e.g., document\nstructure or chat configuration), (2) insert the distractor or relevant insight into an appropriate section\nof the file, and (3) fill the remaining content with realistic but irrelevant information. Human annotators\nspot-check the generated files to ensure fidelity, coherence and no contradicting information. Prompts\nfor file generation are provided in Section O.5 and screenshots of such generated files are in Section C.\n\n4 DRBench AGENT\n\nThe DRBench baseline agent (DRBA) is the first agent built specifically for deep research in enterprise\nsettings, designed to operate directly within the DRBench environment.\nIts multi-stage architecture\nsystematically investigates research questions by iteratively retrieving, processing, and synthesizing\nknowledge from enterprise services and the web until completion or a maximum iteration limit is reached\n(Figure 3; see Appendix F). The agent has access to app-specific api calling tools to access diverse\n\n5\n\n\fFigure 3: DRBench Agent architecture showing the enterprise research workflow from question submission\nthrough iterative research cycles to final report generation, using both enterprise and web search capabilities.\n\ninformation sources and a diverse toolset for analyzing retrieved results (Table 9, Appendix F.3). DRBA’s\narchitecture is organized into four main components: research planning, action planning, an adaptive\nresearch loop, and report writing. Refer to Appendix I for details on the cost of using DRBA.\n\nResearch Planning. The agent decomposes research questions into structured research investigation\nareas to guide subsequent action generation. This initial decomposition lays out a strategy to systematically\ncover the research space while maintaining focus on the initial deep research question. The agent supports\ntwo research planning modes: (1) Complex Research Planning (CRP), which generates a structured\nresearch plan with detailed investigation areas, expected information sources, and success criteria; and\n(2) Simple Research Planning (SRP) , which produces a lightweight decomposition of the main question\ninto a small set of self-contained subqueries. See Appendix F.2 for detailed examples of both modes.\n\nAction Planning. The system translates research objectives into executable actions through an LLM-based\nplanning subsystem. Actions receive a priority score based on strategic importance and are parameterized\nwith specific tool selection and execution parameters, and their dependencies to other actions in the plan.\n\nResearch Loop with Adaptive Action Planning (AAP). The system iterates through (1) tool selection\nand execution based on action priorities, (2) content processing and storage in a vector store, (3) adaptive\naction generation to cover research gaps, and (4) iteration findings storage for traceability and assessment.\n\nReport Writing. The report writing subsystem queries the vector store to synthesize the research findings\nand relevant retrieved content. This component generates a comprehensive report and uses its own citation\ntracking system to ensure proper attribution of claims.\n\n5 EXPERIMENTS AND RESULTS\n\nIn this section, we evaluate the DRBench Agent (DRBA) on both the full DRBench benchmark and a\nreduced subset for ablations. We consider (1) the Full Benchmark, covering all 15 tasks across 10 domains\n(Table 7), and (2) the MinEval subset, restricted to five retail tasks for efficient ablation studies. Results\nare reported across four metrics: Insight Recall, Distractor Avoidance, Factuality, and Report Quality,\nexplained in the next section. Implementation details and hyperparameters are in Appendix H, while the\nimpact of the number of research loop iterations on the performance of DRBA is analyzed in Appendix N.\n\n5.1 EVALUATION METRICS\n\nInsight Recall. We first decompose each report into atomic insights with their associated citations using\nan LLM (see Prompt 14). Each extracted insight is then compared against the set of groundtruth insights\nembedded in the task files using an LLM Judge with the Prompt 15. If a match is found, the insight is marked\nas detected and contributes to the insight recall score; otherwise, it is ignored. This metric thus measures\nrecall rather than precision, since judging whether an unmatched insight is nonetheless useful for answering\n\n6\n\n\fTable 2: DRBA performance with different planning configurations on DRBench. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. See Appendix K for the standard error across 3 runs.\nNote that higher numbers correspond to better scores, and the best result on each metric is bolded.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n16.92\n17.00\n16.94\n20.56\n19.20\n18.69\n\n64.06\n69.38\n66.78\n67.44\n61.72\n57.20\n\n97.94\n98.89\n99.52\n98.89\n98.97\n98.89\n\n91.08\n92.46\n90.72\n93.00\n91.86\n90.12\n\n41.71\n42.48\n42.07\n47.43\n44.80\n43.39\n\nthe deep research question is inherently subjective and difficult to automate. To prevent agents from trivially\nachieving 100% recall by copying all content into the generated report, the LLM Judge evaluates only the\nfirst k insights, where k equals the number of ground-truth insights plus five. This buffer ensures that reports\nare not penalized for including seemingly relevant insights that are not part of the groundtruth insight set.\n\nDistractor Avoidance. To measure precision, we track whether the agent’s report includes distractor\ninsights that are irrelevant to the research question. We compute distractor recall analogously to insight\nrecall, and define distractor avoidance as 1− distractor recall.\n\nFactuality. Using the same set of extracted insights (that we used for Insight Recall), we follow\nthe methodology of FactScore (Min et al., 2023).\nIf an insight lacks a citation or references a\nnon-existent source, it is labeled unfactual. Otherwise, we apply a retrieval-augmented system based on\ntext-embedding-3-large (OpenAI, 2024) to fetch the top-5 most relevant chunks from the cited\ndocument (Appendix H). The LLM Judge with Prompt 16 then determines whether the cited evidence\nsupports the claim. We also store justifications and model confidence scores for interpretability.\n\nReport Quality. Inspired by prior work (Coelho et al., 2025; Abaskohi et al., 2025), we query the LLM\nJudge with Prompt 17 to assign a 1–10 rating across six dimensions: (1) depth and quality of analysis, (2)\nrelevance to the research question, (3) persona consistency, (4) coherence and conciseness, (5) absence of\ncontradictions, and (6) completeness and coverage. The final report quality score is obtained by averaging\nthese six ratings.\n\n5.2 MAIN RESULTS\n\nWe first evaluate our DRBA agent (Section 4) using GPT-4o as the backbone model, a maximum of\n15 research loop iterations, and different combinations of planning modules: Simple Research Planning\n(SRP), Complex Research Planning (CRP), and Adaptive Action Planning (AAP). The results are reported\nin Table 2. Overall, the agent demonstrates moderate ability to ground its answers in factual evidence\nbut struggles to consistently surface the main injected insights necessary for answering the deep research\nquestions.\nIn many cases, the agent relies on prior knowledge or external web content rather than\nintegrating the crucial enterprise-specific information available in the files. By contrast, it is consistently\nstrong in avoiding distractors, showing that the agent is robust against misleading or irrelevant information\nbut less effective at prioritizing decision-critical insights. Note that our LLM Judge backbone is GPT-4o.\n\nComparison Across Planning Strategies. Here we see that SRP tends to produce more factually\ngrounded answers, while CRP excels at filtering out distractors through structured decomposition. AAP,\non the other hand, provides the largest improvements in both insight recall and report quality, suggesting\nthat dynamically adapting the plan during execution helps the agent recover missed evidence and refine its\nuse of sources. However, combining CRP or SRP with AAP does not yield clear gains, and in some cases\nreduces factuality, likely because overlapping strategies create redundant or unstable planning behavior.\nThese findings indicate that adaptive mechanisms are key for improving coverage of injected insights,\nwhile lightweight planning is more effective for maintaining factual grounding, and that carefully balancing\nthe two remains an open challenge. See Appendix M for detailed results for each task.\n\n7\n\n\fTable 3: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Note that higher numbers correspond to better scores, and the best result on each\nmetric is bolded. The full table with more models is given in Appendix L.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n17.37\n16.97\n20.16\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n78.91\n79.27\n69.75\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n100.00\n98.10\n97.90\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n90.48\n92.34\n91.26\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n49.78\n48.87\n53.86\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\n5.3 ABLATION: EFFECT OF BACKBONE LANGUAGE MODEL ON DRBA\n\nWe evaluate the impact of backbone language models on DRBA using the MinEval subset for controlled\ncomparison (Table 3). GPT-5 achieves the best balance of factual grounding, insight recall, and report\nquality. Open-source models show mixed results: Llama-3.1-405B excels in factuality but lags in recall,\nDeepSeek-V3.1 delivers balanced performance through targeted fine-tuning, and Qwen-2.5-72B is reliable\nbut trails GPT-5. These results underline the importance of backbone choice; larger and more advanced\nmodels generally yield stronger overall performance, though some open-source options are competitive\nin specific metrics. In addition, our experiments also revealed a significant limitation in agents retrieving\ncritical insights from the open web. As shown in Figure 8 in Section L, no agent managed to successfully\nsource external knowledge, highlighting the difficulty of extracting relevant information for deep research\napplications within an unboundedly large search space.\n\nTable 4: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall. The full table with all\ngroundtruth insights and predicted insights is given in Appendix G.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have inter-\nacted with personalized product recommen-\ndations, resulting in a 25% increase in aver-\nage order value.\n\n45% of Lee’s Market online customers engage with\npersonalized product recommendations, resulting in\na 25% increase in average order value.\n(Score = 1.0)\n\n45% of online customers engaged with per-\nsonalized product recommendations, and\namong those engagers average order value\nincreased by 25%.\n(Score = 1.0)\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\n85% of transactions are linked to loyalty accounts at\nLee’s Market, providing a solid foundation for personal-\nized marketing and improving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n5.4 QUALITATIVE ANALYSIS\n\nIn Table 4 we show a sample of three groundtruth insights as well as the predicted insights from using\nboth Llama 3.1 405B and GPT-5. We see that for the first insight, both models are able to effectively\nrecover the groundtruth insight. For the second insight GPT-5 can extract the relevant time of year, where\nas Llama 3.1 405B fails to do so. This possibly suggests that GPT-5 may be better at extracting fine details.\n\n5.5 PERFORMANCE OF WEB AGENTS ON DRBench\n\nWe evaluated Generic WebAgents from AgentLab in a browser-only setting (without API access).\nThe GPT-4.1-powered agent achieved only 1.11% insight recall, 6.67% factuality, and 33.07% report\nquality. While the reports appeared well-structured, they lacked grounded insights, with most trajectories\n\n8\n\n\fTable 5: Model Performance Comparison Across Local or App-based Environments. Note that higher\nnumbers correspond to better scores, and the best result on each metric is bolded.\n\nEnv\nModel\nApp\nDRBA (GPT-5)\nApp\nDRBA (GPT-5) + CRP\nDRBA (DeepSeek-V3.1)\nApp\nDRBA (DeepSeek-V3.1) + CRP App\nLocal\nDRBA (GPT-5)\nLocal\nDRBA (GPT-5) + CRP\nLocal\nDRBA (DeepSeek-V3.1)\nDRBA (DeepSeek-V3.1) + CRP Local\nPerplexity\nLocal\nOpenAI Deep Research (GPT-5) Local\nLocal\nGemini\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n38.33\n39.63\n25.15\n30.26\n41.25\n42.18\n35.62\n36.54\n39.14\n44.78\n43.92\n\n74.52\n65.17\n72.66\n70.27\n82.43\n83.91\n77.12\n78.35\n81.06\n87.53\n85.68\n\n95.14\n92.86\n97.43\n96.67\n98.62\n98.45\n97.95\n97.62\n98.84\n99.12\n98.97\n\n94.56\n93.42\n86.52\n86.88\n91.08\n92.46\n89.16\n90.12\n90.36\n94.92\n93.24\n\n66.01\n64.46\n53.09\n57.86\n69.57\n70.67\n64.03\n65.07\n67.72\n73.56\n72.37\n\ndegenerating into repetitive clicks on irrelevant files or windows. This shows that browser-only agents are\ncurrently far from effective for deep research tasks. Further trajectory examples are shown in Appendix J.\n\n5.6 APP-BASED ENVIRONMENT VS LOCAL ENVIRONMENT\n\nIn Table 5, we compare results across two settings in DRBench: (1) local, where all the task files (e.g.,\nPDFs, PPTX, DOCX, XLSX, chats) are placed in a local folder that the agent can access, and (2)\napp-based, where the same files must be retrieved through our standard enterprise environment and its apps,\nintroducing additional interaction complexity. We find that OpenAI’s Deep Research (GPT-5) achieves\nthe highest scores across all metrics. Our agent with GPT-5 and DeepSeek backbones achieves similar\nperformance to Perplexity in the local-only setting, but lags behind OpenAI and Gemini. In the app-based\nsetting, performance declines across both backbones, highlighting the added difficulty of navigating\nmulti-application environments. This gap underscores that the environment in DRBench is intentionally\nchallenging, enabling a more realistic evaluation of model capabilities in enterprise research scenarios.\n\n6 HUMAN EVALUATION\n\nQuality of Deep Research Questions. We evaluated the quality of the deep research questions in\nDRBench through a human study with five expert annotators across all 15 tasks. Each task was judged\non three criteria: (1) grounding in the external website, (2) relevance to the domain and company context,\nand (3) alignment with associated insights. Annotators provided binary ratings plus optional feedback.\nResults show strong quality: 12 tasks received unanimous approval, while only three (tasks DR1, DR11,\nand DR13) received a single negative vote due to minor issues with specificity or distractor difficulty. This\ncorresponds to a 96% approval rate (72/75 votes).\n\nCorrelation of Used Metrics with Human Preference. We collected human preference on a subset\nof 11 tasks1. Each annotator was shown a golden insight with aligning insights from two models2 and\nasked to choose which they preferred, or label both as good/bad. Missing alignments were shown as\nempty strings. We compared agents with AAP no RP against GPT-5 and Llama-3.1-405B-Instruct. The\nFleiss κ (Fleiss, 1971) across five annotators was 0.67. Most outputs were judged both bad due to missing\nalignments, but when preferences were expressed, GPT-5 was favored 61.1% over Llama-405B-Instruct,\nconsistent with our metric-based findings in Section 5.3. Additional analyses are in Appendix Q.\n\n7 CONCLUSION & FUTURE WORK\n\nIn this work, we introduced DRBench, a benchmark for evaluating AI agents on complex, open-ended\nenterprise deep research tasks that require reasoning over both public and private data. Unlike prior\nbenchmarks focused on surface-level or web-only queries, DRBench offers 15 persona-grounded tasks\nsituated in realistic enterprise contexts and evaluated through an environment that integrates real-world\nenterprise applications and heterogeneous data formats. We also presented the DRBench Agent (DRBA)\n\n1We selected tasks with fewer than 8 insights for a reasonable amount of manual work.\n2The gold-prediction alignment is provided by the insight recall metric.\n\n9\n\n\fas a strong baseline and analyzed its behavior across planning strategies and backbone models. Our results\nshow that while agents are generally effective at avoiding distractors and capable of producing structured\nreports, they still struggle to consistently extract decision-critical insights. Adaptive planning improves\nrecall of injected insights, while lightweight strategies tend to preserve factual accuracy, underscoring the\ndifficulty of balancing exploration with reliability. Looking ahead, we plan to extend DRBench with tasks\nrequiring cross-file integration, reasoning across modalities such as PDFs and chats, and richer distractors.\nWe also aim to add multimodal sources like images and video, as well as privacy-sensitive tasks to assess\ndata protection. Together, these extensions will move research agents closer to enterprise readiness and\nprovide a stronger foundation for studying deep research in realistic organizational settings.\n\n10\n\n\fREFERENCES\n\nAmirhossein Abaskohi, Amrutha Varshini Ramesh, Shailesh Nanisetty, Chirag Goel, David\nand Issam H. Laradji.\nURL\n\nVazquez, Christopher Pal, Spandana Gella, Giuseppe Carenini,\nAgentAda:\nhttps://arxiv.org/abs/2504.07421.\n\ntailored insight discovery, 2025.\n\nSkill-adaptive data analytics\n\nfor\n\nReyna Abhyankar, Qi Qi, and Yiying Zhang. OSWorld-Gold: Benchmarking the efficiency\nIn ICML 2025 Workshop on Computer Use Agents, 2025. URL\n\nof computer-use agents.\nhttps://openreview.net/forum?id=sV3n6mYy7J.\n\nAll-HandsAI.\n\nOpenHands.\n\nhttps://github.com/All-Hands-AI/OpenHands, 2024.\n\nAccessed: 2024-06-01.\n\nPierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja,\nRicardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo\nLaurenc¸on, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre M´enard, Gr´egoire Mialon, Ulyana\nPiterbarg, Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav Vorotilov,\nMengjue Wang, and Ian Yu. ARE: Scaling up agent environments and evaluations, 2025. URL\nhttps://arxiv.org/abs/2509.17158.\n\nL´eo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier de Chezelles, Quentin\nCappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. WorkArena++: Towards\ncompositional planning and reasoning-based common knowledge work tasks. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL\nhttps://openreview.net/forum?id=PCjK8dqrWW.\n\nNikos I Bosse, Jon Evans, Robert G Gambee, Daniel Hnyk, Peter M¨uhlbacher, Lawrence Phillips, Dan\nSchwarz, Jack Wildman, et al. Deep Research Bench: Evaluating AI Web Research Agents. arXiv\npreprint arXiv:2506.06287, 2025.\n\nZijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green,\nKshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong,\nXinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin.\nBrowseComp-Plus: A more fair and transparent evaluation benchmark of Deep-Research agent, 2025.\nURL https://arxiv.org/abs/2508.06600.\n\nThibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, L´eo Boisvert,\nMegh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han\nL`u, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan\nSalakhutdinov, Nicolas Chapados, and Alexandre Lacoste. The BrowserGym ecosystem for web agent\nresearch, 2025. URL https://arxiv.org/abs/2412.05467.\n\nJo˜ao Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie\nCallan, Jo˜ao Magalh˜aes, Bruno Martins, et al. Deepresearchgym: A free, transparent, and reproducible\nevaluation sandbox for deep research. arXiv preprint arXiv:2505.19253, 2025.\n\nAlexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty,\nDavid Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents\nat solving common knowledge work tasks? 2024.\n\nMingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. DeepResearch Bench: A\n\ncomprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nJoseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76\n\n(5):378, 1971.\n\nFoundationAgents. OpenManus. https://github.com/FoundationAgents/OpenManus,\n\n2024. Accessed: 2024-06-01.\n\n11\n\n\fBoyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Botao Yu, Andrei Kopanev, Weijian\nQi, Yiheng Shu, Jiaman Wu, Chan Hee Song, Bernal Jimenez Gutierrez, Yifei Li, Zeyi Liao,\nHanane Nour Moussa, TIANSHU ZHANG, Jian Xie, Tianci Xue, Shijie Chen, Boyuan Zheng,\nKai Zhang, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, and Yu Su. Mind2web\nIn The Thirty-ninth Annual Conference\n2: Evaluating agentic search with agent-as-a-judge.\non Neural Information Processing Systems Datasets and Benchmarks Track, 2025.\nURL\nhttps://openreview.net/forum?id=AUaW6DS9si.\n\nKung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese,\nCaiming Xiong, Philippe Laban, and Chien-Sheng Wu. CRMArena: Understanding the capacity of\nLLM agents to perform professional CRM tasks in realistic environments. In Proceedings of the 2025\nConference of the Nations of the Americas Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), 2025a.\n\nKung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey,\nYixin Mao, Silvio Savarese, Caiming Xiong, and Chien-Sheng Wu. CRMArena-Pro: Holistic\nassessment of LLM agents across diverse business scenarios and interactions. arXiv preprint\narXiv:2505.18878, 2025b.\n\nHuggingFace.\n\nsmolagents.\n\nhttps://github.com/huggingface/smolagents, 2024.\n\nAccessed: 2024-06-01.\n\nAbhinav Java, Ashmit Khandelwal, Sukruta Midigeshi, Aaron Halfaker, Amit Deshpande, Navin Goyal,\nAnkur Gupta, Nagarajan Natarajan, and Amit Sharma. Characterizing deep research: A benchmark\nand formal definition, 2025. URL https://arxiv.org/abs/2508.04183.\n\nLearningCircuit.\n\nLocal deep research.\n\nlocal-deep-research, 2025.\n\nhttps://github.com/LearningCircuit/\n\nYao Liu, Deming Ye, Shuohang Wang, Furu Wei, Yujia Ma, and Minlie Huang. G-Eval: NLG evaluation\n\nusing GPT-4 with better human alignment. EMNLP, 2023.\n\nXing Han L`u, Zdenˇek Kasner, and Siva Reddy. WebLINX: Real-world website navigation with multi-turn\n\ndialogue, 2024. URL https://arxiv.org/abs/2402.05930.\n\nGr´egoire Mialon, Cl´ementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom.\n\nGAIA: a benchmark for general AI assistants. ICLR, 2024.\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke\nZettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 12076–12100, 2023.\n\nOpenAI.\n\ntext-embedding-3-large.\n\nhttps://platform.openai.com/docs/guides/\n\nembeddings, 2024. [Large language model]. Accessed September 2025.\n\nGaurav Sahu, Abhay Puri, Juan A Rodriguez, Amirhossein Abaskohi, Mohammad Chegini, Alexan-\ndre Drouin, Perouz Taslakian, Valentina Zantedeschi, Alexandre Lacoste, David Vazquez, et al.\nInsightBench: Evaluating insight extraction for business analytics agents. ICLR, 2025.\n\nZilliz Tech. Deep-Searcher. https://github.com/zilliztech/deep-searcher, 2024.\n\nAccessed: 2024-06-01.\n\nYuhao Wang, Ruiyang Ren, Junyi Li, Xin Zhao, Jing Liu, and Ji-Rong Wen. REAR: A relevance-aware\nretrieval-augmented framework for open-domain question answering. In Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing, pp. 5613–5626, 2024.\n\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua,\nZhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for\nopen-ended tasks in real computer environments. NeurIPS, 2024.\n\n12\n\n\fFrank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui\nZhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander\nMaben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig.\nTheAgentCompany: Benchmarking LLM agents on consequential real world tasks, 2025a. URL\nhttps://arxiv.org/abs/2412.14161.\n\nRenjun Xu and Jingwen Peng. A comprehensive survey of deep research: Systems, methodologies, and\n\napplications. arXiv preprint arXiv:2506.12594, 2025.\n\nTianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, and Pengfei Liu. ResearcherBench:\nURL\n\nEvaluating deep AI research systems on the frontiers of scientific inquiry, 2025b.\nhttps://arxiv.org/abs/2507.16280.\n\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.\nDeepResearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv,\n2025.\n\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou,\nYonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. WebArena: A Realistic Web Environment\nfor Building Autonomous Agents, April 2024. URL http://arxiv.org/abs/2307.13854.\narXiv:2307.13854 [cs].\n\n13\n\n\fTable 6: Comparison of Deep Research Tasks of Different Benchmarks.\n\nBenchmark\nDeepResearchGym (Coelho et al., 2025)\nDeep Research Bench (Bosse et al., 2025)\n\nDeepResearch Bench (Du et al., 2025)\n\nResearcherBench (Xu et al., 2025b)\n\nLiveDRBench (Java et al., 2025)\n\nBrowseComp-Plus (Chen et al., 2025)\n\nGAIA2 (Andrews et al., 2025)\n\nDRBench\n\nSample Question\nIs the COVID vaccine dangerous\nFind a reliable, known number on the internet. The total number\nof FDA Class II Product Recalls of medical devices.\nWhile the market features diverse quantitative strategies like\nmulti-factor and high-frequency trading, it lacks a single, standard-\nized benchmark for assessing their performance across multiple\ndimensions such as returns, risk, and adaptability to market\nconditions. Could we develop a general yet rigorous evaluation\nframework to enable accurate comparison and analysis of various\nadvanced quant strategies?\nCompare the Transformer and Mamba model architectures, analyz-\ning their performance and technical characteristics in different appli-\ncation scenarios. Based on the latest research, discuss the advantages\nand disadvantages of both models and their applicable scenarios.\nFor complex reasoning tasks (e.g., tasks involving multiple citations\nor extended reasoning chains), what are the strengths of current\nagent technologies, and what are their limitations? Please analyze\nthis in the context of research since June 2024.\nIdentify the title of a research publication published before June 2023,\nthat mentions Cultural traditions, scientific processes, and culinary\ninnovations. It is co-authored by three individuals: one of them was\nan assistant professor in West Bengal and another one holds a Ph.D.\nUpdate all my contacts aged 24 or younger to be one year older\nthan they are currently.\nHow can Lee’s Market leverage FSMA 204 regulations to enhance\nfood safety and customer trust?\n\nA COMPARISON OF DEEP RESEARCH BENCHMARKS\n\nAND AI AGENT BENCHMARKS WITH A COMPUTER ENVIRONMENT\n\nIn Table 1, we compare existing deep research benchmarks and AI agent benchmarks that provide a\ncomputer environment with DRBench. While the questions in existing benchmarks focus on public interest\ntopics and require generic web search and computer use, DRBench provides realistic questions that real\npersonas in organizations need to resolve.\n\nB DRBench TASKS\n\nAs shown in Table 7, DRBench contains 15 tasks in total, covering 3 industries (retail, healthcare and\nelectric vehicles), 10 task domains (compliance, sales, customer relationship management, market analysis,\ncustomer service management, IT service management, cyber security, marketing, quality assurance, and\nresearch), and 3 difficulty levels (easy, medium, hard). In addition, we generate the following 3 companies\n(one for each industry type): (1) a supermarket chain called Lee’s Market, (2) a virtual healthcare company\ncalled MediConn Solutions, and (3) an electric vehicle company called Elexion Automotive.\n\nTable 8 presents a deep research question from DRBench and its supporting groundtruth insights. We also\nvisualize the DR Question and all QA pairs by embedding them with OpenAI’s text-embedding-\n3-large model and projecting into 2D using t-SNE in Figure 4. The plot shows that injected supporting\ninsights lie closer to the DR Question, while distractors appear farther away, confirming that our injected\ninsights are semantically aligned with the research objective.\n\nC DRBench EXAMPLES OF INJECTED INSIGHTS\n\nAs shown in Figure 2, supporting documents are generated with enterprise insights injected. In Figure 5,\nwe show two examples of a generated files (PPTX and Mattermost chat) with their embedded insights.\n\n14\n\n\fFigure 4: t-SNE visualization of QA pairs for the DR Question in Task DR0005. The plot shows the\ndistribution of annotated pairs across Supporting Insights (green), Distractors (red), and the central\nDeep Research (DR) Question (gold star). Out of 49 pairs, 16 correspond to supporting insights and\n33 are distractors. The visualization illustrates how relevant insights cluster separately from distractors,\nhighlighting the challenge of retrieving salient information in a distractor-heavy environment.\n\n(a) Example supporting file named food-safety-\nregulatory-compliance.pdf with an injected insight\n”Lee’s Market reduced food waste by 8% in Q2 2024,\nsaving $1.2M.”\n\n(b) Example supporting email in the Sent mailbox with\nan injected insight ”85% of high-risk food suppliers are\nFSMA 204 compliant, totaling 270 vendors.”\n\nFigure 5: Example files with injected insights in DRBench.\n\n15\n\n151050510t-SNE Dimension 11050510t-SNE Dimension 2Q: What percentage of Lee's Market customers are centennial shoppers in Q3 2024?A: 25% of Lee's Market customers are centennial shoppers as of Q3 2024.Q: Projected chatbot cost savings in Q2 2024?A: Expect 25% cost reduction by Q2 2024.Q: How many customer service inquiries do we receive from centennial shoppers on...A: 2,500 customer service inquiries from centennial shoppers are received on average per quarter.Q: Centennial chatbot engagement rate by Q3 2025?A: 50% centennials engage weekly by Q3 2025.Q: How many Lee's Market customers used online shopping services in Q1 2024?A: About 30% of customers used online shopping services, totaling 150,000 users in Q1 2024.Q: Chatbot adoption rate Q1 2025?A: 55% adoption rate by centennials by Q1 2025.Q: Chatbot adoption rate among centennials by Q2 2024?A: 40% centennials prefer chatbots by Q2 2024.Q: How many customer queries does Lee's Market receive monthly in 2024?A: Lee's Market receives an average of 50,000 customer queries per month in 2024.Q: What was Lee's Market employee training budget in Q3 2023?A: Lee's Market allocated $450,000 for employee training, a 15% increase.Q: Annual facility maintenance cost 2024?A: Total maintenance cost was $3.5 million in 2024.Q: How many Lee's Market employees participated in leadership training in Q1 2024?A: 120 employees completed leadership training with a 90% satisfaction rate.Q: How many Lee's Market employees completed IT security training in Q1 2024?A: 85% of employees completed IT security training with a 95% pass rate.Q: What was Lee's Market employee turnover rate in Q2 2024?A: Employee turnover rate averaged 12% with a 5% increase from Q1 2024.Q: What was Lee's Market energy consumption cost in Q2 2024?A: Lee's Market energy consumption cost totaled $850,000 in Q2 2024.Q: What were Lee's Market IT costs in Q4 2024?A: IT costs totaled 3.4M,with1.2M on new hardware.Q: How many new store locations did Lee's Market open in Q3 2024?A: Lee's Market opened 12 new store locations across the US in Q3 2024.Q: What was our workers' compensation claims rate in Q2 2025?A: Workers' compensation claims rate was 2.5% with a total payout of $120,000.Q: What was Lee's Market average store maintenance cost in Q2 2024?A: Average store maintenance cost was $2,000 per month with a 5% reduction.Q: What were Lee's Market facility maintenance costs in Q2 2024?A: Facility maintenance costs totaled 1.2Mwithanaverageof100,000 monthly.Q: What was our employee headcount in Q1 2025?A: We had 5,100 employees as of January 2025.Q: How many employees attended compliance training in Q1 2024?A: Around 300 employees completed compliance training in Q1 2024.Q: What was our facility maintenance cost in Q4 2024?A: Facility maintenance costs totaled $1.8 million in Q4 2024.Q: Facility maintenance expenses Q4 2023?A: Maintenance expenses totaled $1.2 million in Q4 2023.Q: Employee training hours Q3 2024?A: Average training hours per employee were 15 in Q3 2024.Total QA Pairs: 49Supporting: 16 | Distractor: 33 | External: 0Annotated: 8 supporting + 16 distractorShapes:  Supporting   Distractor   ExternalHow can Lee's Market leverage chatbots to enhance the consumer experience for centennial shoppers?DR QuestionSupporting (Insight) (16)Distractor (33)\fTable 7: DRBench Questions and Statistics. Industry: target industry of the deep research question.\nDomain: domain of the deep research task. DR Question: question of the deep research task. Difficulty:\ndifficulty of the task defined based on the rubric mentioned in 3. # Applications: the number of total\napplications in the task environment. # Insights: the number of relevant insights to the deep research\nquestion. # Distractors: the number of non-supportings documents that do not contain relevant insights.\n\nIndustry\nRetail\n\nDomain\nCompliance\n\nRetail\n\nRetail\n\nRetail\n\nRetail\n\nSales\n\nCRM\n\nMarket Analysis\n\nCSM\n\nHealthcare\n\nCompliance\n\nHealthcare\n\nITSM\n\nHealthcare\n\nCybersecurity\n\nHealthcare\n\nCRM\n\nHealthcare\n\nMarketing\n\nElectric Vehicle\n\nCompliance\n\nElectric Vehicle\n\nQuality Assurance\n\nElectric Vehicle\n\nCybersecurity\n\nElectric Vehicle\n\nResearch\n\nElectric Vehicle\n\nCSM\n\nDR Question\nHow can Lee’s Market leverage FSMA 204 reg-\nulations to enhance food safety and customer trust?\nHow can personalization drive sales in the retail\nindustry and what strategies can be used for Lee’s\nMarket in action?\nHow can we leverage data-driven loyalty programs\nto enhance customer engagement?\nWhat are the new trends in the grocery retail\nmarket and what strategies can Lee’s Market adopt\nto remain competitive?\nHow can Lee’s Market\nleverage chatbots to\nenhance the consumer experience for centennial\nshoppers?\nWhat are the key factors influencing MediConn\nSolutions’ decision to accept insurance for tele-\nhealth providers, considering HIPAA compliance\nand state-specific data privacy regulations?\nHow can we leverage ISTM and AI-driven ana-\nlytics to minimize IT service desk workload and\nimprove response times in MediConn Solutions?\nWhat is the impact of third-party data breaches\non MediConn’s virtual healthcare platforms and\npatient data, and what new regulations can be\nimplemented to defend against these breaches?\nHow can MediConn Solutions leverage trendy new\nCRM solutions to improve patient engagement\nand retention, and what new CRM solutions are\nexpected in 2025?\nWhat are the most critical elements of a robust\ndigital presence for a telehealth provider such as\nMediConn Solutions, and how can we optimize\nour website and content marketing strategy to\nattract digital-first patients?\nHow can we balance the need for durability\nand warranty guarantees for EV batteries with\nevolving regulatory requirements, especially ACC\nregulations (ACC II), while staying on track with\nour production timelines through 2035?\nHow can Elexion Automotive’s quality assurance\nprocesses be optimized to address the unique\nchallenges of electric vehicle production, such as\nsoftware and user experience issues, compared to\ngasoline cars?\nHow can Elexion Automotive effectively im-\nplement a cybersecurity strategy for its electric\nvehicles, considering the risks and challenges\nposed by connected and autonomous technologies?\nCan we leverage AI-enhanced battery management\nto improve EV battery lifespan by 15%?\nHow can Elexion Automotive increase customer\ntrust through after-sales support while balancing\nthe need for exceptional customer care with\nefficient and cost-effective service?\n\nDifficulty\neasy\n\n# Applications\n2\n\n# Insights\n3\n\n# Distractors\n7\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\n2\n\n4\n\n3\n\n4\n\n3\n\n3\n\n4\n\n4\n\n4\n\n2\n\n1\n\n3\n\n3\n\n4\n\n4\n\n7\n\n6\n\n16\n\n4\n\n6\n\n7\n\n12\n\n15\n\n3\n\n3\n\n6\n\n7\n\n15\n\n10\n\n15\n\n14\n\n33\n\n8\n\n12\n\n14\n\n24\n\n30\n\n6\n\n6\n\n12\n\n14\n\n30\n\nD DRBENCH ENTERPRISE ENVIRONMENT\n\nThe DRBench Enterprise Environment provides a containerized simulation of realistic enterprise\nresearch settings where employees access confidential company information, personal files, and internal\ncommunications for comprehensive report generation. The environment simulates both a user’s local\nmachine filesystem and provides password-protected access to enterprise services.\n\nTo emulate realistic enterprise research settings, DRBench provides a self-contained Docker environment\nthat integrates commonly used applications: Nextcloud for shared documents, Mattermost for internal\nchat, an IMAP server and Roundcube open-source client for emails, and Filebrowser to emulate local files.\nEach task is initialized by distributing its data across these services, enabling agents to retrieve, analyze,\nand cite information through enterprise-like interfaces rather than static files. This design ensures realistic\ninteraction while maintaining reproducibility and controlled evaluation.\n\n16\n\n\fTable 8: Example Deep Research Question and Supporting Groundtruth Insights\n\nDeep Research Question\n\nHow can Lee’s Market leverage FSMA\n204 regulations to enhance food safety and\ncustomer trust?\n\nSupporting Groundtruth Insight\nU.S. grocers are working to meet the FDA’s FSMA 204 traceability rules\nby January 2026, which require tracking lot codes and key data for high-risk\nfoods to expedite recalls. This compliance is viewed as an “evolutionary\nstep” to modernize grocery operations and enhance food safety.\nBy capturing detailed traceability data, such as lot codes, at every step,\nretailers can meet regulations and gain inventory benefits. This allows grocers\nto know exact expiration dates by lot, enabling them to discount items before\nthey expire, thus reducing food waste and keeping products fresher.\nRegional grocers like Lunds & Byerlys and Raley’s see FSMA 204 as\na chance to enhance their systems and supply chain transparency. They\nbelieve improved traceability will boost customer trust and could signal the\nstart of more extensive future food safety regulations.\nLee’s Market tracks 250 high-risk food products as of Q3 2024, affecting\n30% of inventory.\nLee’s Market reduced food waste by 8% in Q2 2024, saving $1.2M.\n85% of high-risk food suppliers are FSMA 204 compliant, totaling 270\nvendors.\n\nInsight Category\nExternal\n\nExternal\n\nExternal\n\nInternal\n\nInternal\nInternal\n\nD.1 ARCHITECTURE AND SERVICES\n\nThe environment implements a multi-service architecture within a single Docker container. This design\nprioritizes deployment simplicity and cross-platform compatibility while maintaining service isolation.\nThe container orchestrates the following enterprise services:\n\n• Nextcloud: Open-source file sharing and collaboration platform analogous to Microsoft\n\nSharePoint or Google Drive, providing secure document storage with user authentication.\n\n• Mattermost: Open-source team communication platform simulating internal company commu-\nnications similar to Microsoft Teams or Slack, with teams, channels, and persistent chat history.\n\n• FileBrowser: Web-based file manager providing access to the container’s local filesystem,\n\nsimulating employee desktop environments and local document access.\n\n• Email System: Roundcube webmail interface with integrated SMTP (postfix) and IMAP\n\n(dovecot) services for enterprise email communication simulation.\n\n• VNC/NoVNC Desktop: Protocol and browser-based VNC access providing full desktop\nenvironment interaction within the container for comprehensive enterprise workflow simulation.\n\nD.2 TASK LOADING AND DATA DISTRIBUTION\n\nAt initialization, the environment processes task configuration files (env.json) and distributes data\nacross services through automated Python scripts and it makes sure that this source data is only accessible\nthrough the intended applications:\n\n• File Distribution: Documents are placed in appropriate Nextcloud user folders and FileBrowser\n\ndirectories based on task specifications\n\n• Communication Import: Chat histories and team conversations are imported into Mattermost\n\nchannels with proper user attribution\n\n• Email Integration: Email conversations are loaded into the mail system with realistic threading\n\nand metadata\n\n• User Provisioning: Enterprise users are automatically created across all services with consistent\n\nauthentication credentials\n\n17\n\n\ffrom drbench import drbench_enterprise_space, task_loader\n\n# Load task configuration\ntask = task_loader.get_task_from_id(task_id)\n\n# Initialize environment with automatic port allocation\nenv = drbench_enterprise_space.DrBenchEnterpriseSearchSpace(\n\ntask=task.get_path(),\nstart_container=True,\nauto_ports=True\n\n# Prevents port conflicts in parallel execution\n\n)\n\n# Environment provides service discovery\navailable_apps = env.get_available_apps()\n# Returns: {’nextcloud’: {’port’: 8081, ’credentials’: {...}}, ...}\n\n# Pass relevant information to the agent\n\n# Cleanup when research complete\nenv.delete()\n\nListing 1: DrBench Environment Usage\n\nD.3 PYTHON INTEGRATION\n\nThe DrBenchEnterpriseSearchSpace class provides programmatic container management with\nthe following capabilities: container lifecycle management, service access information, task-specific\ndata loading, and automatic cleanup. The typical usage pattern shown in Listing 1 demonstrates these\nintegrated capabilities.\n\nD.4 ENTERPRISE SERVICE APIS\n\nEach service exposes both **web interfaces** for human and web-agent interaction, and **programmatic\nAPIs** for agent access:\n\n• Nextcloud: WebDAV API for file operations, sharing, and metadata retrieval\n\n• Mattermost: REST API for message history, channel management, and user interactions\n\n• Email: IMAP/SMTP protocols for message retrieval and sending\n\n• FileBrowser: HTTP API for filesystem operations and file management\n\nThis dual-access model enables both agent-driven research and human verification of enterprise scenarios,\nsupporting comprehensive evaluation of research capabilities across realistic enterprise information\narchitectures.\n\nE DRBench EXAMPLES OF APPLICATION SCREENSHOTS\n\nFigures 6 and 7 show the applications provided in DRBench environment: File Browser, Mattermost,\nRoundcube, and Nextcloud.\n\nF DRBench AGENT IMPLEMENTATION DETAILS\n\nF.1 DETAILED WORKFLOW\n\nAs depicted in Figure 3, the workflow begins with a Company Employee submitting an enterprise Deep\nResearch Question along with Company Context. The DRBench agent processes this input through several\nkey stages:\n\n18\n\n\f(a) Screenshot of the File Browser interface, displaying\norganized files and folders within the system.\n\n(b) Screenshot of the Mattermost communication\nplatform, showing a discussion channel and user\ninterface elements.\n\nFigure 6: Screenshots of Applications in DRBench environment (Part 1).\n\n(a) Screenshot of the Nextcloud file management system,\nillustrating the file list view with various document types.\n\n(b) Screenshot of Roundcube, an email client, it shows\nan open email in the user’s inbox.\n\nFigure 7: Screenshots of Applications in DRBench environment (Part 2).\n\nStage 1: Research Planning. The agent decomposes the research question into structured research\ninvestigation areas to guide subsequent action generation. This initial decomposition lays out a strategy\nto systematically cover the research space while maintaining focus on the initial deep research question.\n\nStage 2: Action Planning. The Action Planning stage translates the research objectives into executable\nactions through a planning subsystem. This component uses an LLM to create a prioritized sequence\nof actions. Each action is parameterized with specific tool selection and execution parameters, its\ndependencies to other actions in the plan, and a priority score.\n\nStage 3: Research Loop with Adaptive Execution. The Research Loop iterates over the following\nsub-stages until completion: (1) Tool Selection and Execution: The tool selection and execution subsystem\nimplements a sophisticated priority-based selection of actions from the plan at each research iteration step\nand proceeds to execute it with the current research context. (2) Content Processing: If necessary, the action\nwill make use of the content processing subsystem to extract, synthesize, and store retrieved documents and\nwebsites into a vector store to form a task-specific knowledge base that will grow on each iteration. (3) Adap-\ntive Action Planning: After each execution round, the agent analyzes the most recent findings; if coverage\ngaps are detected, new actions are created and added to the plan at this point. This ensures that newly discov-\nered knowledge is taken into account to answer the research question. (4) Iteration Findings Storage: Results\nfrom each iteration are stored in the vector store with rich metadata for traceability and later assessment.\n\nStage 4: Convergence and Completion. The research loop continues until all actions in the plan have\nbeen completed or the maximum iteration limit is reached.\n\n19\n\n\fStage 5: Report Writing. The report writing subsystem queries the vector store to synthesize the\nresearch findings and relevant retrieved content. This component generates a comprehensive report and\nuses its own citation tracking system to ensure proper attribution of claims within the report.\n\nThe vector store serves as the main knowledge integration component, maintaining embeddings of all\nprocessed content and enabling semantic retrieval at the report generation stage. This component is crucial\nin a deep research setting to prevent information loss from early research stages in arbitrarily long research\nsessions.\n\nF.2 RESEARCH PLANNING IMPLEMENTATION\n\nThe Research Planning subsystem offers three operational modes to evaluate the impact of structured\nplanning on research effectiveness:\n\n• Complex Mode: Generates a comprehensive research plan with detailed investigation areas.\nThese areas contain details about the specific research focus, expected information sources, and\nsuccess criteria, among others. Each area includes an importance level and specific business\nintelligence objectives (see Listing 2).\n\n• Simple Mode: Creates focused question decompositions with 4-10 self-contained subqueries\nderived directly from the main research question. Uses straightforward decomposition without\nthe complex enterprise research structure of complex mode. See examples in Listing 4 and\nListing 3 for comparison of different planning modes.\n\n• None: Bypasses structured planning entirely, proceeding directly to action generation based\non the original question. This mode serves as a baseline to measure the added-value of explicit\nplanning stages.\n\nThe planning process begins with the enriched query (Prompt 1) and uses the research planning prompt\n(Prompt 2) to generate structured outputs. In Complex Mode, the system creates detailed investigation areas\nwith enterprise-focused metadata, while Simple Mode produces straightforward question decompositions\nsimilar to existing multi-step reasoning approaches. The resulting plan structure directly feeds into the\nAction Planning System (Section F.3) for executable action generation.\n(cid:7)\n{\n\n(cid:4)\n\n\"area_id\": 1,\n\"research_focus\": \"Core strategic\n\ndomain, market segment, or business hypothesis to investigate\",\n\n\"information_needs\": [\n\n\"What specific intelligence is required for strategic decisions\"\n\n],\n\"knowledge_sources\": [\"internal\", \"external\", \"both\"],\n\"research_approach\n\n\": \"competitive_analysis | market_research | strategic_assessment\n\n| trend_analysis | risk_analysis | performance_benchmarking\",\n\n\"key_concepts\": [\"concept1\", \"concept2\"],\n\"business_rationale\": \"Why this investigation\n\narea is critical for enterprise strategy and decision-making\",\n\n\"expected_insights\": \"What strategic\n\nunderstanding or competitive intelligence this area should provide\",\n\n\"stakeholder_impact\": \"Which\n\nbusiness units or decision-makers will benefit from these insights\",\n\n\"importance_level\": \"critical | important | supplementary\"\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 2: Investigation Area Structure for Full Planning Mode\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"What are the new trends in the grocery retail market\n\nand what strategies can Lee’s Market adopt to remain competitive?\",\n\n\"plan\": {\n\n\"research_investigation_areas\": [\n{\n\n20\n\n\f\"area_id\": 1,\n\"research_focus\": \"Current trends in grocery retail market\",\n\"information_needs\": [\n\"Latest consumer preferences\",\n\"Emerging technologies influencing grocery shopping\",\n\"Sustainability practices in grocery retail\",\n\"Changes in supply chain dynamics\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"trend_analysis\",\n\"key_concepts\n\n\": [\"e-commerce growth\", \"sustainability in supply chains\"],\n\n\"business_rationale\": \"Understanding consumer\n\ntrends and technological advancements that shape shopper behavior is\ncritical for adapting offerings and enhancing customer engagement.\",\n\"expected_insights\": \"Identify specific trends affecting customer\n\nbuying decisions, including the rise of online grocery shopping\nand preferences for sustainable, local, or organic products.\",\n\n\"stakeholder_impact\n\n\": \"Marketing, Product Development, Supply Chain Management\",\n\n\"importance_level\": \"critical\"\n\n},\n{\n\n\"area_id\": 2,\n\"research_focus\": \"Competitive analysis of grocery retailers\",\n\"information_needs\": [\n\"Market share analysis\",\n\"Competitor strengths and weaknesses\",\n\"Innovative strategies adopted by competitors\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"competitive_analysis\",\n\"\n\nkey_concepts\": [\"market positioning\", \"competitive differentiation\"],\n\n\"business_rationale\": \"A comprehensive understanding of\n\ncompetitors allows for strategic positioning and the identification\nof innovative practices that can be adopted or improved upon.\",\n\n\"expected_insights\": \"Detailed profiles of key\n\ncompetitors, including strategic moves they are making to capture\n\nmarket share, which can inform Lee’s Market’s competitive strategy.\",\n\n\"stakeholder_impact\n\n\": \"Executive Leadership, Strategic Planning, Marketing\",\n\n\"importance_level\": \"critical\"\n\n},\n...\n]\n\n}\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 3: Complex Mode Research Plan Example\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"How can we leverage\n\ndata-driven loyalty programs to enhance customer engagement?\",\n\n\"plan\": {\n\n\"mode\": \"simple\",\n\"subqueries\": [\n\"What are the key features\nof successful data-driven loyalty programs in the retail industry?\",\n\"How can data analytics be used to personalize rewards\nand incentives in loyalty programs to increase customer engagement?\",\n\"What types of customer data should be collected and analyzed\nto optimize loyalty programs for a company like Lee’s Market?\",\n\n21\n\n\f\"Which\ntechnology platforms and tools are most effective for implementing\nand managing data-driven loyalty programs in the retail sector?\",\n\"How can Lee’s Market measure the success of their\nloyalty program in terms of customer engagement and sales growth?\",\n\"What are the best practices for integrating a loyalty program\nwith existing marketing strategies to enhance customer experience?\",\n\"How can Lee’s Market ensure customer data\nprivacy and security while leveraging data-driven loyalty programs?\",\n\"What are the potential\nchallenges and limitations of implementing a data-driven loyalty\n\nprogram for a retail company with Lee’s Market’s size and resources?\"\n],\n\"research_methodology\": {\n\"overall_approach\": \"Query decomposition into focused subqueries\"\n}\n\n}\n\n}(cid:6)\n\nListing 4: Simple Mode Research Plan Example\n\n(cid:5)\n\nF.3 ACTION PLANNING SYSTEM\n\nThe Action Planning System translates research objectives into executable actions through an intelligent\nplanning subsystem that manages tools, prioritization, and dependencies.\n\nAvailable Tools Table 9 summarizes the available tools organized by category and their primary purposes.\n\nTable 9: DrBench Agent Tool Categories and Purposes\n\nCategory\n\nTool\n\nPurpose\n\nInformation Retrieval\n\nInternet Search\n\nEnterprise API\n\nURL Fetch\n\nAnalysis\n\nAnalyzer\n\nLocal Processing\n\nLocal\nSearch\n\nDocument\n\nExternal market research, competitive intel-\nligence, and public data analysis. Ideal for\nmarket trends, competitor analysis, industry\nreports, news articles.\n\nAccess to proprietary internal data through\nextensible adapters (Nextcloud, Mattermost,\nemail, FileBrowser). Ideal for internal metrics,\ncommunications, confidential documents.\n\nDirect content extraction from specific URLs.\nIdeal for deep analysis of reports, whitepapers,\ncase studies, competitor websites.\n\nAI-powered synthesis and analysis using\nvector search. Ideal for cross-referencing find-\nings, identifying patterns, generating insights.\n\nSemantic search within locally ingested\ndocuments. Ideal for targeted retrieval from\nlocal files with source references.\n\nPriority Scoring and Dependencies Actions are assigned priority scores (0.0-1.0 scale) based on strategic\nimportance and expected information value. The priority assignment follows enterprise research principles:\n\n• Source Type Prioritization: Enterprise and local sources receive higher priority than external\n\nsources, reflecting the strategic value of proprietary information in competitive analysis.\n\n• Query Specificity: Targeted queries addressing specific investigation areas score higher than\n\nbroad exploratory searches, ensuring focused research execution.\n\n22\n\n\f• Dependency Management: Actions can specify prerequisite relationships where certain\ninformation gathering must precede analysis or synthesis tasks. The scheduler respects these\ndependencies while maximizing parallel execution within each iteration.\n\nF.4 ENTERPRISE INTEGRATION ARCHITECTURE\n\nService Adapters The system implements extensible adapters for enterprise services including Nextcloud\nfile server, Mattermost chat, IMAP email systems, and FileBrowser interfaces. Each adapter handles\nservice-specific authentication, data retrieval, and metadata preservation for proper citation attribution.\n\nSource Prioritization Strategy Enterprise and local sources receive priority multipliers in action scoring,\nreflecting the strategic value of proprietary information. The system maintains source type classification\nthroughout the pipeline to ensure internal intelligence drives analytical conclusions while external sources\nprovide context and validation.\n\nF.5 TOOL SELECTION AND EXECUTION\n\nThe Tool Selection stage implements a priority-based action selection and tool invocation within each\nresearch iteration to execute the action plan.\n\nAction Selection Process At each iteration, the system selects executable actions based on priority\nscores and dependency satisfaction:\n\n• Priority-Based Scheduling: Actions are ranked by their priority scores, with enterprise and local\nsources prioritized over external sources to maximize the value of specific private information.\n\n• Dependency Validation: The scheduler checks that all prerequisite actions have completed\n\nbefore making an action available for execution.\n\n• Sequential Execution: Actions execute one at a time in priority order, maintaining research\n\ncoherence and enabling each action to build upon previous findings.\n\nOnce selected, actions execute through a standardized interface and results are integrated into the research\ncontext, informing the following stage of adaptive action planning.\n\nF.6 ADAPTIVE PLANNING\n\nThe Adaptive Planning system enables dynamic evolution of the action plan by analyzing results after\neach iteration to generate extra actions addressing information gaps.\n\nIt starts by analyzing the most recently completed actions and performs these two substages:\n\nSource analysis and gap classification. The system evaluates the possible imbalances in the action\ncompletion if information came from internal or external sources and identifies possible scenarios to cover.\n\nDynamic action generation. After analyzing the sources and results from the previous actions, the\nsystem makes an LLM call to generate 1-5 extra candidate actions with a specific prioritization. After\ncandidate actions are generated, they go through a deduplication process to make sure the plan didn’t\ncover them already and incorporates the final subset into the priority action plan so they can be considered\nby the scheduler in the following iteration.\n\nF.7 CONTENT PROCESSING AND VECTOR STORE\n\nThe Content Processing system implements a pipeline for unified ingestion of documents in multiple\nformats (PDF, docx, HTML, JSON, plain text formats, etc.) that normalizes and cleans text inside the\ndocuments and websites retrieved during the research. Content is then deduplicated and chunkized, and\nembeddings are computed for each of these chunks.\n\nThe Vector Store implements the storage and retrieval of the content via JSON metadata and NumPy\nembedding metrices, enabling semantic similarity and keyword based searches\n\n23\n\n\fF.8 REPORT GENERATION\n\nMulti-Stage Synthesis Pipeline The Report Generation stage implements a four-stage synthesis\napproach: (1) thematic content clustering via vector searches, (2) source prioritization and deduplication,\n(3) LLM Synthesis with source tracking, and (4) Final report writing and citation resolution.\n\nThe system generates targeted search queries based on the research plan, including specific to ensure that\na predefined set of themes or sections (background, analysis, implementation, trends) are retrieved and\nwritten and prevents redundant analyses.\n\nUnified Citation System The citation system implements deferred resolution to keep consistency of\ncitations from section to section by referencing document IDs in the Vector Store and carrying them over\nfor each piece of synthesized text. A final citation resolution stage will assign the correct numbering to\neach document in the final report.\n\nF.9 DRBA PROMPTS\n\nThe DRBench agent relies on carefully designed prompts to orchestrate enterprise research workflows.\nThese prompts implement the core architectural principles: enterprise context enrichment, structured\nplanning decomposition, priority-based action generation, quantitative synthesis requirements, and adaptive\nresearch capabilities. The following five prompts represent the critical LLM interactions that enable\nsystematic enterprise research with proper source prioritization and citation tracking:\n\n• Enriched Query Generation (Prompt 1): Transforms basic research questions into enterprise-\ncontextualized queries by incorporating company information, stakeholder personas, and business\ncontext to guide subsequent research activities.\n\n• Research Planning (Prompt 2): Decomposes complex research questions into structured\ninvestigation areas with defined information needs, knowledge sources, and business rationales,\nenabling systematic coverage of the research space.\n\n• Action Generation (Prompt 3): Converts research objectives into prioritized executable\nactions with tool specifications and dependency relationships, emphasizing enterprise source\nprioritization over external sources.\n\n• Adaptive Action Generation (Prompt 4): Analyzes research progress to identify coverage\ngaps and source imbalances, generating complementary actions that enhance research depth\nand cross-validate critical findings.\n\n• Report Synthesis (Prompt 5): Orchestrates quantitative-first content synthesis with strict citation\nrequirements, ensuring numerical data leads analytical paragraphs and all claims are properly\nattributed to source documents.\n\nW Query Generation\n\nResearch Question: {dr question}\n\nCompany Context: {company name} is {company desc}.\n\nPersona Context: This analysis is requested by {name}, {role} in\n{department}.\n\nTheir responsibilities include: {responsibilities}.\n\nPrompt 1: Query Enrichment with Enterprise Context.\n\nG QUALITATIVE RESULTS\n\nShown below are some illustrative examples of how metrics are computed for different scenarios on a\ngiven test task.\n\n24\n\n\fW Research Planning (Complex Mode)\n\nDesign a comprehensive enterprise research strategy for:\n\"{question}\"\n\n{tools section}\n\nAs a senior enterprise researcher with deep business\nintelligence expertise, create a thorough investigation\nplan that combines rigorous research methodology with\nstrategic business analysis. Your goal is to provide insights\nthat drive informed decision-making in complex enterprise\nenvironments.\n\nEnterprise Research Design Principles:\n\n- Leverage proprietary internal data as competitive advantage while\nensuring external market context\n- Design investigations that directly inform strategic decisions and\nbusiness outcomes\n- Prioritize research areas that maximize ROI and strategic value to the\norganization\n- Balance comprehensive analysis with focused insights relevant to\nenterprise objectives.\n\nGenerate a JSON object with strategic research investigation\nareas:\n\n{output structure}\n\n.\n\nPrompt 2: Enterprise Research Planning with Investigation Areas. See example of the output structure\nin Listing 2\n\nG.1\n\nINSIGHTS RECALL\n\nWe showcase the insights recall metric using Task DR0002, whose DR Question is “How can\npersonalization drive sales in the retail industry and what strategies can be used for Lee’s Market in action?”\n(also shown in Table 7), which evaluates sales challenges and competitive landscape analysis.\n\nTable 10 shows the overall performance comparison between using Llama 3.1 405B and GPT-5 in\nour agent. Using GPT-5 in our agent results in increasing the insights recall score from 0.14 to 0.43,\nsuccessfully answering 3 out of 7 questions compared to Llama’s 1 out of 7.\n\nTable 10: Insights Recall Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We\nsummarize the number of questions answered successfully and unsuccessfully as well as the overall\ninsights recall score for the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nInsights Recall Score\n\nQuestions Answered\nSuccessfully\n\nQuestions Failed\n\n0.14\n\n1/7\n\n6/7\n\n0.43\n\n3/7\n\n4/7\n\n+0.29\n\n+2\n\n-2\n\nThe question-by-question breakdown in Table 11 reveals the specific questions where each approach\nsucceeded or failed. Both models successfully identified the insight related to online customer engagement,\nbut only GPT-5 was able to identify the number of loyalty program members and the customer data\ncollection rate. Neither model successfully answered the remaining 4 questions, indicating these insights\nmay not have been readily available in the source materials or that agents struggled to find the right insights.\n\n25\n\n\fW Action Generation\n\nGenerate specific executable actions for this research investigation\narea with SOURCE PRIORITIZATION:\n\nResearch Focus: {research focus}\n\nInformation Needs: {information needs}\n\nKnowledge Sources: {knowledge sources}\n\nResearch Approach: {research approach}\n\nAvailable Tools: {available tool names}\n\nTool Selection Guidelines: {tool guidelines}\n\nReturn JSON array of actions with:\n- \"type\": Action type (web search, enterprise api, url fetch, analyzer,\nlocal search)\n- \"description\": Clear description of what this action will accomplish\n- \"parameters\": Tool-specific parameters including query, search type,\netc.\n- \"priority\": Float 0.0-1.0 (enterprise sources: 0.7-1.0, external:\n0.4-0.7)\n- \"expected output\": What information this action should provide\n- \"preferred tool\": Specific tool class name to use\n\nPrompt 3: Priority-Based Action Generation with Source Awareness\n\nTable 11: Question-by-Question Insights Recall Analysis (Task DR0002). We breakdown the results\nquestion by question for the given task, highlighting specifically which question is answered correctly\nor incorrectly for each model.\n\nQuestion\n\nLlama 3.1 405B GPT-5\n\nOnline Customer Engagement with Personalized Recommendations\n\nNumber of Loyalty Program Members\n\nCustomer Data Collection Rate\n\nOnline Sales Growth\n\nEffectiveness of Personalized Marketing\n\nPersonalized Promotions vs Mass Promotions\n\nRetail Media Growth\n\nTotal Insights Recall Score\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0\n\n1.0\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n∆\n\n0.0\n\n+1.0\n\n+1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0/7.0\n\n3.0/7.0\n\n+2.0\n\nIn Table 12 we extend Table 4 and show all the groundtruth insights as well as each of the predicted\ninsights from using both Llama 3.1 405B and GPT-5. As before, we highlight in bold where the model\nwas able to accurately find details relevant to the expected insight, and show all the corresponding scores\nas given in Table 11.\n\nG.2 FACTUALITY\n\nThe factuality metric evaluation uses the same Task DR0002 to assess the accuracy and reliability of\ngenerated content. Table 13 presents the factuality performance comparison, showing that while using\nLlama 3.1 405B achieved 0.41 factuality (7 factual claims out of 17 total claims), where as using GPT-5\nreached 0.65 factuality (13 factual claim out of 20 total claims). This represents a significant improvement\n\n26\n\n\fW Adaptive Action Generation\n\nBased on the research progress so far, suggest new actions with\nINTELLIGENT SOURCE COMPLEMENTARITY:\n\nOriginal Research Query: {research query}\n\nCompleted Actions Summary: {completed actions summary}\n\nRecent Findings Analysis: {findings json}\n\nSource Composition Analysis: {internal findings}\n\nAvailable Tools: {available tool names}\n\nGenerate 1-5 new actions that:\n1. **Address gaps** in current research coverage\n2. **Balance source types** - if findings are heavily external,\nprioritize internal sources and vice versa\n3. **Build on discoveries** - leverage new information to explore\nrelated areas\n4. **Enhance depth** - dive deeper into promising\nfindings\n5. **Cross-validate** - verify critical findings through alternative\nsources\n\nEach action should have:\n- Strategic rationale for why this action is needed\nnow\n- Clear connection to research gaps or promising\nleads\n- Appropriate priority based on strategic value and source\nbalance\n- Specific parameters that build on existing knowledge\n\nPrompt 4: Gap-Driven Adaptive Action Generation\n\nin content reliability. This also highlights that GPT-5 is much better prepared to make accurate claims\nthat are sustained by evidence.\n\nTable 14 provides a detailed breakdown of factual versus unfactual claims. The agent using GPT-5\ngenerated 6 additional factual claims while producing 3 fewer unfactual claims, resulting in a net\nimprovement in accuracy percentage. This demonstrates that GPT-5 may generate a higher proportion\nof factual information than Llama 3.1 405B.\n\nThe impact of these factuality improvements of using GPT-5 over Llama 3.1 405B is summarized in\nTable 15. The 24.0 percentage point improvement in factuality represents enhanced content quality and\nresearch reliability. The increase in 3 total claims shows that GPT-5 can generate more content overall.\n\nH EXPERIMENTAL SETTINGS\n\nAll experiments were conducted on a cluster of 8 NVIDIA A100 GPUs (80GB each). For file generation\nand task construction, we primarily used the Llama-3.1-405B model, with decoding performed\nusing nucleus sampling at a temperature of 0.7 unless otherwise specified. For larger-scale evaluations of\nthe DRBench Agent, we also used closed-source models such as GPT-4o and GPT-5, alongside DeepSeek\nmodels, to enable comparison across open- and closed-source backbones.\n\nTo ensure reproducibility, the DRBench environment was deployed as a self-contained Docker container\nwith all supporting applications (Nextcloud, Mattermost, and Filebrowser) pre-configured. Each task was\n\n27\n\n\fTable 12: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have interacted\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n\n45% of Lee’s Market online customers\nengage with personalized product recom-\nmendations, resulting in a 25% increase in\naverage order value.\n(Score = 1.0)\n\n45% of online customers engaged with\npersonalized product recommendations,\nand among those engagers average order\nvalue increased by 25%.\n(Score = 1.0)\n\nAs of Q3 2024, Lee’s Market has 1.2 million\nloyalty program members.\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\nLee’s Market online sales grew 12% in Q2\n2024 compared to Q2 2023.\n\nRetailers excelling in personalized marketing\nare growing revenues about 10 percentage\npoints faster than their peers, according to\nBCG. By effectively using first-party customer\ndata, these leaders could unlock an estimated\n$570 billion in additional sales, highlighting\nthe importance of data-driven sales strategies\nfor growth.\n\nPersonalized promotions can deliver returns\nthree times higher than mass promotions,\nyet many retailers allocate under 5% of their\npromo budgets to personalization. One major\nchain increased its personalized promo spend\nfrom 1% to 10% by establishing a ”customer\ninvestment council,” resulting in $250 million\nin incremental sales.\n\nRetail media networks are expanding rapidly,\nwith retail media growing at approximately\n25% annually, offering retailers a profitable\nrevenue stream to reinvest in technology, data,\nand personnel. By integrating loyalty data,\nretailers like Sephora, which links 95% of\ntransactions to loyalty accounts, enhance preci-\nsion in product recommendations and provide\na seamless omnichannel experience, boosting\nconversion rates and customer lifetime value.\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nFrom Q2 2024 to Q3 2024, loyalty members\nincreased from 1,050,000 to 1,200,000\n(+150,000; +14.29%), average spend per\nmember rose from 24 to 25 (+4.17%), and\ntotal member spend increased from 25,200,000\nto 30,000,000 (+19.05%).\n(Score = 1.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n45% of Lee’s Market online customers engage\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n(Score = 0.0)\n\nA naive blended online AOV upside of\napproximately 11.25% is derived from 45%\nengagement multiplied by a 25% AOV lift\namong engagers.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nGrocers running data-driven loyalty cam-\npaigns have realized an average 3.8%\nlike-for-like sales uplift.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nsources\n\nindicate POS-enabled\nExternal\npersonalization can lift revenue 5%-15% and\nadvocate personalized e-receipts with relevant\noffers and coupons to extend post-purchase\nengagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 0.0)\n\nTable 13: Factuality Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We show\nthe number of factual and unfactual claims made by each model, as well as the overall factuality score\nfor the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nFactuality Score\n\nFactual Claims\n\nUnfactual Claims\n\n0.41\n\n7\n\n10\n\n0.65\n\n13\n\n7\n\n+0.24\n\n+6 claims\n\n-3 claims\n\n28\n\n\fW Report Synthesis\n\nAs an expert research analyst, synthesize the following content into\na coherent, insightful, and well-supported analysis for the theme:\n\"{theme}\" directly related to the overarching research question:\n\"{original question}\"\n\nSource Priority Guidelines:\n\n1. **\"internal\"**: Highest priority (internal company\ndocuments, proprietary files, confidential reports,\nenterprise chat messages, local documents, CRM data,\ninternal APIs, project management tools). Insights from\nthese sources should form the primary foundation of the\nanalysis.\n2. **\"external\"**: Medium priority (public web sources,\nacademic papers, industry reports, news articles). Use these\nto provide broader context, external validation, or contrasting\nperspectives.\n\n**Synthesis Requirements:**\n* **QUANTITATIVE PRIORITY:** Lead with numerical data, calculations, and\naggregations\n* Extract ALL percentages, costs, metrics, and performance\ndata\n* Perform mathematical operations: aggregate percentages, calculate\nincreases, sum totals\n* Example: \"Finance customers (35%) combined with healthcare\n(40%) represent 75% of regulated industry concerns\n[DOC:doc 1][DOC:doc 2]\"\n\n* **FACT VERIFICATION (CRITICAL):**\n* ONLY state what documents explicitly contain - no inference or\nextrapolation\n* Use exact quotes for key numerical claims: \"As stated in the\ndocument: ’[exact quote]’ [DOC:doc id]\"\n\n* **Citation Usage (Critical):**\n* **Format:** Reference sources by their document ID: \"Internal review\nshows 15% increase [DOC:doc 079c2e0f 1752503636]\"\n* **NEVER HALLUCINATE CITATIONS:** Only use provided doc id\nvalues\n* **Cite every numerical claim and calculation with source\ndocuments**\n\nGenerate 2-4 paragraphs of synthesized analysis with proper inline\ncitations.\n\nPrompt 5: Report Section Synthesis with Citation Requirements\n\nexecuted by instantiating a fresh container to avoid state leakage across runs. We capped the number of\nagent iterations according to the settings described in Section 5, with each iteration limited by a fixed\ncomputational budget.\n\nFor model outputs, we standardized all prompts and evaluation pipelines across backbones, using identical\nresearch questions, company contexts, and injected insight sets. To avoid stochastic variability, we repeated\ngeneration three times per task and reported averaged scores.\n\nFinally, all supporting scripts, environment configurations, and evaluation code are fully containerized,\nenabling consistent replication of our reported results across hardware setups.\n\n29\n\n\fTable 14: Factuality Content Analysis (Task DR0002). We show the number of factual and unfactual\nclaims made by each model, highlighting the factuality accuracy of each model for the given task.\n\nAgent\n\nFactual Unfactual Accuracy\n\nLlama 3.1 405B\n\n7 claims\n\n10 claims\n\nGPT-5\n\nChange\n\n13 claim\n\n7 claims\n\n+6\n\n-3\n\n+24.0%\n\n41.0%\n\n65.0%\n\nTable 15: Factuality Summary (Task DR0002). We summarize the factuality result improvements made\nby using GPT-5 over Llama 3.1 405B.\n\nImpact Category\n\nValue\n\nFactuality Improvement\n\n+24.0 percentage points\n\nClaims Added\n\n3 total claims\n\nAccuracy Enhancement\n\nFrom 41.0% to 65.0%\n\nContent Quality\n\nMore grounded information\n\nTask Domain\n\nTask Industry\n\nSales\n\nRetail\n\nI DATA SYNTHESIS AND DRBA COST DETAILS\n\nTo generate the DRBench tasks, we combined external insight extraction with internal file synthesis. Each\ntask included an average of 10 supporting files spanning heterogeneous formats (PDF, DOCX, PPTX,\nXLSX, and JSONL), with each file containing roughly 5 paragraphs of content. Files were designed\nto embed injected insights while mixing in distractor material, ensuring realistic enterprise complexity\nwithout exceeding practical runtime or storage budgets.\n\nData synthesis was primarily powered by GPT-4o. During task construction, GPT-4o was responsible for (1)\nextracting structured insights from public web sources, (2) adapting these insights into enterprise-grounded\ninterpretations, (3) generating persona-specific deep research questions, and (4) producing file-level content\nwith a balanced mix of insights and distractors. For evaluation, the DRBench Agent (DRBA) used GPT-4o\nas its backbone, with each task typically requiring 15 iterations and approximately 120–150 model calls.\n\nIn terms of cost, GPT-4o-based synthesis of a single task (10 files, 5 paragraphs each, plus metadata)\nconsumed about 30k–40k tokens, while DRBA execution required an additional 50k–70k tokens per\ntask. At current GPT-4o API pricing ($5 per million input tokens and $15 per million output tokens), this\ncorresponds to a per-task cost of approximately $1.5–$3.5 depending on the mix of input/output tokens\nand the iteration budget. This makes large-scale benchmarking feasible at moderate cost, while still being\nsignificantly cheaper than manual authoring or annotation.\n\nWe also note that smaller open-source models such as Llama-3.1-8B-Instruct perform well for file\ngeneration. Unlike GPT-4o, which requires API usage, Llama-3.1-8B can be hosted locally and runs\nefficiently on a single NVIDIA A100 40GB GPU. This provides a cost-effective alternative for generating\nlarge numbers of supporting documents, especially when full closed-source quality is not required.\n\nJ WEB AGENTS FOR DEEP RESEARCH TASKS\n\nSince each of the environments can be access directly through a web user interface (UI), we also exper-\nimented with an agent that can directly interact with the webpages through common browser actions like\nclick, input and scroll, which are executed through playwright3. We implement our web agent\nusing the AgentLab and BrowserGym frameworks (Chezelles et al., 2025) with a GPT-4.14 backbone. Our\nagent is implemented from AgentLab’s GenericAgent, which achieves respectable performance when used\n\n3https://playwright.dev\n4https://openai.com/index/gpt-4-1/\n\n30\n\n\fwith GPT-4o5 as a backbone; it completes 45.5% of the tasks in WorkArena (Drouin et al., 2024), 31.4%\nin WebArena (Zhou et al., 2024) and achieves a step-level reward of 13.7% on WebLINX (L`u et al., 2024).\n\nHyperparameters and Prompt We present the hyperparameters for the agent in tables 16 and 17, which\nare in majority set to the default hyperparameters, except for the maximum number of input tokens (bound\nto a reasonable maximum length) and a higher maximum number of steps (to allow the agent to perform\nmore actions required to write the report). We further update the agent’s action space on the last step to\nonly allow it to reply to the user with a report, ensuring that each trajectory terminates with a report. To\nensure that the agent is aware of the tools it can use, we modify the default system prompt (see prompt 6).\nAdditionally, each task intent is provided alongside information about the user and company (see prompt 7).\n\nResults We find that the GPT-4.1-powered web agent achieves an insights recall and factuality of\n1.11% and 6.67% respectively and a report quality score of 33.07%. Although the high report quality\nindicates that the agent can properly formulate a report, the insights quality is severely limited, with none\nof the claims being backed by useful sources. For example, a DRBench Agent powered by GPT-5 may\nanswer the question What is Lee’s Market’s current food waste reduction rate as of Q2 2024? with An\n8% reduction in food waste in Q2 2024 saved Lee’s Market $1.2 million, indicating that better inventory\ncontrol can yield both safety and financial benefits., which achieves a score of 1.0 for the question. On\nthe other hand, a GPT-4.1-powered agent will provide an unsatisfactory answer, thus achieving an insights\nrecall of 0.0. The most likely cause of this poor performance is the model’s limited capability to properly\ninteract with web interfaces when encountering unfamiliar tools. For instance, the agent may be unfamiliar\nwith the VNC and file browser applications, making it harder for it to correctly select the file it needs\nto use. Moreover, whenever the agent ends up performing an ineffective action (e.g. click on an element\nthat does not trigger any change to the page), it tends to persist by reiterating the same action (see Table\n18), or the same sequence of ineffective actions, despite not achieving anything in the previous steps. As a\nresult, despite a large number of steps, most of the agent’s actions are not helpful towards solving the task.\n\nTable 16: Web Agents Boolean Hyperparameters\n\nValue Flags\n\nTrue\n\nFalse\n\nvision support, use ax tree,\nuse tabs, use focused element, use error logs,\nuse history, use action history, use screenshot,\nuse som, extract visible tag, extract clickable tag,\nuse thinking, use concrete example, use abstract example,\nuse hints, be cautious, add missparsed messages\nuse html, use past error logs, use think history,\nuse diff, filter visible elements only,\nfilter with bid only, filter som only, multiaction,\nstrict, long description, individual examples,\nuse plan, use criticise, use memory, enable chat\n\nK STANDARD ERROR\n\nRestricting to the MinEval subset, we average the results on each task across 3 different runs in Table\n19. We give both the means and standard errors for the insight recall, factuality, distractor avoidance, and\nreport quality.\n\nL COMPLEX MODEL ABLATION RESULTS\n\nExtending our discussion in Section 5.3 to more GPT and Llama models in Table 20, we see that the\nsmaller GPT-5-mini model lags behind but still outperforms earlier closed-source backbones such as\nGPT-4o and GPT-4o-mini, particularly in terms of harmonic mean. In addition, smaller variants of Llama\n\n5https://openai.com/index/hello-gpt-4o/\n\n31\n\n\fW Web Agents System Prompt\n\nYou are an agent trying to solve a web task based on the content of the\npage and user instructions. You can interact with the page and explore,\nand send messages to the user. Each time you submit an action it will\nbe sent to the browser and you will receive a new page.\nYou will be solving tasks that involve Deep Research, by\nnavigating websites and web apps with information useful\nsolving the task. You will need to gather insights from\ndata contained in the services provided and the internet\nto complete your report. You have a maximum of 50 steps\nto complete the task. Before the end, you must use the\naction {send msg to user}, which should contain a final Deep\nResearch report detailing everything the user needs to\nknow.\n\nYou must sustain your claims in files, chats, or emails\nfrom the enterprise environment or in websites you\nsearched. You must provide a citation (or an inline\ncitation, that works too) with the source of those claims\n(e.g. << add citation examples from the documentation\n>>). Do not make up citations if you haven’t retrieved its\ncontent.\n\nHere are some expected agent behavior:\n\n**Enterprise Environment Interaction:**\n- Access Nextcloud files, Mattermost chats, emails, VNC desktop,\netc.\n- Extract relevant information from multiple sources\n- Navigate complex enterprise data landscapes\n\n**Report Requirements:**\n- Synthesize findings into comprehensive research\nreport\n- Include proper citations for all claims (flexible format -\nauto-normalized)\n- Draw meaningful insights and conclusions\n- Ground all statements in available evidence\n\n**Citation Format (Flexible - Auto-Normalized):**\n- Files: ‘quarterly report.pdf‘, ‘shared/budget-analysis.xlsx‘,\n‘Analysis document (reports/analysis.docx)‘\n- URLs: Direct links or ‘[Article Title](https://example.com)‘\n- Emails: ‘Email from alice@company.com on Jan 20,\n2025‘\n- Chat: ‘Mattermost message from john.doe in Compliance team, General\nchannel‘\n\nIf you need navigate the internet (outside of the designated websites),\nyou can use the browser inside noVNC Desktop.\n\nPrompt 6: Extended instructions given to the Deep Research web agent.\n\n32\n\n\fTable 17: Web Agents Non-Boolean Hyperparameters\n\nParameter\n\nValue\n\nchat model.model name\nchat model.max total tokens\nchat model.max input tokens\nchat model.max new tokens\nchat model.temperature\naction.action set.subsets\naction.action set.retry with force true\nflags.max prompt tokens\nflags.max trunc itr\nenv.max steps\n\ngpt-4.1\n32768\n28672\n4096\n0\nwebarena\n\n28672\n20\n50\n\nW Web Agents Task Intent Prompt\n\nHow can Lee’s Market leverage FSMA 204 regulations to enhance food\nsafety and customer trust?\n\nHere is some information about the user:\nID: MNG0003\nFirst name: John\nLast name: Doe\n...\nJustification: As a regulatory affairs manager, John\n...\n\nHere is some information about the company:\nName: Lee’s Market\nAnnual revenue: $500M - $600M\n...\nTarget markets: Asian communities in the U.S. and ...\n\nPrompt 7: Web Agents Task Intent Prompt\n\nTable 18: Web Agents tends to get stuck on cycles of actions, and are unable to backtrack or to restart\nwith a different application.\n\n9. I have been repeatedly clicking\non the “shared” folder (bid ‘194’)\n. . . To proceed with the research\non personalization strategies for\nLee‘s Market, I should open the\n“shared” folder to look for. . .\nclick(‘194’)\n\n10. I have clicked on the “shared”\nfolder multiple times . . .\nI will\ntry clicking on the “shared” folder\nagain to see if it opens and displays\nits contents.\n\nclick(‘190’)\n\n11. I have been repeatedly clicking\non the “shared” folder (bid ‘194’)\nTo proceed, I should try\n. . .\nclicking on the “shared” folder\nagain to attempt to open it and\nview any files . . .\n\nclick(‘194’)\n\n12. I have navigated to the “shared”\nfolder, but I have not yet opened\nit . . . The next logical step is to\nopen the “shared” folder to look\nfor any relevant documents . . .\n\nclick(‘194’)\n\ndegrade metric results further. This further substantiates the claim that larger and more advanced models\ntend to offer a better balance between recall, factuality, and overall report quality.\n\nM QUANTITATIVE RESULTS PER TASK\n\nWe show a detailed breakdown of the insights recall in Table 21, factuality in Table 22, distractor avoidance\nin Table 23 and report quality in Table 24 on the MinEval subset for a variety of models.\n\n33\n\n\fTable 19: DRBA performance with different planning configurations on MinEval. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. Scores are reported for insight recall, factuality, distractor\navoidance, report quality, and the overall harmonic mean.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n.188 ± .038\n.137 ± .04\n.154 ± .03\n.197 ± .03\n.142 ± .04\n.188 ± .05\n\n.665 ± .09\n.622 ± .11\n.705 ± .05\n.604 ± .08\n.504 ± .10\n.691 ± .03\n\n.981 ± .01\n1.00 ± .00\n1.00 ± .00\n.995 ± .39\n.990 ± .39\n1.00 ± .00\n\n.912 ± .004\n.901 ± .006\n.917 ± .004\n.928 ± .005\n.906 ± .007\n.923 ± .006\n\n.448\n.365\n.400\n.454\n.359\n.453\n\nTable 20: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Scores are reported for insight recall, factuality, distractor avoidance, report quality,\nand harmonic mean. Note that higher numbers corresponds to better scores, and the best result on each\nmetric is bolded.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nGPT-5-mini\nGPT-5-mini\nGPT-5-mini\nGPT-4o\nGPT-4o\nGPT-4o\nGPT-4o-mini\nGPT-4o-mini\nGPT-4o-mini\nGPT-OSS-120B\nGPT-OSS-120B\nGPT-OSS-120B\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-70b-Instruct\nLlama-3.1-70b-Instruct\nLlama-3.1-70b-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n25.68\n28.37\n26.57\n17.53\n20.37\n17.31\n13.75\n13.67\n13.08\n22.40\n17.42\n18.31\n17.37\n16.97\n20.16\n18.54\n16.28\n17.02\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n58.76\n58.96\n51.07\n65.43\n62.35\n60.84\n46.68\n55.59\n48.95\n29.24\n27.48\n38.92\n78.91\n79.27\n69.75\n64.64\n69.43\n52.82\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n96.51\n95.81\n94.48\n99.05\n98.57\n98.33\n99.05\n97.14\n97.14\n97.14\n97.14\n98.1\n100.00\n98.10\n97.90\n96.70\n97.62\n98.10\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n84.48\n85.74\n86.28\n92.58\n93.12\n91.62\n84.72\n85.86\n86.04\n84.24\n84.36\n85.44\n90.48\n92.34\n91.26\n85.32\n84.96\n86.16\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n60.21\n63.73\n58.89\n48.47\n53.06\n47.35\n38.33\n39.39\n37.28\n44.82\n38.38\n44.14\n49.78\n48.87\n53.86\n50.08\n46.41\n45.46\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\nN EFFECT OF NUMBER OF ITERATIONS\n\nWe next analyze the effect of varying the number of research loop iterations when using DRBA with\nGPT-5 as the backbone language model. Results for both the baseline configuration without explicit\nplanning and the complex planning setup are shown in Table 25. Overall, increasing the iteration budget\ndoes not guarantee consistent improvements. With no planning, performance initially drops when the\nagent executes more iterations, as additional exploration often introduces noise and distracts from key\ninsights. However, with a larger budget the agent partially recovers, suggesting that a small number of\nadditional iterations can help refine factual grounding, while excessive exploration reduces focus.\n\nFor the complex planning setting, higher iterations improve certain metrics such as factuality, but this\ncomes at the cost of lower insight recall and reduced overall balance. This indicates that while more steps\nallow the agent to verify citations more carefully, they can also lead to fragmented reasoning and overfitting\n\n34\n\n\fFigure 8: Total average Insight Recall scores per model and insight source type computed on all the\nresults available for each model running in Complex Research Plan mode. Insights embedded in enterprise\nsources are more easily retrieved by DRBA in all the models.\n\nto peripheral evidence. The best overall performance emerges at moderate iteration counts, highlighting\nthe importance of carefully tuning the iteration budget rather than simply scaling up the number of steps.\n\nO DATA GENERATION PROMPTS\n\nO.1 COMPANY AND PERSONA DATA GENERATION\n\nIn this section we give the prompts used for company generation 8 and persona generation 9.\n\nW Company Generation Prompt\n\nGenerate a realistic company structure for {company name} in the\n{industry} industry.\n\nCompany size: {size} ({employee range} employees)\n\nThe company should focus on this domain {domain}\n\nEXTERNAL INSIGHTS: {external insights}\n\nReturn ONLY a valid JSON object with this structure:\n{output structure}\n\nMake it realistic for the {industry} industry.\n\nPrompt 8: Company Generation Prompt Template.\n\n35\n\ngpt-5enterprise_fact0.597external_fact0.0deepseek-chat-v3.1enterprise_fact0.472external_fact0.0gpt-5-minienterprise_fact0.444external_fact0.0qwen-2.5-72b-instructenterprise_fact0.417external_fact0.0llama-3.1-405b-instructenterprise_fact0.347external_fact0.0gpt-oss-120benterprise_fact0.333external_fact0.0gpt-4o-minienterprise_fact0.194external_fact0.0llama-3.1-70b-instructenterprise_fact0.194external_fact0.0gpt-4oenterprise_fact0.182external_fact0.0\fTable 21: Mean and standard error of the insight recall metric for the first five tasks, obtained from three\nruns of on DRBench our agent (DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nW Persona Generation Prompt\n\nDR0001\n.222 ± .056\n.222 ± .056\n.278 ± .056\n.111 ± .056\n.222 ± .056\n.000 ± .000\n.111 ± .056\n.167 ± .000\n.111 ± .056\n.000 ± .000\n.056 ± .056\n.000 ± .000\n.111 ± .111\n.056 ± .056\n.000 ± .000\n.167 ± .000\n.222 ± .056\n.111 ± .056\n.167 ± .000\n.056 ± .056\n.167 ± .000\n.167 ± .000\n.278 ± .056\n.167 ± .000\n.222 ± .056\n.111 ± .056\n.167 ± .096\n\nDR0002\n.429 ± .000\n.381 ± .048\n.381 ± .126\n.286 ± .000\n.286 ± .082\n.381 ± .126\n.238 ± .048\n.333 ± .048\n.238 ± .095\n.095 ± .048\n.190 ± .048\n.190 ± .048\n.143 ± .000\n.143 ± .000\n.190 ± .048\n.143 ± .000\n.190 ± .048\n.238 ± .048\n.381 ± .048\n.286 ± .082\n.238 ± .048\n.286 ± .082\n.238 ± .048\n.095 ± .048\n.238 ± .048\n.333 ± .048\n.143 ± .082\n\nDR0003\n.467 ± .033\n.533 ± .033\n.567 ± .067\n.367 ± .033\n.333 ± .033\n.367 ± .067\n.167 ± .033\n.300 ± .000\n.300 ± .100\n.267 ± .033\n.167 ± .067\n.267 ± .033\n.433 ± .033\n.367 ± .033\n.333 ± .067\n.233 ± .088\n.167 ± .033\n.300 ± .058\n.200 ± .000\n.200 ± .058\n.267 ± .033\n.300 ± .058\n.333 ± .033\n.600 ± .058\n.400 ± .058\n.333 ± .088\n.400 ± .058\n\nDR0004\n.519 ± .098\n.370 ± .098\n.370 ± .037\n.222 ± .064\n.296 ± .074\n.370 ± .098\n.185 ± .074\n.148 ± .037\n.111 ± .000\n.185 ± .037\n.148 ± .074\n.074 ± .037\n.222 ± .000\n.148 ± .098\n.111 ± .064\n.185 ± .074\n.111 ± .064\n.148 ± .098\n.074 ± .037\n.185 ± .098\n.074 ± .074\n.259 ± .037\n.148 ± .074\n.370 ± .037\n.259 ± .037\n.259 ± .037\n.333 ± .000\n\nDR0005\n.281 ± .035\n.386 ± .046\n.386 ± .046\n.298 ± .018\n.281 ± .063\n.211 ± .053\n.175 ± .018\n.070 ± .018\n.105 ± .000\n.140 ± .035\n.123 ± .018\n.123 ± .018\n.211 ± .030\n.158 ± .061\n.281 ± .063\n.140 ± .035\n.158 ± .053\n.211 ± .030\n.105 ± .030\n.088 ± .046\n.105 ± .000\n.246 ± .046\n.281 ± .046\n.281 ± .035\n.158 ± .061\n.123 ± .018\n.298 ± .076\n\nGenerate {persona count} diverse employee personas for {company name} in\nthe {industry} industry.\n\nThe personas should focus on this domain: {domain}\n\nCreate diverse roles across seniority levels: Junior, Mid, Senior,\nExecutive\n\nReturn ONLY a valid JSON array with this exact format:\n{output structure}\n\nMake personas realistic with appropriate responsibilities for their\nroles.\n\nPrompt 9: Persona Generation Prompt.\n\nO.2 QUESTION GENERATION\n\nIn this section we give the prompt used to generate our deep research questions 10.\n\n36\n\n\fTable 22: Mean and standard error of the factuality metric for the first five tasks, obtained from our agent\n(DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nSimple\nGPT-4o-mini\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.761 ± .072\n.714 ± .050\n.730 ± .060\n.585 ± .045\n.647 ± .120\n.309 ± .126\n.792 ± .150\n.485 ± .262\n.475 ± .166\n.828 ± .043\n.611 ± .056\n.557 ± .030\n.144 ± .099\n.074 ± .074\n.368 ± .061\n.852 ± .087\n.802 ± .125\n.789 ± .053\n.618 ± .109\n.608 ± .173\n.588 ± .082\n.860 ± .014\n.696 ± .041\n.581 ± .042\n.674 ± .077\n.806 ± .049\n.626 ± .114\n\nDR0002\n.504 ± .075\n.384 ± .076\n.291 ± .094\n.297 ± .119\n.299 ± .056\n.381 ± .161\n.490 ± .110\n.512 ± .131\n.653 ± .097\n.265 ± .133\n.429 ± .092\n.324 ± .169\n.150 ± .035\n.128 ± .072\n.178 ± .078\n.726 ± .158\n.638 ± .202\n.392 ± .154\n.431 ± .160\n.681 ± .069\n.286 ± .108\n.518 ± .085\n.531 ± .086\n.657 ± .024\n.493 ± .109\n.540 ± .174\n.396 ± .056\n\nDR0003\n.866 ± .007\n.848 ± .034\n.782 ± .012\n.705 ± .039\n.624 ± .041\n.699 ± .047\n.827 ± .056\n.813 ± .041\n.704 ± .037\n.800 ± .000\n.622 ± .062\n.580 ± .075\n.386 ± .040\n.410 ± .090\n.564 ± .064\n.803 ± .028\n.800 ± .074\n.792 ± .055\n.684 ± .104\n.800 ± .069\n.686 ± .011\n.818 ± .041\n.922 ± .056\n.838 ± .050\n.866 ± .002\n.741 ± .074\n.723 ± .053\n\nDR0004\n.833 ± .019\n.812 ± .070\n.782 ± .064\n.704 ± .067\n.694 ± .028\n.692 ± .111\n.570 ± .058\n.693 ± .139\n.542 ± .110\n.690 ± .128\n.481 ± .209\n.642 ± .119\n.337 ± .117\n.311 ± .155\n.400 ± .076\n.820 ± .066\n.892 ± .035\n.771 ± .073\n.812 ± .021\n.826 ± .067\n.522 ± .270\n.679 ± .095\n.769 ± .035\n.774 ± .053\n.741 ± .060\n.724 ± .101\n.587 ± .139\n\nDR0005\n.762 ± .077\n.846 ± .029\n.674 ± .121\n.647 ± .098\n.683 ± .020\n.472 ± .114\n.593 ± .204\n.614 ± .121\n.406 ± .020\n.459 ± .235\n.191 ± .046\n.344 ± .144\n.445 ± .051\n.451 ± .080\n.435 ± .190\n.745 ± .022\n.832 ± .083\n.745 ± .100\n.687 ± .073\n.557 ± .143\n.559 ± .240\n.757 ± .057\n.754 ± .082\n.662 ± .098\n.696 ± .060\n.550 ± .148\n.586 ± .055\n\nO.3 PUBLIC SOURCE AND INSIGHT COLLECTION\n\nIn this section we give the prompt used to generate external insights 11. The URLs used for external\ninsight extraction and deep research question creation can be found in Table 26.\n\nO.4\n\nINTERNAL INSIGHT GENERATION\n\nIn this section we give the prompts to generate both internal insights 12 and internal distractors 13.\n\nO.5 FILE GENERATION\n\nIn this section we give the prompts used for generating each of the file types used in our tasks, which\nwe list as follows:\n\n• PDF: Prompts 18, 19, and 20\n\n• Excel: Prompts 21, 22, and 23\n\n• Powerpoint: Prompts 24, 25, and 26\n\n• Email: Prompts 27, 28, and 29\n\n• Chat: Prompts 30, 31, 29, and 33\n\n37\n\n\fTable 23: Mean and standard error of the distractor avoidance metric for the first five tasks, obtained from\nthree runs of on DRBench our agent (DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.857 ± .000\n.857 ± .000\n.905 ± .048\n.905 ± .048\n.857 ± .000\n.857 ± .000\n.952 ± .048\n.952 ± .048\n.952 ± .048\n.952 ± .048\n.857 ± .000\n.857 ± .000\n.857 ± .000\n.857 ± .000\n.905 ± .048\n1.00 ± .000\n.905 ± .048\n.952 ± .048\n.905 ± .048\n.905 ± .048\n.905 ± .048\n.905 ± .048\n.857 ± .000\n.905 ± .048\n1.00 ± .000\n.952 ± .048\n.952 ± .048\n\nDR0002\n.900 ± .058\n1.00 ± .000\n.900 ± .058\n.967 ± .033\n.933 ± .033\n.867 ± .133\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.967 ± .033\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.967 ± .033\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n\nDR0003\n1.00 ± .000\n1.00 ± .000\n.933 ± .000\n.978 ± .022\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.978 ± .022\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.978 ± .022\n\nDR0004\n1.00 ± .000\n1.00 ± .000\n.905 ± .024\n.976 ± .024\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.964 ± .036\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.952 ± .048\n.976 ± .024\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.929 ± .000\n.905 ± .024\n.952 ± .024\n.952 ± .048\n\nDR0005\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n\nP EVALUATION PROMPTS\n\nIn this section we give the prompts for decomposing reports into atomic insights 14, computing insight\nrecall 15, computing factuality 16, and computing report quality 17. These prompts are discussed in detail\nin Section 5.1.\n\nQ HUMAN PREFERENCE EVALUATION\n\nWe calculate a human score for model a task t as:\n\nSa,t = 1\nn\n\n(cid:80)n\n\ni=1sa,i, where sa,i =\n\n(cid:26)1,\n\nif human choice is a or ”both good”\n\n0, otherwise\n\n, where n is the number of gold insights in task t, sa,i is the human score of model a on insight i. Figure9\nshows that the insight recall metric is on par with human decision.\n\n38\n\n\fTable 24: Mean and standard error of the report quality metric for the first five tasks, obtained from three\nruns of DRBench with 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.936 ± .008\n.942 ± .000\n.948 ± .007\n.892 ± .002\n.901 ± .009\n.896 ± .001\n.927 ± .000\n.934 ± .008\n.929 ± .009\n.886 ± .004\n.893 ± .002\n.889 ± .003\n.872 ± .007\n.878 ± .006\n.874 ± .007\n.914 ± .000\n.921 ± .008\n.917 ± .009\n.889 ± .003\n.895 ± .002\n.891 ± .003\n.884 ± .004\n.890 ± .003\n.886 ± .004\n.901 ± .001\n.908 ± .000\n.904 ± .001\n\nDR0002\n.918 ± .004\n.927 ± .008\n.921 ± .001\n.884 ± .007\n.889 ± .002\n.882 ± .006\n.911 ± .003\n.919 ± .000\n.914 ± .002\n.874 ± .008\n.881 ± .005\n.877 ± .006\n.861 ± .001\n.867 ± .009\n.863 ± .000\n.903 ± .003\n.910 ± .001\n.906 ± .002\n.877 ± .007\n.883 ± .005\n.879 ± .006\n.872 ± .008\n.878 ± .006\n.874 ± .007\n.889 ± .005\n.896 ± .003\n.892 ± .004\n\nDR0003\n.924 ± .005\n.936 ± .006\n.940 ± .008\n.879 ± .004\n.887 ± .001\n.884 ± .003\n.903 ± .001\n.911 ± .009\n.905 ± .000\n.861 ± .007\n.867 ± .004\n.864 ± .005\n.849 ± .009\n.854 ± .008\n.851 ± .008\n.897 ± .001\n.904 ± .000\n.899 ± .001\n.869 ± .005\n.874 ± .004\n.871 ± .005\n.864 ± .006\n.870 ± .005\n.866 ± .006\n.881 ± .002\n.888 ± .001\n.884 ± .002\n\nDR0004\n.909 ± .001\n.915 ± .002\n.922 ± .009\n.891 ± .000\n.895 ± .008\n.889 ± .009\n.918 ± .009\n.923 ± .001\n.920 ± .008\n.872 ± .002\n.878 ± .001\n.875 ± .000\n.858 ± .006\n.863 ± .004\n.860 ± .005\n.909 ± .008\n.915 ± .009\n.911 ± .008\n.881 ± .001\n.886 ± .000\n.883 ± .001\n.876 ± .002\n.881 ± .001\n.878 ± .002\n.893 ± .009\n.899 ± .008\n.895 ± .009\n\nDR0005\n.927 ± .009\n.933 ± .007\n.929 ± .008\n.886 ± .005\n.892 ± .003\n.890 ± .001\n.909 ± .002\n.916 ± .000\n.913 ± .009\n.879 ± .006\n.884 ± .003\n.882 ± .004\n.866 ± .008\n.872 ± .007\n.869 ± .006\n.902 ± .002\n.908 ± .000\n.905 ± .001\n.873 ± .004\n.878 ± .003\n.875 ± .004\n.869 ± .005\n.874 ± .004\n.871 ± .005\n.885 ± .003\n.891 ± .002\n.888 ± .003\n\nTable 25: Effect of the number of research loop iterations on the performance of DRBA with GPT-5 as\nthe backbone model, on MinEval.\n\nPlanning Method\nNone\nNone\nNone\nComplex\nComplex\nComplex\n\n# Iterations\n15\n30\n50\n15\n30\n50\n\nInsight Recall\n39.45\n28.80\n37.10\n44.44\n31.61\n38.16\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n72.65\n69.03\n78.84\n62.51\n73.94\n66.05\n\n93.14\n98.57\n100.00\n90.95\n94.38\n94.38\n\n94.56\n96.12\n93.48\n93.12\n94.76\n92.64\n\n66.20\n57.34\n66.30\n66.41\n60.32\n63.76\n\n39\n\n\fW Deep Research Question Generation Prompt\n\nGenerate 3 Deep Research (DR) questions for the following business\ncontext:\n\nPersona: {persona name} - {persona role}\nDepartment: {persona department}\nResponsibilities: {persona responsibilities}\nCompany: {company name} ({company industry})\nDomain: {domain}\n\nExternal Insights: {external insights}\n\nGenerate 3 Deep Research questions that:\n1. Are appropriate for the persona’s role and department\n2. Require analysis of the provided internal insights\n3. Consider the external market context ...\n\nEach question should be 1 sentence of 15 words max, in\nplain english, and end with a question mark. Do not\ninclude any preamble or explanation - return only the JSON\narray.\n\nReturn ONLY a valid JSON array with this structure:\n{output structure}\n\nPrompt 10: Deep Research Question Generation Prompt Template. Subquestions are generated to help\nhuman annotators select good DR questions.\n\nW External Insight Extraction Prompt\n\nYou will be given a report (with url, and date). Based\non the report, generate 3 external insights that summarize\nimportant findings, trends, or takeaways from the\nreport.\n\nOutput Format\n{output structure}\n\nUrl: {url}\nIndustry: {industry}\nDomain: {domain}\nCompany Information: {company information}\n\nImportant notes\nFocus only on insights that are external and grounded in the report.\nInsights should be concise, factual, and directly tied to the retail\nindustry context.\n\nPrompt 11: External Insight Extraction Prompt.\n\n40\n\n\fTable 26: Public URLs For Deep Research Task Creation.\n\nIndustry\n\nDomain\n\nReference\nGrocers on FSMA-204 Compliance (GroceryDive) 2\nGrocery Loyalty & Inflation (EagleEye) 2\nGrocery Trends Outlook 2025 (GroceryDive) 2\nRetail IT Optimization (Thirdera) 2\nChatbots & Grocery Interactions (GroceryDoppio) 2\n\nTelehealth Regulations (HealthcareDive) 2\nFuture of Healthcare CRM (WTT Solutions) 2\nFuture of Telehealth (CHG Healthcare) 2\nHealthcare ITSM (Topdesk) 2\nPatient Engagement Tech (TechTarget) 2\n\nKnowledge Mgmt Retail Knowledge Management (Knowmax) 2\n\nSales\nCybersecurity\nPublic Relations Walmart CSR Strategy (SunriseGeek) 2\n\nPersonalization in Action (BCG) 2\nRetail Cybersecurity Threats (VikingCloud) 2\n\nCompliance\nCRM\nMarket Analysis\nITSM\nCSM\n\nCompliance\nCRM\nMarket Analysis\nITSM\nCSM\n\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles Quality Assurance EV Quality Issues (GreenCars) 2\nAutomobiles\nAutomobiles Market Analysis\n\nSales\nMarketing\nCybersecurity\nCompliance\nCRM\nCSM\nSales\nResearch\nCybersecurity\n\nAsset Mgmt\n\nSales for Digital Health (Medium) 2\nMarketing Telehealth Services (MarketingInsider) 2\nHealthcare Cybersecurity 2024 (AHA) 2\nEvolving EV Regulations (WardsAuto) 2\nSalesforce Automotive Cloud (TechTarget) 2\nEV Aftersales Support (EVReport) 2\nTesla vs Dealerships (TheWeek) 2\nAI for EV Optimization (Here.com) 2\nCybersecurity Risks in Cars (HelpNetSecurity) 2\n\nDigital Twins in Autos (RTInsights) 2\nGlobal EV Outlook 2024 (IEA) 2\n\nKnowledge Mgmt Knowledge Mgmt in Healthcare (C8Health) 2\n\nFigure 9: Comparison of Human Scores and Insight Recall Scores. As can be seen the human evaluation\nresults are aligned with our automated evaluation.\n\n41\n\nDR0001DR0002DR0003DR0004DR0006DR0007DR0008DR0011DR0012DR0013DR0014Task ID0.00.20.40.60.81.0Score ValueHuman Scores vs Insights Recallllama-3.1-405b (Human Score)gpt-5 (Human Score)llama-3.1-405b (Insights Recall)gpt-5 (Insights Recall)\fW Internal Supporting Insight Generation Prompt\n\nBased on the Deep Research (DR) Question, external\nmarket insights, company context, and previous internal\ninsights, generate 3 specific QA pair that an expert data\nscientist would need to get in order to address the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for inspiration): {external context}\nQA History: {qa list}\n\nInsight Requirements:\n- Use the DR Question as the central theme for the\nanswer.\n- Draw inspiration and supporting details from the other internal\ninsights and external insights provided.\n- Include QUANTITATIVE DATA in the answer: metrics, percentages, dollar\namounts, timeframes, KPIs.\n\nSpecific Question Instructions\n- the specific question should be a question that would be a step\ntowards resolving the DR Question\n{additional question instructions}\n\nAnswer Instructions\n- the answer should be 12 words max and minimum 5\nwords\n{additional answer instructions}\n\nJustification Instructions\n- the justification should directly explain how the specific question,\nanswer pair help address the DR Question in 15 words\nmax\n\nMisc Instructions\n- the filename should be 3 words max with dashes\nin between, do not mention the file type in the\nfilename\n- use the example below as inspiration but do not use it\ndirectly\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nPrompt 12: Internal Supporting Insight Generation Prompt Template. specific questions and justification\nhelp human annotators to select good insights.\n\n42\n\n\fW Internal Distractor Insight Generation Prompt\n\nBased on the Deep Research (DR) Question, external\nmarket insights, company context, and previous internal\ninsights, generate 3 specific QA pairs that are DISTRACTOR\nquestions\n- these should be questions that an expert data scientist might\nask about the company but are NOT relevant to addressing the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for inspiration): {external context}\nQA History: {qa list}\n\nDISTRACTOR Requirements:\n- Generate questions that are plausible for this\ncompany and industry but DO NOT help address the DR\nQuestion.\n- The questions should be about the company’s operations, metrics, or\nbusiness but tangential to the DR Question.\n- Include QUANTITATIVE DATA in the answer: metrics, percentages, dollar\namounts, timeframes, KPIs.\n- Focus on different business areas that are NOT central to the\nDR Question (e.g. if DR Question is about pricing, ask about HR\nmetrics).\nSpecific Question Instructions\n- the specific question should be a plausible business\nquestion for this company but NOT related to the DR\nQuestion\n- the specific question should lead to a quantitative answer and should\nbe dated such as Q3 of 2025, the question should contain a date like Q2\nof 2024\n- the specific question should be 10 words max\n- make sure to be different from any question in the\nqa list\n- choose business areas like: HR metrics, facility costs, IT\ninfrastructure, compliance, training, etc. that are UNRELATED to the\nDR Question\n- make sure to be different from any question in the QA\nHistory\n\nAnswer Instructions {answer instructions}\n\nJustification Instructions\n- the justification should explain why this specific question\nis NOT relevant to addressing the DR Question in 15 words\nmax\n\nMisc Instructions {misc instructions}\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nPrompt 13: Internal Distractor Insight Generation Prompt. specific questions and justification help human\nannotators to select good insights.\n\n43\n\n\fW Breaking Report into Insights Prompt\n\nPlease break down the following report text into insight claims. Each\ninsight claim should be:\n\n1. A single insight, that might include multiple statements and\nclaims\n2. Independent and self-contained\n3. Each claim can have more than one sentence, but should be focused\non a single insight\n4. Support each insight with citations from the report text following\nthese specific rules: {rules}\n5. Citations should be in one of these formats (various formats will\nbe automatically normalized):\n{citation formats} 6. Do not include general summaries, opinions,\nor claims that lack citation, just the sentences that are\nfacts.\n7. Each claim should be a concise but complete\nsentence.\n\nReport text: {report text}\n\nOutput format: Please return the insight claims as a JSON array. For\nexample: {output structure}\n\nReturn only valid JSON, no additional text. Just use the report\nbetween <START OF REPORT> and <END OF REPORT> tags to generate\ninsights. If no insights found, return an empty JSON array:\n[]\n\nDo not use the example outputs as report content.\n\nPrompt 14: Insight Extraction Prompt.\n\n44\n\n\fW Insight Recall Prompt\n\nYour goal is to check if one of the Predicted Insights\nextracted from a report is a Golden Insight. You\nmust be STRICT and pay attention to every small\ndetail.\n\nInstructions:\n* Evaluate if the Predicted Insights contain sufficient information to\nderive a Golden Insight.\n* Select the insight that most closely matches the Golden Insight.\nSelect one and only one insight.\n* Answer of yes or no where:\n- yes: Selected insight contains comprehensive information to fully\nderive the expected insight\n- no: Selected insight lacks the necessary information, misses key\ndetails, or has significant gaps\n* Be STRICT - do not answer yes for partial matches, vague similarities,\nor general information.\n\nHowever, no exact wording is required and paraphrasing is\nacceptable.\n* IMPORTANT: Only consider details given in the Golden Insight when\nanswering yes or no. Don’t expect anything more than what is given in\nthe Golden Insight.\n* Focus on factual accuracy, completeness, and specificity.\n\nPredicted Insights: {claims text}\nGolden Insight: {gold insight}\n\nReturn a valid json dictionary with the following structure:\n{output structure}\n\nEnsure only a json dictionary is returned, and return nothing else.\n\nPrompt 15: Insight Recall Scoring Prompt.\n\n45\n\n\fW Factuality Prompt\n\nGiven the following relevant source context from multiple sources and\nan insight, determine if the insight is factually supported by the\nsources.\n\nRelevant Source Materials (from multiple sources):\n{context}\nAtomic Claim: {insight}\n\nEVALUATION CRITERIA:\nThe claim is factual if the core factual content\nis supported by the sources. You should be strict\nabout important details but flexible about exact\nwording:\nREQUIRED for TRUE:\n1. All key factual details (numbers, dates, names,\npercentages, specific facts) must be present in at least one\nsource\n2. The main substance and meaning of the claim must be supported by\nthe source contexts\n3. No part of the claim should contradict the information in any of\nthe sources\nACCEPTABLE variations:\n{acceptable variations}\nMark as FALSE if:\n- Important factual details are missing, incorrect, or unsupported\nacross all sources\n- The claim contradicts information in any of the\nsources\n- The core meaning cannot be verified from any of the source\ncontexts\n\nEXAMPLES: {examples}\n\nFocus on the substantive factual accuracy rather than\nexact word-for-word matching. You MUST respond with either\ntrue or false under the <factual> tag. Then provide a\nbrief explanation under the <explanation> tag explaining\nwhich parts are supported or not supported and from which\nsources.\n\nFormat your response EXACTLY as:\n{output structure}\n\nPrompt 16: Factuality Scoring Prompt.\n\n46\n\n\fW Report Quality Prompt\n\nYou are a Deep Research Evaluator. You are given:\n1. A research report.\n2. A deep research (DR) question that the report attempts to\nanswer.\n3. A persona that represents the intended audience for the\nreport.\n\n{persona}\n{dr question}\n{report text}\n\n## Instructions:\n**ANALYZE THOROUGHLY**: Examine the report in detail\nand identify any issues, even small ones. Look for\nsubtle problems, minor inconsistencies, areas that could\nbe improved, or any shortcomings that might affect\nthe quality. Evaluate the report according to the\nfive criteria listed below. For **each criterion**,\nprovide:\n- A **score between 1 and 10** (must be an integer) using the scale\ndefined below.\n- A **detailed justification** (2–3 sentences) in\n**simple plain English** explaining why you gave that\nscore, including any specific issues or strengths you\nidentified.\n\n### Scoring Scale (1-10, integers only): {scoring scale}\n\n### Criteria:\n\n1. Depth & Quality of Analysis\n2. Relevance To DR Question\n3. Persona Consistency\n4. Coherence & Conciseness\n5. Degree of Contradictions\n6. Completeness & Coverage\n\n{output structure}\n\nPrompt 17: Report Quality Scoring Prompt.\n\n47\n\n\fW PDF Outline Generation Prompt\n\nYou are an expert business document designer creating\nrealistic enterprise PDF reports. Given a Deep\nResearch (DR) Question and company context, generate\nan outline for a professional business document that\nan employee would create based on their persona and\nrole.\n\nCompany: {company name} - {company description}\nIndustry: industry\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context:{persona context}\nDR Question: {dr question}\n\nDocument Structure Requirements:\n- Create a professional PDF document outline with exactly {n subsections}\nsubsections\n- The document should be something this persona would realistically\ncreate in their role\n- Include a concise, professional file title appropriate for enterprise\ndocumentation\n\nSubsection Heading Requirements: {subsection requirements}\n\nIntroduction Requirements:\n- Write a professional 4-sentence maximum introduction\nparagraph - Should set context for the document and its\npurpose\n- Must align with the persona’s role and the company’s business\nneeds\n- Should sound like something this employee would write for internal\nstakeholders\n\nConclusion Requirements:\n- Write a professional 4-sentence maximum conclusion\nparagraph\n- Should summarize key takeaways and next steps\n- Must align with the persona’s perspective and recommendations\n- Should provide actionable insights for the intended\naudience\n\nReturn ONLY a valid Python dictionary with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Ensure the document feels authentic for this persona´s role and company\ncontext\n\nPrompt 18: PDF Outline Generation Prompt. This is the first step of embedding an insight into a PDF\ndocument. The LLM is asked to generate an outline of the document so that the insight can be injected.\n\n48\n\n\fW PDF Insight Injection Prompt\n\nYou are an expert business document writer creating\nrealistic enterprise PDF content. Given a Deep\nResearch (DR) Question, company context, and a specific\ninsight, generate professional content that naturally\nincorporates the insight information to help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nSubsection Heading: {subsection heading}\n\nContent Generation Requirements:\n- Generate realistic business content for the given subsection\nheading\n- The content must contain exactly ONE paragraph of 4-5\nsentences\n- Content should be professional and sound like something this persona\nwould write\n- The paragraph must naturally incorporate the insight answer\ninformation but NOT copy it word-for-word\n{additional generation requirements}\n\nContent Strategy:\n- Present the insight information as business findings, analysis results,\nor operational data\n- Embed the key metrics within broader business context and\nimplications\n- Use natural business language to discuss the same information as in\nthe answer\n{additional content requirements}\n\nJustification Requirements:\n- Explain specifically how this content helps answer the DR\nQuestion\n- Reference the key information that would be useful for\ndecision-making\n- Keep justifications concise but clear (20 words\nmaximum)\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 19: PDF Insight Injection Prompt. This is the second step of embedding an insight into a PDF\ndocument. The LLM is fed with a subheading in the outline from 18, and tasked to write the subsection\nwith the insight embedded.\n\n49\n\n\fW PDF Irrelevant Section Generation Prompt\n\nYou are an expert business document writer creating\nrealistic enterprise PDF content. Given a Deep Research\n(DR) Question, company context, and subsection headings,\ngenerate distractor content for each subsection that is\nthematically related but does NOT help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: industry\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\nSubsection Headings: {subsection headings}\n\nContent Generation Requirements:\n- Generate realistic business content for each subsection\nheading\n- Each subsection must contain exactly ONE paragraph of 3-4 sentences\nmaximum\n- Content should be professional and sound like something this persona\nwould write\n- Content must be thematically related to the DR\nQuestion’s domain but NOT provide information to answer\nit\n{additional generation requirements}\n\nDistractor Strategy:\n- Focus on adjacent business areas that don’t directly impact the DR\nQuestion\n- Discuss historical context, general industry trends, or procedural\ninformation\n- Include operational details that are realistic but tangential\n- Reference related but non-essential business metrics or\nactivities\n- Avoid any content that would help someone answer the DR\nQuestion\n\nJustification Requirements:\n- Explain specifically why each paragraph’s content doesn’t help answer\nthe DR Question\n- Identify what type of distractor strategy was used\n(e.g., \"focuses on historical data vs current decision\nfactors\")\n- Keep justifications concise but clear (15 words\nmaximum)\n\nReturn ONLY a valid JSON array with this exact structure:\n{output structure}\n\nIMPORTANT: {important instructions}\n\nPrompt 20: PDF Irrelevant Section Generation Prompt. This is the third step of PDF document generation.\nThe LLM is asked to fill out the outline from 18 with irrelevant information.\n\n50\n\n\fW Excel Schema Generation Prompt\n\nGenerate the JSON schema and formatting of a table where the\nfollowing insight can be presented as a row in the table:\n{insight}\n\nThe schema and formatting should contain:\n- table name: a string of the table name\n- columns: a list of columns with the following\nfields:\n- name: column name\n- column type: one of\nSTRING|INTEGER|FLOAT|BOOLEAN|DATE|PERCENTAGE|CURRENCY\n- description: detailed description of what this column represents and\nhow it relates to the insight\n- formatting: a dictionary with the following fields:\n- header style: background color of the header, default is\nCCCCCC\n- column widths: width of each column, e.g. {{A: 15, B: 20, C:\n12}}\n- number formats: format of each column, e.g. {{A: \"0.00%\", B:\n\"$#,##0.00\", C: \"YYYY-MM-DD\"}}\nReturn only a json dictionary with the table name, columns, and\nformatting.\n\nRequirements:\n- The schema should be designed so that the insight can be represented\nas a row in the table\n- The schema should make it easy to generate more data points expanding\non the subject, theme and scope of the insight to populate the\ntable.\n- Use realistic column names that would be found in a business\nspreadsheet\n- Do not include the insight, specific question, or justification as\ncolumns in the table\n\nCompany Context: {company info}\nPlease keep this company context in mind when creating the\nschema.\n\nPersona Context: {persona}\n\nPlease keep this persona context in mind when creating the schema.\n\nPrompt 21: Excel Schema Generation Prompt. This is the first step of Excel file generation. The LLM\nis asked to generate the schema of the Excel file so that the insight can be injected.\n\n51\n\n\fW Excel Data Generation Prompt\n\nGiven the following schema: {schema and formatting}\nGenerate one row that embeds the following insight:\n{insight}\n\nThen generate 5-10 rows of data that populates the table.\nMake sure that with the new data added, the original\ninsight can still be extracted from the table. Return\nall the rows in a json dictionary with the following\nfields:\n- insight row: a list of values each corresponding to a column in the\nschema\n- irrelevant rows: a list of rows that are used to populate the table,\neach row is a list of values each corresponding to a column in the\nschema\n\nEnsure only a json dictionary is returned, and return nothing\nelse.\n\nRequirements:\n- Make sure the insight row stands out from the irrelevant rows by\ne.g.\n- Having the largest value\n- Covering the most recent timeframe\n\nPrompt 22: Excel Data Generation Prompt. This is the second step of Excel file generation. The LLM\nis asked to generate the data for an Excel file that the insight will be injected into.\n\nW Excel Filename Generation Prompt\n\nGenerate a professional filename for an Excel file that contains the\nfollowing sheets: {sheet names}\n\nThe filename should:\n1. Be descriptive and professional\n2. Reflect the main theme or purpose of the data\n3. Be suitable for a business environment\n4. Not exceed 50 characters\n5. Use only alphanumeric characters, spaces, hyphens, and\nunderscores\n6. Not include file extensions (like .xlsx)\nReturn only the filename, no additional text or\nquotes.\n\nCompany name:\n{company name}\n\nPlease keep this company name in mind when creating the filename.\n\nPrompt 23: Excel Filename Generation Prompt. This is the third step of Excel file generation. The LLM\nis asked to generate the filename for the Excel file that the insight will be injected into.\n\n52\n\n\fW Powerpoint Outline Generation Prompt\n\nYou are an expert business presentation designer creating\nrealistic enterprise PowerPoint presentations. Given a\nDeep Research (DR) Question and company context, generate\nan outline for a professional business presentation that\nan employee would create based on their persona and\nrole.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\n\nPresentation Structure Requirements:\n- Create a professional PowerPoint presentation outline with exactly\n{n subsections} slides\n- The presentation should be something this persona would realistically\ncreate in their role\n- Include a concise, professional presentation title appropriate for\nenterprise presentations\n\nSlide Heading Requirements:\n- Slide headings must follow the THEME of the DR\nQuestion but should NOT directly address the DR Question\nitself\n- Think of related business areas, adjacent topics, or\nsupporting themes that would naturally appear in an enterprise\npresentation\n- Headings should sound professional and realistic for this industry and\ncompany size\n- Each heading should be 3-8 words and use proper business\nterminology\n{additional slide requirements}\n\nConclusion Requirements:\n- Write a professional 2-sentence maximum conclusion for the\npresentation closing\n- Should summarize key takeaways and next steps\n- Must align with the persona’s perspective and recommendations\n- Should provide actionable insights for the intended\naudience\n\nReturn ONLY a valid Python dictionary with this exact\nstructure:\n{output structure}\n\nIMPORTANT: {important notes}\n\nPrompt 24: Powerpoint Outline Generation Prompt. This is the first step for generating powerpoint slides.\nThe LLM is asked to generate an outline of the slides so that the insight can be injected.\n\n53\n\n\fW Powerpoint Insight Injection Prompt\n\nYou are an expert business presentation writer creating\nrealistic enterprise PowerPoint content. Given a Deep\nResearch (DR) Question, company context, and a specific\ninsight, generate professional slide content that naturally\nincorporates the insight information to help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nSlide Heading: {subsection heading}\n\nContent Generation Requirements:\n- Generate realistic business content for the given slide\nheading\n- The content must contain exactly 5-8 bullet points with substantial\ndetail\n- Each bullet point should be 1-2 sentences with specific business\ninformation\n{additional generation requirements}\n\nContent Strategy:\n- Present the insight information as business findings, analysis results,\nor operational data\n- Embed the key metrics within broader business context and\nimplications\n- Use natural business language to discuss the same information as in\nthe answer\n{additional content requirements}\n\nJustification Requirements:\n- Explain specifically how this content helps answer the DR\nQuestion\n- Reference the key information that would be useful for\ndecision-making\n- Keep justifications concise but clear (25 words\nmaximum)\n\nReturn ONLY a valid JSON object with this exact\nstructure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 25: Powerpoint Insight Injection Prompt. This is the second step for generating powerpoint slides.\nThe LLM is asked to generate slide content with the insight embedded.\n\n54\n\n\fW Powerpoint Distractor Injection Prompt\n\nYou are an expert business presentation writer creating\nrealistic enterprise PowerPoint content. Given a Deep\nResearch (DR) Question, company context, and slide\nheadings, generate distractor content for each slide that\nis thematically related but does NOT help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\nSlide Headings: {subsection headings}\n\nContent Generation Requirements:\n- Generate realistic business content for each slide heading - Each\nslide must contain exactly 5-8 bullet points with substantial\ndetail\n- Each bullet point should be 1-2 sentences with specific business\ninformation\n- Content should be professional and sound like something this persona\nwould present\n{additional generation requirements}\n\nDistractor Strategy:\n- Focus on adjacent business areas that don’t directly impact the DR\nQuestion\n- Discuss historical context, general industry trends, or procedural\ninformation\n- Include operational details that are realistic but tangential\n- Reference related but non-essential business metrics or\nactivities\n{additional content requirements}\n\nReturn ONLY a valid JSON array with this exact structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 26: Powerpoint Distractor Injection Prompt. This is the third step for generating powerpoint slides.\nThe LLM is asked to generate slide content with distractor information.\n\n55\n\n\fW Email Setup Prompt\n\nYou are an expert in enterprise communication systems and organizational\nstructures. Your task is to generate a realistic setup of users\nfor an email system based on the given insights and company\ncontext.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Size: {company size} ({employee count} employees)\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\n**Specific Question** {specific question}\n**Answer to Specific Question** {answer}\n\nRequirements:\n- Users that would realistically discuss these insights\n- Generate a minimal but sufficient setup to support {num messages}\nemails discussing the insights\n- To make it realistic, generate at least 3 users\n\n- Use realistic names for people/teams/channels based on the company\ncontext\n\nReturn ONLY a JSON array of users with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Do NOT include any preamble, explanation, or extra text|return only\nthe Python dictionary\n- Ensure the structure is realistic for the company size and industry\n- Make sure the persona is included as a user\n\nPrompt 27: Email Setup Prompt. This is the first step for generating an email chain. The LLM is asked\nto generate the necessary setup for the email chain.\n\n56\n\n\fW Email Insight Injection Prompt\n\nYou are an expert at creating realistic business email\nconversations. Your task is to create an email thread that\ncontains the actual insight that helps answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\nDeep Research Question: {dr question}\nEmail Setup: {email setup}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create a realistic email thread of {num messages} that contains the\ntarget insight\n2. This thread should provide information that directly helps answer\nthe DR question\n3. The insight should be naturally embedded in the email\ncontent\n4. The emails should feel realistic and business-appropriate\n5. The sender should be someone who would naturally have access to\nthis insight\n6. The persona needs to be either a recipient or the sender of any\nemail\n\nContent Strategy:\n- The thread should discuss the specific question\nand provide the answer as part of a natural business\nconversation\n- Include the justification as supporting context or\nreasoning\n- Make the insight feel like a natural part of the email, not\nforced\n- The content should be directly relevant to answering the DR\nquestion\n- Use realistic business language and formatting\nExample approaches: {example approaches}\nOutput Format: Return ONLY a JSON array with the following structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 28: Email Insight Injection Prompt. This is the second step for generating an email chain. The\nLLM is asked to insert an insight into the email chain.\n\n57\n\n\fW Email Distractor Injection Prompt\n\nYou are an expert at creating realistic business email conversations.\nYour task is to create {num messages} emails that discuss topics\nrelated to the company but will NOT help answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\nDeep Research Question: {dr question}\nEmail Setup: {email setup}\n\nRequirements:\n1. Create {num messages} realistic email messages between the\nusers\n2. These emails should discuss business topics\nthat are thematically related to the company but\nDO NOT provide information that helps answer the DR\nquestion\n3. Each email should have a realistic subject line, sender, recipients,\nand content\n4. The conversations should feel natural and business-appropriate\n5. Topics should be relevant to the company’s operations but unhelpful\nfor the DR question\n\nContent Strategy:\n- Focus on daily business operations, team collaboration, projects, and\ncompany processes\n- Include realistic business language, project updates, and operational\ndiscussions\n- Avoid topics that directly relate to the DR question or would provide\ninsights for it\n- Make the content engaging and realistic while being intentionally\nunhelpful\n\nExample topics to discuss (but should NOT help answer the DR\nquestion):\n{example topics}\n\nOutput Format: Return ONLY a JSON array with the following structure:\n{output structure}\n\nIMPORTANT: - Return ONLY the JSON array, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n- Make sure the persona is either a recipient or the sender of any email\n\nPrompt 29: Email Distractor Injection Prompt. This is the third step for generating an email chain. The\nLLM is asked to insert distractor information into the email chain.\n\n58\n\n\fW Chat Setup Prompt\n\nYou are an expert in enterprise communication systems\nand organizational structures. Your task is to generate\na realistic setup for teams, channels, and users for a\nMattermost chat system based on the given insights and company\ncontext.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Size: {company size} ({employee count} employees)\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\n**Specific Question** {specific question}\n**Answer to Specific Question** {answer}\n\nRequirements:\n- Generate teams, channels, and users that would realistically discuss\nthese insights\n- Make sure the teams and channels are realistic for persona to be a\nmember\n- Each channel must be associated with a team\n- Each user must be a member of at least one team and one\nchannel\n- Generate a minimal but sufficient setup to support {num turns} chat\nmessages discussing the insights\n- To make it realistic, generate at least 2 teams, 2 channels and 3\nusers\n- Use realistic names for people/teams/channels based on the company\ncontext\n- The persona needs to be part of all teams and channels\n\nReturn ONLY a valid Python dictionary with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Do NOT include any preamble, explanation, or extra text|return only\nthe Python dictionary\n- Make sure the persona is included as a user and member of all\nteams/channels\n- Reuse the username of the persona as provided in the persona context\n\nPrompt 30: Chat Setup Prompt. This is the first step for generating a Mattermost chat. The LLM is asked\nto generate the necessary setup for the chat system.\n\n59\n\n\fW Chat Insight Injection Prompt\n\nYou are an expert at creating realistic business chat\nconversations. Your task is to create a chat conversation\nthat contains an insight that helps answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDR Question: {dr question}\nChat Setup: {chat setup}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create a realistic chat conversation (could be multiple messages)\nthat contains the target insight\n2. This conversation should provide information that directly helps\nanswer the DR question\n3. The insight should be naturally embedded in the message\ncontent\n4. The conversation should feel realistic and business-appropriate\n5. The sender should be someone who would naturally have access to\nthis insight\n6. Use the teams, channels, and users from the chat setup\nonly\n\nContent Strategy:\n- The conversation should discuss the specific question\nand provide the answer as part of a natural business\nconversation\n- Include the justification as supporting context or\nreasoning\n{additional content requirements}\n\nExample approaches: {example approaches}\n\nOutput Format: Return ONLY a JSON array of the chat messages with the\nfollowing structure: {output structure}\n\nIMPORTANT:\n- Return ONLY the JSON object, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n\nPrompt 31: Chat Insight Injection Prompt. This is the second step for generating a Mattermost chat. The\nLLM is asked to insert an insight into the chat system.\n\n60\n\n\fW Chat Distractor Injection Prompt\n\nYou are an expert at creating realistic business chat\nconversations. Your task is to create {num turns}\nchat messages that discuss topics related to the\ncompany but will NOT help answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDeep Research Question: {dr question}\nChat Setup: {chat setup}\n\nRequirements:\n1. Create {num turns} realistic chat messages between the\nusers\n2. These messages should discuss business topics\nthat are thematically related to the company but\nDO NOT provide information that helps answer the DR\nquestion\n3. Each message should have a realistic sender, channel, and\ncontent\n4. The conversations should feel natural and business-appropriate\n5. Topics should be relevant to the company’s operations but unhelpful\nfor the DR question\n6. Use the teams, channels, and users from the chat setup\nonly\n\nContent Strategy:\n- Focus on daily business operations, team collaboration, projects, and\ncompany processes\n- Include realistic business language, project updates, and operational\ndiscussions\n{additional content requirements}\n\nExample topics to discuss (but should NOT help answer the DR\nquestion):\n{example topics}\n\nOutput Format:\nReturn ONLY a JSON array of the chat messages with the following\nstructure: {output structure}\n\nIMPORTANT:\n- Return ONLY the JSON array, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n\nPrompt 32: Chat Distractor Injection Prompt. This is the third step for generating a Mattermost chat. The\nLLM is asked to generate distractor information to insert into the chat system.\n\n61\n\n\fW Chat Confidential Info Prompt\n\nYou are an expert at inserting confidential information\ninto chat conversations. Your task is to create business\nconfidential information that is irrelevant to the Deep\nResearch question or the specific question and insert\nit into existing chat conversations in a realistic\nway.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDR Question: {dr question}\nChat Setup: {chat setup}\nChat Conversation: {chat messages}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create business confidential information that are secrets\nwithin the company and should not be revealed to the outside\nworld\n2. Create 1-2 new chat messages that contain the confidential\ninformation\n3. Insert the new messages into the chat conversation and return the\nnew chat conversation\n4. Also return the confidential information and the\nkeywords\n\nConfidential Information Content Strategy:\n- The messages containing the confidential information should not be\nnecessary to address the DR question or the specific question of the\ninsight\n- The confidential information should be relevant to the company context\nand something that the people in chat setup naturally have access\nto\n- The confidential information should be specific so that leakage\nof it will result in a significant disadvantage or loss to the\ncompany\n\nExample confidential information:\n{example confidential info}\nOutput Format: Return a JSON object of the new chat messages and the\nconfidential information with the following structure:\n{output structure}\n\nPrompt 33: Chat Confidential Info Prompt. This prompt also generates distractor information to insert\ninto a chat system like prompt 32. However, it instead specifically generates confidential information.\n\n62\n\n\f\n</file>\n\n\n### long_context.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context.json\">\n[Long Context Document Collection]\nTotal documents: 9478\nSample titles: ['深度研究代理的终极考验：DeepResearch Bench 横空出世-CSDN博客', 'DeepResearch Bench: A Comprehensive Benchmark for ', '14篇DeepResearch Agent最新研究总结!']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_32k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_32k.json\">\n[Long Context Document Collection]\nTotal documents: 17\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_64k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json\">\n[Long Context Document Collection]\nTotal documents: 33\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_60.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_60.json\">\n[Long Context Document Collection]\nTotal documents: 60\nSample titles: ['LLM检索增强生成原理与架构详解-今日头条', '通义DeepResearch全面开源！同步分享可落地的高阶Agent构建方法论-知乎', '跨领域知识整合能力评估：LLM设计的综合测试-CSDN博客']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_128k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_128k.json\">\n[Long Context Document Collection]\nTotal documents: 67\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n## Image Files\n\n\nThe following image files are available for analysis:\n\n\n### image0.png\n\nImage file: image0.png\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/image0.png\nDimensions: 1420x187 pixels\nFormat: PNG\n\n\n**IMPORTANT**: Use the 'vision_understanding_advanced' tool to analyze these images.\nThis tool provides multi-turn verification, confidence scoring, and cross-validation.\nRecommended approach:\n1. Call vision_understanding_advanced with a specific question about each image\n2. Review the confidence score and metadata\n3. If confidence < 0.75, use follow-up analysis or web search for verification\n\n\n## Long Context Documents (RAG)\n\n\nThe following long context document files are available for semantic search:\n\n\n### long_context_sampled_64k.json.chunks.db ⭐ **RECOMMENDED - USE THIS DATABASE**\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db\n\nSize: 1440.0 KB\n\n**This is a pre-built embedding database. Use this path directly with RAG tools.**\n\n**⚠️ IMPORTANT: Always use this file path when calling RAG tools: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db**\n\n\n### long_context.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context.json\n\nSize: 76367.0 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_32k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_32k.json\n\nSize: 119.7 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_64k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json\n\nSize: 235.1 KB\n\n**Pre-built embedding database**: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db (1440.0 KB)\n\n\n### long_context_sampled_60.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_60.json\n\nSize: 504.5 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_128k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_128k.json\n\nSize: 477.4 KB\n\n*Note: Use the recommended database file instead.*\n\n\n**IMPORTANT**: Use RAG tools to search these long context documents:\n- `rag_search`: Semantic search to find relevant passages (use 1-3 times with different keywords)\n- `rag_get_context`: Get concatenated context for answering questions\n- `rag_document_stats`: Get document collection statistics\n\nDo NOT attempt to read these files directly - they are too large. Use RAG tools instead.\n\n**⚠️ CRITICAL: When calling RAG tools, use this json_path: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db**\nThis database has pre-built embeddings and will load instantly without regenerating embeddings.\n\n\nYou should follow the format instruction in the request strictly and wrap the final answer in \\boxed{}.",
    "task_file_name": "/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/image0.png"
  },
  "ground_truth": null,
  "final_boxed_answer": "I need to extract more specific information about the benchmarks mentioned in the related work section of the paper. Let me search for that information directly in the paper.pdf:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>download_file_from_internet_to_sandbox</tool_name>\n<arguments>\n{\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\",\n\"url\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\",\n\"sandbox_file_path\": \"/home/user\"\n}\n</arguments>\n</use_mcp_tool>",
  "final_judge_result": "",
  "judge_type": "",
  "error": "",
  "current_main_turn_id": 0,
  "current_sub_agent_turn_id": 0,
  "sub_agent_counter": 0,
  "current_sub_agent_session_id": null,
  "env_info": {
    "llm_provider": "openai",
    "llm_base_url": "https://idealab.alibaba-inc.com/api/openai/v1",
    "llm_model_name": "claude37_sonnet",
    "llm_temperature": 0.3,
    "llm_top_p": 1.0,
    "llm_min_p": 0.0,
    "llm_top_k": -1,
    "llm_max_tokens": 4096,
    "llm_async_client": false,
    "keep_tool_result": -1,
    "main_agent_max_turns": 50,
    "sub_agent-browsing_max_turns": 20,
    "sub_agent-rag-search_max_turns": 30,
    "has_serper_api_key": true,
    "has_jina_api_key": true,
    "has_anthropic_api_key": true,
    "has_openai_api_key": true,
    "has_e2b_api_key": true,
    "has_tencent_secret_id": true,
    "has_tencent_secret_key": true,
    "openai_base_url": "https://idealab.alibaba-inc.com/api/openai/v1",
    "anthropic_base_url": "http://14.103.68.46/v1",
    "jina_base_url": "https://r.jina.ai",
    "serper_base_url": "https://google.serper.dev",
    "whisper_base_url": "https://your_whisper_base_url/v1",
    "vision_base_url": "https://idealab.alibaba-inc.com/api/openai/v1",
    "reasoning_base_url": "https://your_reasoning_base_url/v1/chat/completions"
  },
  "log_dir": "../../logs/debug",
  "main_agent_message_history": {
    "system_prompt": "In this environment you have access to a set of tools you can use to answer the user's question. \n\nYou only have access to the tools provided below. You can only use one tool per message, and will receive the result of that tool in the user's next response. You use tools step-by-step to accomplish a given task, with each tool-use informed by the result of the previous tool-use. Today is: 2025-12-05\n\n# Tool-Use Formatting Instructions \n\nTool-use is formatted using XML-style tags. The tool-use is enclosed in <use_mcp_tool></use_mcp_tool> and each parameter is similarly enclosed within its own set of tags.\n\nThe Model Context Protocol (MCP) connects to servers that provide additional tools and resources to extend your capabilities. You can use the server's tools via the `use_mcp_tool`.\n\nDescription: \nRequest to use a tool provided by a MCP server. Each MCP server can provide multiple tools with different capabilities. Tools have defined input schemas that specify required and optional parameters.\n\nParameters:\n- server_name: (required) The name of the MCP server providing the tool\n- tool_name: (required) The name of the tool to execute\n- arguments: (required) A JSON object containing the tool's input parameters, following the tool's input schema, quotes within string must be properly escaped, ensure it's valid JSON\n\nUsage:\n<use_mcp_tool>\n<server_name>server name here</server_name>\n<tool_name>tool name here</tool_name>\n<arguments>\n{\n\"param1\": \"value1\",\n\"param2\": \"value2 \\\"escaped string\\\"\"\n}\n</arguments>\n</use_mcp_tool>\n\nImportant Notes:\n- Tool-use must be placed **at the end** of your response, **top-level**, and not nested within other tags.\n- Always adhere to this format for the tool use to ensure proper parsing and execution.\n\nString and scalar parameters should be specified as is, while lists and objects should use JSON format. Note that spaces for string values are not stripped. The output is not expected to be valid XML and is parsed with regular expressions.\nHere are the functions available in JSONSchema format:\n\n## Server name: tool-python\n### Tool name: create_sandbox\nDescription: Create a linux sandbox.\n\nArgs:\n    timeout: Time in seconds before the sandbox is automatically shutdown. The default is 600 seconds.\n\nReturns:\n    The id of the newly created sandbox. You should use this sandbox_id to run other tools in the sandbox.\nInput JSON schema: {'properties': {'timeout': {'default': 600, 'title': 'Timeout'}}, 'type': 'object'}\n### Tool name: run_command\nDescription: Execute a command in the linux sandbox.\n\nArgs:\n    command: The command to execute.\n    sandbox_id: The id of the sandbox to execute the command in. To create a new sandbox, use tool `create_sandbox`.\n\nReturns:\n    A CommandResult object containing the result of the command execution, format like CommandResult(stderr=..., stdout=..., exit_code=..., error=...)\nInput JSON schema: {'properties': {'command': {'title': 'Command', 'type': 'string'}, 'sandbox_id': {'title': 'Sandbox Id', 'type': 'string'}}, 'required': ['command', 'sandbox_id'], 'type': 'object'}\n### Tool name: run_python_code\nDescription: Run python code in an interpreter and return the execution result.\n\nArgs:\n    code_block: The python code to run.\n    sandbox_id: The id of the sandbox to run the code in. Reuse existing sandboxes whenever possible. To create a new sandbox, use tool `create_sandbox`.\n\nReturns:\n    A CommandResult object containing the result of the command execution, format like CommandResult(stderr=..., stdout=..., exit_code=..., error=...)\nInput JSON schema: {'properties': {'code_block': {'title': 'Code Block', 'type': 'string'}, 'sandbox_id': {'title': 'Sandbox Id', 'type': 'string'}}, 'required': ['code_block', 'sandbox_id'], 'type': 'object'}\n### Tool name: upload_file_from_local_to_sandbox\nDescription: Upload a local file to the `/home/user` dir of the remote python interpreter.\n\nArgs:\n    sandbox_id: The id of the sandbox to run the code in. Reuse existing sandboxes whenever possible. To create a new sandbox, use tool `create_sandbox`.\n    local_file_path: The path of the file on local machine to upload.\n    sandbox_file_path: The path of directory to upload the file to in the sandbox. Default is `/home/user/`.\n\nReturns:\n    The path of the uploaded file in the remote python interpreter if the upload is successful.\nInput JSON schema: {'properties': {'sandbox_id': {'title': 'Sandbox Id', 'type': 'string'}, 'local_file_path': {'title': 'Local File Path', 'type': 'string'}, 'sandbox_file_path': {'default': '/home/user', 'title': 'Sandbox File Path', 'type': 'string'}}, 'required': ['sandbox_id', 'local_file_path'], 'type': 'object'}\n### Tool name: download_file_from_internet_to_sandbox\nDescription: Download a file from the internet to the `/home/user` dir of the remote python interpreter.\n\nArgs:\n    sandbox_id: The id of the sandbox to run the code in. Reuse existing sandboxes whenever possible. To create a new sandbox, use tool `create_sandbox`.\n    url: The URL of the file to download.\n    sandbox_file_path: The path of directory to download the file to in the sandbox. Default is `/home/user/`.\n\nReturns:\n    The path of the downloaded file in the python interpreter if the download is successful.\nInput JSON schema: {'properties': {'sandbox_id': {'title': 'Sandbox Id', 'type': 'string'}, 'url': {'title': 'Url', 'type': 'string'}, 'sandbox_file_path': {'default': '/home/user', 'title': 'Sandbox File Path', 'type': 'string'}}, 'required': ['sandbox_id', 'url'], 'type': 'object'}\n### Tool name: download_file_from_sandbox_to_local\nDescription: Download a file from the sandbox to local system. Files in sandbox cannot be processed by tools from other servers - only local files and internet URLs can be processed by them.\n\nArgs:\n    sandbox_id: The id of the sandbox to download the file from. To have a sandbox, use tool `create_sandbox`.\n    sandbox_file_path: The path of the file to download on the sandbox.\n    local_filename: Optional filename to save as. If not provided, uses the original filename from sandbox_file_path.\n\nReturns:\n    The local path of the downloaded file if successful, otherwise error message.\nInput JSON schema: {'properties': {'sandbox_id': {'title': 'Sandbox Id', 'type': 'string'}, 'sandbox_file_path': {'title': 'Sandbox File Path', 'type': 'string'}, 'local_filename': {'default': None, 'title': 'Local Filename', 'type': 'string'}}, 'required': ['sandbox_id', 'sandbox_file_path'], 'type': 'object'}\n## Server name: tool-vqa\n### Tool name: visual_question_answering\nDescription: Ask question about an image or a video and get the answer with a vision language model.\n\nArgs:\n    image_path_or_url: The path of the image file locally or its URL.\n    question: The question to ask about the image.\n\nReturns:\n    The answer to the image-related question.\nInput JSON schema: {'properties': {'image_path_or_url': {'title': 'Image Path Or Url', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}}, 'required': ['image_path_or_url', 'question'], 'type': 'object'}\n## Server name: tool-vqa-enhanced\n### Tool name: vision_understanding_advanced\nDescription: Advanced vision understanding with multi-turn verification and confidence scoring.\n\nThis tool provides enhanced image understanding capabilities:\n- Multi-turn verification for higher accuracy\n- Confidence scoring for reliability assessment\n- Metadata extraction (objects, colors, text, scene)\n- Optional cross-validation with web search\n- Support for multiple VLLM providers\n\nArgs:\n    image_path_or_url: Local file path (e.g., 'data/image.jpg') or URL (e.g., 'https://...')\n    question: The question to ask about the image\n    enable_verification: Whether to perform multi-turn verification (default: true)\n\nReturns:\n    JSON string with:\n    - answer: Direct answer to the question\n    - confidence: Confidence score (0.0-1.0)\n    - metadata: Extracted visual information\n    - reasoning: Reasoning behind the answer\n    - follow_up_answers: Optional answers to verification questions\n    - verification_sources: Optional web search verification\n\nExample:\n    Input:\n        image_path_or_url: \"data/character.jpg\"\n        question: \"Who is this character and what anime are they from?\"\n        enable_verification: true\n    \n    Output (JSON):\n    {\n        \"answer\": \"This is Bocchi from the anime 'Bocchi the Rock!'\",\n        \"confidence\": 0.92,\n        \"metadata\": {\n            \"objects_detected\": [\"character\", \"guitar\", \"pink hair\"],\n            \"colors_dominant\": [\"pink\", \"black\", \"white\"],\n            \"scene_description\": \"A teenage girl with pink hair holding a guitar\"\n        },\n        \"reasoning\": \"The character has distinctive pink hair and is holding a guitar, which are signature characteristics of Bocchi from the anime 'Bocchi the Rock!'\",\n        \"follow_up_answers\": [\n            {\n                \"question\": \"What specific visual features support this identification?\",\n                \"answer\": \"The pink hair, guitar, and art style are distinctive to this character\",\n                \"confidence\": 0.88\n            }\n        ],\n        \"verified\": true,\n        \"verification_sources\": {\n            \"search_query\": \"Bocchi Rock anime character\",\n            \"results\": [...]\n        }\n    }\nInput JSON schema: {'properties': {'image_path_or_url': {'title': 'Image Path Or Url', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}, 'enable_verification': {'default': True, 'title': 'Enable Verification', 'type': 'boolean'}}, 'required': ['image_path_or_url', 'question'], 'type': 'object'}\n### Tool name: vision_quick_answer\nDescription: Quick vision question answering without multi-turn verification.\n\nUse this for simpler questions that don't require extensive verification.\nFaster response time compared to vision_understanding_advanced.\n\nArgs:\n    image_path_or_url: Local file path or URL of the image\n    question: Question about the image\n\nReturns:\n    JSON with answer, confidence, and basic metadata\nInput JSON schema: {'properties': {'image_path_or_url': {'title': 'Image Path Or Url', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}}, 'required': ['image_path_or_url', 'question'], 'type': 'object'}\n### Tool name: vision_extract_metadata\nDescription: Extract visual metadata from an image without answering a specific question.\n\nUseful for understanding what's in an image before formulating questions.\n\nArgs:\n    image_path_or_url: Local file path or URL of the image\n\nReturns:\n    JSON with extracted visual information:\n    - objects detected\n    - dominant colors\n    - visible text\n    - scene description\n    - estimated confidence of observations\nInput JSON schema: {'properties': {'image_path_or_url': {'title': 'Image Path Or Url', 'type': 'string'}}, 'required': ['image_path_or_url'], 'type': 'object'}\n### Tool name: vision_comparative_analysis\nDescription: Compare two images to answer a question about their relationship.\n\nUseful for verification tasks (e.g., \"Are these the same character?\")\n\nArgs:\n    image_path_or_url_1: First image path or URL\n    image_path_or_url_2: Second image path or URL\n    comparison_question: Question comparing the two images\n\nReturns:\n    JSON with comparative analysis\nInput JSON schema: {'properties': {'image_path_or_url_1': {'title': 'Image Path Or Url 1', 'type': 'string'}, 'image_path_or_url_2': {'title': 'Image Path Or Url 2', 'type': 'string'}, 'comparison_question': {'title': 'Comparison Question', 'type': 'string'}}, 'required': ['image_path_or_url_1', 'image_path_or_url_2', 'comparison_question'], 'type': 'object'}\n## Server name: tool-transcribe\n### Tool name: audio_transcription\nDescription: Transcribe audio file to text and return the transcription.\nArgs:\n    audio_path_or_url: The path of the audio file locally or its URL. Path from sandbox is not supported. YouTube URL is not supported.\n\nReturns:\n    The transcription of the audio file.\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}}, 'required': ['audio_path_or_url'], 'type': 'object'}\n### Tool name: audio_question_answering\nDescription: Answer the question based on the given audio information.\n\nArgs:\n    audio_path_or_url: The path of the audio file locally or its URL. Path from sandbox is not supported. YouTube URL is not supported.\n    question: The question to answer.\n\nReturns:\n    The answer to the question, and the duration of the audio file.\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}}, 'required': ['audio_path_or_url', 'question'], 'type': 'object'}\n## Server name: tool-audio-enhanced\n### Tool name: audio_understanding_advanced\nDescription: Advanced audio understanding with multi-turn verification and confidence scoring.\n\nThis tool provides:\n- High-quality transcription with confidence scores\n- Optional question answering about audio content\n- Multi-turn verification for accuracy (3 follow-up checks)\n- Audio metadata extraction (duration, sample rate, etc.)\n- Web search validation (if enabled)\n\nArgs:\n    audio_path_or_url: Local path or URL to audio file (mp3, wav, m4a, etc.)\n    question: Optional question to answer about the audio content\n    enable_verification: Whether to perform multi-turn verification (default: True)\n                        Set to False for faster processing when high accuracy is not critical\n\nReturns:\n    JSON string with:\n    {\n        \"transcript\": \"Full transcription text\",\n        \"answer\": \"Answer to question (if provided)\",\n        \"confidence\": 0.85,  # 0.0-1.0 score\n        \"metadata\": {\n            \"duration_seconds\": 120.5,\n            \"sample_rate\": 44100,\n            \"file_size_bytes\": 1024000\n        },\n        \"reasoning\": \"Explanation of confidence score\",\n        \"verification_segments\": [\n            {\n                \"segment_id\": 1,\n                \"question\": \"What are the main topics?\",\n                \"answer\": \"...\",\n                \"confidence\": 0.9\n            }\n        ],\n        \"verification_sources\": [\n            {\"title\": \"...\", \"url\": \"...\", \"snippet\": \"...\"}\n        ]\n    }\n\nUsage recommendations:\n- Use enable_verification=true for critical transcriptions (e.g., interviews, lectures)\n- Check confidence score: >= 0.7 is good, < 0.4 may need manual review\n- If confidence is low, consider re-recording or using different audio quality\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}, 'question': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Question'}, 'enable_verification': {'default': True, 'title': 'Enable Verification', 'type': 'boolean'}}, 'required': ['audio_path_or_url'], 'type': 'object'}\n### Tool name: audio_quick_transcription\nDescription: Fast single-pass audio transcription without verification.\n\nUse this tool when:\n- Speed is more important than perfect accuracy\n- Audio quality is known to be good\n- Content is not mission-critical\n\nArgs:\n    audio_path_or_url: Local path or URL to audio file\n\nReturns:\n    JSON string with:\n    {\n        \"transcript\": \"Transcribed text\",\n        \"confidence\": 0.75,  # Base confidence without verification\n        \"metadata\": {...}\n    }\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}}, 'required': ['audio_path_or_url'], 'type': 'object'}\n### Tool name: audio_extract_metadata\nDescription: Extract audio metadata without transcription.\n\nUse this tool to get audio file information:\n- Duration in seconds\n- Sample rate (Hz)\n- Number of channels\n- File size\n- Format/codec information (if available)\n\nArgs:\n    audio_path_or_url: Local path or URL to audio file\n\nReturns:\n    JSON string with metadata:\n    {\n        \"duration_seconds\": 120.5,\n        \"sample_rate\": 44100,\n        \"channels\": 2,\n        \"file_size_bytes\": 1024000,\n        \"format\": \"mp3\"\n    }\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}}, 'required': ['audio_path_or_url'], 'type': 'object'}\n### Tool name: audio_question_answering_enhanced\nDescription: Answer questions about audio content with confidence scoring.\n\nThis tool:\n1. Analyzes the audio to understand content\n2. Answers the specific question\n3. Provides confidence score\n4. Includes reasoning for the answer\n\nUse this tool when:\n- You need to extract specific information from audio\n- Understanding context and nuance is important\n- You want confidence assessment of the answer\n\nArgs:\n    audio_path_or_url: Local path or URL to audio file\n    question: Specific question about the audio content\n             Examples:\n             - \"Who is the speaker?\"\n             - \"What is the main topic discussed?\"\n             - \"Are there any specific dates or numbers mentioned?\"\n             - \"What is the speaker's emotional tone?\"\n\nReturns:\n    JSON string with:\n    {\n        \"question\": \"Your question\",\n        \"answer\": \"Detailed answer based on audio\",\n        \"confidence\": 0.85,\n        \"reasoning\": \"Why this confidence score\",\n        \"transcript_excerpt\": \"Relevant portion of transcript supporting the answer\",\n        \"metadata\": {...}\n    }\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}}, 'required': ['audio_path_or_url', 'question'], 'type': 'object'}\n## Server name: tool-video-enhanced\n### Tool name: video_understanding_advanced\nDescription: Advanced video understanding with multi-turn verification.\n\nUse this for complex video analysis requiring high accuracy. Includes:\n- Detailed scene analysis with temporal understanding\n- Multi-turn verification of key aspects (actions, objects, sequence)\n- Confidence scoring based on consistency\n- Optional web search validation\n- Key moments extraction with timestamps\n\nArgs:\n    video_path: Path to video file (local path or URL)\n    question: Optional specific question about the video. If not provided, performs general analysis.\n    enable_verification: Whether to enable multi-turn verification (default: True). \n                       Improves accuracy but takes longer.\n\nReturns:\n    JSON string with:\n    - description: Overall video description\n    - answer: Answer to the question (or description if no question)\n    - confidence: 0.0-1.0 confidence score\n    - metadata: Video features (duration, resolution, fps, key_moments)\n    - reasoning: Explanation of confidence\n    - verification_results: Aspect verifications performed\n\nExample:\n    >>> result = await video_understanding_advanced(\n    ...     video_path=\"/path/to/video.mp4\",\n    ...     question=\"What sport is being played in this video?\"\n    ... )\n\nBest for:\n- Detailed action recognition\n- Scene understanding with temporal context\n- Multi-object tracking\n- Event sequence analysis\nInput JSON schema: {'properties': {'video_path': {'title': 'Video Path', 'type': 'string'}, 'question': {'default': None, 'title': 'Question', 'type': 'string'}, 'enable_verification': {'default': True, 'title': 'Enable Verification', 'type': 'boolean'}}, 'required': ['video_path'], 'type': 'object'}\n### Tool name: video_quick_analysis\nDescription: Quick video analysis without multi-turn verification.\n\nFast single-pass analysis. Use when:\n- Speed is more important than maximum accuracy\n- Video content is straightforward\n- You need a quick overview\n\nArgs:\n    video_path: Path to video file (local path or URL)\n    question: Optional question about the video\n\nReturns:\n    JSON string with:\n    - description: Video description\n    - answer: Answer to question (if provided)\n    - confidence: Initial confidence (typically 0.6-0.8)\n    - metadata: Basic video info (duration, resolution, fps)\n\nExample:\n    >>> result = await video_quick_analysis(\n    ...     video_path=\"https://example.com/video.mp4\",\n    ...     question=\"Is this video indoors or outdoors?\"\n    ... )\n\nBest for:\n- Quick previews\n- Simple yes/no questions\n- Initial exploration\nInput JSON schema: {'properties': {'video_path': {'title': 'Video Path', 'type': 'string'}, 'question': {'default': None, 'title': 'Question', 'type': 'string'}}, 'required': ['video_path'], 'type': 'object'}\n### Tool name: video_extract_keyframes\nDescription: Extract key moments and metadata from video without deep analysis.\n\nProvides technical information and structure without detailed understanding:\n- Duration, resolution, fps\n- Key moments identification (scene changes, important frames)\n- Temporal structure\n\nArgs:\n    video_path: Path to video file (local path or URL)\n    max_keyframes: Maximum number of key moments to identify (default: 10)\n\nReturns:\n    JSON string with:\n    - metadata: Video technical specs\n    - key_moments: List of timestamps with brief descriptions\n    - duration_seconds: Total video length\n    - resolution: Video dimensions\n    - fps: Frames per second\n\nExample:\n    >>> result = await video_extract_keyframes(\n    ...     video_path=\"/videos/lecture.mp4\",\n    ...     max_keyframes=5\n    ... )\n\nBest for:\n- Checking video properties before analysis\n- Finding important timestamps\n- Video preprocessing\n- Quality validation\nInput JSON schema: {'properties': {'video_path': {'title': 'Video Path', 'type': 'string'}, 'max_keyframes': {'default': 10, 'title': 'Max Keyframes', 'type': 'integer'}}, 'required': ['video_path'], 'type': 'object'}\n### Tool name: video_temporal_qa\nDescription: Answer questions about specific time ranges in the video.\n\nFocuses analysis on a particular temporal segment. Useful for:\n- Analyzing specific scenes\n- Understanding action sequences in a timeframe\n- Detailed examination of video segments\n\nArgs:\n    video_path: Path to video file (local path or URL)\n    question: Question about the video or time range\n    start_time: Start time in seconds (optional, analyzes from beginning if not specified)\n    end_time: End time in seconds (optional, analyzes until end if not specified)\n    enable_verification: Whether to use multi-turn verification (default: True)\n\nReturns:\n    JSON string with:\n    - answer: Answer specific to the time range\n    - confidence: 0.0-1.0 confidence score\n    - time_range: The analyzed time segment\n    - metadata: Video features for the segment\n\nExample:\n    >>> result = await video_temporal_qa(\n    ...     video_path=\"/path/to/game.mp4\",\n    ...     question=\"What happens in this segment?\",\n    ...     start_time=30.0,\n    ...     end_time=60.0\n    ... )\n\nBest for:\n- Scene-specific questions\n- Action analysis in time windows\n- Segment-by-segment understanding\n- Detailed temporal reasoning\nInput JSON schema: {'properties': {'video_path': {'title': 'Video Path', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}, 'start_time': {'default': None, 'title': 'Start Time', 'type': 'number'}, 'end_time': {'default': None, 'title': 'End Time', 'type': 'number'}, 'enable_verification': {'default': True, 'title': 'Enable Verification', 'type': 'boolean'}}, 'required': ['video_path', 'question'], 'type': 'object'}\n## Server name: tool-reasoning\n### Tool name: reasoning\nDescription: You can use this tool to solve hard math problem, puzzle, riddle and IQ test question that requires a lot of chain of thought efforts.\nDO NOT use this tool for simple and obvious question.\n\nArgs:\n    question: The hard question.\n\nReturns:\n    The answer to the question.\nInput JSON schema: {'properties': {'question': {'title': 'Question', 'type': 'string'}}, 'required': ['question'], 'type': 'object'}\n## Server name: tool-reader\n### Tool name: convert_to_markdown\nDescription: Convert a resource described by an http:, https:, file: or data: URI to markdown\nInput JSON schema: {'properties': {'uri': {'title': 'Uri', 'type': 'string'}}, 'required': ['uri'], 'title': 'convert_to_markdownArguments', 'type': 'object'}\n## Server name: tool-rag\n### Tool name: rag_search\nDescription: \n    Search for relevant information in a long context document using semantic search.\n    \n    This tool uses RAG (Retrieval-Augmented Generation) with OpenAI embeddings\n    to find the most relevant passages from a large document collection.\n    Embeddings are cached in SQLite for fast subsequent queries.\n    \n    By default, uses diverse search to ensure results come from multiple different\n    documents, not just the most similar chunks from a few documents.\n    \n    Args:\n        query: The search query - what information you're looking for\n        json_path: Path to the long_context.json file containing the documents\n        top_k: Number of top results to return (default: 10)\n        diverse: If True, ensure results come from different documents (default: True)\n        min_docs: Minimum number of different documents to include when diverse=True (default: 5)\n        max_per_doc: Maximum chunks per document when diverse=True (default: 2)\n        \n    Returns:\n        Formatted search results with relevant passages and their sources\n    \nInput JSON schema: {'properties': {'query': {'title': 'Query', 'type': 'string'}, 'json_path': {'title': 'Json Path', 'type': 'string'}, 'top_k': {'default': 10, 'title': 'Top K', 'type': 'integer'}, 'diverse': {'default': True, 'title': 'Diverse', 'type': 'boolean'}, 'min_docs': {'default': 5, 'title': 'Min Docs', 'type': 'integer'}, 'max_per_doc': {'default': 2, 'title': 'Max Per Doc', 'type': 'integer'}}, 'required': ['query', 'json_path'], 'title': 'rag_searchArguments', 'type': 'object'}\n### Tool name: rag_get_context\nDescription: \n    Get relevant context from a long document for answering a specific question.\n    \n    This tool retrieves the most relevant passages that can help answer the query,\n    formatted as context that can be used for further analysis.\n    Each passage includes source information for proper citation.\n    \n    By default, uses diverse search to ensure context comes from multiple different\n    documents, not just the most similar chunks from a few documents.\n    \n    Args:\n        query: The question or topic to find context for\n        json_path: Path to the long_context.json file\n        max_tokens: Maximum approximate tokens of context to return (default: 4000)\n        top_k: Number of documents to consider (default: 10)\n        diverse: If True, ensure results come from different documents (default: True)\n        min_docs: Minimum number of different documents to include when diverse=True (default: 5)\n        max_per_doc: Maximum chunks per document when diverse=True (default: 2)\n        \n    Returns:\n        Concatenated relevant context passages with source information\n    \nInput JSON schema: {'properties': {'query': {'title': 'Query', 'type': 'string'}, 'json_path': {'title': 'Json Path', 'type': 'string'}, 'max_tokens': {'default': 4000, 'title': 'Max Tokens', 'type': 'integer'}, 'top_k': {'default': 10, 'title': 'Top K', 'type': 'integer'}, 'diverse': {'default': True, 'title': 'Diverse', 'type': 'boolean'}, 'min_docs': {'default': 5, 'title': 'Min Docs', 'type': 'integer'}, 'max_per_doc': {'default': 2, 'title': 'Max Per Doc', 'type': 'integer'}}, 'required': ['query', 'json_path'], 'title': 'rag_get_contextArguments', 'type': 'object'}\n### Tool name: rag_document_stats\nDescription: \n    Get statistics about the loaded document collection.\n    \n    Args:\n        json_path: Path to the long_context.json file\n        \n    Returns:\n        Statistics about the document collection (number of documents, chunks, cache info, etc.)\n    \nInput JSON schema: {'properties': {'json_path': {'title': 'Json Path', 'type': 'string'}}, 'required': ['json_path'], 'title': 'rag_document_statsArguments', 'type': 'object'}\n## Server name: agent-browsing\n### Tool name: search_and_browse\nDescription: This tool is an agent that performs the subtask of searching and browsing the web for specific missing information and generating the desired answer. The subtask should be clearly defined, include relevant background, and focus on factual gaps. It does not perform vague or speculative subtasks. \nArgs: \n\tsubtask: the subtask to be performed. \nReturns: \n\tthe result of the subtask. \nInput JSON schema: {'type': 'object', 'properties': {'subtask': {'title': 'Subtask', 'type': 'string'}}, 'required': ['subtask'], 'title': 'search_and_browseArguments'}\n\n# General Objective\n\nYou accomplish a given task iteratively, breaking it down into clear steps and working through them methodically.\n\n## Task Strategy\n\n1. Analyze the user's request and set clear, achievable sub-goals. Prioritize these sub-goals in a logical order.\n2. Start with a concise, numbered, step-by-step plan (e.g., 1., 2., 3.) outlining how you will solve the task before taking any action. Each sub-goal should correspond to a distinct step in your task-solving process.\n3. Work through these sub-goals sequentially. After each step, the user may provide tool-use feedback, reflect on the results and revise your plan if needed. If you encounter new information or challenges, adjust your approach accordingly. Revisit previous steps to ensure earlier sub-goals or clues have not been overlooked.\n4. You have access to a wide range of powerful tools. Use them strategically to accomplish each sub-goal.\n\n**IMPORTANT - Thorough Analysis Requirement**:\n- For complex tasks involving document analysis, literature review, or report generation, you MUST work through **at least 7-8 turns** of analysis before producing the final answer.\n- Do NOT rush to conclusions after only 2-3 turns. Take time to:\n  - Explore different aspects of the source materials\n  - Perform multiple RAG searches with different keywords\n  - Cross-reference information from different sources\n  - Verify and validate findings\n  - Build a comprehensive understanding before synthesizing\n- Each turn should focus on a specific aspect or sub-goal\n- Only produce the final report after thorough multi-turn exploration\n\n## Tool-Use Guidelines\n\n1. Each step must involve a single tool call, unless the task is already solved. \n2. Before each tool call:\n- Briefly summarize and analyze what is currently known.\n- Identify what is missing, uncertain, or unreliable.\n- Be concise; do not repeat the same analysis across steps.\n- Choose the most relevant tool for the current sub-goal, and explain why this tool is necessary at this point.\n- Verify whether all required parameters are either explicitly provided or can be clearly and reasonably inferred from context.\n- Do not guess or use placeholder values for missing inputs.\n- Skip optional parameters unless they are explicitly specified.\n3. All tool queries must include full, self-contained context. Tools do not retain memory between calls. Include all relevant information from earlier steps in each query.\n4. Avoid broad, vague, or speculative queries. Every tool call should aim to retrieve new, actionable information that clearly advances the task.\n5. Even if a tool result does not directly answer the question, extract and summarize any partial information, patterns, constraints, or keywords that can help guide future steps.\n\n## Tool-Use Communication Rules\n\n1. Do not include tool results in your response — the user will provide them.\n2. Do not present the final answer until the entire task is complete.\n3. Do not mention tool names.\n4. Do not engage in unnecessary back-and-forth or end with vague offers of help. Do not end your responses with questions or generic prompts.\n5. Do not use tools that do not exist.\n6. Unless otherwise requested, respond in the same language as the user's message.\n7. If the task does not require tool use, answer the user directly.\n\n\n\n# Agent Specific Objective\n\nYou are a task-solving agent that uses tools step-by-step to answer the user's question. Your goal is to provide complete, accurate and well-reasoned answers using additional tools.\n\n",
    "message_history": [
      {
        "role": "system",
        "content": "In this environment you have access to a set of tools you can use to answer the user's question. \n\nYou only have access to the tools provided below. You can only use one tool per message, and will receive the result of that tool in the user's next response. You use tools step-by-step to accomplish a given task, with each tool-use informed by the result of the previous tool-use. Today is: 2025-12-05\n\n# Tool-Use Formatting Instructions \n\nTool-use is formatted using XML-style tags. The tool-use is enclosed in <use_mcp_tool></use_mcp_tool> and each parameter is similarly enclosed within its own set of tags.\n\nThe Model Context Protocol (MCP) connects to servers that provide additional tools and resources to extend your capabilities. You can use the server's tools via the `use_mcp_tool`.\n\nDescription: \nRequest to use a tool provided by a MCP server. Each MCP server can provide multiple tools with different capabilities. Tools have defined input schemas that specify required and optional parameters.\n\nParameters:\n- server_name: (required) The name of the MCP server providing the tool\n- tool_name: (required) The name of the tool to execute\n- arguments: (required) A JSON object containing the tool's input parameters, following the tool's input schema, quotes within string must be properly escaped, ensure it's valid JSON\n\nUsage:\n<use_mcp_tool>\n<server_name>server name here</server_name>\n<tool_name>tool name here</tool_name>\n<arguments>\n{\n\"param1\": \"value1\",\n\"param2\": \"value2 \\\"escaped string\\\"\"\n}\n</arguments>\n</use_mcp_tool>\n\nImportant Notes:\n- Tool-use must be placed **at the end** of your response, **top-level**, and not nested within other tags.\n- Always adhere to this format for the tool use to ensure proper parsing and execution.\n\nString and scalar parameters should be specified as is, while lists and objects should use JSON format. Note that spaces for string values are not stripped. The output is not expected to be valid XML and is parsed with regular expressions.\nHere are the functions available in JSONSchema format:\n\n## Server name: tool-python\n### Tool name: create_sandbox\nDescription: Create a linux sandbox.\n\nArgs:\n    timeout: Time in seconds before the sandbox is automatically shutdown. The default is 600 seconds.\n\nReturns:\n    The id of the newly created sandbox. You should use this sandbox_id to run other tools in the sandbox.\nInput JSON schema: {'properties': {'timeout': {'default': 600, 'title': 'Timeout'}}, 'type': 'object'}\n### Tool name: run_command\nDescription: Execute a command in the linux sandbox.\n\nArgs:\n    command: The command to execute.\n    sandbox_id: The id of the sandbox to execute the command in. To create a new sandbox, use tool `create_sandbox`.\n\nReturns:\n    A CommandResult object containing the result of the command execution, format like CommandResult(stderr=..., stdout=..., exit_code=..., error=...)\nInput JSON schema: {'properties': {'command': {'title': 'Command', 'type': 'string'}, 'sandbox_id': {'title': 'Sandbox Id', 'type': 'string'}}, 'required': ['command', 'sandbox_id'], 'type': 'object'}\n### Tool name: run_python_code\nDescription: Run python code in an interpreter and return the execution result.\n\nArgs:\n    code_block: The python code to run.\n    sandbox_id: The id of the sandbox to run the code in. Reuse existing sandboxes whenever possible. To create a new sandbox, use tool `create_sandbox`.\n\nReturns:\n    A CommandResult object containing the result of the command execution, format like CommandResult(stderr=..., stdout=..., exit_code=..., error=...)\nInput JSON schema: {'properties': {'code_block': {'title': 'Code Block', 'type': 'string'}, 'sandbox_id': {'title': 'Sandbox Id', 'type': 'string'}}, 'required': ['code_block', 'sandbox_id'], 'type': 'object'}\n### Tool name: upload_file_from_local_to_sandbox\nDescription: Upload a local file to the `/home/user` dir of the remote python interpreter.\n\nArgs:\n    sandbox_id: The id of the sandbox to run the code in. Reuse existing sandboxes whenever possible. To create a new sandbox, use tool `create_sandbox`.\n    local_file_path: The path of the file on local machine to upload.\n    sandbox_file_path: The path of directory to upload the file to in the sandbox. Default is `/home/user/`.\n\nReturns:\n    The path of the uploaded file in the remote python interpreter if the upload is successful.\nInput JSON schema: {'properties': {'sandbox_id': {'title': 'Sandbox Id', 'type': 'string'}, 'local_file_path': {'title': 'Local File Path', 'type': 'string'}, 'sandbox_file_path': {'default': '/home/user', 'title': 'Sandbox File Path', 'type': 'string'}}, 'required': ['sandbox_id', 'local_file_path'], 'type': 'object'}\n### Tool name: download_file_from_internet_to_sandbox\nDescription: Download a file from the internet to the `/home/user` dir of the remote python interpreter.\n\nArgs:\n    sandbox_id: The id of the sandbox to run the code in. Reuse existing sandboxes whenever possible. To create a new sandbox, use tool `create_sandbox`.\n    url: The URL of the file to download.\n    sandbox_file_path: The path of directory to download the file to in the sandbox. Default is `/home/user/`.\n\nReturns:\n    The path of the downloaded file in the python interpreter if the download is successful.\nInput JSON schema: {'properties': {'sandbox_id': {'title': 'Sandbox Id', 'type': 'string'}, 'url': {'title': 'Url', 'type': 'string'}, 'sandbox_file_path': {'default': '/home/user', 'title': 'Sandbox File Path', 'type': 'string'}}, 'required': ['sandbox_id', 'url'], 'type': 'object'}\n### Tool name: download_file_from_sandbox_to_local\nDescription: Download a file from the sandbox to local system. Files in sandbox cannot be processed by tools from other servers - only local files and internet URLs can be processed by them.\n\nArgs:\n    sandbox_id: The id of the sandbox to download the file from. To have a sandbox, use tool `create_sandbox`.\n    sandbox_file_path: The path of the file to download on the sandbox.\n    local_filename: Optional filename to save as. If not provided, uses the original filename from sandbox_file_path.\n\nReturns:\n    The local path of the downloaded file if successful, otherwise error message.\nInput JSON schema: {'properties': {'sandbox_id': {'title': 'Sandbox Id', 'type': 'string'}, 'sandbox_file_path': {'title': 'Sandbox File Path', 'type': 'string'}, 'local_filename': {'default': None, 'title': 'Local Filename', 'type': 'string'}}, 'required': ['sandbox_id', 'sandbox_file_path'], 'type': 'object'}\n## Server name: tool-vqa\n### Tool name: visual_question_answering\nDescription: Ask question about an image or a video and get the answer with a vision language model.\n\nArgs:\n    image_path_or_url: The path of the image file locally or its URL.\n    question: The question to ask about the image.\n\nReturns:\n    The answer to the image-related question.\nInput JSON schema: {'properties': {'image_path_or_url': {'title': 'Image Path Or Url', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}}, 'required': ['image_path_or_url', 'question'], 'type': 'object'}\n## Server name: tool-vqa-enhanced\n### Tool name: vision_understanding_advanced\nDescription: Advanced vision understanding with multi-turn verification and confidence scoring.\n\nThis tool provides enhanced image understanding capabilities:\n- Multi-turn verification for higher accuracy\n- Confidence scoring for reliability assessment\n- Metadata extraction (objects, colors, text, scene)\n- Optional cross-validation with web search\n- Support for multiple VLLM providers\n\nArgs:\n    image_path_or_url: Local file path (e.g., 'data/image.jpg') or URL (e.g., 'https://...')\n    question: The question to ask about the image\n    enable_verification: Whether to perform multi-turn verification (default: true)\n\nReturns:\n    JSON string with:\n    - answer: Direct answer to the question\n    - confidence: Confidence score (0.0-1.0)\n    - metadata: Extracted visual information\n    - reasoning: Reasoning behind the answer\n    - follow_up_answers: Optional answers to verification questions\n    - verification_sources: Optional web search verification\n\nExample:\n    Input:\n        image_path_or_url: \"data/character.jpg\"\n        question: \"Who is this character and what anime are they from?\"\n        enable_verification: true\n    \n    Output (JSON):\n    {\n        \"answer\": \"This is Bocchi from the anime 'Bocchi the Rock!'\",\n        \"confidence\": 0.92,\n        \"metadata\": {\n            \"objects_detected\": [\"character\", \"guitar\", \"pink hair\"],\n            \"colors_dominant\": [\"pink\", \"black\", \"white\"],\n            \"scene_description\": \"A teenage girl with pink hair holding a guitar\"\n        },\n        \"reasoning\": \"The character has distinctive pink hair and is holding a guitar, which are signature characteristics of Bocchi from the anime 'Bocchi the Rock!'\",\n        \"follow_up_answers\": [\n            {\n                \"question\": \"What specific visual features support this identification?\",\n                \"answer\": \"The pink hair, guitar, and art style are distinctive to this character\",\n                \"confidence\": 0.88\n            }\n        ],\n        \"verified\": true,\n        \"verification_sources\": {\n            \"search_query\": \"Bocchi Rock anime character\",\n            \"results\": [...]\n        }\n    }\nInput JSON schema: {'properties': {'image_path_or_url': {'title': 'Image Path Or Url', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}, 'enable_verification': {'default': True, 'title': 'Enable Verification', 'type': 'boolean'}}, 'required': ['image_path_or_url', 'question'], 'type': 'object'}\n### Tool name: vision_quick_answer\nDescription: Quick vision question answering without multi-turn verification.\n\nUse this for simpler questions that don't require extensive verification.\nFaster response time compared to vision_understanding_advanced.\n\nArgs:\n    image_path_or_url: Local file path or URL of the image\n    question: Question about the image\n\nReturns:\n    JSON with answer, confidence, and basic metadata\nInput JSON schema: {'properties': {'image_path_or_url': {'title': 'Image Path Or Url', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}}, 'required': ['image_path_or_url', 'question'], 'type': 'object'}\n### Tool name: vision_extract_metadata\nDescription: Extract visual metadata from an image without answering a specific question.\n\nUseful for understanding what's in an image before formulating questions.\n\nArgs:\n    image_path_or_url: Local file path or URL of the image\n\nReturns:\n    JSON with extracted visual information:\n    - objects detected\n    - dominant colors\n    - visible text\n    - scene description\n    - estimated confidence of observations\nInput JSON schema: {'properties': {'image_path_or_url': {'title': 'Image Path Or Url', 'type': 'string'}}, 'required': ['image_path_or_url'], 'type': 'object'}\n### Tool name: vision_comparative_analysis\nDescription: Compare two images to answer a question about their relationship.\n\nUseful for verification tasks (e.g., \"Are these the same character?\")\n\nArgs:\n    image_path_or_url_1: First image path or URL\n    image_path_or_url_2: Second image path or URL\n    comparison_question: Question comparing the two images\n\nReturns:\n    JSON with comparative analysis\nInput JSON schema: {'properties': {'image_path_or_url_1': {'title': 'Image Path Or Url 1', 'type': 'string'}, 'image_path_or_url_2': {'title': 'Image Path Or Url 2', 'type': 'string'}, 'comparison_question': {'title': 'Comparison Question', 'type': 'string'}}, 'required': ['image_path_or_url_1', 'image_path_or_url_2', 'comparison_question'], 'type': 'object'}\n## Server name: tool-transcribe\n### Tool name: audio_transcription\nDescription: Transcribe audio file to text and return the transcription.\nArgs:\n    audio_path_or_url: The path of the audio file locally or its URL. Path from sandbox is not supported. YouTube URL is not supported.\n\nReturns:\n    The transcription of the audio file.\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}}, 'required': ['audio_path_or_url'], 'type': 'object'}\n### Tool name: audio_question_answering\nDescription: Answer the question based on the given audio information.\n\nArgs:\n    audio_path_or_url: The path of the audio file locally or its URL. Path from sandbox is not supported. YouTube URL is not supported.\n    question: The question to answer.\n\nReturns:\n    The answer to the question, and the duration of the audio file.\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}}, 'required': ['audio_path_or_url', 'question'], 'type': 'object'}\n## Server name: tool-audio-enhanced\n### Tool name: audio_understanding_advanced\nDescription: Advanced audio understanding with multi-turn verification and confidence scoring.\n\nThis tool provides:\n- High-quality transcription with confidence scores\n- Optional question answering about audio content\n- Multi-turn verification for accuracy (3 follow-up checks)\n- Audio metadata extraction (duration, sample rate, etc.)\n- Web search validation (if enabled)\n\nArgs:\n    audio_path_or_url: Local path or URL to audio file (mp3, wav, m4a, etc.)\n    question: Optional question to answer about the audio content\n    enable_verification: Whether to perform multi-turn verification (default: True)\n                        Set to False for faster processing when high accuracy is not critical\n\nReturns:\n    JSON string with:\n    {\n        \"transcript\": \"Full transcription text\",\n        \"answer\": \"Answer to question (if provided)\",\n        \"confidence\": 0.85,  # 0.0-1.0 score\n        \"metadata\": {\n            \"duration_seconds\": 120.5,\n            \"sample_rate\": 44100,\n            \"file_size_bytes\": 1024000\n        },\n        \"reasoning\": \"Explanation of confidence score\",\n        \"verification_segments\": [\n            {\n                \"segment_id\": 1,\n                \"question\": \"What are the main topics?\",\n                \"answer\": \"...\",\n                \"confidence\": 0.9\n            }\n        ],\n        \"verification_sources\": [\n            {\"title\": \"...\", \"url\": \"...\", \"snippet\": \"...\"}\n        ]\n    }\n\nUsage recommendations:\n- Use enable_verification=true for critical transcriptions (e.g., interviews, lectures)\n- Check confidence score: >= 0.7 is good, < 0.4 may need manual review\n- If confidence is low, consider re-recording or using different audio quality\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}, 'question': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Question'}, 'enable_verification': {'default': True, 'title': 'Enable Verification', 'type': 'boolean'}}, 'required': ['audio_path_or_url'], 'type': 'object'}\n### Tool name: audio_quick_transcription\nDescription: Fast single-pass audio transcription without verification.\n\nUse this tool when:\n- Speed is more important than perfect accuracy\n- Audio quality is known to be good\n- Content is not mission-critical\n\nArgs:\n    audio_path_or_url: Local path or URL to audio file\n\nReturns:\n    JSON string with:\n    {\n        \"transcript\": \"Transcribed text\",\n        \"confidence\": 0.75,  # Base confidence without verification\n        \"metadata\": {...}\n    }\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}}, 'required': ['audio_path_or_url'], 'type': 'object'}\n### Tool name: audio_extract_metadata\nDescription: Extract audio metadata without transcription.\n\nUse this tool to get audio file information:\n- Duration in seconds\n- Sample rate (Hz)\n- Number of channels\n- File size\n- Format/codec information (if available)\n\nArgs:\n    audio_path_or_url: Local path or URL to audio file\n\nReturns:\n    JSON string with metadata:\n    {\n        \"duration_seconds\": 120.5,\n        \"sample_rate\": 44100,\n        \"channels\": 2,\n        \"file_size_bytes\": 1024000,\n        \"format\": \"mp3\"\n    }\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}}, 'required': ['audio_path_or_url'], 'type': 'object'}\n### Tool name: audio_question_answering_enhanced\nDescription: Answer questions about audio content with confidence scoring.\n\nThis tool:\n1. Analyzes the audio to understand content\n2. Answers the specific question\n3. Provides confidence score\n4. Includes reasoning for the answer\n\nUse this tool when:\n- You need to extract specific information from audio\n- Understanding context and nuance is important\n- You want confidence assessment of the answer\n\nArgs:\n    audio_path_or_url: Local path or URL to audio file\n    question: Specific question about the audio content\n             Examples:\n             - \"Who is the speaker?\"\n             - \"What is the main topic discussed?\"\n             - \"Are there any specific dates or numbers mentioned?\"\n             - \"What is the speaker's emotional tone?\"\n\nReturns:\n    JSON string with:\n    {\n        \"question\": \"Your question\",\n        \"answer\": \"Detailed answer based on audio\",\n        \"confidence\": 0.85,\n        \"reasoning\": \"Why this confidence score\",\n        \"transcript_excerpt\": \"Relevant portion of transcript supporting the answer\",\n        \"metadata\": {...}\n    }\nInput JSON schema: {'properties': {'audio_path_or_url': {'title': 'Audio Path Or Url', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}}, 'required': ['audio_path_or_url', 'question'], 'type': 'object'}\n## Server name: tool-video-enhanced\n### Tool name: video_understanding_advanced\nDescription: Advanced video understanding with multi-turn verification.\n\nUse this for complex video analysis requiring high accuracy. Includes:\n- Detailed scene analysis with temporal understanding\n- Multi-turn verification of key aspects (actions, objects, sequence)\n- Confidence scoring based on consistency\n- Optional web search validation\n- Key moments extraction with timestamps\n\nArgs:\n    video_path: Path to video file (local path or URL)\n    question: Optional specific question about the video. If not provided, performs general analysis.\n    enable_verification: Whether to enable multi-turn verification (default: True). \n                       Improves accuracy but takes longer.\n\nReturns:\n    JSON string with:\n    - description: Overall video description\n    - answer: Answer to the question (or description if no question)\n    - confidence: 0.0-1.0 confidence score\n    - metadata: Video features (duration, resolution, fps, key_moments)\n    - reasoning: Explanation of confidence\n    - verification_results: Aspect verifications performed\n\nExample:\n    >>> result = await video_understanding_advanced(\n    ...     video_path=\"/path/to/video.mp4\",\n    ...     question=\"What sport is being played in this video?\"\n    ... )\n\nBest for:\n- Detailed action recognition\n- Scene understanding with temporal context\n- Multi-object tracking\n- Event sequence analysis\nInput JSON schema: {'properties': {'video_path': {'title': 'Video Path', 'type': 'string'}, 'question': {'default': None, 'title': 'Question', 'type': 'string'}, 'enable_verification': {'default': True, 'title': 'Enable Verification', 'type': 'boolean'}}, 'required': ['video_path'], 'type': 'object'}\n### Tool name: video_quick_analysis\nDescription: Quick video analysis without multi-turn verification.\n\nFast single-pass analysis. Use when:\n- Speed is more important than maximum accuracy\n- Video content is straightforward\n- You need a quick overview\n\nArgs:\n    video_path: Path to video file (local path or URL)\n    question: Optional question about the video\n\nReturns:\n    JSON string with:\n    - description: Video description\n    - answer: Answer to question (if provided)\n    - confidence: Initial confidence (typically 0.6-0.8)\n    - metadata: Basic video info (duration, resolution, fps)\n\nExample:\n    >>> result = await video_quick_analysis(\n    ...     video_path=\"https://example.com/video.mp4\",\n    ...     question=\"Is this video indoors or outdoors?\"\n    ... )\n\nBest for:\n- Quick previews\n- Simple yes/no questions\n- Initial exploration\nInput JSON schema: {'properties': {'video_path': {'title': 'Video Path', 'type': 'string'}, 'question': {'default': None, 'title': 'Question', 'type': 'string'}}, 'required': ['video_path'], 'type': 'object'}\n### Tool name: video_extract_keyframes\nDescription: Extract key moments and metadata from video without deep analysis.\n\nProvides technical information and structure without detailed understanding:\n- Duration, resolution, fps\n- Key moments identification (scene changes, important frames)\n- Temporal structure\n\nArgs:\n    video_path: Path to video file (local path or URL)\n    max_keyframes: Maximum number of key moments to identify (default: 10)\n\nReturns:\n    JSON string with:\n    - metadata: Video technical specs\n    - key_moments: List of timestamps with brief descriptions\n    - duration_seconds: Total video length\n    - resolution: Video dimensions\n    - fps: Frames per second\n\nExample:\n    >>> result = await video_extract_keyframes(\n    ...     video_path=\"/videos/lecture.mp4\",\n    ...     max_keyframes=5\n    ... )\n\nBest for:\n- Checking video properties before analysis\n- Finding important timestamps\n- Video preprocessing\n- Quality validation\nInput JSON schema: {'properties': {'video_path': {'title': 'Video Path', 'type': 'string'}, 'max_keyframes': {'default': 10, 'title': 'Max Keyframes', 'type': 'integer'}}, 'required': ['video_path'], 'type': 'object'}\n### Tool name: video_temporal_qa\nDescription: Answer questions about specific time ranges in the video.\n\nFocuses analysis on a particular temporal segment. Useful for:\n- Analyzing specific scenes\n- Understanding action sequences in a timeframe\n- Detailed examination of video segments\n\nArgs:\n    video_path: Path to video file (local path or URL)\n    question: Question about the video or time range\n    start_time: Start time in seconds (optional, analyzes from beginning if not specified)\n    end_time: End time in seconds (optional, analyzes until end if not specified)\n    enable_verification: Whether to use multi-turn verification (default: True)\n\nReturns:\n    JSON string with:\n    - answer: Answer specific to the time range\n    - confidence: 0.0-1.0 confidence score\n    - time_range: The analyzed time segment\n    - metadata: Video features for the segment\n\nExample:\n    >>> result = await video_temporal_qa(\n    ...     video_path=\"/path/to/game.mp4\",\n    ...     question=\"What happens in this segment?\",\n    ...     start_time=30.0,\n    ...     end_time=60.0\n    ... )\n\nBest for:\n- Scene-specific questions\n- Action analysis in time windows\n- Segment-by-segment understanding\n- Detailed temporal reasoning\nInput JSON schema: {'properties': {'video_path': {'title': 'Video Path', 'type': 'string'}, 'question': {'title': 'Question', 'type': 'string'}, 'start_time': {'default': None, 'title': 'Start Time', 'type': 'number'}, 'end_time': {'default': None, 'title': 'End Time', 'type': 'number'}, 'enable_verification': {'default': True, 'title': 'Enable Verification', 'type': 'boolean'}}, 'required': ['video_path', 'question'], 'type': 'object'}\n## Server name: tool-reasoning\n### Tool name: reasoning\nDescription: You can use this tool to solve hard math problem, puzzle, riddle and IQ test question that requires a lot of chain of thought efforts.\nDO NOT use this tool for simple and obvious question.\n\nArgs:\n    question: The hard question.\n\nReturns:\n    The answer to the question.\nInput JSON schema: {'properties': {'question': {'title': 'Question', 'type': 'string'}}, 'required': ['question'], 'type': 'object'}\n## Server name: tool-reader\n### Tool name: convert_to_markdown\nDescription: Convert a resource described by an http:, https:, file: or data: URI to markdown\nInput JSON schema: {'properties': {'uri': {'title': 'Uri', 'type': 'string'}}, 'required': ['uri'], 'title': 'convert_to_markdownArguments', 'type': 'object'}\n## Server name: tool-rag\n### Tool name: rag_search\nDescription: \n    Search for relevant information in a long context document using semantic search.\n    \n    This tool uses RAG (Retrieval-Augmented Generation) with OpenAI embeddings\n    to find the most relevant passages from a large document collection.\n    Embeddings are cached in SQLite for fast subsequent queries.\n    \n    By default, uses diverse search to ensure results come from multiple different\n    documents, not just the most similar chunks from a few documents.\n    \n    Args:\n        query: The search query - what information you're looking for\n        json_path: Path to the long_context.json file containing the documents\n        top_k: Number of top results to return (default: 10)\n        diverse: If True, ensure results come from different documents (default: True)\n        min_docs: Minimum number of different documents to include when diverse=True (default: 5)\n        max_per_doc: Maximum chunks per document when diverse=True (default: 2)\n        \n    Returns:\n        Formatted search results with relevant passages and their sources\n    \nInput JSON schema: {'properties': {'query': {'title': 'Query', 'type': 'string'}, 'json_path': {'title': 'Json Path', 'type': 'string'}, 'top_k': {'default': 10, 'title': 'Top K', 'type': 'integer'}, 'diverse': {'default': True, 'title': 'Diverse', 'type': 'boolean'}, 'min_docs': {'default': 5, 'title': 'Min Docs', 'type': 'integer'}, 'max_per_doc': {'default': 2, 'title': 'Max Per Doc', 'type': 'integer'}}, 'required': ['query', 'json_path'], 'title': 'rag_searchArguments', 'type': 'object'}\n### Tool name: rag_get_context\nDescription: \n    Get relevant context from a long document for answering a specific question.\n    \n    This tool retrieves the most relevant passages that can help answer the query,\n    formatted as context that can be used for further analysis.\n    Each passage includes source information for proper citation.\n    \n    By default, uses diverse search to ensure context comes from multiple different\n    documents, not just the most similar chunks from a few documents.\n    \n    Args:\n        query: The question or topic to find context for\n        json_path: Path to the long_context.json file\n        max_tokens: Maximum approximate tokens of context to return (default: 4000)\n        top_k: Number of documents to consider (default: 10)\n        diverse: If True, ensure results come from different documents (default: True)\n        min_docs: Minimum number of different documents to include when diverse=True (default: 5)\n        max_per_doc: Maximum chunks per document when diverse=True (default: 2)\n        \n    Returns:\n        Concatenated relevant context passages with source information\n    \nInput JSON schema: {'properties': {'query': {'title': 'Query', 'type': 'string'}, 'json_path': {'title': 'Json Path', 'type': 'string'}, 'max_tokens': {'default': 4000, 'title': 'Max Tokens', 'type': 'integer'}, 'top_k': {'default': 10, 'title': 'Top K', 'type': 'integer'}, 'diverse': {'default': True, 'title': 'Diverse', 'type': 'boolean'}, 'min_docs': {'default': 5, 'title': 'Min Docs', 'type': 'integer'}, 'max_per_doc': {'default': 2, 'title': 'Max Per Doc', 'type': 'integer'}}, 'required': ['query', 'json_path'], 'title': 'rag_get_contextArguments', 'type': 'object'}\n### Tool name: rag_document_stats\nDescription: \n    Get statistics about the loaded document collection.\n    \n    Args:\n        json_path: Path to the long_context.json file\n        \n    Returns:\n        Statistics about the document collection (number of documents, chunks, cache info, etc.)\n    \nInput JSON schema: {'properties': {'json_path': {'title': 'Json Path', 'type': 'string'}}, 'required': ['json_path'], 'title': 'rag_document_statsArguments', 'type': 'object'}\n## Server name: agent-browsing\n### Tool name: search_and_browse\nDescription: This tool is an agent that performs the subtask of searching and browsing the web for specific missing information and generating the desired answer. The subtask should be clearly defined, include relevant background, and focus on factual gaps. It does not perform vague or speculative subtasks. \nArgs: \n\tsubtask: the subtask to be performed. \nReturns: \n\tthe result of the subtask. \nInput JSON schema: {'type': 'object', 'properties': {'subtask': {'title': 'Subtask', 'type': 'string'}}, 'required': ['subtask'], 'title': 'search_and_browseArguments'}\n\n# General Objective\n\nYou accomplish a given task iteratively, breaking it down into clear steps and working through them methodically.\n\n## Task Strategy\n\n1. Analyze the user's request and set clear, achievable sub-goals. Prioritize these sub-goals in a logical order.\n2. Start with a concise, numbered, step-by-step plan (e.g., 1., 2., 3.) outlining how you will solve the task before taking any action. Each sub-goal should correspond to a distinct step in your task-solving process.\n3. Work through these sub-goals sequentially. After each step, the user may provide tool-use feedback, reflect on the results and revise your plan if needed. If you encounter new information or challenges, adjust your approach accordingly. Revisit previous steps to ensure earlier sub-goals or clues have not been overlooked.\n4. You have access to a wide range of powerful tools. Use them strategically to accomplish each sub-goal.\n\n**IMPORTANT - Thorough Analysis Requirement**:\n- For complex tasks involving document analysis, literature review, or report generation, you MUST work through **at least 7-8 turns** of analysis before producing the final answer.\n- Do NOT rush to conclusions after only 2-3 turns. Take time to:\n  - Explore different aspects of the source materials\n  - Perform multiple RAG searches with different keywords\n  - Cross-reference information from different sources\n  - Verify and validate findings\n  - Build a comprehensive understanding before synthesizing\n- Each turn should focus on a specific aspect or sub-goal\n- Only produce the final report after thorough multi-turn exploration\n\n## Tool-Use Guidelines\n\n1. Each step must involve a single tool call, unless the task is already solved. \n2. Before each tool call:\n- Briefly summarize and analyze what is currently known.\n- Identify what is missing, uncertain, or unreliable.\n- Be concise; do not repeat the same analysis across steps.\n- Choose the most relevant tool for the current sub-goal, and explain why this tool is necessary at this point.\n- Verify whether all required parameters are either explicitly provided or can be clearly and reasonably inferred from context.\n- Do not guess or use placeholder values for missing inputs.\n- Skip optional parameters unless they are explicitly specified.\n3. All tool queries must include full, self-contained context. Tools do not retain memory between calls. Include all relevant information from earlier steps in each query.\n4. Avoid broad, vague, or speculative queries. Every tool call should aim to retrieve new, actionable information that clearly advances the task.\n5. Even if a tool result does not directly answer the question, extract and summarize any partial information, patterns, constraints, or keywords that can help guide future steps.\n\n## Tool-Use Communication Rules\n\n1. Do not include tool results in your response — the user will provide them.\n2. Do not present the final answer until the entire task is complete.\n3. Do not mention tool names.\n4. Do not engage in unnecessary back-and-forth or end with vague offers of help. Do not end your responses with questions or generic prompts.\n5. Do not use tools that do not exist.\n6. Unless otherwise requested, respond in the same language as the user's message.\n7. If the task does not require tool use, answer the user directly.\n\n\n\n# Agent Specific Objective\n\nYou are a task-solving agent that uses tools step-by-step to answer the user's question. Your goal is to provide complete, accurate and well-reasoned answers using additional tools.\n\n"
      },
      {
        "role": "user",
        "content": "# Task\n\n假设你是一名科研工作者，正在为某人工智能研究团队开展大模型评测基准的系统性梳理工作。请根据提供的材料，撰写一份《Deep Research相关Benchmark数据集调研报告》。\n具体而言，依据图片image0中所示的字段要求，提取并结构化提供的pdf论文在相关工作部分提到的每篇论文对应的Benchmark特征，其中请忽略“数据集example”指标。\n报告需整合文献内容与公开可查的权威技术资料（如论文原文、官方GitHub仓库、学术数据库或评测平台），\n确保所有信息真实、准确、可验证，并在文中注明数据来源。\n报告语言应采用正式书面语，结构清晰，包含引言、调研方法、各论文Benchmark信息汇总表、分析与观察、参考资料等部分，\n最终形成一份可用于团队技术决策的标准化调研文档，字数在1500至2000之间。\n\n\n## Folder Contents Summary\n\nFolder: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001\nTotal files: 12\n  - data: 5\n  - document: 1\n  - image: 1\n  - other: 5\n\n\n## Document Contents\n\n\n### paper.pdf\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\">\n5\n2\n0\n2\n\np\ne\nS\n0\n3\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n2\n7\n1\n0\n0\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nDRBench: A REALISTIC\n\nBENCHMARK FOR ENTERPRISE DEEP RESEARCH\n\nAmirhossein Abaskohi1,2 Tianyi Chen1 Miguel Mu˜noz-M´armol1 Curtis Fox1,2\nAmrutha Varshini Ramesh1,2 ´Etienne Marcotte1 Xing Han L`u3,4 Nicolas Chapados1\n\nSpandana Gella1,3 Christopher Pal1,3,5 Alexandre Drouin1,3\n\nIssam H. Laradji1,2\n\n1ServiceNow Research\n\n2University of British Columbia\n\n3Mila – Quebec AI Institute\n\n4McGill University\n\n5Canada CIFAR AI Chair\n\nABSTRACT\n\nWe introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended\ndeep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple\nquestions or web-only queries, DRBench evaluates agents on multi-step queries (for\nexample, “What changes should we make to our product roadmap to ensure compliance\nwith this standard?”) that require identifying supporting facts from both the public web\nand private company knowledge base. Each task is grounded in realistic user personas\nand enterprise context, spanning a heterogeneous search space that includes productivity\nsoftware, cloud file systems, emails, chat conversations, and the open web. Tasks\nare generated through a carefully designed synthesis pipeline with human-in-the-loop\nverification, and agents are evaluated on their ability to recall relevant insights, maintain\nfactual accuracy, and produce coherent, well-structured reports. We release 15 deep\nresearch tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We\ndemonstrate the effectiveness of DRBench by evaluating diverse DR agents across open-\nand closed-source models (such as GPT, Llama, and Qwen) and DR strategies, high-\nlighting their strengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.\n\n1\n\nINTRODUCTION\n\nOrganizations today face a strong need to find useful insights in a world full of overwhelming information.\nValuable insights are often hidden in noisy data, which can contain many distracting or irrelevant details that\nobscure the insights that really matter. This challenge is present in enterprise settings, where data is spread\nacross many applications and stored in different formats (e.g., PDFs, spreadsheets, emails, and internal tools)\nmaking extracting relevant information difficult. To uncover these hidden, valuable insights, one must con-\nduct what is known as deep research. This task involves asking high-level strategic questions (e.g, “What\nchanges should we make to our roadmap to remain compliant?”), planning sub-questions, retrieving and\nevaluating relevant materials, and producing a clear, actionable summary grounded in data sources (Zheng\net al., 2025; Xu & Peng, 2025; Du et al., 2025). These tasks are typically performed by domain experts\nusing a mix of search engines, communication platforms, and business applications in iterative, high-effort\nworkflows (Mialon et al., 2024), which unfortunately require a significant amount of human effort.\n\nOne promising solution to reducing this human effort is agent-based deep research, which uses autonomous\nsoftware agents to search, extract, and synthesize information across fragmented sources into an\ninsightful report. Recently, LLM-based agents have emerged as promising assistants for deep research.\nSystems such as Local Deep Researcher (LearningCircuit, 2025), Deep-Searcher (Tech, 2024), and\nDeepResearcher (Zheng et al., 2025) propose modular agent pipelines that combine retrieval, reasoning,\nand summarization over documents and web sources. Architectures like OpenHands (All-HandsAI, 2024),\nOpenManus (FoundationAgents, 2024), and smolagents (HuggingFace, 2024) extend these capabilities\nto include collaboration, multi-modal search, and complex tool use in enterprise workflows (Xu & Peng,\n2025). Despite these advances, evaluating such systems remains an open challenge.\n\nMost existing benchmarks evaluate narrow aspects such as report factuality (Coelho et al., 2025), web-only\nsynthesis (Bosse et al., 2025), or tabular analytics (Sahu et al., 2025), but they do not assess whether agents\nidentify the most salient insights, remain faithful to retrieved evidence, or adapt to enterprise contexts. To\naddress these limitations, we introduce DRBench, a benchmark designed to evaluate LLM agents on open-\n\n1\n\n \n \n \n \n \n \n\fFigure 1: DRBench pipeline. 1 The Task Context defines the deep research question grounded by the\ncompany and persona given to the agent. 2 Task Data, including both distractor and injected groundtruth\ninsights in different formats (PDFs, DOCX, PPTX, XLSX, chats, etc.) are loaded into the enterprise\nenvironment’s applications. 3 The DRBenchAgent accesses both public web sources and local enterprise\ndata to extract relevant insights for the research question. 4 It produces a structured research report,\nwhich is 5 evaluated for Insight Recall (detecting injected groundtruth insights), Factuality (verifying\nclaims are correctly cited), and Report Quality.\n\nended, multi-step and long-horizon deep research tasks grounded in realistic enterprise contexts. As Fig. 1 il-\nlustrates, DRBench includes a suite of queries grounded in user personas and organizational scenarios, requir-\ning agents to search across real applications such as cloud file storage (Nextcloud), enterprise chat (Matter-\nmost), and user file systems, and to reason over formats like spreadsheets, slide decks, and PDFs. Our evalu-\nation framework introduces three scoring axes using LLM-as-a-judge methods inspired by G-Eval (Liu et al.,\n2023): (1) Insight Recall and Distractor Avoidance, which together evaluate whether the agent surfaces the\nmost salient injected insights while avoiding distractor content; (2) Factuality, which uses a TREC-RAG\npipeline (Wang et al., 2024) to verify whether claims are correctly grounded in their cited sources; and (3) Re-\nport Quality, which measures the coherence, completeness, and overall readability of the synthesized report.\n\nWe conduct a comparative study of agent architectures inspired by recent work (Zheng et al., 2025;\nXu & Peng, 2025; LearningCircuit, 2025; Zheng et al., 2025), analyzing how well they perform on\nDRBench across planning, insight identification, and grounding on facts. Our results show that while\nagents are competent at document retrieval and summarization, they often miss high-value insights, cite\nirrelevant evidence, or produce incoherent explanations, highlighting the limitations of current architectures\nand the need for more targeted innovation.\n\nOur contributions are as follows: (1) We introduce DRBench, the first benchmark for evaluating LLM\nagents on complex enterprise deep research tasks combining public web sources with private organizational\ndata; (2) We provide a suite of 15 high-level research tasks with 114 sub-questions spanning 10 domains,\nincluding Sales, Cybersecurity, and Compliance, each grounded in realistic company contexts and personas;\n(3) We design a reproducible enterprise environment integrating realistic enterprise applications like chat,\ncloud storage, emails, and documents; (4) We propose a scalable pipeline that generates realistic research\nquestions and insights by combining web facts with synthesized internal data; and (5) We develop an\nevaluation framework that scores agent reports on insight recall and distractor avoidance, factuality, and\noverall report quality.\n\n2 RELATED WORK\n\nDeep Research Benchmarks. With the growing capabilities of LLMs in research and reasoning tasks,\nseveral benchmarks have emerged to evaluate their performance in realistic, multi-step and long-horizon sce-\nnarios, including Deep Research Bench (Bosse et al., 2025), DeepResearch Bench (Du et al., 2025), Deep-\nResearchGym (Coelho et al., 2025), ResearcherBench (Xu et al., 2025b), Mind2Web2 (Gou et al., 2025)\nand GAIA (Mialon et al., 2024). As summarized in Table 1, these efforts typically focus on web-only tasks,\n\n2\n\nLoading to Environment2Load Ground Truth Insights Factuality Insight RecallReport QualityIs each claim in the reportproperly cited?Were the injected insightsdiscovered? Is the report clear to read andcomplete in addressing the points?5PersonaCompanyDR QuestionLee's Market: a regional Asian supermarket chainJohn Doe: Regulatory Aﬀairs ManagerNextCloudFileShareFile SystemLocal FilesMattermostChatRoundCubeEmailEnterprise Data3ReportGeneration41Context5(a) Tracking lot codes helps grocers reduce food waste and keep products fresh. [Cite: grocerydive.com](b) Lee's Market tracks 250 high- risk food products as of Q3 2024, aﬀecting 30% of inventory. [Cite: food- safety- compliance.pdf] ...(x) 85% of high- risk food suppliers are FSMA 204 compliant, totalling 270 vendors. [Cite: RoundCube email]EvaluationTask ContextHow can Lee's Market leverage FSMA 204 regulations to enhance food safety and customer trust?3Public DataReportAccessingWebsitesChat ConversationsPresentations:PPTXTask DataTabular Data:CSV, XLSXDocuments: Docx, PDF, MarkdownE- mailsI) 8% reduction in food waste during Q2 2024, saving $1.2 million....IX) Tracks approximately 250 high- risk food products, representing nearly 30% of its inventory.Injected InsightsSynthesized Files with Distractor Content and Injected InsightsLoad Sources5\fTable 1: Comparison of deep research benchmarks (top) and AI agent benchmarks with a computer environ-\nment (middle). Columns report dataset size, whether both public and local data are required, the provided\nenvironment type, task domains, task description, and evaluation method. Unlike prior work, DRBench com-\nbines public web retrieval with local enterprise data in realistic enterprise applications and evaluates both\ninsight recall, distractor avoidance and report quality. Task Description: types of tasks covered by the\nbenchmark: WR for Web Research, DR for Deep Research with both public and local data, CU for\nComputer Use and/or Mobile Use. DRBench has 114 total # groundtruth insights that need to be extracted\nto address the 15 DR Questions. Example groundtruth insights can be found at Table 8 in Appendix A.\n\nBenchmark\n\n# groundtruth\n\nPublic &\nLocal Data\n\nProvides\nEnv\n\nTask\nDomain\n\nTask\nDescription\n\nMain Evaluation Method\n\nDeep Research Bench (Bosse et al., 2025)\nDeepResearch Bench (Du et al., 2025)\nDeepResearchGym (Coelho et al., 2025)\nResearcherBench (Xu et al., 2025b)\nLiveDRBench (Java et al., 2025)\nBrowseComp-Plus (Chen et al., 2025)\nMind2Web 2 (Gou et al., 2025)\nGAIA (Mialon et al., 2024)\nGAIA2 (Andrews et al., 2025)\n\nTheAgentCompany (Xu et al., 2025a)\nOSWorld (Xie et al., 2024)\n\nDRBench\n\n89\n100\n1,000\n65\n100\n1,005\n130\n466\n963\n\n175\n369\n\n114\n\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n\n✗\n✗\n\n✓\n\n✓\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n\n✓\n✓\n\n✓\n\nWR\nWR\nWR\n\nGeneric WR & CU Answer Accuracy\nGeneric\nGeneric\nAI\n\nInsight Recall\nDocument Retrieval\nInsight Recall, Factuality\nGeneric WR & CU Insight Precision, Recall\nGeneric\nGeneric\nGeneric\nGeneric\n\nAnswer Accuracy, URL Recall\nPartial Completion\nAnswer Accuracy\nAction Accuracy\n\nWR\nWR\nWR\nCU\n\nEnterprise\nGeneric\n\nEnterprise\n\nCU\nCU\n\nDR\n\nTask Completion, Efficiency\nTask Completion\n\nInsight Recall\n\nmeasuring retrieval or synthesis quality in controlled but limited settings. In contrast, DRBench is the first to\ncombine web retrieval with local enterprise data, grounding tasks in persona- and domain-specific contexts.\n\nEnterprise Environments. Realistic enterprise environments have become an important testbed for\nevaluating agents in complex multi-application workflows. CRMArena-Pro (Huang et al., 2025a;b) targets\nsales and CPQ pipelines through persona-grounded dialogues, but is limited to conversational sales\nworkflows. OSWorld (Xie et al., 2024) and OSWorld-Gold (Abhyankar et al., 2025) benchmark agents\nin general-purpose desktop environments, using applications such as Microsoft Word and Excel, yet their\nfocus remains on computer task execution rather than enterprise deep research. TheAgentCompany (Xu\net al., 2025a) evaluates collaboration among autonomous agents for programming, browsing, and\ncommunication, though the tasks are computer-use focused and do not assess deep research capabilities.\nWorkArena (Drouin et al., 2024; Boisvert et al., 2024) offers a realistic enterprise environment with\nknowledge work tasks for web agents, though it does not support evaluation of deep research capabilities.\nIn contrast, DRBench offers a domain-grounded enterprise environment with applications that would\nrealistically be encountered in organizations. Tasks are tied to concrete personas and roles, requiring agents\nto search, reason, and synthesize insights across diverse formats, including spreadsheets, PDFs, wikis,\nemails, and presentations, reflecting realistic enterprise deep research.\n\nDeep Research Agents. A growing line of work explores agents for multi-step search and synthesis\nacross diverse information sources. LangChain’s Local Deep Researcher (LearningCircuit, 2025) and\nZilliz’s Deep-Searcher provide modular pipelines for iterative querying and summarization, while DeepRe-\nsearcher (Zheng et al., 2025) uses RL to enable planning, cross-validation, and self-reflection. Commercial\nsystems such as Gemini Deep Research and Manus.ai synthesize web-based reports with citations,\nand open-source frameworks like OpenHands (All-HandsAI, 2024), OpenManus (FoundationAgents,\n2024), and smolagents (HuggingFace, 2024) offer alternative architectures. Recent work also introduces\ntask-agnostic frameworks for long-form synthesis and evaluation paradigms such as Mind2Web 2 (Gou\net al., 2025), which treat agents as judges of browsing trajectories. Building on these efforts, DRBench an-\nalyzes their strengths and limitations in enterprise contexts, showing that current agents still fall short in\nconsistently extracting and grounding critical insights within complex, heterogeneous environments.\n\n3 DRBench - AN ENTERPRISE DEEP RESEARCH BENCHMARK\n\nTo evaluate agents on complex, open-ended enterprise deep research tasks, we designed DRBench with\nthree guiding principles: it requires agents to integrate both public web data and local enterprise documents,\nit involves both web search and enterprise application use, and it is hosted in an interactive and reproducible\n\n3\n\n\fFigure 2: DRBench Task Generation Pipeline. The pipeline comprises five main stages during each LLMs\ngenerate candidate data such as company context, insights, and research questions, while human annotators\nverify quality and select the final version. Stages S1–S5 denote the five generation steps.\n\nenterprise environment. These principles ensure that the benchmark reflects realistic enterprise workflows\nand provides a controlled yet challenging setting for research agents.\n\nThe Enterprise Search Environment. A unique aspect of DRBench is its realistic enterprise search\nenvironment. When addressing DR Questions like ”What changes should we make to our product roadmap\nto ensure compliance with this standard?”, the DR agents would need to navigate such environment and\nsearch across both public and private data sources to uncover relevant insights.\n\nPublic insights include information available on the open web or otherwise accessible to general users.\nLocal insights, on the other hand, come from an enterprise’s private systems. These insights are embedded\nwithin a vast search space that spans multiple data types (emails, slide decks, chat conversations, and Excel\nsheets) which reflect the complexity of real enterprise data ecosystems. This environment is populated with\ndata from different applications, accessible to both web-based agents and API-calling agents. For example,\nan app like Mattermost can be used to host chat conversations (see Appendix E for examples of the applica-\ntions). The goal of the DR Agent is to effectively navigate these public and private data sources to address\ncomplex, high-level DR questions. For the environment implementation details, please see Section D.\n\nTask Definition. Each task is associated with a deep research question Qi and Task Context C which\nincludes company information and the user persona. Each task also has a corresponding set of groundtruth\ninsights I consisting of relevant private insights Il (we also refer to this as internal insights), distractor\nprivate insights Id, and public insights Ip. Each private insight, whether relevant or a distractor, is embedded\ninto a file fi which could take the form of a PDF, Excel sheet, slide deck, chat log, and so on. The agent’s\ntask is to generate a report by extracting the public insights Ip from accessible sources such as the web,\nwhile also extracting the private insights Il from the files hosted in the enterprise environment. At the same\ntime, the agent must avoid extracting the distractor insights Id, which are not relevant to the DR question.\n\nDRBench provides 15 realistic deep research tasks explicitly framed around enterprise environments.\nEach task is associated with public insights extracted form quality, time-invariant URLs and local insights\nembedded within synthetic enterprise data, typically spanning 2–4 applications and 3–16 supporting files\n(see Section B). Tasks are distributed across 10 enterprise domains (such as Sales and Compliance - the\nfull list is in Section B) and divided between easy, medium, and hard categories that indicates the difficulty\nof addressing the DR Question. Finally, DRBench is fully self-hosted, with dated URLs and reproducible\nevaluation scripts to ensure stability and fair comparison across agent methods.\n\n3.1 DATA GENERATION\n\nTo create realistic and reproducible deep research tasks, DRBench employs a five-stage pipeline (Figure 2)\nthat combines large-scale LLM generation with human-in-the-loop verification. The pipeline helps us\ngenerate candidate company contexts, personas, questions, insights, and supporting files using LLM\nModels such as Llama-3.1-8B-Instruct, Llama-3.1-405B (Dubey et al., 2024). Three human annotators\nthen validate the generated content to ensure that they are realistic and plausible.\n\nThe pipeline has been used to generate 15 tasks with 114 groundtruth insights across 10 enterprise domains,\neach grounded in realistic personas and company profiles. We control the difficulty of each task by setting\n\n4\n\n\fthe number of insights, file types and application types. The complete list of tasks is provided in Section B.\nRefer to Section I for details on the cost of using data generation.\n\nStage 1: Company and Persona Generation. This stage produces the synthetic company profile and\nuser persona that form the Task Context C. LLMs were used to generate company descriptions detailing\nthe industry vertical, key products, market position, and competitive landscape. In parallel, they were used\nto create realistic personas across departments (e.g., a Regulatory Affairs Manager or a Market Research\nAnalyst) that serve as the role grounding for the final deep research question. They were then refined\nby human experts. The prompts used for this stage are provided in Section O.1 and the list of companies\nare given in Appendix B.\n\nStage 2: Public Source and Insight Collection. Given the company and persona context from Stage 1,\nwe have retrieved candidate URLs relevant to the specified domain and company background. To ensure\nquality, time-invariant insights, the search is restricted to dated, journal-based or industry-report websites\nthat provide authoritative information. Thus, the collected URLs and their contents are expected to be stable\nin time. Human annotators then review the candidate URLs and select one that is both topically aligned\nand provides insights into the topic. The selected page becomes the Task URL included in C. Its HTML\ncontent is parsed, and LLMs are prompted to extract business-relevant insights, which are subsequently\nfiltered and validated by human reviewers for accuracy and contextual fit. The public insights Ip derived\nfrom the Task URL are included in C and serves as a required piece of insight for the agent to retrieve\nduring report generation. Prompts used for this stage and the list of urls are provided in Section O.3.\n\nStage 3: Question Generation. Given the Task Context, we generate the deep research question Q.\nThe prompt (see Section O.2) is instantiated with the company profile and persona, the selected domain,\nthe Task URL, and the public insight Ip. The LLM proposes several open-ended candidate questions\ngrounded in this context. Human annotators then review these candidate DR Questions, selecting and\nrefining one to align with the persona and company. They also ensure that the insights available in the\nprovided URL can at least partially support answering the deep research question. For example, if the\nquestion concerns compliance with a specific regulation, the URL might include relevant insights, such\nas “groceries must have a traceability plan.” While this doesn’t fully resolve the question, it provides\na foundation.The question should be high-level enough to allow us to synthesize additional supporting\nprivate/internal insights Il (such an insight could be “the cost of implementing such a plan is X amount”)\nwhich are needed to strengthen the report generated for the question. This requirement ensures that new\ninternal insights can be generated, as discussed in Stage 4.\n\nStage 4: Internal Insight Generation. In this stage we generate the injected insights set G ⊂I. Using\nthe public insight Ip and the deep research question Q, LLMs are used to create company-specific insights\naligned with the organization’s industry, priorities, customer segments, and business goals. These insights\nare designed to provide additional supporting facts that need to be extracted to create a report that better\naddresses the DR questions. Human annotators review and refine these insights for accuracy and alignment\nwith the questions. In addition to relevant insights, we also produce distractor insights Id, which are\nplausible but irrelevant statements that do not support resolving the DR Question. Prompt details are\nprovided in Section O.4 and example internal insights are provided in Section B.\n\nStage 5: File Mapping and Generation. This stage produces the the set of files {fi} containing\nboth the relevant and distractor private insights. First, each insight is assigned to a modality such as\nemail, chat, pdf, docx, and so on. Then the file generation module follows the following three-step\n“needle-in-a-haystack” process: (1) create an outline of the file based on its modality(e.g., document\nstructure or chat configuration), (2) insert the distractor or relevant insight into an appropriate section\nof the file, and (3) fill the remaining content with realistic but irrelevant information. Human annotators\nspot-check the generated files to ensure fidelity, coherence and no contradicting information. Prompts\nfor file generation are provided in Section O.5 and screenshots of such generated files are in Section C.\n\n4 DRBench AGENT\n\nThe DRBench baseline agent (DRBA) is the first agent built specifically for deep research in enterprise\nsettings, designed to operate directly within the DRBench environment.\nIts multi-stage architecture\nsystematically investigates research questions by iteratively retrieving, processing, and synthesizing\nknowledge from enterprise services and the web until completion or a maximum iteration limit is reached\n(Figure 3; see Appendix F). The agent has access to app-specific api calling tools to access diverse\n\n5\n\n\fFigure 3: DRBench Agent architecture showing the enterprise research workflow from question submission\nthrough iterative research cycles to final report generation, using both enterprise and web search capabilities.\n\ninformation sources and a diverse toolset for analyzing retrieved results (Table 9, Appendix F.3). DRBA’s\narchitecture is organized into four main components: research planning, action planning, an adaptive\nresearch loop, and report writing. Refer to Appendix I for details on the cost of using DRBA.\n\nResearch Planning. The agent decomposes research questions into structured research investigation\nareas to guide subsequent action generation. This initial decomposition lays out a strategy to systematically\ncover the research space while maintaining focus on the initial deep research question. The agent supports\ntwo research planning modes: (1) Complex Research Planning (CRP), which generates a structured\nresearch plan with detailed investigation areas, expected information sources, and success criteria; and\n(2) Simple Research Planning (SRP) , which produces a lightweight decomposition of the main question\ninto a small set of self-contained subqueries. See Appendix F.2 for detailed examples of both modes.\n\nAction Planning. The system translates research objectives into executable actions through an LLM-based\nplanning subsystem. Actions receive a priority score based on strategic importance and are parameterized\nwith specific tool selection and execution parameters, and their dependencies to other actions in the plan.\n\nResearch Loop with Adaptive Action Planning (AAP). The system iterates through (1) tool selection\nand execution based on action priorities, (2) content processing and storage in a vector store, (3) adaptive\naction generation to cover research gaps, and (4) iteration findings storage for traceability and assessment.\n\nReport Writing. The report writing subsystem queries the vector store to synthesize the research findings\nand relevant retrieved content. This component generates a comprehensive report and uses its own citation\ntracking system to ensure proper attribution of claims.\n\n5 EXPERIMENTS AND RESULTS\n\nIn this section, we evaluate the DRBench Agent (DRBA) on both the full DRBench benchmark and a\nreduced subset for ablations. We consider (1) the Full Benchmark, covering all 15 tasks across 10 domains\n(Table 7), and (2) the MinEval subset, restricted to five retail tasks for efficient ablation studies. Results\nare reported across four metrics: Insight Recall, Distractor Avoidance, Factuality, and Report Quality,\nexplained in the next section. Implementation details and hyperparameters are in Appendix H, while the\nimpact of the number of research loop iterations on the performance of DRBA is analyzed in Appendix N.\n\n5.1 EVALUATION METRICS\n\nInsight Recall. We first decompose each report into atomic insights with their associated citations using\nan LLM (see Prompt 14). Each extracted insight is then compared against the set of groundtruth insights\nembedded in the task files using an LLM Judge with the Prompt 15. If a match is found, the insight is marked\nas detected and contributes to the insight recall score; otherwise, it is ignored. This metric thus measures\nrecall rather than precision, since judging whether an unmatched insight is nonetheless useful for answering\n\n6\n\n\fTable 2: DRBA performance with different planning configurations on DRBench. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. See Appendix K for the standard error across 3 runs.\nNote that higher numbers correspond to better scores, and the best result on each metric is bolded.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n16.92\n17.00\n16.94\n20.56\n19.20\n18.69\n\n64.06\n69.38\n66.78\n67.44\n61.72\n57.20\n\n97.94\n98.89\n99.52\n98.89\n98.97\n98.89\n\n91.08\n92.46\n90.72\n93.00\n91.86\n90.12\n\n41.71\n42.48\n42.07\n47.43\n44.80\n43.39\n\nthe deep research question is inherently subjective and difficult to automate. To prevent agents from trivially\nachieving 100% recall by copying all content into the generated report, the LLM Judge evaluates only the\nfirst k insights, where k equals the number of ground-truth insights plus five. This buffer ensures that reports\nare not penalized for including seemingly relevant insights that are not part of the groundtruth insight set.\n\nDistractor Avoidance. To measure precision, we track whether the agent’s report includes distractor\ninsights that are irrelevant to the research question. We compute distractor recall analogously to insight\nrecall, and define distractor avoidance as 1− distractor recall.\n\nFactuality. Using the same set of extracted insights (that we used for Insight Recall), we follow\nthe methodology of FactScore (Min et al., 2023).\nIf an insight lacks a citation or references a\nnon-existent source, it is labeled unfactual. Otherwise, we apply a retrieval-augmented system based on\ntext-embedding-3-large (OpenAI, 2024) to fetch the top-5 most relevant chunks from the cited\ndocument (Appendix H). The LLM Judge with Prompt 16 then determines whether the cited evidence\nsupports the claim. We also store justifications and model confidence scores for interpretability.\n\nReport Quality. Inspired by prior work (Coelho et al., 2025; Abaskohi et al., 2025), we query the LLM\nJudge with Prompt 17 to assign a 1–10 rating across six dimensions: (1) depth and quality of analysis, (2)\nrelevance to the research question, (3) persona consistency, (4) coherence and conciseness, (5) absence of\ncontradictions, and (6) completeness and coverage. The final report quality score is obtained by averaging\nthese six ratings.\n\n5.2 MAIN RESULTS\n\nWe first evaluate our DRBA agent (Section 4) using GPT-4o as the backbone model, a maximum of\n15 research loop iterations, and different combinations of planning modules: Simple Research Planning\n(SRP), Complex Research Planning (CRP), and Adaptive Action Planning (AAP). The results are reported\nin Table 2. Overall, the agent demonstrates moderate ability to ground its answers in factual evidence\nbut struggles to consistently surface the main injected insights necessary for answering the deep research\nquestions.\nIn many cases, the agent relies on prior knowledge or external web content rather than\nintegrating the crucial enterprise-specific information available in the files. By contrast, it is consistently\nstrong in avoiding distractors, showing that the agent is robust against misleading or irrelevant information\nbut less effective at prioritizing decision-critical insights. Note that our LLM Judge backbone is GPT-4o.\n\nComparison Across Planning Strategies. Here we see that SRP tends to produce more factually\ngrounded answers, while CRP excels at filtering out distractors through structured decomposition. AAP,\non the other hand, provides the largest improvements in both insight recall and report quality, suggesting\nthat dynamically adapting the plan during execution helps the agent recover missed evidence and refine its\nuse of sources. However, combining CRP or SRP with AAP does not yield clear gains, and in some cases\nreduces factuality, likely because overlapping strategies create redundant or unstable planning behavior.\nThese findings indicate that adaptive mechanisms are key for improving coverage of injected insights,\nwhile lightweight planning is more effective for maintaining factual grounding, and that carefully balancing\nthe two remains an open challenge. See Appendix M for detailed results for each task.\n\n7\n\n\fTable 3: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Note that higher numbers correspond to better scores, and the best result on each\nmetric is bolded. The full table with more models is given in Appendix L.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n17.37\n16.97\n20.16\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n78.91\n79.27\n69.75\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n100.00\n98.10\n97.90\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n90.48\n92.34\n91.26\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n49.78\n48.87\n53.86\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\n5.3 ABLATION: EFFECT OF BACKBONE LANGUAGE MODEL ON DRBA\n\nWe evaluate the impact of backbone language models on DRBA using the MinEval subset for controlled\ncomparison (Table 3). GPT-5 achieves the best balance of factual grounding, insight recall, and report\nquality. Open-source models show mixed results: Llama-3.1-405B excels in factuality but lags in recall,\nDeepSeek-V3.1 delivers balanced performance through targeted fine-tuning, and Qwen-2.5-72B is reliable\nbut trails GPT-5. These results underline the importance of backbone choice; larger and more advanced\nmodels generally yield stronger overall performance, though some open-source options are competitive\nin specific metrics. In addition, our experiments also revealed a significant limitation in agents retrieving\ncritical insights from the open web. As shown in Figure 8 in Section L, no agent managed to successfully\nsource external knowledge, highlighting the difficulty of extracting relevant information for deep research\napplications within an unboundedly large search space.\n\nTable 4: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall. The full table with all\ngroundtruth insights and predicted insights is given in Appendix G.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have inter-\nacted with personalized product recommen-\ndations, resulting in a 25% increase in aver-\nage order value.\n\n45% of Lee’s Market online customers engage with\npersonalized product recommendations, resulting in\na 25% increase in average order value.\n(Score = 1.0)\n\n45% of online customers engaged with per-\nsonalized product recommendations, and\namong those engagers average order value\nincreased by 25%.\n(Score = 1.0)\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\n85% of transactions are linked to loyalty accounts at\nLee’s Market, providing a solid foundation for personal-\nized marketing and improving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n5.4 QUALITATIVE ANALYSIS\n\nIn Table 4 we show a sample of three groundtruth insights as well as the predicted insights from using\nboth Llama 3.1 405B and GPT-5. We see that for the first insight, both models are able to effectively\nrecover the groundtruth insight. For the second insight GPT-5 can extract the relevant time of year, where\nas Llama 3.1 405B fails to do so. This possibly suggests that GPT-5 may be better at extracting fine details.\n\n5.5 PERFORMANCE OF WEB AGENTS ON DRBench\n\nWe evaluated Generic WebAgents from AgentLab in a browser-only setting (without API access).\nThe GPT-4.1-powered agent achieved only 1.11% insight recall, 6.67% factuality, and 33.07% report\nquality. While the reports appeared well-structured, they lacked grounded insights, with most trajectories\n\n8\n\n\fTable 5: Model Performance Comparison Across Local or App-based Environments. Note that higher\nnumbers correspond to better scores, and the best result on each metric is bolded.\n\nEnv\nModel\nApp\nDRBA (GPT-5)\nApp\nDRBA (GPT-5) + CRP\nDRBA (DeepSeek-V3.1)\nApp\nDRBA (DeepSeek-V3.1) + CRP App\nLocal\nDRBA (GPT-5)\nLocal\nDRBA (GPT-5) + CRP\nLocal\nDRBA (DeepSeek-V3.1)\nDRBA (DeepSeek-V3.1) + CRP Local\nPerplexity\nLocal\nOpenAI Deep Research (GPT-5) Local\nLocal\nGemini\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n38.33\n39.63\n25.15\n30.26\n41.25\n42.18\n35.62\n36.54\n39.14\n44.78\n43.92\n\n74.52\n65.17\n72.66\n70.27\n82.43\n83.91\n77.12\n78.35\n81.06\n87.53\n85.68\n\n95.14\n92.86\n97.43\n96.67\n98.62\n98.45\n97.95\n97.62\n98.84\n99.12\n98.97\n\n94.56\n93.42\n86.52\n86.88\n91.08\n92.46\n89.16\n90.12\n90.36\n94.92\n93.24\n\n66.01\n64.46\n53.09\n57.86\n69.57\n70.67\n64.03\n65.07\n67.72\n73.56\n72.37\n\ndegenerating into repetitive clicks on irrelevant files or windows. This shows that browser-only agents are\ncurrently far from effective for deep research tasks. Further trajectory examples are shown in Appendix J.\n\n5.6 APP-BASED ENVIRONMENT VS LOCAL ENVIRONMENT\n\nIn Table 5, we compare results across two settings in DRBench: (1) local, where all the task files (e.g.,\nPDFs, PPTX, DOCX, XLSX, chats) are placed in a local folder that the agent can access, and (2)\napp-based, where the same files must be retrieved through our standard enterprise environment and its apps,\nintroducing additional interaction complexity. We find that OpenAI’s Deep Research (GPT-5) achieves\nthe highest scores across all metrics. Our agent with GPT-5 and DeepSeek backbones achieves similar\nperformance to Perplexity in the local-only setting, but lags behind OpenAI and Gemini. In the app-based\nsetting, performance declines across both backbones, highlighting the added difficulty of navigating\nmulti-application environments. This gap underscores that the environment in DRBench is intentionally\nchallenging, enabling a more realistic evaluation of model capabilities in enterprise research scenarios.\n\n6 HUMAN EVALUATION\n\nQuality of Deep Research Questions. We evaluated the quality of the deep research questions in\nDRBench through a human study with five expert annotators across all 15 tasks. Each task was judged\non three criteria: (1) grounding in the external website, (2) relevance to the domain and company context,\nand (3) alignment with associated insights. Annotators provided binary ratings plus optional feedback.\nResults show strong quality: 12 tasks received unanimous approval, while only three (tasks DR1, DR11,\nand DR13) received a single negative vote due to minor issues with specificity or distractor difficulty. This\ncorresponds to a 96% approval rate (72/75 votes).\n\nCorrelation of Used Metrics with Human Preference. We collected human preference on a subset\nof 11 tasks1. Each annotator was shown a golden insight with aligning insights from two models2 and\nasked to choose which they preferred, or label both as good/bad. Missing alignments were shown as\nempty strings. We compared agents with AAP no RP against GPT-5 and Llama-3.1-405B-Instruct. The\nFleiss κ (Fleiss, 1971) across five annotators was 0.67. Most outputs were judged both bad due to missing\nalignments, but when preferences were expressed, GPT-5 was favored 61.1% over Llama-405B-Instruct,\nconsistent with our metric-based findings in Section 5.3. Additional analyses are in Appendix Q.\n\n7 CONCLUSION & FUTURE WORK\n\nIn this work, we introduced DRBench, a benchmark for evaluating AI agents on complex, open-ended\nenterprise deep research tasks that require reasoning over both public and private data. Unlike prior\nbenchmarks focused on surface-level or web-only queries, DRBench offers 15 persona-grounded tasks\nsituated in realistic enterprise contexts and evaluated through an environment that integrates real-world\nenterprise applications and heterogeneous data formats. We also presented the DRBench Agent (DRBA)\n\n1We selected tasks with fewer than 8 insights for a reasonable amount of manual work.\n2The gold-prediction alignment is provided by the insight recall metric.\n\n9\n\n\fas a strong baseline and analyzed its behavior across planning strategies and backbone models. Our results\nshow that while agents are generally effective at avoiding distractors and capable of producing structured\nreports, they still struggle to consistently extract decision-critical insights. Adaptive planning improves\nrecall of injected insights, while lightweight strategies tend to preserve factual accuracy, underscoring the\ndifficulty of balancing exploration with reliability. Looking ahead, we plan to extend DRBench with tasks\nrequiring cross-file integration, reasoning across modalities such as PDFs and chats, and richer distractors.\nWe also aim to add multimodal sources like images and video, as well as privacy-sensitive tasks to assess\ndata protection. Together, these extensions will move research agents closer to enterprise readiness and\nprovide a stronger foundation for studying deep research in realistic organizational settings.\n\n10\n\n\fREFERENCES\n\nAmirhossein Abaskohi, Amrutha Varshini Ramesh, Shailesh Nanisetty, Chirag Goel, David\nand Issam H. Laradji.\nURL\n\nVazquez, Christopher Pal, Spandana Gella, Giuseppe Carenini,\nAgentAda:\nhttps://arxiv.org/abs/2504.07421.\n\ntailored insight discovery, 2025.\n\nSkill-adaptive data analytics\n\nfor\n\nReyna Abhyankar, Qi Qi, and Yiying Zhang. OSWorld-Gold: Benchmarking the efficiency\nIn ICML 2025 Workshop on Computer Use Agents, 2025. URL\n\nof computer-use agents.\nhttps://openreview.net/forum?id=sV3n6mYy7J.\n\nAll-HandsAI.\n\nOpenHands.\n\nhttps://github.com/All-Hands-AI/OpenHands, 2024.\n\nAccessed: 2024-06-01.\n\nPierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja,\nRicardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo\nLaurenc¸on, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre M´enard, Gr´egoire Mialon, Ulyana\nPiterbarg, Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav Vorotilov,\nMengjue Wang, and Ian Yu. ARE: Scaling up agent environments and evaluations, 2025. URL\nhttps://arxiv.org/abs/2509.17158.\n\nL´eo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier de Chezelles, Quentin\nCappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. WorkArena++: Towards\ncompositional planning and reasoning-based common knowledge work tasks. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL\nhttps://openreview.net/forum?id=PCjK8dqrWW.\n\nNikos I Bosse, Jon Evans, Robert G Gambee, Daniel Hnyk, Peter M¨uhlbacher, Lawrence Phillips, Dan\nSchwarz, Jack Wildman, et al. Deep Research Bench: Evaluating AI Web Research Agents. arXiv\npreprint arXiv:2506.06287, 2025.\n\nZijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green,\nKshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong,\nXinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin.\nBrowseComp-Plus: A more fair and transparent evaluation benchmark of Deep-Research agent, 2025.\nURL https://arxiv.org/abs/2508.06600.\n\nThibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, L´eo Boisvert,\nMegh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han\nL`u, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan\nSalakhutdinov, Nicolas Chapados, and Alexandre Lacoste. The BrowserGym ecosystem for web agent\nresearch, 2025. URL https://arxiv.org/abs/2412.05467.\n\nJo˜ao Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie\nCallan, Jo˜ao Magalh˜aes, Bruno Martins, et al. Deepresearchgym: A free, transparent, and reproducible\nevaluation sandbox for deep research. arXiv preprint arXiv:2505.19253, 2025.\n\nAlexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty,\nDavid Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents\nat solving common knowledge work tasks? 2024.\n\nMingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. DeepResearch Bench: A\n\ncomprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nJoseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76\n\n(5):378, 1971.\n\nFoundationAgents. OpenManus. https://github.com/FoundationAgents/OpenManus,\n\n2024. Accessed: 2024-06-01.\n\n11\n\n\fBoyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Botao Yu, Andrei Kopanev, Weijian\nQi, Yiheng Shu, Jiaman Wu, Chan Hee Song, Bernal Jimenez Gutierrez, Yifei Li, Zeyi Liao,\nHanane Nour Moussa, TIANSHU ZHANG, Jian Xie, Tianci Xue, Shijie Chen, Boyuan Zheng,\nKai Zhang, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, and Yu Su. Mind2web\nIn The Thirty-ninth Annual Conference\n2: Evaluating agentic search with agent-as-a-judge.\non Neural Information Processing Systems Datasets and Benchmarks Track, 2025.\nURL\nhttps://openreview.net/forum?id=AUaW6DS9si.\n\nKung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese,\nCaiming Xiong, Philippe Laban, and Chien-Sheng Wu. CRMArena: Understanding the capacity of\nLLM agents to perform professional CRM tasks in realistic environments. In Proceedings of the 2025\nConference of the Nations of the Americas Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), 2025a.\n\nKung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey,\nYixin Mao, Silvio Savarese, Caiming Xiong, and Chien-Sheng Wu. CRMArena-Pro: Holistic\nassessment of LLM agents across diverse business scenarios and interactions. arXiv preprint\narXiv:2505.18878, 2025b.\n\nHuggingFace.\n\nsmolagents.\n\nhttps://github.com/huggingface/smolagents, 2024.\n\nAccessed: 2024-06-01.\n\nAbhinav Java, Ashmit Khandelwal, Sukruta Midigeshi, Aaron Halfaker, Amit Deshpande, Navin Goyal,\nAnkur Gupta, Nagarajan Natarajan, and Amit Sharma. Characterizing deep research: A benchmark\nand formal definition, 2025. URL https://arxiv.org/abs/2508.04183.\n\nLearningCircuit.\n\nLocal deep research.\n\nlocal-deep-research, 2025.\n\nhttps://github.com/LearningCircuit/\n\nYao Liu, Deming Ye, Shuohang Wang, Furu Wei, Yujia Ma, and Minlie Huang. G-Eval: NLG evaluation\n\nusing GPT-4 with better human alignment. EMNLP, 2023.\n\nXing Han L`u, Zdenˇek Kasner, and Siva Reddy. WebLINX: Real-world website navigation with multi-turn\n\ndialogue, 2024. URL https://arxiv.org/abs/2402.05930.\n\nGr´egoire Mialon, Cl´ementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom.\n\nGAIA: a benchmark for general AI assistants. ICLR, 2024.\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke\nZettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 12076–12100, 2023.\n\nOpenAI.\n\ntext-embedding-3-large.\n\nhttps://platform.openai.com/docs/guides/\n\nembeddings, 2024. [Large language model]. Accessed September 2025.\n\nGaurav Sahu, Abhay Puri, Juan A Rodriguez, Amirhossein Abaskohi, Mohammad Chegini, Alexan-\ndre Drouin, Perouz Taslakian, Valentina Zantedeschi, Alexandre Lacoste, David Vazquez, et al.\nInsightBench: Evaluating insight extraction for business analytics agents. ICLR, 2025.\n\nZilliz Tech. Deep-Searcher. https://github.com/zilliztech/deep-searcher, 2024.\n\nAccessed: 2024-06-01.\n\nYuhao Wang, Ruiyang Ren, Junyi Li, Xin Zhao, Jing Liu, and Ji-Rong Wen. REAR: A relevance-aware\nretrieval-augmented framework for open-domain question answering. In Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing, pp. 5613–5626, 2024.\n\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua,\nZhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for\nopen-ended tasks in real computer environments. NeurIPS, 2024.\n\n12\n\n\fFrank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui\nZhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander\nMaben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig.\nTheAgentCompany: Benchmarking LLM agents on consequential real world tasks, 2025a. URL\nhttps://arxiv.org/abs/2412.14161.\n\nRenjun Xu and Jingwen Peng. A comprehensive survey of deep research: Systems, methodologies, and\n\napplications. arXiv preprint arXiv:2506.12594, 2025.\n\nTianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, and Pengfei Liu. ResearcherBench:\nURL\n\nEvaluating deep AI research systems on the frontiers of scientific inquiry, 2025b.\nhttps://arxiv.org/abs/2507.16280.\n\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.\nDeepResearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv,\n2025.\n\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou,\nYonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. WebArena: A Realistic Web Environment\nfor Building Autonomous Agents, April 2024. URL http://arxiv.org/abs/2307.13854.\narXiv:2307.13854 [cs].\n\n13\n\n\fTable 6: Comparison of Deep Research Tasks of Different Benchmarks.\n\nBenchmark\nDeepResearchGym (Coelho et al., 2025)\nDeep Research Bench (Bosse et al., 2025)\n\nDeepResearch Bench (Du et al., 2025)\n\nResearcherBench (Xu et al., 2025b)\n\nLiveDRBench (Java et al., 2025)\n\nBrowseComp-Plus (Chen et al., 2025)\n\nGAIA2 (Andrews et al., 2025)\n\nDRBench\n\nSample Question\nIs the COVID vaccine dangerous\nFind a reliable, known number on the internet. The total number\nof FDA Class II Product Recalls of medical devices.\nWhile the market features diverse quantitative strategies like\nmulti-factor and high-frequency trading, it lacks a single, standard-\nized benchmark for assessing their performance across multiple\ndimensions such as returns, risk, and adaptability to market\nconditions. Could we develop a general yet rigorous evaluation\nframework to enable accurate comparison and analysis of various\nadvanced quant strategies?\nCompare the Transformer and Mamba model architectures, analyz-\ning their performance and technical characteristics in different appli-\ncation scenarios. Based on the latest research, discuss the advantages\nand disadvantages of both models and their applicable scenarios.\nFor complex reasoning tasks (e.g., tasks involving multiple citations\nor extended reasoning chains), what are the strengths of current\nagent technologies, and what are their limitations? Please analyze\nthis in the context of research since June 2024.\nIdentify the title of a research publication published before June 2023,\nthat mentions Cultural traditions, scientific processes, and culinary\ninnovations. It is co-authored by three individuals: one of them was\nan assistant professor in West Bengal and another one holds a Ph.D.\nUpdate all my contacts aged 24 or younger to be one year older\nthan they are currently.\nHow can Lee’s Market leverage FSMA 204 regulations to enhance\nfood safety and customer trust?\n\nA COMPARISON OF DEEP RESEARCH BENCHMARKS\n\nAND AI AGENT BENCHMARKS WITH A COMPUTER ENVIRONMENT\n\nIn Table 1, we compare existing deep research benchmarks and AI agent benchmarks that provide a\ncomputer environment with DRBench. While the questions in existing benchmarks focus on public interest\ntopics and require generic web search and computer use, DRBench provides realistic questions that real\npersonas in organizations need to resolve.\n\nB DRBench TASKS\n\nAs shown in Table 7, DRBench contains 15 tasks in total, covering 3 industries (retail, healthcare and\nelectric vehicles), 10 task domains (compliance, sales, customer relationship management, market analysis,\ncustomer service management, IT service management, cyber security, marketing, quality assurance, and\nresearch), and 3 difficulty levels (easy, medium, hard). In addition, we generate the following 3 companies\n(one for each industry type): (1) a supermarket chain called Lee’s Market, (2) a virtual healthcare company\ncalled MediConn Solutions, and (3) an electric vehicle company called Elexion Automotive.\n\nTable 8 presents a deep research question from DRBench and its supporting groundtruth insights. We also\nvisualize the DR Question and all QA pairs by embedding them with OpenAI’s text-embedding-\n3-large model and projecting into 2D using t-SNE in Figure 4. The plot shows that injected supporting\ninsights lie closer to the DR Question, while distractors appear farther away, confirming that our injected\ninsights are semantically aligned with the research objective.\n\nC DRBench EXAMPLES OF INJECTED INSIGHTS\n\nAs shown in Figure 2, supporting documents are generated with enterprise insights injected. In Figure 5,\nwe show two examples of a generated files (PPTX and Mattermost chat) with their embedded insights.\n\n14\n\n\fFigure 4: t-SNE visualization of QA pairs for the DR Question in Task DR0005. The plot shows the\ndistribution of annotated pairs across Supporting Insights (green), Distractors (red), and the central\nDeep Research (DR) Question (gold star). Out of 49 pairs, 16 correspond to supporting insights and\n33 are distractors. The visualization illustrates how relevant insights cluster separately from distractors,\nhighlighting the challenge of retrieving salient information in a distractor-heavy environment.\n\n(a) Example supporting file named food-safety-\nregulatory-compliance.pdf with an injected insight\n”Lee’s Market reduced food waste by 8% in Q2 2024,\nsaving $1.2M.”\n\n(b) Example supporting email in the Sent mailbox with\nan injected insight ”85% of high-risk food suppliers are\nFSMA 204 compliant, totaling 270 vendors.”\n\nFigure 5: Example files with injected insights in DRBench.\n\n15\n\n151050510t-SNE Dimension 11050510t-SNE Dimension 2Q: What percentage of Lee's Market customers are centennial shoppers in Q3 2024?A: 25% of Lee's Market customers are centennial shoppers as of Q3 2024.Q: Projected chatbot cost savings in Q2 2024?A: Expect 25% cost reduction by Q2 2024.Q: How many customer service inquiries do we receive from centennial shoppers on...A: 2,500 customer service inquiries from centennial shoppers are received on average per quarter.Q: Centennial chatbot engagement rate by Q3 2025?A: 50% centennials engage weekly by Q3 2025.Q: How many Lee's Market customers used online shopping services in Q1 2024?A: About 30% of customers used online shopping services, totaling 150,000 users in Q1 2024.Q: Chatbot adoption rate Q1 2025?A: 55% adoption rate by centennials by Q1 2025.Q: Chatbot adoption rate among centennials by Q2 2024?A: 40% centennials prefer chatbots by Q2 2024.Q: How many customer queries does Lee's Market receive monthly in 2024?A: Lee's Market receives an average of 50,000 customer queries per month in 2024.Q: What was Lee's Market employee training budget in Q3 2023?A: Lee's Market allocated $450,000 for employee training, a 15% increase.Q: Annual facility maintenance cost 2024?A: Total maintenance cost was $3.5 million in 2024.Q: How many Lee's Market employees participated in leadership training in Q1 2024?A: 120 employees completed leadership training with a 90% satisfaction rate.Q: How many Lee's Market employees completed IT security training in Q1 2024?A: 85% of employees completed IT security training with a 95% pass rate.Q: What was Lee's Market employee turnover rate in Q2 2024?A: Employee turnover rate averaged 12% with a 5% increase from Q1 2024.Q: What was Lee's Market energy consumption cost in Q2 2024?A: Lee's Market energy consumption cost totaled $850,000 in Q2 2024.Q: What were Lee's Market IT costs in Q4 2024?A: IT costs totaled 3.4M,with1.2M on new hardware.Q: How many new store locations did Lee's Market open in Q3 2024?A: Lee's Market opened 12 new store locations across the US in Q3 2024.Q: What was our workers' compensation claims rate in Q2 2025?A: Workers' compensation claims rate was 2.5% with a total payout of $120,000.Q: What was Lee's Market average store maintenance cost in Q2 2024?A: Average store maintenance cost was $2,000 per month with a 5% reduction.Q: What were Lee's Market facility maintenance costs in Q2 2024?A: Facility maintenance costs totaled 1.2Mwithanaverageof100,000 monthly.Q: What was our employee headcount in Q1 2025?A: We had 5,100 employees as of January 2025.Q: How many employees attended compliance training in Q1 2024?A: Around 300 employees completed compliance training in Q1 2024.Q: What was our facility maintenance cost in Q4 2024?A: Facility maintenance costs totaled $1.8 million in Q4 2024.Q: Facility maintenance expenses Q4 2023?A: Maintenance expenses totaled $1.2 million in Q4 2023.Q: Employee training hours Q3 2024?A: Average training hours per employee were 15 in Q3 2024.Total QA Pairs: 49Supporting: 16 | Distractor: 33 | External: 0Annotated: 8 supporting + 16 distractorShapes:  Supporting   Distractor   ExternalHow can Lee's Market leverage chatbots to enhance the consumer experience for centennial shoppers?DR QuestionSupporting (Insight) (16)Distractor (33)\fTable 7: DRBench Questions and Statistics. Industry: target industry of the deep research question.\nDomain: domain of the deep research task. DR Question: question of the deep research task. Difficulty:\ndifficulty of the task defined based on the rubric mentioned in 3. # Applications: the number of total\napplications in the task environment. # Insights: the number of relevant insights to the deep research\nquestion. # Distractors: the number of non-supportings documents that do not contain relevant insights.\n\nIndustry\nRetail\n\nDomain\nCompliance\n\nRetail\n\nRetail\n\nRetail\n\nRetail\n\nSales\n\nCRM\n\nMarket Analysis\n\nCSM\n\nHealthcare\n\nCompliance\n\nHealthcare\n\nITSM\n\nHealthcare\n\nCybersecurity\n\nHealthcare\n\nCRM\n\nHealthcare\n\nMarketing\n\nElectric Vehicle\n\nCompliance\n\nElectric Vehicle\n\nQuality Assurance\n\nElectric Vehicle\n\nCybersecurity\n\nElectric Vehicle\n\nResearch\n\nElectric Vehicle\n\nCSM\n\nDR Question\nHow can Lee’s Market leverage FSMA 204 reg-\nulations to enhance food safety and customer trust?\nHow can personalization drive sales in the retail\nindustry and what strategies can be used for Lee’s\nMarket in action?\nHow can we leverage data-driven loyalty programs\nto enhance customer engagement?\nWhat are the new trends in the grocery retail\nmarket and what strategies can Lee’s Market adopt\nto remain competitive?\nHow can Lee’s Market\nleverage chatbots to\nenhance the consumer experience for centennial\nshoppers?\nWhat are the key factors influencing MediConn\nSolutions’ decision to accept insurance for tele-\nhealth providers, considering HIPAA compliance\nand state-specific data privacy regulations?\nHow can we leverage ISTM and AI-driven ana-\nlytics to minimize IT service desk workload and\nimprove response times in MediConn Solutions?\nWhat is the impact of third-party data breaches\non MediConn’s virtual healthcare platforms and\npatient data, and what new regulations can be\nimplemented to defend against these breaches?\nHow can MediConn Solutions leverage trendy new\nCRM solutions to improve patient engagement\nand retention, and what new CRM solutions are\nexpected in 2025?\nWhat are the most critical elements of a robust\ndigital presence for a telehealth provider such as\nMediConn Solutions, and how can we optimize\nour website and content marketing strategy to\nattract digital-first patients?\nHow can we balance the need for durability\nand warranty guarantees for EV batteries with\nevolving regulatory requirements, especially ACC\nregulations (ACC II), while staying on track with\nour production timelines through 2035?\nHow can Elexion Automotive’s quality assurance\nprocesses be optimized to address the unique\nchallenges of electric vehicle production, such as\nsoftware and user experience issues, compared to\ngasoline cars?\nHow can Elexion Automotive effectively im-\nplement a cybersecurity strategy for its electric\nvehicles, considering the risks and challenges\nposed by connected and autonomous technologies?\nCan we leverage AI-enhanced battery management\nto improve EV battery lifespan by 15%?\nHow can Elexion Automotive increase customer\ntrust through after-sales support while balancing\nthe need for exceptional customer care with\nefficient and cost-effective service?\n\nDifficulty\neasy\n\n# Applications\n2\n\n# Insights\n3\n\n# Distractors\n7\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\n2\n\n4\n\n3\n\n4\n\n3\n\n3\n\n4\n\n4\n\n4\n\n2\n\n1\n\n3\n\n3\n\n4\n\n4\n\n7\n\n6\n\n16\n\n4\n\n6\n\n7\n\n12\n\n15\n\n3\n\n3\n\n6\n\n7\n\n15\n\n10\n\n15\n\n14\n\n33\n\n8\n\n12\n\n14\n\n24\n\n30\n\n6\n\n6\n\n12\n\n14\n\n30\n\nD DRBENCH ENTERPRISE ENVIRONMENT\n\nThe DRBench Enterprise Environment provides a containerized simulation of realistic enterprise\nresearch settings where employees access confidential company information, personal files, and internal\ncommunications for comprehensive report generation. The environment simulates both a user’s local\nmachine filesystem and provides password-protected access to enterprise services.\n\nTo emulate realistic enterprise research settings, DRBench provides a self-contained Docker environment\nthat integrates commonly used applications: Nextcloud for shared documents, Mattermost for internal\nchat, an IMAP server and Roundcube open-source client for emails, and Filebrowser to emulate local files.\nEach task is initialized by distributing its data across these services, enabling agents to retrieve, analyze,\nand cite information through enterprise-like interfaces rather than static files. This design ensures realistic\ninteraction while maintaining reproducibility and controlled evaluation.\n\n16\n\n\fTable 8: Example Deep Research Question and Supporting Groundtruth Insights\n\nDeep Research Question\n\nHow can Lee’s Market leverage FSMA\n204 regulations to enhance food safety and\ncustomer trust?\n\nSupporting Groundtruth Insight\nU.S. grocers are working to meet the FDA’s FSMA 204 traceability rules\nby January 2026, which require tracking lot codes and key data for high-risk\nfoods to expedite recalls. This compliance is viewed as an “evolutionary\nstep” to modernize grocery operations and enhance food safety.\nBy capturing detailed traceability data, such as lot codes, at every step,\nretailers can meet regulations and gain inventory benefits. This allows grocers\nto know exact expiration dates by lot, enabling them to discount items before\nthey expire, thus reducing food waste and keeping products fresher.\nRegional grocers like Lunds & Byerlys and Raley’s see FSMA 204 as\na chance to enhance their systems and supply chain transparency. They\nbelieve improved traceability will boost customer trust and could signal the\nstart of more extensive future food safety regulations.\nLee’s Market tracks 250 high-risk food products as of Q3 2024, affecting\n30% of inventory.\nLee’s Market reduced food waste by 8% in Q2 2024, saving $1.2M.\n85% of high-risk food suppliers are FSMA 204 compliant, totaling 270\nvendors.\n\nInsight Category\nExternal\n\nExternal\n\nExternal\n\nInternal\n\nInternal\nInternal\n\nD.1 ARCHITECTURE AND SERVICES\n\nThe environment implements a multi-service architecture within a single Docker container. This design\nprioritizes deployment simplicity and cross-platform compatibility while maintaining service isolation.\nThe container orchestrates the following enterprise services:\n\n• Nextcloud: Open-source file sharing and collaboration platform analogous to Microsoft\n\nSharePoint or Google Drive, providing secure document storage with user authentication.\n\n• Mattermost: Open-source team communication platform simulating internal company commu-\nnications similar to Microsoft Teams or Slack, with teams, channels, and persistent chat history.\n\n• FileBrowser: Web-based file manager providing access to the container’s local filesystem,\n\nsimulating employee desktop environments and local document access.\n\n• Email System: Roundcube webmail interface with integrated SMTP (postfix) and IMAP\n\n(dovecot) services for enterprise email communication simulation.\n\n• VNC/NoVNC Desktop: Protocol and browser-based VNC access providing full desktop\nenvironment interaction within the container for comprehensive enterprise workflow simulation.\n\nD.2 TASK LOADING AND DATA DISTRIBUTION\n\nAt initialization, the environment processes task configuration files (env.json) and distributes data\nacross services through automated Python scripts and it makes sure that this source data is only accessible\nthrough the intended applications:\n\n• File Distribution: Documents are placed in appropriate Nextcloud user folders and FileBrowser\n\ndirectories based on task specifications\n\n• Communication Import: Chat histories and team conversations are imported into Mattermost\n\nchannels with proper user attribution\n\n• Email Integration: Email conversations are loaded into the mail system with realistic threading\n\nand metadata\n\n• User Provisioning: Enterprise users are automatically created across all services with consistent\n\nauthentication credentials\n\n17\n\n\ffrom drbench import drbench_enterprise_space, task_loader\n\n# Load task configuration\ntask = task_loader.get_task_from_id(task_id)\n\n# Initialize environment with automatic port allocation\nenv = drbench_enterprise_space.DrBenchEnterpriseSearchSpace(\n\ntask=task.get_path(),\nstart_container=True,\nauto_ports=True\n\n# Prevents port conflicts in parallel execution\n\n)\n\n# Environment provides service discovery\navailable_apps = env.get_available_apps()\n# Returns: {’nextcloud’: {’port’: 8081, ’credentials’: {...}}, ...}\n\n# Pass relevant information to the agent\n\n# Cleanup when research complete\nenv.delete()\n\nListing 1: DrBench Environment Usage\n\nD.3 PYTHON INTEGRATION\n\nThe DrBenchEnterpriseSearchSpace class provides programmatic container management with\nthe following capabilities: container lifecycle management, service access information, task-specific\ndata loading, and automatic cleanup. The typical usage pattern shown in Listing 1 demonstrates these\nintegrated capabilities.\n\nD.4 ENTERPRISE SERVICE APIS\n\nEach service exposes both **web interfaces** for human and web-agent interaction, and **programmatic\nAPIs** for agent access:\n\n• Nextcloud: WebDAV API for file operations, sharing, and metadata retrieval\n\n• Mattermost: REST API for message history, channel management, and user interactions\n\n• Email: IMAP/SMTP protocols for message retrieval and sending\n\n• FileBrowser: HTTP API for filesystem operations and file management\n\nThis dual-access model enables both agent-driven research and human verification of enterprise scenarios,\nsupporting comprehensive evaluation of research capabilities across realistic enterprise information\narchitectures.\n\nE DRBench EXAMPLES OF APPLICATION SCREENSHOTS\n\nFigures 6 and 7 show the applications provided in DRBench environment: File Browser, Mattermost,\nRoundcube, and Nextcloud.\n\nF DRBench AGENT IMPLEMENTATION DETAILS\n\nF.1 DETAILED WORKFLOW\n\nAs depicted in Figure 3, the workflow begins with a Company Employee submitting an enterprise Deep\nResearch Question along with Company Context. The DRBench agent processes this input through several\nkey stages:\n\n18\n\n\f(a) Screenshot of the File Browser interface, displaying\norganized files and folders within the system.\n\n(b) Screenshot of the Mattermost communication\nplatform, showing a discussion channel and user\ninterface elements.\n\nFigure 6: Screenshots of Applications in DRBench environment (Part 1).\n\n(a) Screenshot of the Nextcloud file management system,\nillustrating the file list view with various document types.\n\n(b) Screenshot of Roundcube, an email client, it shows\nan open email in the user’s inbox.\n\nFigure 7: Screenshots of Applications in DRBench environment (Part 2).\n\nStage 1: Research Planning. The agent decomposes the research question into structured research\ninvestigation areas to guide subsequent action generation. This initial decomposition lays out a strategy\nto systematically cover the research space while maintaining focus on the initial deep research question.\n\nStage 2: Action Planning. The Action Planning stage translates the research objectives into executable\nactions through a planning subsystem. This component uses an LLM to create a prioritized sequence\nof actions. Each action is parameterized with specific tool selection and execution parameters, its\ndependencies to other actions in the plan, and a priority score.\n\nStage 3: Research Loop with Adaptive Execution. The Research Loop iterates over the following\nsub-stages until completion: (1) Tool Selection and Execution: The tool selection and execution subsystem\nimplements a sophisticated priority-based selection of actions from the plan at each research iteration step\nand proceeds to execute it with the current research context. (2) Content Processing: If necessary, the action\nwill make use of the content processing subsystem to extract, synthesize, and store retrieved documents and\nwebsites into a vector store to form a task-specific knowledge base that will grow on each iteration. (3) Adap-\ntive Action Planning: After each execution round, the agent analyzes the most recent findings; if coverage\ngaps are detected, new actions are created and added to the plan at this point. This ensures that newly discov-\nered knowledge is taken into account to answer the research question. (4) Iteration Findings Storage: Results\nfrom each iteration are stored in the vector store with rich metadata for traceability and later assessment.\n\nStage 4: Convergence and Completion. The research loop continues until all actions in the plan have\nbeen completed or the maximum iteration limit is reached.\n\n19\n\n\fStage 5: Report Writing. The report writing subsystem queries the vector store to synthesize the\nresearch findings and relevant retrieved content. This component generates a comprehensive report and\nuses its own citation tracking system to ensure proper attribution of claims within the report.\n\nThe vector store serves as the main knowledge integration component, maintaining embeddings of all\nprocessed content and enabling semantic retrieval at the report generation stage. This component is crucial\nin a deep research setting to prevent information loss from early research stages in arbitrarily long research\nsessions.\n\nF.2 RESEARCH PLANNING IMPLEMENTATION\n\nThe Research Planning subsystem offers three operational modes to evaluate the impact of structured\nplanning on research effectiveness:\n\n• Complex Mode: Generates a comprehensive research plan with detailed investigation areas.\nThese areas contain details about the specific research focus, expected information sources, and\nsuccess criteria, among others. Each area includes an importance level and specific business\nintelligence objectives (see Listing 2).\n\n• Simple Mode: Creates focused question decompositions with 4-10 self-contained subqueries\nderived directly from the main research question. Uses straightforward decomposition without\nthe complex enterprise research structure of complex mode. See examples in Listing 4 and\nListing 3 for comparison of different planning modes.\n\n• None: Bypasses structured planning entirely, proceeding directly to action generation based\non the original question. This mode serves as a baseline to measure the added-value of explicit\nplanning stages.\n\nThe planning process begins with the enriched query (Prompt 1) and uses the research planning prompt\n(Prompt 2) to generate structured outputs. In Complex Mode, the system creates detailed investigation areas\nwith enterprise-focused metadata, while Simple Mode produces straightforward question decompositions\nsimilar to existing multi-step reasoning approaches. The resulting plan structure directly feeds into the\nAction Planning System (Section F.3) for executable action generation.\n(cid:7)\n{\n\n(cid:4)\n\n\"area_id\": 1,\n\"research_focus\": \"Core strategic\n\ndomain, market segment, or business hypothesis to investigate\",\n\n\"information_needs\": [\n\n\"What specific intelligence is required for strategic decisions\"\n\n],\n\"knowledge_sources\": [\"internal\", \"external\", \"both\"],\n\"research_approach\n\n\": \"competitive_analysis | market_research | strategic_assessment\n\n| trend_analysis | risk_analysis | performance_benchmarking\",\n\n\"key_concepts\": [\"concept1\", \"concept2\"],\n\"business_rationale\": \"Why this investigation\n\narea is critical for enterprise strategy and decision-making\",\n\n\"expected_insights\": \"What strategic\n\nunderstanding or competitive intelligence this area should provide\",\n\n\"stakeholder_impact\": \"Which\n\nbusiness units or decision-makers will benefit from these insights\",\n\n\"importance_level\": \"critical | important | supplementary\"\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 2: Investigation Area Structure for Full Planning Mode\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"What are the new trends in the grocery retail market\n\nand what strategies can Lee’s Market adopt to remain competitive?\",\n\n\"plan\": {\n\n\"research_investigation_areas\": [\n{\n\n20\n\n\f\"area_id\": 1,\n\"research_focus\": \"Current trends in grocery retail market\",\n\"information_needs\": [\n\"Latest consumer preferences\",\n\"Emerging technologies influencing grocery shopping\",\n\"Sustainability practices in grocery retail\",\n\"Changes in supply chain dynamics\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"trend_analysis\",\n\"key_concepts\n\n\": [\"e-commerce growth\", \"sustainability in supply chains\"],\n\n\"business_rationale\": \"Understanding consumer\n\ntrends and technological advancements that shape shopper behavior is\ncritical for adapting offerings and enhancing customer engagement.\",\n\"expected_insights\": \"Identify specific trends affecting customer\n\nbuying decisions, including the rise of online grocery shopping\nand preferences for sustainable, local, or organic products.\",\n\n\"stakeholder_impact\n\n\": \"Marketing, Product Development, Supply Chain Management\",\n\n\"importance_level\": \"critical\"\n\n},\n{\n\n\"area_id\": 2,\n\"research_focus\": \"Competitive analysis of grocery retailers\",\n\"information_needs\": [\n\"Market share analysis\",\n\"Competitor strengths and weaknesses\",\n\"Innovative strategies adopted by competitors\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"competitive_analysis\",\n\"\n\nkey_concepts\": [\"market positioning\", \"competitive differentiation\"],\n\n\"business_rationale\": \"A comprehensive understanding of\n\ncompetitors allows for strategic positioning and the identification\nof innovative practices that can be adopted or improved upon.\",\n\n\"expected_insights\": \"Detailed profiles of key\n\ncompetitors, including strategic moves they are making to capture\n\nmarket share, which can inform Lee’s Market’s competitive strategy.\",\n\n\"stakeholder_impact\n\n\": \"Executive Leadership, Strategic Planning, Marketing\",\n\n\"importance_level\": \"critical\"\n\n},\n...\n]\n\n}\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 3: Complex Mode Research Plan Example\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"How can we leverage\n\ndata-driven loyalty programs to enhance customer engagement?\",\n\n\"plan\": {\n\n\"mode\": \"simple\",\n\"subqueries\": [\n\"What are the key features\nof successful data-driven loyalty programs in the retail industry?\",\n\"How can data analytics be used to personalize rewards\nand incentives in loyalty programs to increase customer engagement?\",\n\"What types of customer data should be collected and analyzed\nto optimize loyalty programs for a company like Lee’s Market?\",\n\n21\n\n\f\"Which\ntechnology platforms and tools are most effective for implementing\nand managing data-driven loyalty programs in the retail sector?\",\n\"How can Lee’s Market measure the success of their\nloyalty program in terms of customer engagement and sales growth?\",\n\"What are the best practices for integrating a loyalty program\nwith existing marketing strategies to enhance customer experience?\",\n\"How can Lee’s Market ensure customer data\nprivacy and security while leveraging data-driven loyalty programs?\",\n\"What are the potential\nchallenges and limitations of implementing a data-driven loyalty\n\nprogram for a retail company with Lee’s Market’s size and resources?\"\n],\n\"research_methodology\": {\n\"overall_approach\": \"Query decomposition into focused subqueries\"\n}\n\n}\n\n}(cid:6)\n\nListing 4: Simple Mode Research Plan Example\n\n(cid:5)\n\nF.3 ACTION PLANNING SYSTEM\n\nThe Action Planning System translates research objectives into executable actions through an intelligent\nplanning subsystem that manages tools, prioritization, and dependencies.\n\nAvailable Tools Table 9 summarizes the available tools organized by category and their primary purposes.\n\nTable 9: DrBench Agent Tool Categories and Purposes\n\nCategory\n\nTool\n\nPurpose\n\nInformation Retrieval\n\nInternet Search\n\nEnterprise API\n\nURL Fetch\n\nAnalysis\n\nAnalyzer\n\nLocal Processing\n\nLocal\nSearch\n\nDocument\n\nExternal market research, competitive intel-\nligence, and public data analysis. Ideal for\nmarket trends, competitor analysis, industry\nreports, news articles.\n\nAccess to proprietary internal data through\nextensible adapters (Nextcloud, Mattermost,\nemail, FileBrowser). Ideal for internal metrics,\ncommunications, confidential documents.\n\nDirect content extraction from specific URLs.\nIdeal for deep analysis of reports, whitepapers,\ncase studies, competitor websites.\n\nAI-powered synthesis and analysis using\nvector search. Ideal for cross-referencing find-\nings, identifying patterns, generating insights.\n\nSemantic search within locally ingested\ndocuments. Ideal for targeted retrieval from\nlocal files with source references.\n\nPriority Scoring and Dependencies Actions are assigned priority scores (0.0-1.0 scale) based on strategic\nimportance and expected information value. The priority assignment follows enterprise research principles:\n\n• Source Type Prioritization: Enterprise and local sources receive higher priority than external\n\nsources, reflecting the strategic value of proprietary information in competitive analysis.\n\n• Query Specificity: Targeted queries addressing specific investigation areas score higher than\n\nbroad exploratory searches, ensuring focused research execution.\n\n22\n\n\f• Dependency Management: Actions can specify prerequisite relationships where certain\ninformation gathering must precede analysis or synthesis tasks. The scheduler respects these\ndependencies while maximizing parallel execution within each iteration.\n\nF.4 ENTERPRISE INTEGRATION ARCHITECTURE\n\nService Adapters The system implements extensible adapters for enterprise services including Nextcloud\nfile server, Mattermost chat, IMAP email systems, and FileBrowser interfaces. Each adapter handles\nservice-specific authentication, data retrieval, and metadata preservation for proper citation attribution.\n\nSource Prioritization Strategy Enterprise and local sources receive priority multipliers in action scoring,\nreflecting the strategic value of proprietary information. The system maintains source type classification\nthroughout the pipeline to ensure internal intelligence drives analytical conclusions while external sources\nprovide context and validation.\n\nF.5 TOOL SELECTION AND EXECUTION\n\nThe Tool Selection stage implements a priority-based action selection and tool invocation within each\nresearch iteration to execute the action plan.\n\nAction Selection Process At each iteration, the system selects executable actions based on priority\nscores and dependency satisfaction:\n\n• Priority-Based Scheduling: Actions are ranked by their priority scores, with enterprise and local\nsources prioritized over external sources to maximize the value of specific private information.\n\n• Dependency Validation: The scheduler checks that all prerequisite actions have completed\n\nbefore making an action available for execution.\n\n• Sequential Execution: Actions execute one at a time in priority order, maintaining research\n\ncoherence and enabling each action to build upon previous findings.\n\nOnce selected, actions execute through a standardized interface and results are integrated into the research\ncontext, informing the following stage of adaptive action planning.\n\nF.6 ADAPTIVE PLANNING\n\nThe Adaptive Planning system enables dynamic evolution of the action plan by analyzing results after\neach iteration to generate extra actions addressing information gaps.\n\nIt starts by analyzing the most recently completed actions and performs these two substages:\n\nSource analysis and gap classification. The system evaluates the possible imbalances in the action\ncompletion if information came from internal or external sources and identifies possible scenarios to cover.\n\nDynamic action generation. After analyzing the sources and results from the previous actions, the\nsystem makes an LLM call to generate 1-5 extra candidate actions with a specific prioritization. After\ncandidate actions are generated, they go through a deduplication process to make sure the plan didn’t\ncover them already and incorporates the final subset into the priority action plan so they can be considered\nby the scheduler in the following iteration.\n\nF.7 CONTENT PROCESSING AND VECTOR STORE\n\nThe Content Processing system implements a pipeline for unified ingestion of documents in multiple\nformats (PDF, docx, HTML, JSON, plain text formats, etc.) that normalizes and cleans text inside the\ndocuments and websites retrieved during the research. Content is then deduplicated and chunkized, and\nembeddings are computed for each of these chunks.\n\nThe Vector Store implements the storage and retrieval of the content via JSON metadata and NumPy\nembedding metrices, enabling semantic similarity and keyword based searches\n\n23\n\n\fF.8 REPORT GENERATION\n\nMulti-Stage Synthesis Pipeline The Report Generation stage implements a four-stage synthesis\napproach: (1) thematic content clustering via vector searches, (2) source prioritization and deduplication,\n(3) LLM Synthesis with source tracking, and (4) Final report writing and citation resolution.\n\nThe system generates targeted search queries based on the research plan, including specific to ensure that\na predefined set of themes or sections (background, analysis, implementation, trends) are retrieved and\nwritten and prevents redundant analyses.\n\nUnified Citation System The citation system implements deferred resolution to keep consistency of\ncitations from section to section by referencing document IDs in the Vector Store and carrying them over\nfor each piece of synthesized text. A final citation resolution stage will assign the correct numbering to\neach document in the final report.\n\nF.9 DRBA PROMPTS\n\nThe DRBench agent relies on carefully designed prompts to orchestrate enterprise research workflows.\nThese prompts implement the core architectural principles: enterprise context enrichment, structured\nplanning decomposition, priority-based action generation, quantitative synthesis requirements, and adaptive\nresearch capabilities. The following five prompts represent the critical LLM interactions that enable\nsystematic enterprise research with proper source prioritization and citation tracking:\n\n• Enriched Query Generation (Prompt 1): Transforms basic research questions into enterprise-\ncontextualized queries by incorporating company information, stakeholder personas, and business\ncontext to guide subsequent research activities.\n\n• Research Planning (Prompt 2): Decomposes complex research questions into structured\ninvestigation areas with defined information needs, knowledge sources, and business rationales,\nenabling systematic coverage of the research space.\n\n• Action Generation (Prompt 3): Converts research objectives into prioritized executable\nactions with tool specifications and dependency relationships, emphasizing enterprise source\nprioritization over external sources.\n\n• Adaptive Action Generation (Prompt 4): Analyzes research progress to identify coverage\ngaps and source imbalances, generating complementary actions that enhance research depth\nand cross-validate critical findings.\n\n• Report Synthesis (Prompt 5): Orchestrates quantitative-first content synthesis with strict citation\nrequirements, ensuring numerical data leads analytical paragraphs and all claims are properly\nattributed to source documents.\n\nW Query Generation\n\nResearch Question: {dr question}\n\nCompany Context: {company name} is {company desc}.\n\nPersona Context: This analysis is requested by {name}, {role} in\n{department}.\n\nTheir responsibilities include: {responsibilities}.\n\nPrompt 1: Query Enrichment with Enterprise Context.\n\nG QUALITATIVE RESULTS\n\nShown below are some illustrative examples of how metrics are computed for different scenarios on a\ngiven test task.\n\n24\n\n\fW Research Planning (Complex Mode)\n\nDesign a comprehensive enterprise research strategy for:\n\"{question}\"\n\n{tools section}\n\nAs a senior enterprise researcher with deep business\nintelligence expertise, create a thorough investigation\nplan that combines rigorous research methodology with\nstrategic business analysis. Your goal is to provide insights\nthat drive informed decision-making in complex enterprise\nenvironments.\n\nEnterprise Research Design Principles:\n\n- Leverage proprietary internal data as competitive advantage while\nensuring external market context\n- Design investigations that directly inform strategic decisions and\nbusiness outcomes\n- Prioritize research areas that maximize ROI and strategic value to the\norganization\n- Balance comprehensive analysis with focused insights relevant to\nenterprise objectives.\n\nGenerate a JSON object with strategic research investigation\nareas:\n\n{output structure}\n\n.\n\nPrompt 2: Enterprise Research Planning with Investigation Areas. See example of the output structure\nin Listing 2\n\nG.1\n\nINSIGHTS RECALL\n\nWe showcase the insights recall metric using Task DR0002, whose DR Question is “How can\npersonalization drive sales in the retail industry and what strategies can be used for Lee’s Market in action?”\n(also shown in Table 7), which evaluates sales challenges and competitive landscape analysis.\n\nTable 10 shows the overall performance comparison between using Llama 3.1 405B and GPT-5 in\nour agent. Using GPT-5 in our agent results in increasing the insights recall score from 0.14 to 0.43,\nsuccessfully answering 3 out of 7 questions compared to Llama’s 1 out of 7.\n\nTable 10: Insights Recall Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We\nsummarize the number of questions answered successfully and unsuccessfully as well as the overall\ninsights recall score for the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nInsights Recall Score\n\nQuestions Answered\nSuccessfully\n\nQuestions Failed\n\n0.14\n\n1/7\n\n6/7\n\n0.43\n\n3/7\n\n4/7\n\n+0.29\n\n+2\n\n-2\n\nThe question-by-question breakdown in Table 11 reveals the specific questions where each approach\nsucceeded or failed. Both models successfully identified the insight related to online customer engagement,\nbut only GPT-5 was able to identify the number of loyalty program members and the customer data\ncollection rate. Neither model successfully answered the remaining 4 questions, indicating these insights\nmay not have been readily available in the source materials or that agents struggled to find the right insights.\n\n25\n\n\fW Action Generation\n\nGenerate specific executable actions for this research investigation\narea with SOURCE PRIORITIZATION:\n\nResearch Focus: {research focus}\n\nInformation Needs: {information needs}\n\nKnowledge Sources: {knowledge sources}\n\nResearch Approach: {research approach}\n\nAvailable Tools: {available tool names}\n\nTool Selection Guidelines: {tool guidelines}\n\nReturn JSON array of actions with:\n- \"type\": Action type (web search, enterprise api, url fetch, analyzer,\nlocal search)\n- \"description\": Clear description of what this action will accomplish\n- \"parameters\": Tool-specific parameters including query, search type,\netc.\n- \"priority\": Float 0.0-1.0 (enterprise sources: 0.7-1.0, external:\n0.4-0.7)\n- \"expected output\": What information this action should provide\n- \"preferred tool\": Specific tool class name to use\n\nPrompt 3: Priority-Based Action Generation with Source Awareness\n\nTable 11: Question-by-Question Insights Recall Analysis (Task DR0002). We breakdown the results\nquestion by question for the given task, highlighting specifically which question is answered correctly\nor incorrectly for each model.\n\nQuestion\n\nLlama 3.1 405B GPT-5\n\nOnline Customer Engagement with Personalized Recommendations\n\nNumber of Loyalty Program Members\n\nCustomer Data Collection Rate\n\nOnline Sales Growth\n\nEffectiveness of Personalized Marketing\n\nPersonalized Promotions vs Mass Promotions\n\nRetail Media Growth\n\nTotal Insights Recall Score\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0\n\n1.0\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n∆\n\n0.0\n\n+1.0\n\n+1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0/7.0\n\n3.0/7.0\n\n+2.0\n\nIn Table 12 we extend Table 4 and show all the groundtruth insights as well as each of the predicted\ninsights from using both Llama 3.1 405B and GPT-5. As before, we highlight in bold where the model\nwas able to accurately find details relevant to the expected insight, and show all the corresponding scores\nas given in Table 11.\n\nG.2 FACTUALITY\n\nThe factuality metric evaluation uses the same Task DR0002 to assess the accuracy and reliability of\ngenerated content. Table 13 presents the factuality performance comparison, showing that while using\nLlama 3.1 405B achieved 0.41 factuality (7 factual claims out of 17 total claims), where as using GPT-5\nreached 0.65 factuality (13 factual claim out of 20 total claims). This represents a significant improvement\n\n26\n\n\fW Adaptive Action Generation\n\nBased on the research progress so far, suggest new actions with\nINTELLIGENT SOURCE COMPLEMENTARITY:\n\nOriginal Research Query: {research query}\n\nCompleted Actions Summary: {completed actions summary}\n\nRecent Findings Analysis: {findings json}\n\nSource Composition Analysis: {internal findings}\n\nAvailable Tools: {available tool names}\n\nGenerate 1-5 new actions that:\n1. **Address gaps** in current research coverage\n2. **Balance source types** - if findings are heavily external,\nprioritize internal sources and vice versa\n3. **Build on discoveries** - leverage new information to explore\nrelated areas\n4. **Enhance depth** - dive deeper into promising\nfindings\n5. **Cross-validate** - verify critical findings through alternative\nsources\n\nEach action should have:\n- Strategic rationale for why this action is needed\nnow\n- Clear connection to research gaps or promising\nleads\n- Appropriate priority based on strategic value and source\nbalance\n- Specific parameters that build on existing knowledge\n\nPrompt 4: Gap-Driven Adaptive Action Generation\n\nin content reliability. This also highlights that GPT-5 is much better prepared to make accurate claims\nthat are sustained by evidence.\n\nTable 14 provides a detailed breakdown of factual versus unfactual claims. The agent using GPT-5\ngenerated 6 additional factual claims while producing 3 fewer unfactual claims, resulting in a net\nimprovement in accuracy percentage. This demonstrates that GPT-5 may generate a higher proportion\nof factual information than Llama 3.1 405B.\n\nThe impact of these factuality improvements of using GPT-5 over Llama 3.1 405B is summarized in\nTable 15. The 24.0 percentage point improvement in factuality represents enhanced content quality and\nresearch reliability. The increase in 3 total claims shows that GPT-5 can generate more content overall.\n\nH EXPERIMENTAL SETTINGS\n\nAll experiments were conducted on a cluster of 8 NVIDIA A100 GPUs (80GB each). For file generation\nand task construction, we primarily used the Llama-3.1-405B model, with decoding performed\nusing nucleus sampling at a temperature of 0.7 unless otherwise specified. For larger-scale evaluations of\nthe DRBench Agent, we also used closed-source models such as GPT-4o and GPT-5, alongside DeepSeek\nmodels, to enable comparison across open- and closed-source backbones.\n\nTo ensure reproducibility, the DRBench environment was deployed as a self-contained Docker container\nwith all supporting applications (Nextcloud, Mattermost, and Filebrowser) pre-configured. Each task was\n\n27\n\n\fTable 12: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have interacted\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n\n45% of Lee’s Market online customers\nengage with personalized product recom-\nmendations, resulting in a 25% increase in\naverage order value.\n(Score = 1.0)\n\n45% of online customers engaged with\npersonalized product recommendations,\nand among those engagers average order\nvalue increased by 25%.\n(Score = 1.0)\n\nAs of Q3 2024, Lee’s Market has 1.2 million\nloyalty program members.\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\nLee’s Market online sales grew 12% in Q2\n2024 compared to Q2 2023.\n\nRetailers excelling in personalized marketing\nare growing revenues about 10 percentage\npoints faster than their peers, according to\nBCG. By effectively using first-party customer\ndata, these leaders could unlock an estimated\n$570 billion in additional sales, highlighting\nthe importance of data-driven sales strategies\nfor growth.\n\nPersonalized promotions can deliver returns\nthree times higher than mass promotions,\nyet many retailers allocate under 5% of their\npromo budgets to personalization. One major\nchain increased its personalized promo spend\nfrom 1% to 10% by establishing a ”customer\ninvestment council,” resulting in $250 million\nin incremental sales.\n\nRetail media networks are expanding rapidly,\nwith retail media growing at approximately\n25% annually, offering retailers a profitable\nrevenue stream to reinvest in technology, data,\nand personnel. By integrating loyalty data,\nretailers like Sephora, which links 95% of\ntransactions to loyalty accounts, enhance preci-\nsion in product recommendations and provide\na seamless omnichannel experience, boosting\nconversion rates and customer lifetime value.\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nFrom Q2 2024 to Q3 2024, loyalty members\nincreased from 1,050,000 to 1,200,000\n(+150,000; +14.29%), average spend per\nmember rose from 24 to 25 (+4.17%), and\ntotal member spend increased from 25,200,000\nto 30,000,000 (+19.05%).\n(Score = 1.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n45% of Lee’s Market online customers engage\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n(Score = 0.0)\n\nA naive blended online AOV upside of\napproximately 11.25% is derived from 45%\nengagement multiplied by a 25% AOV lift\namong engagers.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nGrocers running data-driven loyalty cam-\npaigns have realized an average 3.8%\nlike-for-like sales uplift.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nsources\n\nindicate POS-enabled\nExternal\npersonalization can lift revenue 5%-15% and\nadvocate personalized e-receipts with relevant\noffers and coupons to extend post-purchase\nengagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 0.0)\n\nTable 13: Factuality Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We show\nthe number of factual and unfactual claims made by each model, as well as the overall factuality score\nfor the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nFactuality Score\n\nFactual Claims\n\nUnfactual Claims\n\n0.41\n\n7\n\n10\n\n0.65\n\n13\n\n7\n\n+0.24\n\n+6 claims\n\n-3 claims\n\n28\n\n\fW Report Synthesis\n\nAs an expert research analyst, synthesize the following content into\na coherent, insightful, and well-supported analysis for the theme:\n\"{theme}\" directly related to the overarching research question:\n\"{original question}\"\n\nSource Priority Guidelines:\n\n1. **\"internal\"**: Highest priority (internal company\ndocuments, proprietary files, confidential reports,\nenterprise chat messages, local documents, CRM data,\ninternal APIs, project management tools). Insights from\nthese sources should form the primary foundation of the\nanalysis.\n2. **\"external\"**: Medium priority (public web sources,\nacademic papers, industry reports, news articles). Use these\nto provide broader context, external validation, or contrasting\nperspectives.\n\n**Synthesis Requirements:**\n* **QUANTITATIVE PRIORITY:** Lead with numerical data, calculations, and\naggregations\n* Extract ALL percentages, costs, metrics, and performance\ndata\n* Perform mathematical operations: aggregate percentages, calculate\nincreases, sum totals\n* Example: \"Finance customers (35%) combined with healthcare\n(40%) represent 75% of regulated industry concerns\n[DOC:doc 1][DOC:doc 2]\"\n\n* **FACT VERIFICATION (CRITICAL):**\n* ONLY state what documents explicitly contain - no inference or\nextrapolation\n* Use exact quotes for key numerical claims: \"As stated in the\ndocument: ’[exact quote]’ [DOC:doc id]\"\n\n* **Citation Usage (Critical):**\n* **Format:** Reference sources by their document ID: \"Internal review\nshows 15% increase [DOC:doc 079c2e0f 1752503636]\"\n* **NEVER HALLUCINATE CITATIONS:** Only use provided doc id\nvalues\n* **Cite every numerical claim and calculation with source\ndocuments**\n\nGenerate 2-4 paragraphs of synthesized analysis with proper inline\ncitations.\n\nPrompt 5: Report Section Synthesis with Citation Requirements\n\nexecuted by instantiating a fresh container to avoid state leakage across runs. We capped the number of\nagent iterations according to the settings described in Section 5, with each iteration limited by a fixed\ncomputational budget.\n\nFor model outputs, we standardized all prompts and evaluation pipelines across backbones, using identical\nresearch questions, company contexts, and injected insight sets. To avoid stochastic variability, we repeated\ngeneration three times per task and reported averaged scores.\n\nFinally, all supporting scripts, environment configurations, and evaluation code are fully containerized,\nenabling consistent replication of our reported results across hardware setups.\n\n29\n\n\fTable 14: Factuality Content Analysis (Task DR0002). We show the number of factual and unfactual\nclaims made by each model, highlighting the factuality accuracy of each model for the given task.\n\nAgent\n\nFactual Unfactual Accuracy\n\nLlama 3.1 405B\n\n7 claims\n\n10 claims\n\nGPT-5\n\nChange\n\n13 claim\n\n7 claims\n\n+6\n\n-3\n\n+24.0%\n\n41.0%\n\n65.0%\n\nTable 15: Factuality Summary (Task DR0002). We summarize the factuality result improvements made\nby using GPT-5 over Llama 3.1 405B.\n\nImpact Category\n\nValue\n\nFactuality Improvement\n\n+24.0 percentage points\n\nClaims Added\n\n3 total claims\n\nAccuracy Enhancement\n\nFrom 41.0% to 65.0%\n\nContent Quality\n\nMore grounded information\n\nTask Domain\n\nTask Industry\n\nSales\n\nRetail\n\nI DATA SYNTHESIS AND DRBA COST DETAILS\n\nTo generate the DRBench tasks, we combined external insight extraction with internal file synthesis. Each\ntask included an average of 10 supporting files spanning heterogeneous formats (PDF, DOCX, PPTX,\nXLSX, and JSONL), with each file containing roughly 5 paragraphs of content. Files were designed\nto embed injected insights while mixing in distractor material, ensuring realistic enterprise complexity\nwithout exceeding practical runtime or storage budgets.\n\nData synthesis was primarily powered by GPT-4o. During task construction, GPT-4o was responsible for (1)\nextracting structured insights from public web sources, (2) adapting these insights into enterprise-grounded\ninterpretations, (3) generating persona-specific deep research questions, and (4) producing file-level content\nwith a balanced mix of insights and distractors. For evaluation, the DRBench Agent (DRBA) used GPT-4o\nas its backbone, with each task typically requiring 15 iterations and approximately 120–150 model calls.\n\nIn terms of cost, GPT-4o-based synthesis of a single task (10 files, 5 paragraphs each, plus metadata)\nconsumed about 30k–40k tokens, while DRBA execution required an additional 50k–70k tokens per\ntask. At current GPT-4o API pricing ($5 per million input tokens and $15 per million output tokens), this\ncorresponds to a per-task cost of approximately $1.5–$3.5 depending on the mix of input/output tokens\nand the iteration budget. This makes large-scale benchmarking feasible at moderate cost, while still being\nsignificantly cheaper than manual authoring or annotation.\n\nWe also note that smaller open-source models such as Llama-3.1-8B-Instruct perform well for file\ngeneration. Unlike GPT-4o, which requires API usage, Llama-3.1-8B can be hosted locally and runs\nefficiently on a single NVIDIA A100 40GB GPU. This provides a cost-effective alternative for generating\nlarge numbers of supporting documents, especially when full closed-source quality is not required.\n\nJ WEB AGENTS FOR DEEP RESEARCH TASKS\n\nSince each of the environments can be access directly through a web user interface (UI), we also exper-\nimented with an agent that can directly interact with the webpages through common browser actions like\nclick, input and scroll, which are executed through playwright3. We implement our web agent\nusing the AgentLab and BrowserGym frameworks (Chezelles et al., 2025) with a GPT-4.14 backbone. Our\nagent is implemented from AgentLab’s GenericAgent, which achieves respectable performance when used\n\n3https://playwright.dev\n4https://openai.com/index/gpt-4-1/\n\n30\n\n\fwith GPT-4o5 as a backbone; it completes 45.5% of the tasks in WorkArena (Drouin et al., 2024), 31.4%\nin WebArena (Zhou et al., 2024) and achieves a step-level reward of 13.7% on WebLINX (L`u et al., 2024).\n\nHyperparameters and Prompt We present the hyperparameters for the agent in tables 16 and 17, which\nare in majority set to the default hyperparameters, except for the maximum number of input tokens (bound\nto a reasonable maximum length) and a higher maximum number of steps (to allow the agent to perform\nmore actions required to write the report). We further update the agent’s action space on the last step to\nonly allow it to reply to the user with a report, ensuring that each trajectory terminates with a report. To\nensure that the agent is aware of the tools it can use, we modify the default system prompt (see prompt 6).\nAdditionally, each task intent is provided alongside information about the user and company (see prompt 7).\n\nResults We find that the GPT-4.1-powered web agent achieves an insights recall and factuality of\n1.11% and 6.67% respectively and a report quality score of 33.07%. Although the high report quality\nindicates that the agent can properly formulate a report, the insights quality is severely limited, with none\nof the claims being backed by useful sources. For example, a DRBench Agent powered by GPT-5 may\nanswer the question What is Lee’s Market’s current food waste reduction rate as of Q2 2024? with An\n8% reduction in food waste in Q2 2024 saved Lee’s Market $1.2 million, indicating that better inventory\ncontrol can yield both safety and financial benefits., which achieves a score of 1.0 for the question. On\nthe other hand, a GPT-4.1-powered agent will provide an unsatisfactory answer, thus achieving an insights\nrecall of 0.0. The most likely cause of this poor performance is the model’s limited capability to properly\ninteract with web interfaces when encountering unfamiliar tools. For instance, the agent may be unfamiliar\nwith the VNC and file browser applications, making it harder for it to correctly select the file it needs\nto use. Moreover, whenever the agent ends up performing an ineffective action (e.g. click on an element\nthat does not trigger any change to the page), it tends to persist by reiterating the same action (see Table\n18), or the same sequence of ineffective actions, despite not achieving anything in the previous steps. As a\nresult, despite a large number of steps, most of the agent’s actions are not helpful towards solving the task.\n\nTable 16: Web Agents Boolean Hyperparameters\n\nValue Flags\n\nTrue\n\nFalse\n\nvision support, use ax tree,\nuse tabs, use focused element, use error logs,\nuse history, use action history, use screenshot,\nuse som, extract visible tag, extract clickable tag,\nuse thinking, use concrete example, use abstract example,\nuse hints, be cautious, add missparsed messages\nuse html, use past error logs, use think history,\nuse diff, filter visible elements only,\nfilter with bid only, filter som only, multiaction,\nstrict, long description, individual examples,\nuse plan, use criticise, use memory, enable chat\n\nK STANDARD ERROR\n\nRestricting to the MinEval subset, we average the results on each task across 3 different runs in Table\n19. We give both the means and standard errors for the insight recall, factuality, distractor avoidance, and\nreport quality.\n\nL COMPLEX MODEL ABLATION RESULTS\n\nExtending our discussion in Section 5.3 to more GPT and Llama models in Table 20, we see that the\nsmaller GPT-5-mini model lags behind but still outperforms earlier closed-source backbones such as\nGPT-4o and GPT-4o-mini, particularly in terms of harmonic mean. In addition, smaller variants of Llama\n\n5https://openai.com/index/hello-gpt-4o/\n\n31\n\n\fW Web Agents System Prompt\n\nYou are an agent trying to solve a web task based on the content of the\npage and user instructions. You can interact with the page and explore,\nand send messages to the user. Each time you submit an action it will\nbe sent to the browser and you will receive a new page.\nYou will be solving tasks that involve Deep Research, by\nnavigating websites and web apps with information useful\nsolving the task. You will need to gather insights from\ndata contained in the services provided and the internet\nto complete your report. You have a maximum of 50 steps\nto complete the task. Before the end, you must use the\naction {send msg to user}, which should contain a final Deep\nResearch report detailing everything the user needs to\nknow.\n\nYou must sustain your claims in files, chats, or emails\nfrom the enterprise environment or in websites you\nsearched. You must provide a citation (or an inline\ncitation, that works too) with the source of those claims\n(e.g. << add citation examples from the documentation\n>>). Do not make up citations if you haven’t retrieved its\ncontent.\n\nHere are some expected agent behavior:\n\n**Enterprise Environment Interaction:**\n- Access Nextcloud files, Mattermost chats, emails, VNC desktop,\netc.\n- Extract relevant information from multiple sources\n- Navigate complex enterprise data landscapes\n\n**Report Requirements:**\n- Synthesize findings into comprehensive research\nreport\n- Include proper citations for all claims (flexible format -\nauto-normalized)\n- Draw meaningful insights and conclusions\n- Ground all statements in available evidence\n\n**Citation Format (Flexible - Auto-Normalized):**\n- Files: ‘quarterly report.pdf‘, ‘shared/budget-analysis.xlsx‘,\n‘Analysis document (reports/analysis.docx)‘\n- URLs: Direct links or ‘[Article Title](https://example.com)‘\n- Emails: ‘Email from alice@company.com on Jan 20,\n2025‘\n- Chat: ‘Mattermost message from john.doe in Compliance team, General\nchannel‘\n\nIf you need navigate the internet (outside of the designated websites),\nyou can use the browser inside noVNC Desktop.\n\nPrompt 6: Extended instructions given to the Deep Research web agent.\n\n32\n\n\fTable 17: Web Agents Non-Boolean Hyperparameters\n\nParameter\n\nValue\n\nchat model.model name\nchat model.max total tokens\nchat model.max input tokens\nchat model.max new tokens\nchat model.temperature\naction.action set.subsets\naction.action set.retry with force true\nflags.max prompt tokens\nflags.max trunc itr\nenv.max steps\n\ngpt-4.1\n32768\n28672\n4096\n0\nwebarena\n\n28672\n20\n50\n\nW Web Agents Task Intent Prompt\n\nHow can Lee’s Market leverage FSMA 204 regulations to enhance food\nsafety and customer trust?\n\nHere is some information about the user:\nID: MNG0003\nFirst name: John\nLast name: Doe\n...\nJustification: As a regulatory affairs manager, John\n...\n\nHere is some information about the company:\nName: Lee’s Market\nAnnual revenue: $500M - $600M\n...\nTarget markets: Asian communities in the U.S. and ...\n\nPrompt 7: Web Agents Task Intent Prompt\n\nTable 18: Web Agents tends to get stuck on cycles of actions, and are unable to backtrack or to restart\nwith a different application.\n\n9. I have been repeatedly clicking\non the “shared” folder (bid ‘194’)\n. . . To proceed with the research\non personalization strategies for\nLee‘s Market, I should open the\n“shared” folder to look for. . .\nclick(‘194’)\n\n10. I have clicked on the “shared”\nfolder multiple times . . .\nI will\ntry clicking on the “shared” folder\nagain to see if it opens and displays\nits contents.\n\nclick(‘190’)\n\n11. I have been repeatedly clicking\non the “shared” folder (bid ‘194’)\nTo proceed, I should try\n. . .\nclicking on the “shared” folder\nagain to attempt to open it and\nview any files . . .\n\nclick(‘194’)\n\n12. I have navigated to the “shared”\nfolder, but I have not yet opened\nit . . . The next logical step is to\nopen the “shared” folder to look\nfor any relevant documents . . .\n\nclick(‘194’)\n\ndegrade metric results further. This further substantiates the claim that larger and more advanced models\ntend to offer a better balance between recall, factuality, and overall report quality.\n\nM QUANTITATIVE RESULTS PER TASK\n\nWe show a detailed breakdown of the insights recall in Table 21, factuality in Table 22, distractor avoidance\nin Table 23 and report quality in Table 24 on the MinEval subset for a variety of models.\n\n33\n\n\fTable 19: DRBA performance with different planning configurations on MinEval. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. Scores are reported for insight recall, factuality, distractor\navoidance, report quality, and the overall harmonic mean.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n.188 ± .038\n.137 ± .04\n.154 ± .03\n.197 ± .03\n.142 ± .04\n.188 ± .05\n\n.665 ± .09\n.622 ± .11\n.705 ± .05\n.604 ± .08\n.504 ± .10\n.691 ± .03\n\n.981 ± .01\n1.00 ± .00\n1.00 ± .00\n.995 ± .39\n.990 ± .39\n1.00 ± .00\n\n.912 ± .004\n.901 ± .006\n.917 ± .004\n.928 ± .005\n.906 ± .007\n.923 ± .006\n\n.448\n.365\n.400\n.454\n.359\n.453\n\nTable 20: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Scores are reported for insight recall, factuality, distractor avoidance, report quality,\nand harmonic mean. Note that higher numbers corresponds to better scores, and the best result on each\nmetric is bolded.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nGPT-5-mini\nGPT-5-mini\nGPT-5-mini\nGPT-4o\nGPT-4o\nGPT-4o\nGPT-4o-mini\nGPT-4o-mini\nGPT-4o-mini\nGPT-OSS-120B\nGPT-OSS-120B\nGPT-OSS-120B\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-70b-Instruct\nLlama-3.1-70b-Instruct\nLlama-3.1-70b-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n25.68\n28.37\n26.57\n17.53\n20.37\n17.31\n13.75\n13.67\n13.08\n22.40\n17.42\n18.31\n17.37\n16.97\n20.16\n18.54\n16.28\n17.02\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n58.76\n58.96\n51.07\n65.43\n62.35\n60.84\n46.68\n55.59\n48.95\n29.24\n27.48\n38.92\n78.91\n79.27\n69.75\n64.64\n69.43\n52.82\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n96.51\n95.81\n94.48\n99.05\n98.57\n98.33\n99.05\n97.14\n97.14\n97.14\n97.14\n98.1\n100.00\n98.10\n97.90\n96.70\n97.62\n98.10\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n84.48\n85.74\n86.28\n92.58\n93.12\n91.62\n84.72\n85.86\n86.04\n84.24\n84.36\n85.44\n90.48\n92.34\n91.26\n85.32\n84.96\n86.16\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n60.21\n63.73\n58.89\n48.47\n53.06\n47.35\n38.33\n39.39\n37.28\n44.82\n38.38\n44.14\n49.78\n48.87\n53.86\n50.08\n46.41\n45.46\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\nN EFFECT OF NUMBER OF ITERATIONS\n\nWe next analyze the effect of varying the number of research loop iterations when using DRBA with\nGPT-5 as the backbone language model. Results for both the baseline configuration without explicit\nplanning and the complex planning setup are shown in Table 25. Overall, increasing the iteration budget\ndoes not guarantee consistent improvements. With no planning, performance initially drops when the\nagent executes more iterations, as additional exploration often introduces noise and distracts from key\ninsights. However, with a larger budget the agent partially recovers, suggesting that a small number of\nadditional iterations can help refine factual grounding, while excessive exploration reduces focus.\n\nFor the complex planning setting, higher iterations improve certain metrics such as factuality, but this\ncomes at the cost of lower insight recall and reduced overall balance. This indicates that while more steps\nallow the agent to verify citations more carefully, they can also lead to fragmented reasoning and overfitting\n\n34\n\n\fFigure 8: Total average Insight Recall scores per model and insight source type computed on all the\nresults available for each model running in Complex Research Plan mode. Insights embedded in enterprise\nsources are more easily retrieved by DRBA in all the models.\n\nto peripheral evidence. The best overall performance emerges at moderate iteration counts, highlighting\nthe importance of carefully tuning the iteration budget rather than simply scaling up the number of steps.\n\nO DATA GENERATION PROMPTS\n\nO.1 COMPANY AND PERSONA DATA GENERATION\n\nIn this section we give the prompts used for company generation 8 and persona generation 9.\n\nW Company Generation Prompt\n\nGenerate a realistic company structure for {company name} in the\n{industry} industry.\n\nCompany size: {size} ({employee range} employees)\n\nThe company should focus on this domain {domain}\n\nEXTERNAL INSIGHTS: {external insights}\n\nReturn ONLY a valid JSON object with this structure:\n{output structure}\n\nMake it realistic for the {industry} industry.\n\nPrompt 8: Company Generation Prompt Template.\n\n35\n\ngpt-5enterprise_fact0.597external_fact0.0deepseek-chat-v3.1enterprise_fact0.472external_fact0.0gpt-5-minienterprise_fact0.444external_fact0.0qwen-2.5-72b-instructenterprise_fact0.417external_fact0.0llama-3.1-405b-instructenterprise_fact0.347external_fact0.0gpt-oss-120benterprise_fact0.333external_fact0.0gpt-4o-minienterprise_fact0.194external_fact0.0llama-3.1-70b-instructenterprise_fact0.194external_fact0.0gpt-4oenterprise_fact0.182external_fact0.0\fTable 21: Mean and standard error of the insight recall metric for the first five tasks, obtained from three\nruns of on DRBench our agent (DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nW Persona Generation Prompt\n\nDR0001\n.222 ± .056\n.222 ± .056\n.278 ± .056\n.111 ± .056\n.222 ± .056\n.000 ± .000\n.111 ± .056\n.167 ± .000\n.111 ± .056\n.000 ± .000\n.056 ± .056\n.000 ± .000\n.111 ± .111\n.056 ± .056\n.000 ± .000\n.167 ± .000\n.222 ± .056\n.111 ± .056\n.167 ± .000\n.056 ± .056\n.167 ± .000\n.167 ± .000\n.278 ± .056\n.167 ± .000\n.222 ± .056\n.111 ± .056\n.167 ± .096\n\nDR0002\n.429 ± .000\n.381 ± .048\n.381 ± .126\n.286 ± .000\n.286 ± .082\n.381 ± .126\n.238 ± .048\n.333 ± .048\n.238 ± .095\n.095 ± .048\n.190 ± .048\n.190 ± .048\n.143 ± .000\n.143 ± .000\n.190 ± .048\n.143 ± .000\n.190 ± .048\n.238 ± .048\n.381 ± .048\n.286 ± .082\n.238 ± .048\n.286 ± .082\n.238 ± .048\n.095 ± .048\n.238 ± .048\n.333 ± .048\n.143 ± .082\n\nDR0003\n.467 ± .033\n.533 ± .033\n.567 ± .067\n.367 ± .033\n.333 ± .033\n.367 ± .067\n.167 ± .033\n.300 ± .000\n.300 ± .100\n.267 ± .033\n.167 ± .067\n.267 ± .033\n.433 ± .033\n.367 ± .033\n.333 ± .067\n.233 ± .088\n.167 ± .033\n.300 ± .058\n.200 ± .000\n.200 ± .058\n.267 ± .033\n.300 ± .058\n.333 ± .033\n.600 ± .058\n.400 ± .058\n.333 ± .088\n.400 ± .058\n\nDR0004\n.519 ± .098\n.370 ± .098\n.370 ± .037\n.222 ± .064\n.296 ± .074\n.370 ± .098\n.185 ± .074\n.148 ± .037\n.111 ± .000\n.185 ± .037\n.148 ± .074\n.074 ± .037\n.222 ± .000\n.148 ± .098\n.111 ± .064\n.185 ± .074\n.111 ± .064\n.148 ± .098\n.074 ± .037\n.185 ± .098\n.074 ± .074\n.259 ± .037\n.148 ± .074\n.370 ± .037\n.259 ± .037\n.259 ± .037\n.333 ± .000\n\nDR0005\n.281 ± .035\n.386 ± .046\n.386 ± .046\n.298 ± .018\n.281 ± .063\n.211 ± .053\n.175 ± .018\n.070 ± .018\n.105 ± .000\n.140 ± .035\n.123 ± .018\n.123 ± .018\n.211 ± .030\n.158 ± .061\n.281 ± .063\n.140 ± .035\n.158 ± .053\n.211 ± .030\n.105 ± .030\n.088 ± .046\n.105 ± .000\n.246 ± .046\n.281 ± .046\n.281 ± .035\n.158 ± .061\n.123 ± .018\n.298 ± .076\n\nGenerate {persona count} diverse employee personas for {company name} in\nthe {industry} industry.\n\nThe personas should focus on this domain: {domain}\n\nCreate diverse roles across seniority levels: Junior, Mid, Senior,\nExecutive\n\nReturn ONLY a valid JSON array with this exact format:\n{output structure}\n\nMake personas realistic with appropriate responsibilities for their\nroles.\n\nPrompt 9: Persona Generation Prompt.\n\nO.2 QUESTION GENERATION\n\nIn this section we give the prompt used to generate our deep research questions 10.\n\n36\n\n\fTable 22: Mean and standard error of the factuality metric for the first five tasks, obtained from our agent\n(DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nSimple\nGPT-4o-mini\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.761 ± .072\n.714 ± .050\n.730 ± .060\n.585 ± .045\n.647 ± .120\n.309 ± .126\n.792 ± .150\n.485 ± .262\n.475 ± .166\n.828 ± .043\n.611 ± .056\n.557 ± .030\n.144 ± .099\n.074 ± .074\n.368 ± .061\n.852 ± .087\n.802 ± .125\n.789 ± .053\n.618 ± .109\n.608 ± .173\n.588 ± .082\n.860 ± .014\n.696 ± .041\n.581 ± .042\n.674 ± .077\n.806 ± .049\n.626 ± .114\n\nDR0002\n.504 ± .075\n.384 ± .076\n.291 ± .094\n.297 ± .119\n.299 ± .056\n.381 ± .161\n.490 ± .110\n.512 ± .131\n.653 ± .097\n.265 ± .133\n.429 ± .092\n.324 ± .169\n.150 ± .035\n.128 ± .072\n.178 ± .078\n.726 ± .158\n.638 ± .202\n.392 ± .154\n.431 ± .160\n.681 ± .069\n.286 ± .108\n.518 ± .085\n.531 ± .086\n.657 ± .024\n.493 ± .109\n.540 ± .174\n.396 ± .056\n\nDR0003\n.866 ± .007\n.848 ± .034\n.782 ± .012\n.705 ± .039\n.624 ± .041\n.699 ± .047\n.827 ± .056\n.813 ± .041\n.704 ± .037\n.800 ± .000\n.622 ± .062\n.580 ± .075\n.386 ± .040\n.410 ± .090\n.564 ± .064\n.803 ± .028\n.800 ± .074\n.792 ± .055\n.684 ± .104\n.800 ± .069\n.686 ± .011\n.818 ± .041\n.922 ± .056\n.838 ± .050\n.866 ± .002\n.741 ± .074\n.723 ± .053\n\nDR0004\n.833 ± .019\n.812 ± .070\n.782 ± .064\n.704 ± .067\n.694 ± .028\n.692 ± .111\n.570 ± .058\n.693 ± .139\n.542 ± .110\n.690 ± .128\n.481 ± .209\n.642 ± .119\n.337 ± .117\n.311 ± .155\n.400 ± .076\n.820 ± .066\n.892 ± .035\n.771 ± .073\n.812 ± .021\n.826 ± .067\n.522 ± .270\n.679 ± .095\n.769 ± .035\n.774 ± .053\n.741 ± .060\n.724 ± .101\n.587 ± .139\n\nDR0005\n.762 ± .077\n.846 ± .029\n.674 ± .121\n.647 ± .098\n.683 ± .020\n.472 ± .114\n.593 ± .204\n.614 ± .121\n.406 ± .020\n.459 ± .235\n.191 ± .046\n.344 ± .144\n.445 ± .051\n.451 ± .080\n.435 ± .190\n.745 ± .022\n.832 ± .083\n.745 ± .100\n.687 ± .073\n.557 ± .143\n.559 ± .240\n.757 ± .057\n.754 ± .082\n.662 ± .098\n.696 ± .060\n.550 ± .148\n.586 ± .055\n\nO.3 PUBLIC SOURCE AND INSIGHT COLLECTION\n\nIn this section we give the prompt used to generate external insights 11. The URLs used for external\ninsight extraction and deep research question creation can be found in Table 26.\n\nO.4\n\nINTERNAL INSIGHT GENERATION\n\nIn this section we give the prompts to generate both internal insights 12 and internal distractors 13.\n\nO.5 FILE GENERATION\n\nIn this section we give the prompts used for generating each of the file types used in our tasks, which\nwe list as follows:\n\n• PDF: Prompts 18, 19, and 20\n\n• Excel: Prompts 21, 22, and 23\n\n• Powerpoint: Prompts 24, 25, and 26\n\n• Email: Prompts 27, 28, and 29\n\n• Chat: Prompts 30, 31, 29, and 33\n\n37\n\n\fTable 23: Mean and standard error of the distractor avoidance metric for the first five tasks, obtained from\nthree runs of on DRBench our agent (DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.857 ± .000\n.857 ± .000\n.905 ± .048\n.905 ± .048\n.857 ± .000\n.857 ± .000\n.952 ± .048\n.952 ± .048\n.952 ± .048\n.952 ± .048\n.857 ± .000\n.857 ± .000\n.857 ± .000\n.857 ± .000\n.905 ± .048\n1.00 ± .000\n.905 ± .048\n.952 ± .048\n.905 ± .048\n.905 ± .048\n.905 ± .048\n.905 ± .048\n.857 ± .000\n.905 ± .048\n1.00 ± .000\n.952 ± .048\n.952 ± .048\n\nDR0002\n.900 ± .058\n1.00 ± .000\n.900 ± .058\n.967 ± .033\n.933 ± .033\n.867 ± .133\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.967 ± .033\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.967 ± .033\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n\nDR0003\n1.00 ± .000\n1.00 ± .000\n.933 ± .000\n.978 ± .022\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.978 ± .022\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.978 ± .022\n\nDR0004\n1.00 ± .000\n1.00 ± .000\n.905 ± .024\n.976 ± .024\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.964 ± .036\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.952 ± .048\n.976 ± .024\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.929 ± .000\n.905 ± .024\n.952 ± .024\n.952 ± .048\n\nDR0005\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n\nP EVALUATION PROMPTS\n\nIn this section we give the prompts for decomposing reports into atomic insights 14, computing insight\nrecall 15, computing factuality 16, and computing report quality 17. These prompts are discussed in detail\nin Section 5.1.\n\nQ HUMAN PREFERENCE EVALUATION\n\nWe calculate a human score for model a task t as:\n\nSa,t = 1\nn\n\n(cid:80)n\n\ni=1sa,i, where sa,i =\n\n(cid:26)1,\n\nif human choice is a or ”both good”\n\n0, otherwise\n\n, where n is the number of gold insights in task t, sa,i is the human score of model a on insight i. Figure9\nshows that the insight recall metric is on par with human decision.\n\n38\n\n\fTable 24: Mean and standard error of the report quality metric for the first five tasks, obtained from three\nruns of DRBench with 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.936 ± .008\n.942 ± .000\n.948 ± .007\n.892 ± .002\n.901 ± .009\n.896 ± .001\n.927 ± .000\n.934 ± .008\n.929 ± .009\n.886 ± .004\n.893 ± .002\n.889 ± .003\n.872 ± .007\n.878 ± .006\n.874 ± .007\n.914 ± .000\n.921 ± .008\n.917 ± .009\n.889 ± .003\n.895 ± .002\n.891 ± .003\n.884 ± .004\n.890 ± .003\n.886 ± .004\n.901 ± .001\n.908 ± .000\n.904 ± .001\n\nDR0002\n.918 ± .004\n.927 ± .008\n.921 ± .001\n.884 ± .007\n.889 ± .002\n.882 ± .006\n.911 ± .003\n.919 ± .000\n.914 ± .002\n.874 ± .008\n.881 ± .005\n.877 ± .006\n.861 ± .001\n.867 ± .009\n.863 ± .000\n.903 ± .003\n.910 ± .001\n.906 ± .002\n.877 ± .007\n.883 ± .005\n.879 ± .006\n.872 ± .008\n.878 ± .006\n.874 ± .007\n.889 ± .005\n.896 ± .003\n.892 ± .004\n\nDR0003\n.924 ± .005\n.936 ± .006\n.940 ± .008\n.879 ± .004\n.887 ± .001\n.884 ± .003\n.903 ± .001\n.911 ± .009\n.905 ± .000\n.861 ± .007\n.867 ± .004\n.864 ± .005\n.849 ± .009\n.854 ± .008\n.851 ± .008\n.897 ± .001\n.904 ± .000\n.899 ± .001\n.869 ± .005\n.874 ± .004\n.871 ± .005\n.864 ± .006\n.870 ± .005\n.866 ± .006\n.881 ± .002\n.888 ± .001\n.884 ± .002\n\nDR0004\n.909 ± .001\n.915 ± .002\n.922 ± .009\n.891 ± .000\n.895 ± .008\n.889 ± .009\n.918 ± .009\n.923 ± .001\n.920 ± .008\n.872 ± .002\n.878 ± .001\n.875 ± .000\n.858 ± .006\n.863 ± .004\n.860 ± .005\n.909 ± .008\n.915 ± .009\n.911 ± .008\n.881 ± .001\n.886 ± .000\n.883 ± .001\n.876 ± .002\n.881 ± .001\n.878 ± .002\n.893 ± .009\n.899 ± .008\n.895 ± .009\n\nDR0005\n.927 ± .009\n.933 ± .007\n.929 ± .008\n.886 ± .005\n.892 ± .003\n.890 ± .001\n.909 ± .002\n.916 ± .000\n.913 ± .009\n.879 ± .006\n.884 ± .003\n.882 ± .004\n.866 ± .008\n.872 ± .007\n.869 ± .006\n.902 ± .002\n.908 ± .000\n.905 ± .001\n.873 ± .004\n.878 ± .003\n.875 ± .004\n.869 ± .005\n.874 ± .004\n.871 ± .005\n.885 ± .003\n.891 ± .002\n.888 ± .003\n\nTable 25: Effect of the number of research loop iterations on the performance of DRBA with GPT-5 as\nthe backbone model, on MinEval.\n\nPlanning Method\nNone\nNone\nNone\nComplex\nComplex\nComplex\n\n# Iterations\n15\n30\n50\n15\n30\n50\n\nInsight Recall\n39.45\n28.80\n37.10\n44.44\n31.61\n38.16\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n72.65\n69.03\n78.84\n62.51\n73.94\n66.05\n\n93.14\n98.57\n100.00\n90.95\n94.38\n94.38\n\n94.56\n96.12\n93.48\n93.12\n94.76\n92.64\n\n66.20\n57.34\n66.30\n66.41\n60.32\n63.76\n\n39\n\n\fW Deep Research Question Generation Prompt\n\nGenerate 3 Deep Research (DR) questions for the following business\ncontext:\n\nPersona: {persona name} - {persona role}\nDepartment: {persona department}\nResponsibilities: {persona responsibilities}\nCompany: {company name} ({company industry})\nDomain: {domain}\n\nExternal Insights: {external insights}\n\nGenerate 3 Deep Research questions that:\n1. Are appropriate for the persona’s role and department\n2. Require analysis of the provided internal insights\n3. Consider the external market context ...\n\nEach question should be 1 sentence of 15 words max, in\nplain english, and end with a question mark. Do not\ninclude any preamble or explanation - return only the JSON\narray.\n\nReturn ONLY a valid JSON array with this structure:\n{output structure}\n\nPrompt 10: Deep Research Question Generation Prompt Template. Subquestions are generated to help\nhuman annotators select good DR questions.\n\nW External Insight Extraction Prompt\n\nYou will be given a report (with url, and date). Based\non the report, generate 3 external insights that summarize\nimportant findings, trends, or takeaways from the\nreport.\n\nOutput Format\n{output structure}\n\nUrl: {url}\nIndustry: {industry}\nDomain: {domain}\nCompany Information: {company information}\n\nImportant notes\nFocus only on insights that are external and grounded in the report.\nInsights should be concise, factual, and directly tied to the retail\nindustry context.\n\nPrompt 11: External Insight Extraction Prompt.\n\n40\n\n\fTable 26: Public URLs For Deep Research Task Creation.\n\nIndustry\n\nDomain\n\nReference\nGrocers on FSMA-204 Compliance (GroceryDive) 2\nGrocery Loyalty & Inflation (EagleEye) 2\nGrocery Trends Outlook 2025 (GroceryDive) 2\nRetail IT Optimization (Thirdera) 2\nChatbots & Grocery Interactions (GroceryDoppio) 2\n\nTelehealth Regulations (HealthcareDive) 2\nFuture of Healthcare CRM (WTT Solutions) 2\nFuture of Telehealth (CHG Healthcare) 2\nHealthcare ITSM (Topdesk) 2\nPatient Engagement Tech (TechTarget) 2\n\nKnowledge Mgmt Retail Knowledge Management (Knowmax) 2\n\nSales\nCybersecurity\nPublic Relations Walmart CSR Strategy (SunriseGeek) 2\n\nPersonalization in Action (BCG) 2\nRetail Cybersecurity Threats (VikingCloud) 2\n\nCompliance\nCRM\nMarket Analysis\nITSM\nCSM\n\nCompliance\nCRM\nMarket Analysis\nITSM\nCSM\n\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles Quality Assurance EV Quality Issues (GreenCars) 2\nAutomobiles\nAutomobiles Market Analysis\n\nSales\nMarketing\nCybersecurity\nCompliance\nCRM\nCSM\nSales\nResearch\nCybersecurity\n\nAsset Mgmt\n\nSales for Digital Health (Medium) 2\nMarketing Telehealth Services (MarketingInsider) 2\nHealthcare Cybersecurity 2024 (AHA) 2\nEvolving EV Regulations (WardsAuto) 2\nSalesforce Automotive Cloud (TechTarget) 2\nEV Aftersales Support (EVReport) 2\nTesla vs Dealerships (TheWeek) 2\nAI for EV Optimization (Here.com) 2\nCybersecurity Risks in Cars (HelpNetSecurity) 2\n\nDigital Twins in Autos (RTInsights) 2\nGlobal EV Outlook 2024 (IEA) 2\n\nKnowledge Mgmt Knowledge Mgmt in Healthcare (C8Health) 2\n\nFigure 9: Comparison of Human Scores and Insight Recall Scores. As can be seen the human evaluation\nresults are aligned with our automated evaluation.\n\n41\n\nDR0001DR0002DR0003DR0004DR0006DR0007DR0008DR0011DR0012DR0013DR0014Task ID0.00.20.40.60.81.0Score ValueHuman Scores vs Insights Recallllama-3.1-405b (Human Score)gpt-5 (Human Score)llama-3.1-405b (Insights Recall)gpt-5 (Insights Recall)\fW Internal Supporting Insight Generation Prompt\n\nBased on the Deep Research (DR) Question, external\nmarket insights, company context, and previous internal\ninsights, generate 3 specific QA pair that an expert data\nscientist would need to get in order to address the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for inspiration): {external context}\nQA History: {qa list}\n\nInsight Requirements:\n- Use the DR Question as the central theme for the\nanswer.\n- Draw inspiration and supporting details from the other internal\ninsights and external insights provided.\n- Include QUANTITATIVE DATA in the answer: metrics, percentages, dollar\namounts, timeframes, KPIs.\n\nSpecific Question Instructions\n- the specific question should be a question that would be a step\ntowards resolving the DR Question\n{additional question instructions}\n\nAnswer Instructions\n- the answer should be 12 words max and minimum 5\nwords\n{additional answer instructions}\n\nJustification Instructions\n- the justification should directly explain how the specific question,\nanswer pair help address the DR Question in 15 words\nmax\n\nMisc Instructions\n- the filename should be 3 words max with dashes\nin between, do not mention the file type in the\nfilename\n- use the example below as inspiration but do not use it\ndirectly\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nPrompt 12: Internal Supporting Insight Generation Prompt Template. specific questions and justification\nhelp human annotators to select good insights.\n\n42\n\n\fW Internal Distractor Insight Generation Prompt\n\nBased on the Deep Research (DR) Question, external\nmarket insights, company context, and previous internal\ninsights, generate 3 specific QA pairs that are DISTRACTOR\nquestions\n- these should be questions that an expert data scientist might\nask about the company but are NOT relevant to addressing the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for inspiration): {external context}\nQA History: {qa list}\n\nDISTRACTOR Requirements:\n- Generate questions that are plausible for this\ncompany and industry but DO NOT help address the DR\nQuestion.\n- The questions should be about the company’s operations, metrics, or\nbusiness but tangential to the DR Question.\n- Include QUANTITATIVE DATA in the answer: metrics, percentages, dollar\namounts, timeframes, KPIs.\n- Focus on different business areas that are NOT central to the\nDR Question (e.g. if DR Question is about pricing, ask about HR\nmetrics).\nSpecific Question Instructions\n- the specific question should be a plausible business\nquestion for this company but NOT related to the DR\nQuestion\n- the specific question should lead to a quantitative answer and should\nbe dated such as Q3 of 2025, the question should contain a date like Q2\nof 2024\n- the specific question should be 10 words max\n- make sure to be different from any question in the\nqa list\n- choose business areas like: HR metrics, facility costs, IT\ninfrastructure, compliance, training, etc. that are UNRELATED to the\nDR Question\n- make sure to be different from any question in the QA\nHistory\n\nAnswer Instructions {answer instructions}\n\nJustification Instructions\n- the justification should explain why this specific question\nis NOT relevant to addressing the DR Question in 15 words\nmax\n\nMisc Instructions {misc instructions}\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nPrompt 13: Internal Distractor Insight Generation Prompt. specific questions and justification help human\nannotators to select good insights.\n\n43\n\n\fW Breaking Report into Insights Prompt\n\nPlease break down the following report text into insight claims. Each\ninsight claim should be:\n\n1. A single insight, that might include multiple statements and\nclaims\n2. Independent and self-contained\n3. Each claim can have more than one sentence, but should be focused\non a single insight\n4. Support each insight with citations from the report text following\nthese specific rules: {rules}\n5. Citations should be in one of these formats (various formats will\nbe automatically normalized):\n{citation formats} 6. Do not include general summaries, opinions,\nor claims that lack citation, just the sentences that are\nfacts.\n7. Each claim should be a concise but complete\nsentence.\n\nReport text: {report text}\n\nOutput format: Please return the insight claims as a JSON array. For\nexample: {output structure}\n\nReturn only valid JSON, no additional text. Just use the report\nbetween <START OF REPORT> and <END OF REPORT> tags to generate\ninsights. If no insights found, return an empty JSON array:\n[]\n\nDo not use the example outputs as report content.\n\nPrompt 14: Insight Extraction Prompt.\n\n44\n\n\fW Insight Recall Prompt\n\nYour goal is to check if one of the Predicted Insights\nextracted from a report is a Golden Insight. You\nmust be STRICT and pay attention to every small\ndetail.\n\nInstructions:\n* Evaluate if the Predicted Insights contain sufficient information to\nderive a Golden Insight.\n* Select the insight that most closely matches the Golden Insight.\nSelect one and only one insight.\n* Answer of yes or no where:\n- yes: Selected insight contains comprehensive information to fully\nderive the expected insight\n- no: Selected insight lacks the necessary information, misses key\ndetails, or has significant gaps\n* Be STRICT - do not answer yes for partial matches, vague similarities,\nor general information.\n\nHowever, no exact wording is required and paraphrasing is\nacceptable.\n* IMPORTANT: Only consider details given in the Golden Insight when\nanswering yes or no. Don’t expect anything more than what is given in\nthe Golden Insight.\n* Focus on factual accuracy, completeness, and specificity.\n\nPredicted Insights: {claims text}\nGolden Insight: {gold insight}\n\nReturn a valid json dictionary with the following structure:\n{output structure}\n\nEnsure only a json dictionary is returned, and return nothing else.\n\nPrompt 15: Insight Recall Scoring Prompt.\n\n45\n\n\fW Factuality Prompt\n\nGiven the following relevant source context from multiple sources and\nan insight, determine if the insight is factually supported by the\nsources.\n\nRelevant Source Materials (from multiple sources):\n{context}\nAtomic Claim: {insight}\n\nEVALUATION CRITERIA:\nThe claim is factual if the core factual content\nis supported by the sources. You should be strict\nabout important details but flexible about exact\nwording:\nREQUIRED for TRUE:\n1. All key factual details (numbers, dates, names,\npercentages, specific facts) must be present in at least one\nsource\n2. The main substance and meaning of the claim must be supported by\nthe source contexts\n3. No part of the claim should contradict the information in any of\nthe sources\nACCEPTABLE variations:\n{acceptable variations}\nMark as FALSE if:\n- Important factual details are missing, incorrect, or unsupported\nacross all sources\n- The claim contradicts information in any of the\nsources\n- The core meaning cannot be verified from any of the source\ncontexts\n\nEXAMPLES: {examples}\n\nFocus on the substantive factual accuracy rather than\nexact word-for-word matching. You MUST respond with either\ntrue or false under the <factual> tag. Then provide a\nbrief explanation under the <explanation> tag explaining\nwhich parts are supported or not supported and from which\nsources.\n\nFormat your response EXACTLY as:\n{output structure}\n\nPrompt 16: Factuality Scoring Prompt.\n\n46\n\n\fW Report Quality Prompt\n\nYou are a Deep Research Evaluator. You are given:\n1. A research report.\n2. A deep research (DR) question that the report attempts to\nanswer.\n3. A persona that represents the intended audience for the\nreport.\n\n{persona}\n{dr question}\n{report text}\n\n## Instructions:\n**ANALYZE THOROUGHLY**: Examine the report in detail\nand identify any issues, even small ones. Look for\nsubtle problems, minor inconsistencies, areas that could\nbe improved, or any shortcomings that might affect\nthe quality. Evaluate the report according to the\nfive criteria listed below. For **each criterion**,\nprovide:\n- A **score between 1 and 10** (must be an integer) using the scale\ndefined below.\n- A **detailed justification** (2–3 sentences) in\n**simple plain English** explaining why you gave that\nscore, including any specific issues or strengths you\nidentified.\n\n### Scoring Scale (1-10, integers only): {scoring scale}\n\n### Criteria:\n\n1. Depth & Quality of Analysis\n2. Relevance To DR Question\n3. Persona Consistency\n4. Coherence & Conciseness\n5. Degree of Contradictions\n6. Completeness & Coverage\n\n{output structure}\n\nPrompt 17: Report Quality Scoring Prompt.\n\n47\n\n\fW PDF Outline Generation Prompt\n\nYou are an expert business document designer creating\nrealistic enterprise PDF reports. Given a Deep\nResearch (DR) Question and company context, generate\nan outline for a professional business document that\nan employee would create based on their persona and\nrole.\n\nCompany: {company name} - {company description}\nIndustry: industry\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context:{persona context}\nDR Question: {dr question}\n\nDocument Structure Requirements:\n- Create a professional PDF document outline with exactly {n subsections}\nsubsections\n- The document should be something this persona would realistically\ncreate in their role\n- Include a concise, professional file title appropriate for enterprise\ndocumentation\n\nSubsection Heading Requirements: {subsection requirements}\n\nIntroduction Requirements:\n- Write a professional 4-sentence maximum introduction\nparagraph - Should set context for the document and its\npurpose\n- Must align with the persona’s role and the company’s business\nneeds\n- Should sound like something this employee would write for internal\nstakeholders\n\nConclusion Requirements:\n- Write a professional 4-sentence maximum conclusion\nparagraph\n- Should summarize key takeaways and next steps\n- Must align with the persona’s perspective and recommendations\n- Should provide actionable insights for the intended\naudience\n\nReturn ONLY a valid Python dictionary with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Ensure the document feels authentic for this persona´s role and company\ncontext\n\nPrompt 18: PDF Outline Generation Prompt. This is the first step of embedding an insight into a PDF\ndocument. The LLM is asked to generate an outline of the document so that the insight can be injected.\n\n48\n\n\fW PDF Insight Injection Prompt\n\nYou are an expert business document writer creating\nrealistic enterprise PDF content. Given a Deep\nResearch (DR) Question, company context, and a specific\ninsight, generate professional content that naturally\nincorporates the insight information to help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nSubsection Heading: {subsection heading}\n\nContent Generation Requirements:\n- Generate realistic business content for the given subsection\nheading\n- The content must contain exactly ONE paragraph of 4-5\nsentences\n- Content should be professional and sound like something this persona\nwould write\n- The paragraph must naturally incorporate the insight answer\ninformation but NOT copy it word-for-word\n{additional generation requirements}\n\nContent Strategy:\n- Present the insight information as business findings, analysis results,\nor operational data\n- Embed the key metrics within broader business context and\nimplications\n- Use natural business language to discuss the same information as in\nthe answer\n{additional content requirements}\n\nJustification Requirements:\n- Explain specifically how this content helps answer the DR\nQuestion\n- Reference the key information that would be useful for\ndecision-making\n- Keep justifications concise but clear (20 words\nmaximum)\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 19: PDF Insight Injection Prompt. This is the second step of embedding an insight into a PDF\ndocument. The LLM is fed with a subheading in the outline from 18, and tasked to write the subsection\nwith the insight embedded.\n\n49\n\n\fW PDF Irrelevant Section Generation Prompt\n\nYou are an expert business document writer creating\nrealistic enterprise PDF content. Given a Deep Research\n(DR) Question, company context, and subsection headings,\ngenerate distractor content for each subsection that is\nthematically related but does NOT help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: industry\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\nSubsection Headings: {subsection headings}\n\nContent Generation Requirements:\n- Generate realistic business content for each subsection\nheading\n- Each subsection must contain exactly ONE paragraph of 3-4 sentences\nmaximum\n- Content should be professional and sound like something this persona\nwould write\n- Content must be thematically related to the DR\nQuestion’s domain but NOT provide information to answer\nit\n{additional generation requirements}\n\nDistractor Strategy:\n- Focus on adjacent business areas that don’t directly impact the DR\nQuestion\n- Discuss historical context, general industry trends, or procedural\ninformation\n- Include operational details that are realistic but tangential\n- Reference related but non-essential business metrics or\nactivities\n- Avoid any content that would help someone answer the DR\nQuestion\n\nJustification Requirements:\n- Explain specifically why each paragraph’s content doesn’t help answer\nthe DR Question\n- Identify what type of distractor strategy was used\n(e.g., \"focuses on historical data vs current decision\nfactors\")\n- Keep justifications concise but clear (15 words\nmaximum)\n\nReturn ONLY a valid JSON array with this exact structure:\n{output structure}\n\nIMPORTANT: {important instructions}\n\nPrompt 20: PDF Irrelevant Section Generation Prompt. This is the third step of PDF document generation.\nThe LLM is asked to fill out the outline from 18 with irrelevant information.\n\n50\n\n\fW Excel Schema Generation Prompt\n\nGenerate the JSON schema and formatting of a table where the\nfollowing insight can be presented as a row in the table:\n{insight}\n\nThe schema and formatting should contain:\n- table name: a string of the table name\n- columns: a list of columns with the following\nfields:\n- name: column name\n- column type: one of\nSTRING|INTEGER|FLOAT|BOOLEAN|DATE|PERCENTAGE|CURRENCY\n- description: detailed description of what this column represents and\nhow it relates to the insight\n- formatting: a dictionary with the following fields:\n- header style: background color of the header, default is\nCCCCCC\n- column widths: width of each column, e.g. {{A: 15, B: 20, C:\n12}}\n- number formats: format of each column, e.g. {{A: \"0.00%\", B:\n\"$#,##0.00\", C: \"YYYY-MM-DD\"}}\nReturn only a json dictionary with the table name, columns, and\nformatting.\n\nRequirements:\n- The schema should be designed so that the insight can be represented\nas a row in the table\n- The schema should make it easy to generate more data points expanding\non the subject, theme and scope of the insight to populate the\ntable.\n- Use realistic column names that would be found in a business\nspreadsheet\n- Do not include the insight, specific question, or justification as\ncolumns in the table\n\nCompany Context: {company info}\nPlease keep this company context in mind when creating the\nschema.\n\nPersona Context: {persona}\n\nPlease keep this persona context in mind when creating the schema.\n\nPrompt 21: Excel Schema Generation Prompt. This is the first step of Excel file generation. The LLM\nis asked to generate the schema of the Excel file so that the insight can be injected.\n\n51\n\n\fW Excel Data Generation Prompt\n\nGiven the following schema: {schema and formatting}\nGenerate one row that embeds the following insight:\n{insight}\n\nThen generate 5-10 rows of data that populates the table.\nMake sure that with the new data added, the original\ninsight can still be extracted from the table. Return\nall the rows in a json dictionary with the following\nfields:\n- insight row: a list of values each corresponding to a column in the\nschema\n- irrelevant rows: a list of rows that are used to populate the table,\neach row is a list of values each corresponding to a column in the\nschema\n\nEnsure only a json dictionary is returned, and return nothing\nelse.\n\nRequirements:\n- Make sure the insight row stands out from the irrelevant rows by\ne.g.\n- Having the largest value\n- Covering the most recent timeframe\n\nPrompt 22: Excel Data Generation Prompt. This is the second step of Excel file generation. The LLM\nis asked to generate the data for an Excel file that the insight will be injected into.\n\nW Excel Filename Generation Prompt\n\nGenerate a professional filename for an Excel file that contains the\nfollowing sheets: {sheet names}\n\nThe filename should:\n1. Be descriptive and professional\n2. Reflect the main theme or purpose of the data\n3. Be suitable for a business environment\n4. Not exceed 50 characters\n5. Use only alphanumeric characters, spaces, hyphens, and\nunderscores\n6. Not include file extensions (like .xlsx)\nReturn only the filename, no additional text or\nquotes.\n\nCompany name:\n{company name}\n\nPlease keep this company name in mind when creating the filename.\n\nPrompt 23: Excel Filename Generation Prompt. This is the third step of Excel file generation. The LLM\nis asked to generate the filename for the Excel file that the insight will be injected into.\n\n52\n\n\fW Powerpoint Outline Generation Prompt\n\nYou are an expert business presentation designer creating\nrealistic enterprise PowerPoint presentations. Given a\nDeep Research (DR) Question and company context, generate\nan outline for a professional business presentation that\nan employee would create based on their persona and\nrole.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\n\nPresentation Structure Requirements:\n- Create a professional PowerPoint presentation outline with exactly\n{n subsections} slides\n- The presentation should be something this persona would realistically\ncreate in their role\n- Include a concise, professional presentation title appropriate for\nenterprise presentations\n\nSlide Heading Requirements:\n- Slide headings must follow the THEME of the DR\nQuestion but should NOT directly address the DR Question\nitself\n- Think of related business areas, adjacent topics, or\nsupporting themes that would naturally appear in an enterprise\npresentation\n- Headings should sound professional and realistic for this industry and\ncompany size\n- Each heading should be 3-8 words and use proper business\nterminology\n{additional slide requirements}\n\nConclusion Requirements:\n- Write a professional 2-sentence maximum conclusion for the\npresentation closing\n- Should summarize key takeaways and next steps\n- Must align with the persona’s perspective and recommendations\n- Should provide actionable insights for the intended\naudience\n\nReturn ONLY a valid Python dictionary with this exact\nstructure:\n{output structure}\n\nIMPORTANT: {important notes}\n\nPrompt 24: Powerpoint Outline Generation Prompt. This is the first step for generating powerpoint slides.\nThe LLM is asked to generate an outline of the slides so that the insight can be injected.\n\n53\n\n\fW Powerpoint Insight Injection Prompt\n\nYou are an expert business presentation writer creating\nrealistic enterprise PowerPoint content. Given a Deep\nResearch (DR) Question, company context, and a specific\ninsight, generate professional slide content that naturally\nincorporates the insight information to help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nSlide Heading: {subsection heading}\n\nContent Generation Requirements:\n- Generate realistic business content for the given slide\nheading\n- The content must contain exactly 5-8 bullet points with substantial\ndetail\n- Each bullet point should be 1-2 sentences with specific business\ninformation\n{additional generation requirements}\n\nContent Strategy:\n- Present the insight information as business findings, analysis results,\nor operational data\n- Embed the key metrics within broader business context and\nimplications\n- Use natural business language to discuss the same information as in\nthe answer\n{additional content requirements}\n\nJustification Requirements:\n- Explain specifically how this content helps answer the DR\nQuestion\n- Reference the key information that would be useful for\ndecision-making\n- Keep justifications concise but clear (25 words\nmaximum)\n\nReturn ONLY a valid JSON object with this exact\nstructure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 25: Powerpoint Insight Injection Prompt. This is the second step for generating powerpoint slides.\nThe LLM is asked to generate slide content with the insight embedded.\n\n54\n\n\fW Powerpoint Distractor Injection Prompt\n\nYou are an expert business presentation writer creating\nrealistic enterprise PowerPoint content. Given a Deep\nResearch (DR) Question, company context, and slide\nheadings, generate distractor content for each slide that\nis thematically related but does NOT help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\nSlide Headings: {subsection headings}\n\nContent Generation Requirements:\n- Generate realistic business content for each slide heading - Each\nslide must contain exactly 5-8 bullet points with substantial\ndetail\n- Each bullet point should be 1-2 sentences with specific business\ninformation\n- Content should be professional and sound like something this persona\nwould present\n{additional generation requirements}\n\nDistractor Strategy:\n- Focus on adjacent business areas that don’t directly impact the DR\nQuestion\n- Discuss historical context, general industry trends, or procedural\ninformation\n- Include operational details that are realistic but tangential\n- Reference related but non-essential business metrics or\nactivities\n{additional content requirements}\n\nReturn ONLY a valid JSON array with this exact structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 26: Powerpoint Distractor Injection Prompt. This is the third step for generating powerpoint slides.\nThe LLM is asked to generate slide content with distractor information.\n\n55\n\n\fW Email Setup Prompt\n\nYou are an expert in enterprise communication systems and organizational\nstructures. Your task is to generate a realistic setup of users\nfor an email system based on the given insights and company\ncontext.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Size: {company size} ({employee count} employees)\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\n**Specific Question** {specific question}\n**Answer to Specific Question** {answer}\n\nRequirements:\n- Users that would realistically discuss these insights\n- Generate a minimal but sufficient setup to support {num messages}\nemails discussing the insights\n- To make it realistic, generate at least 3 users\n\n- Use realistic names for people/teams/channels based on the company\ncontext\n\nReturn ONLY a JSON array of users with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Do NOT include any preamble, explanation, or extra text|return only\nthe Python dictionary\n- Ensure the structure is realistic for the company size and industry\n- Make sure the persona is included as a user\n\nPrompt 27: Email Setup Prompt. This is the first step for generating an email chain. The LLM is asked\nto generate the necessary setup for the email chain.\n\n56\n\n\fW Email Insight Injection Prompt\n\nYou are an expert at creating realistic business email\nconversations. Your task is to create an email thread that\ncontains the actual insight that helps answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\nDeep Research Question: {dr question}\nEmail Setup: {email setup}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create a realistic email thread of {num messages} that contains the\ntarget insight\n2. This thread should provide information that directly helps answer\nthe DR question\n3. The insight should be naturally embedded in the email\ncontent\n4. The emails should feel realistic and business-appropriate\n5. The sender should be someone who would naturally have access to\nthis insight\n6. The persona needs to be either a recipient or the sender of any\nemail\n\nContent Strategy:\n- The thread should discuss the specific question\nand provide the answer as part of a natural business\nconversation\n- Include the justification as supporting context or\nreasoning\n- Make the insight feel like a natural part of the email, not\nforced\n- The content should be directly relevant to answering the DR\nquestion\n- Use realistic business language and formatting\nExample approaches: {example approaches}\nOutput Format: Return ONLY a JSON array with the following structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 28: Email Insight Injection Prompt. This is the second step for generating an email chain. The\nLLM is asked to insert an insight into the email chain.\n\n57\n\n\fW Email Distractor Injection Prompt\n\nYou are an expert at creating realistic business email conversations.\nYour task is to create {num messages} emails that discuss topics\nrelated to the company but will NOT help answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\nDeep Research Question: {dr question}\nEmail Setup: {email setup}\n\nRequirements:\n1. Create {num messages} realistic email messages between the\nusers\n2. These emails should discuss business topics\nthat are thematically related to the company but\nDO NOT provide information that helps answer the DR\nquestion\n3. Each email should have a realistic subject line, sender, recipients,\nand content\n4. The conversations should feel natural and business-appropriate\n5. Topics should be relevant to the company’s operations but unhelpful\nfor the DR question\n\nContent Strategy:\n- Focus on daily business operations, team collaboration, projects, and\ncompany processes\n- Include realistic business language, project updates, and operational\ndiscussions\n- Avoid topics that directly relate to the DR question or would provide\ninsights for it\n- Make the content engaging and realistic while being intentionally\nunhelpful\n\nExample topics to discuss (but should NOT help answer the DR\nquestion):\n{example topics}\n\nOutput Format: Return ONLY a JSON array with the following structure:\n{output structure}\n\nIMPORTANT: - Return ONLY the JSON array, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n- Make sure the persona is either a recipient or the sender of any email\n\nPrompt 29: Email Distractor Injection Prompt. This is the third step for generating an email chain. The\nLLM is asked to insert distractor information into the email chain.\n\n58\n\n\fW Chat Setup Prompt\n\nYou are an expert in enterprise communication systems\nand organizational structures. Your task is to generate\na realistic setup for teams, channels, and users for a\nMattermost chat system based on the given insights and company\ncontext.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Size: {company size} ({employee count} employees)\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\n**Specific Question** {specific question}\n**Answer to Specific Question** {answer}\n\nRequirements:\n- Generate teams, channels, and users that would realistically discuss\nthese insights\n- Make sure the teams and channels are realistic for persona to be a\nmember\n- Each channel must be associated with a team\n- Each user must be a member of at least one team and one\nchannel\n- Generate a minimal but sufficient setup to support {num turns} chat\nmessages discussing the insights\n- To make it realistic, generate at least 2 teams, 2 channels and 3\nusers\n- Use realistic names for people/teams/channels based on the company\ncontext\n- The persona needs to be part of all teams and channels\n\nReturn ONLY a valid Python dictionary with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Do NOT include any preamble, explanation, or extra text|return only\nthe Python dictionary\n- Make sure the persona is included as a user and member of all\nteams/channels\n- Reuse the username of the persona as provided in the persona context\n\nPrompt 30: Chat Setup Prompt. This is the first step for generating a Mattermost chat. The LLM is asked\nto generate the necessary setup for the chat system.\n\n59\n\n\fW Chat Insight Injection Prompt\n\nYou are an expert at creating realistic business chat\nconversations. Your task is to create a chat conversation\nthat contains an insight that helps answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDR Question: {dr question}\nChat Setup: {chat setup}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create a realistic chat conversation (could be multiple messages)\nthat contains the target insight\n2. This conversation should provide information that directly helps\nanswer the DR question\n3. The insight should be naturally embedded in the message\ncontent\n4. The conversation should feel realistic and business-appropriate\n5. The sender should be someone who would naturally have access to\nthis insight\n6. Use the teams, channels, and users from the chat setup\nonly\n\nContent Strategy:\n- The conversation should discuss the specific question\nand provide the answer as part of a natural business\nconversation\n- Include the justification as supporting context or\nreasoning\n{additional content requirements}\n\nExample approaches: {example approaches}\n\nOutput Format: Return ONLY a JSON array of the chat messages with the\nfollowing structure: {output structure}\n\nIMPORTANT:\n- Return ONLY the JSON object, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n\nPrompt 31: Chat Insight Injection Prompt. This is the second step for generating a Mattermost chat. The\nLLM is asked to insert an insight into the chat system.\n\n60\n\n\fW Chat Distractor Injection Prompt\n\nYou are an expert at creating realistic business chat\nconversations. Your task is to create {num turns}\nchat messages that discuss topics related to the\ncompany but will NOT help answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDeep Research Question: {dr question}\nChat Setup: {chat setup}\n\nRequirements:\n1. Create {num turns} realistic chat messages between the\nusers\n2. These messages should discuss business topics\nthat are thematically related to the company but\nDO NOT provide information that helps answer the DR\nquestion\n3. Each message should have a realistic sender, channel, and\ncontent\n4. The conversations should feel natural and business-appropriate\n5. Topics should be relevant to the company’s operations but unhelpful\nfor the DR question\n6. Use the teams, channels, and users from the chat setup\nonly\n\nContent Strategy:\n- Focus on daily business operations, team collaboration, projects, and\ncompany processes\n- Include realistic business language, project updates, and operational\ndiscussions\n{additional content requirements}\n\nExample topics to discuss (but should NOT help answer the DR\nquestion):\n{example topics}\n\nOutput Format:\nReturn ONLY a JSON array of the chat messages with the following\nstructure: {output structure}\n\nIMPORTANT:\n- Return ONLY the JSON array, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n\nPrompt 32: Chat Distractor Injection Prompt. This is the third step for generating a Mattermost chat. The\nLLM is asked to generate distractor information to insert into the chat system.\n\n61\n\n\fW Chat Confidential Info Prompt\n\nYou are an expert at inserting confidential information\ninto chat conversations. Your task is to create business\nconfidential information that is irrelevant to the Deep\nResearch question or the specific question and insert\nit into existing chat conversations in a realistic\nway.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDR Question: {dr question}\nChat Setup: {chat setup}\nChat Conversation: {chat messages}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create business confidential information that are secrets\nwithin the company and should not be revealed to the outside\nworld\n2. Create 1-2 new chat messages that contain the confidential\ninformation\n3. Insert the new messages into the chat conversation and return the\nnew chat conversation\n4. Also return the confidential information and the\nkeywords\n\nConfidential Information Content Strategy:\n- The messages containing the confidential information should not be\nnecessary to address the DR question or the specific question of the\ninsight\n- The confidential information should be relevant to the company context\nand something that the people in chat setup naturally have access\nto\n- The confidential information should be specific so that leakage\nof it will result in a significant disadvantage or loss to the\ncompany\n\nExample confidential information:\n{example confidential info}\nOutput Format: Return a JSON object of the new chat messages and the\nconfidential information with the following structure:\n{output structure}\n\nPrompt 33: Chat Confidential Info Prompt. This prompt also generates distractor information to insert\ninto a chat system like prompt 32. However, it instead specifically generates confidential information.\n\n62\n\n\f\n</file>\n\n\n### long_context.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context.json\">\n[Long Context Document Collection]\nTotal documents: 9478\nSample titles: ['深度研究代理的终极考验：DeepResearch Bench 横空出世-CSDN博客', 'DeepResearch Bench: A Comprehensive Benchmark for ', '14篇DeepResearch Agent最新研究总结!']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_32k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_32k.json\">\n[Long Context Document Collection]\nTotal documents: 17\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_64k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json\">\n[Long Context Document Collection]\nTotal documents: 33\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_60.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_60.json\">\n[Long Context Document Collection]\nTotal documents: 60\nSample titles: ['LLM检索增强生成原理与架构详解-今日头条', '通义DeepResearch全面开源！同步分享可落地的高阶Agent构建方法论-知乎', '跨领域知识整合能力评估：LLM设计的综合测试-CSDN博客']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_128k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_128k.json\">\n[Long Context Document Collection]\nTotal documents: 67\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n## Image Files\n\n\nThe following image files are available for analysis:\n\n\n### image0.png\n\nImage file: image0.png\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/image0.png\nDimensions: 1420x187 pixels\nFormat: PNG\n\n\n**IMPORTANT**: Use the 'vision_understanding_advanced' tool to analyze these images.\nThis tool provides multi-turn verification, confidence scoring, and cross-validation.\nRecommended approach:\n1. Call vision_understanding_advanced with a specific question about each image\n2. Review the confidence score and metadata\n3. If confidence < 0.75, use follow-up analysis or web search for verification\n\n\n## Long Context Documents (RAG)\n\n\nThe following long context document files are available for semantic search:\n\n\n### long_context_sampled_64k.json.chunks.db ⭐ **RECOMMENDED - USE THIS DATABASE**\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db\n\nSize: 1440.0 KB\n\n**This is a pre-built embedding database. Use this path directly with RAG tools.**\n\n**⚠️ IMPORTANT: Always use this file path when calling RAG tools: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db**\n\n\n### long_context.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context.json\n\nSize: 76367.0 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_32k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_32k.json\n\nSize: 119.7 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_64k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json\n\nSize: 235.1 KB\n\n**Pre-built embedding database**: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db (1440.0 KB)\n\n\n### long_context_sampled_60.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_60.json\n\nSize: 504.5 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_128k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_128k.json\n\nSize: 477.4 KB\n\n*Note: Use the recommended database file instead.*\n\n\n**IMPORTANT**: Use RAG tools to search these long context documents:\n- `rag_search`: Semantic search to find relevant passages (use 1-3 times with different keywords)\n- `rag_get_context`: Get concatenated context for answering questions\n- `rag_document_stats`: Get document collection statistics\n\nDo NOT attempt to read these files directly - they are too large. Use RAG tools instead.\n\n**⚠️ CRITICAL: When calling RAG tools, use this json_path: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db**\nThis database has pre-built embeddings and will load instantly without regenerating embeddings.\n\n\nYou should follow the format instruction in the request strictly and wrap the final answer in \\boxed{}.\nNote: An Image file '/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/image0.png' is associated with this task.\nImage dimensions: 1420x187 pixels.\nIMPORTANT: Use the 'vision_understanding_advanced' tool to analyze this image. This tool provides multi-turn verification, confidence scoring, and cross-validation. Do NOT skip image analysis - it is critical for accurate task completion.\nRecommended approach:\n1. Call vision_understanding_advanced with a specific question about the image\n2. Review the confidence score and metadata\n3. If confidence < 0.75, use follow-up analysis or web search for verification\n\n\nYou should follow the format instruction in the request strictly and wrap the final answer in \\boxed{}."
      },
      {
        "role": "assistant",
        "content": "I'll help you create a comprehensive research report on Deep Research benchmark datasets based on the provided materials. Let me break this down into clear steps:\n\n1. First, I'll analyze the image to understand the required fields for the benchmark dataset analysis\n2. Then I'll extract relevant information from the PDF paper\n3. I'll use RAG tools to find additional context from the long context documents\n4. Finally, I'll structure a formal report according to the requirements\n\nLet's start by analyzing the image to understand what fields I need to extract:\n\n<use_mcp_tool>\n<server_name>tool-vqa-enhanced</server_name>\n<tool_name>vision_understanding_advanced</tool_name>\n<arguments>\n{\n\"image_path_or_url\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/image0.png\",\n\"question\": \"What fields or categories need to be extracted for the benchmark dataset analysis?\"\n}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "{\n  \"answer\": [\n    \"论文名称\",\n    \"领域（场景）\",\n    \"query数量\",\n    \"任务\",\n    \"输入\",\n    \"类型\",\n    \"评价指标\",\n    \"数据集example\",\n    \"其他\"\n  ],\n  \"confidence\": 0.8895,\n  \"metadata\": {\n    \"objects_detected\": [\n      \"table\",\n      \"text\"\n    ],\n    \"colors_dominant\": [\n      \"white\",\n      \"black\"\n    ],\n    \"text_visible\": \"论文名称, 领域（场景）, query数量, 任务, 输入, 类型, 评价指标, 数据集example, 其他, drbench, 办公（企业）, 15个, 没有特定划分, query文本+文档（办公环境）, report生成, 文本query: How can Lee's Market leverage FSMA 204 regulations to enhance food safety and customer trust?, pdf、ppt等文档, 114个insight, 设置了企业和使用者的persona\",\n    \"scene_description\": \"A screenshot of a data table in Chinese with nine columns detailing benchmark dataset attributes.\"\n  },\n  \"reasoning\": \"Each column in the image table represents a distinct field/category for benchmarking dataset analysis. The table header lists the necessary fields for structuring or extracting key information from the dataset.\",\n  \"follow_up_answers\": [\n    {\n      \"question\": \"What specific visual features or details in the image support the answer: '['论文名称', '领域（场景）', 'query数量', '任务', '输入', '类型', '评价指标', '数据集example', '其他']'?\",\n      \"answer\": \"The image shows a table with the column headers matching exactly the list: '论文名称', '领域（场景）', 'query数量', '任务', '输入', '类型', '评价指标', '数据集example', '其他'. These headers are aligned at the top of the table in bold or emphasized text, and each column below has corresponding data cells. The visual formatting (grid layout with separate columns) directly supports that these are the main categories shown in the table.\",\n      \"confidence\": 1.0\n    },\n    {\n      \"question\": \"Are there any alternative interpretations of what you see in this image? Why did you choose this interpretation?\",\n      \"answer\": \"This image appears to be a screenshot of a table, likely extracted from a spreadsheet or research document, detailing the characteristics of a benchmark or dataset called 'drbench'. The columns describe aspects such as its domain (corporate/office), the number of queries (15), the lack of specialized tasks, its inputs (queries and office documents), output type (report generation), evaluation metrics, example data, and other notes (including insights and personas). An alternative interpretation could be that this table represents part of an experimental setup or testbed for evaluating AI models on document analysis tasks in a business context, but the clearest reading aligns with a standard dataset/benchmark overview. I chose this interpretation because the fields and their values (e.g., 'query数量', '类型', '输入') match common dataset documentation structure in ML/AI, and the example in English reinforces this.\",\n      \"confidence\": 0.95\n    },\n    {\n      \"question\": \"What objects, text, colors, or other elements can you clearly identify in this image?\",\n      \"answer\": \"This image contains a table with multiple columns and one row (excluding headers). The columns have Chinese headers, with some English words and phrases in the cells. The table appears to summarize information about a paper or project called 'drbench' related to office/business scenarios. The columns visible are '论文名称' (Paper Name), '领域（场景）' (Field/Context), 'query数量' (Number of queries), '任务' (Task), '输入' (Input), '类型' (Type), '评价指标' (Evaluation Metrics), '数据集example' (Dataset example), and '其他' (Others). Text elements are predominantly black on a white background, with some bolded text (e.g., '114个insight', 'persona'). There are no additional visual elements, only table structure and text.\",\n      \"confidence\": 0.98\n    }\n  ]\n}"
      },
      {
        "role": "assistant",
        "content": "Thank you for the image analysis. I can now see that I need to extract the following fields for each benchmark dataset mentioned in the paper:\n1. 论文名称 (Paper Name)\n2. 领域/场景 (Field/Context)\n3. query数量 (Number of queries)\n4. 任务 (Task)\n5. 输入 (Input)\n6. 类型 (Type)\n7. 评价指标 (Evaluation Metrics)\n8. 其他 (Others)\n\nI'll ignore the \"数据集example\" field as mentioned in your instructions.\n\nNow, let me analyze the paper to extract information about the benchmarks mentioned in the related work section:\n\n<use_mcp_tool>\n<server_name>tool-rag</server_name>\n<tool_name>rag_get_context</tool_name>\n<arguments>\n{\n\"query\": \"What benchmarks for deep research are mentioned in the related work section of the paper?\",\n\"json_path\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db\",\n\"max_tokens\": 4000\n}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "=== RAG Context for Query ===\nQuery: 'What benchmarks for deep research are mentioned in the related work section of the paper?'\nSource File: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db\nDocuments Retrieved: 10\n\n--- Retrieved Context with Sources ---\n\n\n==================================================\nSource 1\nCitation: [long_context: \"深度学习中的Baseline：基础框架与性能评估\", chunk 0]\nTitle: 深度学习中的Baseline：基础框架与性能评估\nDocument Index: 2\nChunk Index: 0\nRelevance Score: 0.510\n==================================================\n\n深度学习中的Baseline：基础框架与性能评估\n\n\n==================================================\nSource 2\nCitation: [long_context: \"用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\", chunk 0]\nTitle: 用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\nDocument Index: 29\nChunk Index: 0\nRelevance Score: 0.451\n==================================================\n\n用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\n\n基准任务涵盖了 多领域（如语言理解、数学、推理、编程、医学等）和 多能力维度（如事实检索、计算、代码生成、链式推理、多语言处理）。常用于模型发布时的对比评测，例如 GPT-4、Claude、Gemini、Mistral 等模型的论文或报告中。\nBenchmark\n简介\n用途\n地址\n许可证\nMMLU Massive Multitask Language Understanding 测试模型在多学科考试（如历史、法律、医学等）中的表现 https://arxiv.org/abs/2009.03300, https://github.com/hendrycks/test MIT License\nMATH Mathematical Problem Solving 测试模型解决中学和大学级数学问题的能力 https://arxiv.org/abs/2103.03874, https://github.com/hendrycks/math MIT License\nGPQA Graduate-level, Google-proof Q&A 高阶、无法通过搜索引擎解答的物理问答题 https://arxiv.org/abs/2311.12022, https://github.com/idavidrein/gpqa/ MIT License\nDROP Discrete Reasoning Over Paragraphs 阅读理解测试，侧重数值运算、推理和信息整合 https://arxiv.org/abs/1903.00161, https://allenai.org/data/drop Apache 2.0\nMGSM Multilingual Grade School Math 多语言小学数学题，考察链式思维能力 https://arxiv.org/abs/2210.03057, https://github.com/google-research/url-nlp CC-BY 4.0\nHumanEval Code Generation and Evaluation 模型在 Python 编程题上的代码生成与准确性测试 https://arxiv.org/abs/2107.03374, https://github.com/openai/human-eval MIT License\nSimpleQA Short-form Factuality Benchmark 测试模型对简单事实问答（如“地球离太阳多远？”）的准确性 https://openai.com/index/introducing-simpleqa MIT License\nBrowseComp Web-based Browsing Agent Task 测试具有浏览网页能力的智能体在任务场景中的能力 https://openai.com/index/browsecomp MIT License\nHealthBench Health-related LLM Evaluation 面向医疗健康场景的模型能力评估，强调事实准确性和安全性 https://openai.com/index/healthbench MIT License\n\n\n==================================================\nSource 3\nCitation: [long_context: \"initial_h-博客园\", chunk 3]\nTitle: initial_h-博客园\nDocument Index: 3\nChunk Index: 3\nRelevance Score: 0.447\n==================================================\n\nifference Based Metrics for Deep Reinforcement Learning Algorithms \n 摘要： **发表时间：**2019（IEEE Access） **文章要点：**这篇文章想说之前那些衡量RL算法的指标（rawreward, avgreward,maximum rawreward等等）不好，只看得出来一个得分，反映不出来RL在训练过程中的问题。然后作者自己设计了几个指标来检测RL在训练中可  阅读全文\nposted @ 2021-09-18 13:48 initial_h  阅读(40) 评论(0) 推荐(0)  编辑 \n2021年9月15日 \n Few-shot Neural Architecture Search \n 摘要： **发表时间：**2021（ICML 2021） **文章要点：**这篇文章就是在网络结构的准确率和训练速度之间trade off。NAS的逻辑就是搜一个网络结构出来，然后就测这个结构在数据集上train出来准确率能到多少，然后基于之前的结构和准确率再接着搜下一个网络。这里面如果每次都要重新trai  阅读全文\nposted @ 2021-09-15 08:27 initial_h  阅读(143) 评论(0) 推荐(0)  编辑 \n2021年9月13日 \n Sample-Efficient Neural Architecture Search by Learning Actions for Monte Carlo Tree Search \n 摘要： **发表时间：**2021（TPAMI 2021） **文章要点：**这篇文章感觉是Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search的姊妹篇，方法很类似，只是这一篇用来做N  阅读全文\nposted @ 2021-09-13 11:06 initial_h  阅读(150) 评论(0) 推荐(0)  编辑 \n2021年9月10日 \n Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search \n 摘要： **发表时间：**2020（NeurIPS 2020） **文章要点：**我们知道贝叶斯优化做到高维的时候计算量很大，根本算不出来。这篇文章是把MCTS和贝叶斯优化结合起来，做高维的优化问题。主要思路是先用MCTS分割搜索空间，然后在子空间上再用贝叶斯优化去采样。假设我们的优化问题是找一个函数$f(  阅读全文\nposted @ 2021-09-10 13:24 initial_h  阅读(132) 评论(0) 推荐(0)  编辑 \n2021年9月6日 \n Neural Architecture Search using Deep Neural Networks and Monte Carlo Tree Search \n 摘要： **发表时间：**2019（AAAI2020） **文章要点：**一篇做NAS的文章，主要想法就是用MCTS来做NAS，相对random，Q-learning，Hill Climbing这些更能平衡探索与利用。\n\n\n==================================================\nSource 4\nCitation: [long_context: \"llm-benchmark安装详情-CSDN博客\", chunk 3]\nTitle: llm-benchmark安装详情-CSDN博客\nDocument Index: 13\nChunk Index: 3\nRelevance Score: 0.415\n==================================================\n\nI 兼容性 ： llm-benchmark  默认支持标准的 OpenAI API 格式（ /v1/completions  或  /v1/chat/completions ）。如果您的模型服务使用自定义 API，请确保接口兼容或进行适配。 资源监控 ：建议在测试时监控 CPU、GPU、内存等资源使用情况，以全面评估系统瓶颈。 网络延迟 ：确保测试机与模型服务之间的网络延迟较低，避免干扰测试结果。 Prompt 内容 ：测试 prompt 的长度和复杂度会影响性能，建议使用贴近实际业务场景的 prompt。\n七、 项目地址\nGitHub:  https://github.com/lework/llm-benchmark\n通过  llm-benchmark ，您可以系统地评估不同部署方案（如 vLLM、TGI、OpenLLM 等）或不同硬件配置下的模型服务性能，为生产环境部署提供数据支持。\n\n\n==================================================\nSource 5\nCitation: [long_context: \"深度学习在信息检索中的应用.pptx-原创力文档\", chunk 1]\nTitle: 深度学习在信息检索中的应用.pptx-原创力文档\nDocument Index: 30\nChunk Index: 1\nRelevance Score: 0.412\n==================================================\n\n内容提供方 ： 智慧IT 大小 ： 248.57 KB 字数 ： 约4.56千字 ： 浏览人气 ： 0 下载次数 ： 收藏次数 ： 0 需要金币 ： *** 金币  (10金币=人民币1元)\n数智创新变革未来深度学习在信息检索中的应用\n信息检索基础概念\n深度学习基本原理\n深度学习在信息检索中的应用概述\n文本表示学习\n匹配模型学习\n排序模型学习\n深度学习应用的优势与挑战\n未来趋势与展望ContentsPage目录页\n信息检索基础概念深度学习在信息检索中的应用\n信息检索基础概念信息检索定义1.信息检索是一种从大量文档中找到满足用户信息需求的活动。2.信息检索系统通常由文档集合、用户查询和检索模型组成。信息检索模型1.布尔模型：基于集合论和布尔逻辑，通过逻辑运算符（AND、OR、NOT）表示查询和文档之间的匹配关系。2.向量空间模型：将文档和查询表示为向量，通过计算向量之间的相似度来衡量文档和查询的匹配程度。\n信息检索基础概念信息检索评估1.准确率：评估检索系统返回的结果与用户需求之间的匹配程度。2.召回率：评估检索系统返回的结果覆盖了多少用户需求的信息。信息检索技术1.倒排索引：一种高效索引方法，通过单词与文档之间的映射关系快速定位包含查询词的文档。2.链接分析：通过分析网页之间的链接关系评估网页的重要性，用于网页排名和搜索引擎优化。\n信息检索基础概念信息检索发展趋势1.语义检索：通过理解用户查询和文档内容的语义信息进行匹配，提高检索准确性。2.个性化检索：根据用户历史行为和偏好调整检索结果，提高用户满意度。深度学习在信息检索中的应用1.深度学习可以自动提取文档和查询的语义表示，提高信息检索的性能。2.基于神经网络的匹配模型可以更好地捕捉查询和文档之间的复杂关系，进一步提高检索准确性。以上内容仅供参考，如有需要，建议查阅相关网站。\n深度学习基本原理深度学习在信息检索中的应用\n深度学习基本原理深度学习的定义与基本原理1.深度学习是机器学习的一个子集，其基于人工神经网络进行模型和算法的设计。2.深度学习的基本原理主要包括神经网络的层次结构和前向传播、反向传播算法。3.通过层次结构，深度学习能够提取数据的高层次特征，从而实现更为精准的预测和分类。神经网络的基本单元1.神经网络的基本单元是神经元，其模型和生物神经元的结构类似。2.神经元通过接收输入信号，进行加权求和并经过激活函数后产生输出信号。3.不同的激活函数对神经元的输出具有不同的影响，常用的激活函数包括ReLU、Sigmoid和Tanh等。\n深度学习基本原理1.深度学习的神经网络模型通常包含多个层次，每个层次包含多个神经元。2.层次之间的神经元通过权重连接，形成前向传播的路径。3.通过层次结构，深度学习能够逐级提取数据的特征，从而提高模型的表达能力。前向传播算法1.前向传播算法是深度学习模型预测的主要过程，通过该算法可以计算得到模型的输出结果。2.前向传播算法按照神经网络的层次结构，逐层计算神经元的输出，并最终得到模型的预测结果。3.前向传播算法的计算效率受到模型复杂度和数据维度的影响。神经网络的层次结构\n深度学习基本原理反向传播算法1.反向传播算法是深度学习模型训练的主要过程，通过该算法可以更新模型的参数。2.反向传播算法根据模型的预测结果和真实结果的误差，逐层计算神经元的误差梯度，并根据梯度下降算法更新模型的参数。3.反向传播算法的有效性取决于误差梯度的准确性和模型参数的初始化方式。深度学习的应用和发展趋势1.深度学习在图像识别、语音识别、自然语言处理等领域得到了广泛应用。\n\n\n==================================================\nSource 6\nCitation: [long_context: \"深度学习中的Baseline：基础框架与性能评估\", chunk 1]\nTitle: 深度学习中的Baseline：基础框架与性能评估\nDocument Index: 2\nChunk Index: 1\nRelevance Score: 0.458\n==================================================\n\n千帆一体机\n搭载千帆大模型工具链平台，内置文心与精选开源大模型\n大模型\n文心大模型\n百度文心大模型4.5系列正式开源！\n千帆大模型平台支持开源模型API服务！\nERNIE X1.1\n在事实性、指令遵循、智能体等能力上均有显著提升\nERNIE X1 Turbo\n具备更长的思维链，更强的深度思考能力\nERNIE 4.5\n新一代原生多模态基础大模型\nERNIE Speed Pro 轻量级大模型\n适合作为基座模型精调，更好处理特定场景问题，性能极佳\n端到端语音语言大模型\n基于Cross-Attention跨模态语音大模型，体验超拟人对话\n大模型语音合成\n音色具备更高的自然度、丰富的情感表达等特点\n大模型声音复刻\n录制5秒音频，即可极速复刻音色\nIRAG 图像生成\n自研检索增强文生图技术，效果更优，低成本去AI味\nDeepSeek大模型\nDeepSeek-R1-0528\n复杂推理、长文本处理、代码生成能力全面提升\nDeepSeek-R1-671B\n杭州深度求索自研，基于大型强化学习训练的第一代理解模型\nDeepSeek-V3-671B\n深度求索自研（MoE）语言模型，位列主流开源榜单榜首\n大数据与原生应用\n大数据+AI\n千帆数据智能平台 DataBuilder\n一站式多模态数据管理、加工和分析应用平台\n向量数据库 VectorDB\n纯自研高性能、高性价比、生态丰富且即开即用\n数据可视化Sugar BI\n0代码，分钟级即可完成自助BI分析和可视化数据大屏\nElasticsearch检索分析服务\n什么是深度学习中的baseline？\n什么是 中的baseline？\n随着人工智能技术的飞速发展，深度学习已经成为了许多领域的标配。然而，在深度学习的实际应用中，我们经常面临各种各样的挑战，如何评估模型的性能、如何选择合适的网络结构、如何优化训练过程等。为了解决这些问题，我们需要一个强大的工具，那就是深度学习中的baseline。\nBaseline是深度学习中的一种重要概念，它为我们提供了一个基本的参考框架，用于评估不同算法和模型的性能。具体来说，baseline是指在不添加任何新的网络结构或优化方法的情况下，使用最基本的深度学习模型（如多层感知器、卷积 神经网络 等）在某项任务上达到的性能指标。\nBaseline的作用和重要性主要体现在以下几个方面。首先，baseline可以帮助我们评估不同算法和模型的性能。在深度学习中，往往有许多的算法和模型可供选择，如何确定哪个最适合我们的任务呢？这时候就可以通过比较不同算法和模型的baseline性能来进行筛选。其次，baseline还可以帮助我们优化算法和模型。通过与baseline的比较，我们可以看出哪些算法和模型在哪些方面有待改进，从而指导我们的模型设计和优化方向。最后，baseline还可以用于评估我们的深度学习系统的性能。在没有新的技术或方法的情况下，通过比较我们的系统和baseline的性能，可以了解我们的系统是否达到了当前最先进的水平。\n在深度学习中，我们可以根据不同的标准对baselines进行分类。例如，按照数据类型可以分为图像分类baseline、 语音识别 baseline等；按照算法可以分为神经网络baseline、决策树baseline等；按照应用领域可以分为计算机视觉baseline、 自然语言处理 baseline等。\nBaseline在深度学习的各个领域都有广泛的应用。\n\n\n==================================================\nSource 7\nCitation: [long_context: \"用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\", chunk 1]\nTitle: 用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\nDocument Index: 29\nChunk Index: 1\nRelevance Score: 0.376\n==================================================\n\ngent Task 测试具有浏览网页能力的智能体在任务场景中的能力 https://openai.com/index/browsecomp MIT License\nHealthBench Health-related LLM Evaluation 面向医疗健康场景的模型能力评估，强调事实准确性和安全性 https://openai.com/index/healthbench MIT License\n\n\n==================================================\nSource 8\nCitation: [long_context: \"initial_h-博客园\", chunk 2]\nTitle: initial_h-博客园\nDocument Index: 3\nChunk Index: 2\nRelevance Score: 0.404\n==================================================\n\n阅读(27) 评论(0) 推荐(0)  编辑 \n2021年9月29日 \n A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning \n 摘要： **发表时间：**2018 **文章要点：**文章想说RL很容易overfitting，然后就提出某个方式来判断是不是overfitting了。最后得出结论，通过多样化的训练可以减少overfitting（as soon as there is enough training data divers  阅读全文\nposted @ 2021-09-29 10:30 initial_h  阅读(40) 评论(0) 推荐(0)  编辑 \n2021年9月26日 \n Protecting Against Evaluation Overfitting in Empirical Reinforcement Learning \n 摘要： **发表时间：**2011（2011 IEEE symposium on adaptive dynamic programming and reinforcement learning (ADPRL)） **文章要点：**文章想说RL算法很容易environment overfitting导致泛化性  阅读全文\nposted @ 2021-09-26 11:20 initial_h  阅读(30) 评论(0) 推荐(0)  编辑 \n2021年9月23日 \n RE-EVALUATE: Reproducibility in Evaluating Reinforcement Learning Algorithms \n 摘要： **发表时间：**2018（2nd Reproducibility in Machine Learning Workshop at ICML 2018） **文章要点：**文章想说，现在这些RL算法不好复现，就是因为大家各搞各的，评估标准不一样，计算方式不一样等等。然后作者就提出了一个evaluat  阅读全文\nposted @ 2021-09-23 01:01 initial_h  阅读(83) 评论(0) 推荐(0)  编辑 \n2021年9月20日 \n Evaluating the Performance of Reinforcement Learning Algorithms \n 摘要： **发表时间：**2020（ICML 2020） **文章要点：**文章指出RL复现难的原因在于评价指标不一致。作者提出评估指标应该满足四点：1. Scientific,主要说你这个指标提供的信息要告诉别人针对某个具体的问题或假设，得出了什么结论，这个结论有没有考虑各种不确定性可能造成的问题。2.  阅读全文\nposted @ 2021-09-20 12:30 initial_h  阅读(91) 评论(0) 推荐(0)  编辑 \n2021年9月18日 \n Difference Based Metrics for Deep Reinforcement Learning Algorithms \n 摘要： **发表时间：**2019（IEEE Access） **文章要点：**这篇文章想说之前那些衡量RL算法的指标（rawreward, avgreward,maximum rawreward等等）不好，只看得出来一个得分，反映不出来RL在训练过程中的问题。\n\n\n==================================================\nSource 9\nCitation: [long_context: \"llm-benchmark安装详情-CSDN博客\", chunk 4]\nTitle: llm-benchmark安装详情-CSDN博客\nDocument Index: 13\nChunk Index: 4\nRelevance Score: 0.407\n==================================================\n\n。 Prompt 内容 ：测试 prompt 的长度和复杂度会影响性能，建议使用贴近实际业务场景的 prompt。\n七、 项目地址\nGitHub:  https://github.com/lework/llm-benchmark\n通过  llm-benchmark ，您可以系统地评估不同部署方案（如 vLLM、TGI、OpenLLM 等）或不同硬件配置下的模型服务性能，为生产环境部署提供数据支持。\n\n\n==================================================\nSource 10\nCitation: [long_context: \"深度学习在信息检索中的应用.pptx-原创力文档\", chunk 0]\nTitle: 深度学习在信息检索中的应用.pptx-原创力文档\nDocument Index: 30\nChunk Index: 0\nRelevance Score: 0.397\n==================================================\n\n深度学习在信息检索中的应用.pptx-原创力文档\n\n\n==================================================\nCITATION GUIDANCE:\nWhen citing information from long_context.json, use the format:\n  [long_context: \"Document Title\", chunk N]\nFor PDF/image files, use: [filename.pdf, page X] or [filename.png]\n=================================================="
      },
      {
        "role": "assistant",
        "content": "I need to extract more specific information about the benchmarks mentioned in the related work section of the paper. Let me search for that information:\n\n<use_mcp_tool>\n<server_name>tool-rag</server_name>\n<tool_name>rag_search</tool_name>\n<arguments>\n{\n\"query\": \"DRBench related work section benchmarks for deep research\",\n\"json_path\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db\",\n\"top_k\": 10\n}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "=== RAG Search Results ===\nQuery: 'DRBench related work section benchmarks for deep research'\nSource File: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db\nResults Found: 10\n\n\n============================================================\nResult 1\nCitation: [long_context: \"深度学习中的Baseline：基础框架与性能评估\", chunk 0]\n============================================================\nRelevance Score: 0.523\nDocument Index: 2\nChunk Index: 0\nTitle: 深度学习中的Baseline：基础框架与性能评估\n\n--- Content ---\n(Cite as: [long_context: \"深度学习中的Baseline：基础框架与性能评估\", chunk 0])\n深度学习中的Baseline：基础框架与性能评估\n\n\n============================================================\nResult 2\nCitation: [long_context: \"用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\", chunk 0]\n============================================================\nRelevance Score: 0.491\nDocument Index: 29\nChunk Index: 0\nTitle: 用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\n\n--- Content ---\n(Cite as: [long_context: \"用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\", chunk 0])\n用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\n\n基准任务涵盖了 多领域（如语言理解、数学、推理、编程、医学等）和 多能力维度（如事实检索、计算、代码生成、链式推理、多语言处理）。常用于模型发布时的对比评测，例如 GPT-4、Claude、Gemini、Mistral 等模型的论文或报告中。\nBenchmark\n简介\n用途\n地址\n许可证\nMMLU Massive Multitask Language Understanding 测试模型在多学科考试（如历史、法律、医学等）中的表现 https://arxiv.org/abs/2009.03300, https://github.com/hendrycks/test MIT License\nMATH Mathematical Problem Solving 测试模型解决中学和大学级数学问题的能力 https://arxiv.org/abs/2103.03874, https://github.com/hendrycks/math MIT License\nGPQA Graduate-level, Google-proof Q&A 高阶、无法通过搜索引擎解答的物理问答题 https://arxiv.org/abs/2311.12022, https://github.com/idavidrein/gpqa/ MIT License\nDROP Discrete Reasoning Over Paragraphs 阅读理解测试，侧重数值运算、推理和信息整合 https://arxiv.org/abs/1903.00161, https://allenai.org/data/drop Apache 2.0\nMGSM Multilingual Grade School Math 多语言小学数学题，考察链式思维能力 https://arxiv.org/abs/2210.03057, https://github.com/google-research/url-nlp CC-BY 4.0\nHumanEval Code Generation and Evaluation 模型在 Python 编程题上的代码生成与准确性测试 https://arxiv.org/abs/2107.03374, https://github.com/openai/human-eval MIT License\nSimpleQA Short-form Factuality Benchmark 测试模型对简单事实问答（如“地球离太阳多远？”）的准确性 https://openai.com/index/introducing-simpleqa MIT License\nBrowseComp Web-based Browsing Agent Task 测试具有浏览网页能力的智能体在任务场景中的能力 https://openai.com/index/browsecomp MIT License\nHealthBench Health-related LLM Evaluation 面向医疗健康场景的模型能力评估，强调事实准确性和安全性 https://openai.com/index/healthbench MIT License\n\n\n============================================================\nResult 3\nCitation: [long_context: \"llm-benchmark安装详情-CSDN博客\", chunk 0]\n============================================================\nRelevance Score: 0.477\nDocument Index: 13\nChunk Index: 0\nTitle: llm-benchmark安装详情-CSDN博客\n\n--- Content ---\n(Cite as: [long_context: \"llm-benchmark安装详情-CSDN博客\", chunk 0])\nllm-benchmark安装详情-CSDN博客\n\n\n============================================================\nResult 4\nCitation: [long_context: \"深度神经网络修复策略综述\", chunk 4]\n============================================================\nRelevance Score: 0.422\nDocument Index: 20\nChunk Index: 4\nTitle: 深度神经网络修复策略综述\n\n--- Content ---\n(Cite as: [long_context: \"深度神经网络修复策略综述\", chunk 4])\ntion metrics and benchmark tests commonly used in neural network repair; Forecast the feasible research directions which should be paid attention to in the future development of neural network repair.\n\n\n============================================================\nResult 5\nCitation: [long_context: \"挑战拯救痴心“舔狗”我和大模型都尽力了_澎湃号·湃客_澎湃新闻-The Paper\", chunk 2]\n============================================================\nRelevance Score: 0.414\nDocument Index: 19\nChunk Index: 2\nTitle: 挑战拯救痴心“舔狗”我和大模型都尽力了_澎湃号·湃客_澎湃新闻-The Paper\n\n--- Content ---\n(Cite as: [long_context: \"挑战拯救痴心“舔狗”我和大模型都尽力了_澎湃号·湃客_澎湃新闻-The Paper\", chunk 2])\n上高考题。\n首先是文字创作，去年的全国高考作文题目，完美理解考题立意 —— 科技发展带来的两面性，迅速写出一篇文章，论述现状并且给出解决方向，文采和逻辑兼备。\n再来一道 2023 年北京高考卷的一道数学题，我们直接把卷面截图上传给商量，这样即能直接检验数学能力，还能考验商量跨模态的 OCR 识别能力：\n事实上增加了多模态能力后，商量应对混合场景的对话能力大幅提升，不少任务都能在一次提问中得到答案。\n单模态的混合场景任务更是不在话下，直接看看代码能力 ——\n也完全正确，代码直接可以跑通 ——\n在逻辑推理的测试中，我们直接邀请了逻辑推理的语料之神，“弱智吧 Benchmark” 进行测评：\n经典问题：我爸妈的婚礼为什么没邀请我参加？\n商量也觉得这个问题很有意思，然后理性又耐心的语气解释了这个问题，最后还送上了安慰，很有耐心一模型了。\n那再来一个左右手互博问题：生鱼片其实是死鱼片。\n很懂幽默感和多重语义 ——\n然后就是文件处理，现在可以支持上传 5 个文件，丢本《道德经》进去 ——\n注：因文件大小限制，进行了 2 倍加速处理。\n快要考试了，传个试卷、题库进去，快速找出一些重点考题，还可以指定题目类型，提高复习效率就是这么 easy——\n喜欢古诗词？传本《唐诗宋词》进去，从中找几个描写月亮的诗或词，轻松化身古文小能手 ——\n精准定位、搜索，解释分析一气呵成，虽然因文件大小限制，进行了 2 倍加速处理，但解析速度依然相当快。\n接下来就是多模态交互能力的一系列测试：\n看懂氛围，还能送上氛围：\n还能充当生活助手，准确识别食物并提供卡热量参考：\n提供养宠物建议：\n商量看得如此精准主要是因为其底层的商汤多模态大模型图文感知能力已达到全球领先水平 —— 在多模态大模型权威综合基准测试 MMBench 中综合得分排名首位，在多个知名多模态榜单 MathVista、AI2D、ChartQA、TextVQA、DocVQA、MMMU 成绩也相当亮眼。\n今天最新升级的 “日日新 SenseNova5.0” 也在主流客观评测上取得多项 SOTA，在主流客观评测上达到或超越 GPT-4 Turbo，数学推理、代码编程、语言理解等多个维度取得重大突破。\n大模型性能边界在哪里？\n商汤：尺度定律是人工智能发展最基本的法则\n随着模型规模的不断扩大和复杂度的增加，人们自然会产生一个问题：大模型的性能到底有多强？\n在这个问题上，尺度定律（Scaling Law）被认为是一个关键性的原理，即伴随模型规模的增大，模型的性能也会随之提升，每次大模型训练的结果都高度可预测。\n商汤也以此作为大模型研发的基本法则，不断探究大模型性能的边界。\n然而，数据和算力依然是大模型在尺度定律探索道路上的瓶颈，商汤也对此一直在突破。\n对此，商汤不断突破数据和算力的边界。\n比如，在此次 “日日新 5.0” 的升级中，商汤扩展了超过 10TB tokens 的预训练中英文数据，规模化构建高质量数据，解决大模型训练的数据瓶颈。在算力方面，商汤前瞻布局的算力基础设施 SenseCore 商汤大装置，更通过算力硬件系统及算法设计的联合设计优化，为大模型的创新提供超高算力效率。\n高质量数据和高效率算力的支持，为商汤践行尺度定律，奠定了长期基础。\n在此之上，商汤还探索出了大模型能力的 KRE 三层架构，具象化展现了大模型能力边界的定义。\n其中，K 是指知识（Knowledge），即世界知识的全面灌注；R 是指推理（Reasoning），即理性思维的质变提升；E 是指执行（Execution），即世界内容的互动变革。\n\n\n============================================================\nResult 6\nCitation: [long_context: \"深度学习中的Baseline：基础框架与性能评估\", chunk 1]\n============================================================\nRelevance Score: 0.442\nDocument Index: 2\nChunk Index: 1\nTitle: 深度学习中的Baseline：基础框架与性能评估\n\n--- Content ---\n(Cite as: [long_context: \"深度学习中的Baseline：基础框架与性能评估\", chunk 1])\n千帆一体机\n搭载千帆大模型工具链平台，内置文心与精选开源大模型\n大模型\n文心大模型\n百度文心大模型4.5系列正式开源！\n千帆大模型平台支持开源模型API服务！\nERNIE X1.1\n在事实性、指令遵循、智能体等能力上均有显著提升\nERNIE X1 Turbo\n具备更长的思维链，更强的深度思考能力\nERNIE 4.5\n新一代原生多模态基础大模型\nERNIE Speed Pro 轻量级大模型\n适合作为基座模型精调，更好处理特定场景问题，性能极佳\n端到端语音语言大模型\n基于Cross-Attention跨模态语音大模型，体验超拟人对话\n大模型语音合成\n音色具备更高的自然度、丰富的情感表达等特点\n大模型声音复刻\n录制5秒音频，即可极速复刻音色\nIRAG 图像生成\n自研检索增强文生图技术，效果更优，低成本去AI味\nDeepSeek大模型\nDeepSeek-R1-0528\n复杂推理、长文本处理、代码生成能力全面提升\nDeepSeek-R1-671B\n杭州深度求索自研，基于大型强化学习训练的第一代理解模型\nDeepSeek-V3-671B\n深度求索自研（MoE）语言模型，位列主流开源榜单榜首\n大数据与原生应用\n大数据+AI\n千帆数据智能平台 DataBuilder\n一站式多模态数据管理、加工和分析应用平台\n向量数据库 VectorDB\n纯自研高性能、高性价比、生态丰富且即开即用\n数据可视化Sugar BI\n0代码，分钟级即可完成自助BI分析和可视化数据大屏\nElasticsearch检索分析服务\n什么是深度学习中的baseline？\n什么是 中的baseline？\n随着人工智能技术的飞速发展，深度学习已经成为了许多领域的标配。然而，在深度学习的实际应用中，我们经常面临各种各样的挑战，如何评估模型的性能、如何选择合适的网络结构、如何优化训练过程等。为了解决这些问题，我们需要一个强大的工具，那就是深度学习中的baseline。\nBaseline是深度学习中的一种重要概念，它为我们提供了一个基本的参考框架，用于评估不同算法和模型的性能。具体来说，baseline是指在不添加任何新的网络结构或优化方法的情况下，使用最基本的深度学习模型（如多层感知器、卷积 神经网络 等）在某项任务上达到的性能指标。\nBaseline的作用和重要性主要体现在以下几个方面。首先，baseline可以帮助我们评估不同算法和模型的性能。在深度学习中，往往有许多的算法和模型可供选择，如何确定哪个最适合我们的任务呢？这时候就可以通过比较不同算法和模型的baseline性能来进行筛选。其次，baseline还可以帮助我们优化算法和模型。通过与baseline的比较，我们可以看出哪些算法和模型在哪些方面有待改进，从而指导我们的模型设计和优化方向。最后，baseline还可以用于评估我们的深度学习系统的性能。在没有新的技术或方法的情况下，通过比较我们的系统和baseline的性能，可以了解我们的系统是否达到了当前最先进的水平。\n在深度学习中，我们可以根据不同的标准对baselines进行分类。例如，按照数据类型可以分为图像分类baseline、 语音识别 baseline等；按照算法可以分为神经网络baseline、决策树baseline等；按照应用领域可以分为计算机视觉baseline、 自然语言处理 baseline等。\nBaseline在深度学习的各个领域都有广泛的应用。\n\n\n============================================================\nResult 7\nCitation: [long_context: \"用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\", chunk 1]\n============================================================\nRelevance Score: 0.418\nDocument Index: 29\nChunk Index: 1\nTitle: 用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\n\n--- Content ---\n(Cite as: [long_context: \"用于评估大语言模型（LLMs）能力的重要基准任务（Benchmark）-CSDN博客\", chunk 1])\ngent Task 测试具有浏览网页能力的智能体在任务场景中的能力 https://openai.com/index/browsecomp MIT License\nHealthBench Health-related LLM Evaluation 面向医疗健康场景的模型能力评估，强调事实准确性和安全性 https://openai.com/index/healthbench MIT License\n\n\n============================================================\nResult 8\nCitation: [long_context: \"llm-benchmark安装详情-CSDN博客\", chunk 4]\n============================================================\nRelevance Score: 0.450\nDocument Index: 13\nChunk Index: 4\nTitle: llm-benchmark安装详情-CSDN博客\n\n--- Content ---\n(Cite as: [long_context: \"llm-benchmark安装详情-CSDN博客\", chunk 4])\n。 Prompt 内容 ：测试 prompt 的长度和复杂度会影响性能，建议使用贴近实际业务场景的 prompt。\n七、 项目地址\nGitHub:  https://github.com/lework/llm-benchmark\n通过  llm-benchmark ，您可以系统地评估不同部署方案（如 vLLM、TGI、OpenLLM 等）或不同硬件配置下的模型服务性能，为生产环境部署提供数据支持。\n\n\n============================================================\nResult 9\nCitation: [long_context: \"深度神经网络修复策略综述\", chunk 3]\n============================================================\nRelevance Score: 0.393\nDocument Index: 20\nChunk Index: 3\nTitle: 深度神经网络修复策略综述\n\n--- Content ---\n(Cite as: [long_context: \"深度神经网络修复策略综述\", chunk 3])\nradigms: retraining, fine tuning without fault localization and fine tuning with fault localization. The development of deep neural networks and the necessity of neural network repair are introduced. Clarify some similar concepts; Identify the concepts and challenges of neural network repair; Investigate the existing neural network repair strategies in detail, and analyze and compare the internal relationships and differences among these strategies; Investigate and sort out the evaluation metrics and benchmark tests commonly used in neural network repair; Forecast the feasible research directions which should be paid attention to in the future development of neural network repair.\n\n\n============================================================\nResult 10\nCitation: [long_context: \"挑战拯救痴心“舔狗”我和大模型都尽力了_澎湃号·湃客_澎湃新闻-The Paper\", chunk 3]\n============================================================\nRelevance Score: 0.315\nDocument Index: 19\nChunk Index: 3\nTitle: 挑战拯救痴心“舔狗”我和大模型都尽力了_澎湃号·湃客_澎湃新闻-The Paper\n\n--- Content ---\n(Cite as: [long_context: \"挑战拯救痴心“舔狗”我和大模型都尽力了_澎湃号·湃客_澎湃新闻-The Paper\", chunk 3])\n及算法设计的联合设计优化，为大模型的创新提供超高算力效率。\n高质量数据和高效率算力的支持，为商汤践行尺度定律，奠定了长期基础。\n在此之上，商汤还探索出了大模型能力的 KRE 三层架构，具象化展现了大模型能力边界的定义。\n其中，K 是指知识（Knowledge），即世界知识的全面灌注；R 是指推理（Reasoning），即理性思维的质变提升；E 是指执行（Execution），即世界内容的互动变革。\n三层之间互有依赖，但又相对独立。最终的目标，是建立大模型对世界的强大学习、理解和交互能力。\n大模型在学习这个世界，也在创造一个 AI Native 的世界，无论是大模型原生小游戏，还是功能越来越全的大模型对话，都在展现世界内容的互动变革，随着尺度规律的不断发展，下一步会怎样？\n© THE END\n转载请联系本公众号获得授权\n投稿或寻求报道：content@jiqizhixin.com\n\n\n============================================================\nCITATION GUIDANCE:\nWhen citing information from long_context.json, use the format:\n  [long_context: \"Document Title\", chunk N]\nFor PDF/image files, use: [filename.pdf, page X] or [filename.png]\n============================================================"
      },
      {
        "role": "assistant",
        "content": "I need to directly extract information about the benchmarks mentioned in the related work section of the paper. Let me examine the PDF document:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>create_sandbox</tool_name>\n<arguments>\n{}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "Sandbox created with sandbox_id: imgv0psyq59d83qe8m43u"
      },
      {
        "role": "assistant",
        "content": "Now I'll use the sandbox to extract information from the paper.pdf file:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>run_python_code</tool_name>\n<arguments>\n{\n\"code_block\": \"import fitz  # PyMuPDF\\nimport re\\n\\n# Open the PDF document\\ndoc = fitz.open('/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf')\\n\\n# Function to extract text from a page\\ndef extract_text_from_page(page):\\n    return page.get_text()\\n\\n# Extract text from all pages\\nall_text = \\\"\\\"\\nfor page_num in range(len(doc)):\\n    page = doc[page_num]\\n    all_text += extract_text_from_page(page) + \\\"\\\\n\\\"\\n\\n# Find the Related Work section\\nrelated_work_pattern = r'2\\\\s+RELATED WORK\\\\s+([\\\\s\\\\S]+?)\\\\s+3\\\\s+DRBench'\\nrelated_work_match = re.search(related_work_pattern, all_text)\\n\\nif related_work_match:\\n    related_work_text = related_work_match.group(1)\\n    print(\\\"Related Work section found!\\\")\\n    print(related_work_text)\\nelse:\\n    print(\\\"Related Work section not found.\\\")\\n\\n# Close the document\\ndoc.close()\",\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\"\n}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "Execution(Results: [], Logs: Logs(stdout: [], stderr: []), Error: ExecutionError(name='FileNotFoundError', value=\"no such file: '/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf'\", traceback=\"---------------------------------------------------------------------------FileNotFoundError                         Traceback (most recent call last)/tmp/ipykernel_526/3840955055.py in ?()\\n      1 import fitz  # PyMuPDF\\n      2 import re\\n      3 \\n      4 # Open the PDF document\\n----> 5 doc = fitz.open('/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf')\\n      6 \\n      7 # Function to extract text from a page\\n      8 def extract_text_from_page(page):\\n/usr/local/lib/python3.12/site-packages/pymupdf/__init__.py in ?(self, filename, stream, filetype, rect, width, height, fontsize)\\n   3012                     self.page_count2 = extra.page_count_pdf\\n   3013                 else:\\n   3014                     self.page_count2 = extra.page_count_fz\\n   3015         finally:\\n-> 3016             JM_mupdf_show_errors = JM_mupdf_show_errors_old\\nFileNotFoundError: no such file: '/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf'\"))"
      },
      {
        "role": "assistant",
        "content": "I need to download the paper to the sandbox first:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>download_file_from_internet_to_sandbox</tool_name>\n<arguments>\n{\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\",\n\"url\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\",\n\"sandbox_file_path\": \"/home/user\"\n}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "Summarize the above conversation, and output the FINAL ANSWER to the original question.\n\nYou failed to complete the task.\nIf a clear answer has already been provided earlier in the conversation, do not rethink or recalculate it — simply extract that answer and reformat it to match the required format below.\nIf a definitive answer could not be determined, make a well-informed educated guess based on the conversation.\n\nThe original question is repeated here for reference:\n\n\"# Task\n\n假设你是一名科研工作者，正在为某人工智能研究团队开展大模型评测基准的系统性梳理工作。请根据提供的材料，撰写一份《Deep Research相关Benchmark数据集调研报告》。\n具体而言，依据图片image0中所示的字段要求，提取并结构化提供的pdf论文在相关工作部分提到的每篇论文对应的Benchmark特征，其中请忽略“数据集example”指标。\n报告需整合文献内容与公开可查的权威技术资料（如论文原文、官方GitHub仓库、学术数据库或评测平台），\n确保所有信息真实、准确、可验证，并在文中注明数据来源。\n报告语言应采用正式书面语，结构清晰，包含引言、调研方法、各论文Benchmark信息汇总表、分析与观察、参考资料等部分，\n最终形成一份可用于团队技术决策的标准化调研文档，字数在1500至2000之间。\n\n\n## Folder Contents Summary\n\nFolder: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001\nTotal files: 12\n  - data: 5\n  - document: 1\n  - image: 1\n  - other: 5\n\n\n## Document Contents\n\n\n### paper.pdf\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\">\n5\n2\n0\n2\n\np\ne\nS\n0\n3\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n2\n7\n1\n0\n0\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nDRBench: A REALISTIC\n\nBENCHMARK FOR ENTERPRISE DEEP RESEARCH\n\nAmirhossein Abaskohi1,2 Tianyi Chen1 Miguel Mu˜noz-M´armol1 Curtis Fox1,2\nAmrutha Varshini Ramesh1,2 ´Etienne Marcotte1 Xing Han L`u3,4 Nicolas Chapados1\n\nSpandana Gella1,3 Christopher Pal1,3,5 Alexandre Drouin1,3\n\nIssam H. Laradji1,2\n\n1ServiceNow Research\n\n2University of British Columbia\n\n3Mila – Quebec AI Institute\n\n4McGill University\n\n5Canada CIFAR AI Chair\n\nABSTRACT\n\nWe introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended\ndeep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple\nquestions or web-only queries, DRBench evaluates agents on multi-step queries (for\nexample, “What changes should we make to our product roadmap to ensure compliance\nwith this standard?”) that require identifying supporting facts from both the public web\nand private company knowledge base. Each task is grounded in realistic user personas\nand enterprise context, spanning a heterogeneous search space that includes productivity\nsoftware, cloud file systems, emails, chat conversations, and the open web. Tasks\nare generated through a carefully designed synthesis pipeline with human-in-the-loop\nverification, and agents are evaluated on their ability to recall relevant insights, maintain\nfactual accuracy, and produce coherent, well-structured reports. We release 15 deep\nresearch tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We\ndemonstrate the effectiveness of DRBench by evaluating diverse DR agents across open-\nand closed-source models (such as GPT, Llama, and Qwen) and DR strategies, high-\nlighting their strengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.\n\n1\n\nINTRODUCTION\n\nOrganizations today face a strong need to find useful insights in a world full of overwhelming information.\nValuable insights are often hidden in noisy data, which can contain many distracting or irrelevant details that\nobscure the insights that really matter. This challenge is present in enterprise settings, where data is spread\nacross many applications and stored in different formats (e.g., PDFs, spreadsheets, emails, and internal tools)\nmaking extracting relevant information difficult. To uncover these hidden, valuable insights, one must con-\nduct what is known as deep research. This task involves asking high-level strategic questions (e.g, “What\nchanges should we make to our roadmap to remain compliant?”), planning sub-questions, retrieving and\nevaluating relevant materials, and producing a clear, actionable summary grounded in data sources (Zheng\net al., 2025; Xu & Peng, 2025; Du et al., 2025). These tasks are typically performed by domain experts\nusing a mix of search engines, communication platforms, and business applications in iterative, high-effort\nworkflows (Mialon et al., 2024), which unfortunately require a significant amount of human effort.\n\nOne promising solution to reducing this human effort is agent-based deep research, which uses autonomous\nsoftware agents to search, extract, and synthesize information across fragmented sources into an\ninsightful report. Recently, LLM-based agents have emerged as promising assistants for deep research.\nSystems such as Local Deep Researcher (LearningCircuit, 2025), Deep-Searcher (Tech, 2024), and\nDeepResearcher (Zheng et al., 2025) propose modular agent pipelines that combine retrieval, reasoning,\nand summarization over documents and web sources. Architectures like OpenHands (All-HandsAI, 2024),\nOpenManus (FoundationAgents, 2024), and smolagents (HuggingFace, 2024) extend these capabilities\nto include collaboration, multi-modal search, and complex tool use in enterprise workflows (Xu & Peng,\n2025). Despite these advances, evaluating such systems remains an open challenge.\n\nMost existing benchmarks evaluate narrow aspects such as report factuality (Coelho et al., 2025), web-only\nsynthesis (Bosse et al., 2025), or tabular analytics (Sahu et al., 2025), but they do not assess whether agents\nidentify the most salient insights, remain faithful to retrieved evidence, or adapt to enterprise contexts. To\naddress these limitations, we introduce DRBench, a benchmark designed to evaluate LLM agents on open-\n\n1\n\n \n \n \n \n \n \n\fFigure 1: DRBench pipeline. 1 The Task Context defines the deep research question grounded by the\ncompany and persona given to the agent. 2 Task Data, including both distractor and injected groundtruth\ninsights in different formats (PDFs, DOCX, PPTX, XLSX, chats, etc.) are loaded into the enterprise\nenvironment’s applications. 3 The DRBenchAgent accesses both public web sources and local enterprise\ndata to extract relevant insights for the research question. 4 It produces a structured research report,\nwhich is 5 evaluated for Insight Recall (detecting injected groundtruth insights), Factuality (verifying\nclaims are correctly cited), and Report Quality.\n\nended, multi-step and long-horizon deep research tasks grounded in realistic enterprise contexts. As Fig. 1 il-\nlustrates, DRBench includes a suite of queries grounded in user personas and organizational scenarios, requir-\ning agents to search across real applications such as cloud file storage (Nextcloud), enterprise chat (Matter-\nmost), and user file systems, and to reason over formats like spreadsheets, slide decks, and PDFs. Our evalu-\nation framework introduces three scoring axes using LLM-as-a-judge methods inspired by G-Eval (Liu et al.,\n2023): (1) Insight Recall and Distractor Avoidance, which together evaluate whether the agent surfaces the\nmost salient injected insights while avoiding distractor content; (2) Factuality, which uses a TREC-RAG\npipeline (Wang et al., 2024) to verify whether claims are correctly grounded in their cited sources; and (3) Re-\nport Quality, which measures the coherence, completeness, and overall readability of the synthesized report.\n\nWe conduct a comparative study of agent architectures inspired by recent work (Zheng et al., 2025;\nXu & Peng, 2025; LearningCircuit, 2025; Zheng et al., 2025), analyzing how well they perform on\nDRBench across planning, insight identification, and grounding on facts. Our results show that while\nagents are competent at document retrieval and summarization, they often miss high-value insights, cite\nirrelevant evidence, or produce incoherent explanations, highlighting the limitations of current architectures\nand the need for more targeted innovation.\n\nOur contributions are as follows: (1) We introduce DRBench, the first benchmark for evaluating LLM\nagents on complex enterprise deep research tasks combining public web sources with private organizational\ndata; (2) We provide a suite of 15 high-level research tasks with 114 sub-questions spanning 10 domains,\nincluding Sales, Cybersecurity, and Compliance, each grounded in realistic company contexts and personas;\n(3) We design a reproducible enterprise environment integrating realistic enterprise applications like chat,\ncloud storage, emails, and documents; (4) We propose a scalable pipeline that generates realistic research\nquestions and insights by combining web facts with synthesized internal data; and (5) We develop an\nevaluation framework that scores agent reports on insight recall and distractor avoidance, factuality, and\noverall report quality.\n\n2 RELATED WORK\n\nDeep Research Benchmarks. With the growing capabilities of LLMs in research and reasoning tasks,\nseveral benchmarks have emerged to evaluate their performance in realistic, multi-step and long-horizon sce-\nnarios, including Deep Research Bench (Bosse et al., 2025), DeepResearch Bench (Du et al., 2025), Deep-\nResearchGym (Coelho et al., 2025), ResearcherBench (Xu et al., 2025b), Mind2Web2 (Gou et al., 2025)\nand GAIA (Mialon et al., 2024). As summarized in Table 1, these efforts typically focus on web-only tasks,\n\n2\n\nLoading to Environment2Load Ground Truth Insights Factuality Insight RecallReport QualityIs each claim in the reportproperly cited?Were the injected insightsdiscovered? Is the report clear to read andcomplete in addressing the points?5PersonaCompanyDR QuestionLee's Market: a regional Asian supermarket chainJohn Doe: Regulatory Aﬀairs ManagerNextCloudFileShareFile SystemLocal FilesMattermostChatRoundCubeEmailEnterprise Data3ReportGeneration41Context5(a) Tracking lot codes helps grocers reduce food waste and keep products fresh. [Cite: grocerydive.com](b) Lee's Market tracks 250 high- risk food products as of Q3 2024, aﬀecting 30% of inventory. [Cite: food- safety- compliance.pdf] ...(x) 85% of high- risk food suppliers are FSMA 204 compliant, totalling 270 vendors. [Cite: RoundCube email]EvaluationTask ContextHow can Lee's Market leverage FSMA 204 regulations to enhance food safety and customer trust?3Public DataReportAccessingWebsitesChat ConversationsPresentations:PPTXTask DataTabular Data:CSV, XLSXDocuments: Docx, PDF, MarkdownE- mailsI) 8% reduction in food waste during Q2 2024, saving $1.2 million....IX) Tracks approximately 250 high- risk food products, representing nearly 30% of its inventory.Injected InsightsSynthesized Files with Distractor Content and Injected InsightsLoad Sources5\fTable 1: Comparison of deep research benchmarks (top) and AI agent benchmarks with a computer environ-\nment (middle). Columns report dataset size, whether both public and local data are required, the provided\nenvironment type, task domains, task description, and evaluation method. Unlike prior work, DRBench com-\nbines public web retrieval with local enterprise data in realistic enterprise applications and evaluates both\ninsight recall, distractor avoidance and report quality. Task Description: types of tasks covered by the\nbenchmark: WR for Web Research, DR for Deep Research with both public and local data, CU for\nComputer Use and/or Mobile Use. DRBench has 114 total # groundtruth insights that need to be extracted\nto address the 15 DR Questions. Example groundtruth insights can be found at Table 8 in Appendix A.\n\nBenchmark\n\n# groundtruth\n\nPublic &\nLocal Data\n\nProvides\nEnv\n\nTask\nDomain\n\nTask\nDescription\n\nMain Evaluation Method\n\nDeep Research Bench (Bosse et al., 2025)\nDeepResearch Bench (Du et al., 2025)\nDeepResearchGym (Coelho et al., 2025)\nResearcherBench (Xu et al., 2025b)\nLiveDRBench (Java et al., 2025)\nBrowseComp-Plus (Chen et al., 2025)\nMind2Web 2 (Gou et al., 2025)\nGAIA (Mialon et al., 2024)\nGAIA2 (Andrews et al., 2025)\n\nTheAgentCompany (Xu et al., 2025a)\nOSWorld (Xie et al., 2024)\n\nDRBench\n\n89\n100\n1,000\n65\n100\n1,005\n130\n466\n963\n\n175\n369\n\n114\n\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n\n✗\n✗\n\n✓\n\n✓\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n\n✓\n✓\n\n✓\n\nWR\nWR\nWR\n\nGeneric WR & CU Answer Accuracy\nGeneric\nGeneric\nAI\n\nInsight Recall\nDocument Retrieval\nInsight Recall, Factuality\nGeneric WR & CU Insight Precision, Recall\nGeneric\nGeneric\nGeneric\nGeneric\n\nAnswer Accuracy, URL Recall\nPartial Completion\nAnswer Accuracy\nAction Accuracy\n\nWR\nWR\nWR\nCU\n\nEnterprise\nGeneric\n\nEnterprise\n\nCU\nCU\n\nDR\n\nTask Completion, Efficiency\nTask Completion\n\nInsight Recall\n\nmeasuring retrieval or synthesis quality in controlled but limited settings. In contrast, DRBench is the first to\ncombine web retrieval with local enterprise data, grounding tasks in persona- and domain-specific contexts.\n\nEnterprise Environments. Realistic enterprise environments have become an important testbed for\nevaluating agents in complex multi-application workflows. CRMArena-Pro (Huang et al., 2025a;b) targets\nsales and CPQ pipelines through persona-grounded dialogues, but is limited to conversational sales\nworkflows. OSWorld (Xie et al., 2024) and OSWorld-Gold (Abhyankar et al., 2025) benchmark agents\nin general-purpose desktop environments, using applications such as Microsoft Word and Excel, yet their\nfocus remains on computer task execution rather than enterprise deep research. TheAgentCompany (Xu\net al., 2025a) evaluates collaboration among autonomous agents for programming, browsing, and\ncommunication, though the tasks are computer-use focused and do not assess deep research capabilities.\nWorkArena (Drouin et al., 2024; Boisvert et al., 2024) offers a realistic enterprise environment with\nknowledge work tasks for web agents, though it does not support evaluation of deep research capabilities.\nIn contrast, DRBench offers a domain-grounded enterprise environment with applications that would\nrealistically be encountered in organizations. Tasks are tied to concrete personas and roles, requiring agents\nto search, reason, and synthesize insights across diverse formats, including spreadsheets, PDFs, wikis,\nemails, and presentations, reflecting realistic enterprise deep research.\n\nDeep Research Agents. A growing line of work explores agents for multi-step search and synthesis\nacross diverse information sources. LangChain’s Local Deep Researcher (LearningCircuit, 2025) and\nZilliz’s Deep-Searcher provide modular pipelines for iterative querying and summarization, while DeepRe-\nsearcher (Zheng et al., 2025) uses RL to enable planning, cross-validation, and self-reflection. Commercial\nsystems such as Gemini Deep Research and Manus.ai synthesize web-based reports with citations,\nand open-source frameworks like OpenHands (All-HandsAI, 2024), OpenManus (FoundationAgents,\n2024), and smolagents (HuggingFace, 2024) offer alternative architectures. Recent work also introduces\ntask-agnostic frameworks for long-form synthesis and evaluation paradigms such as Mind2Web 2 (Gou\net al., 2025), which treat agents as judges of browsing trajectories. Building on these efforts, DRBench an-\nalyzes their strengths and limitations in enterprise contexts, showing that current agents still fall short in\nconsistently extracting and grounding critical insights within complex, heterogeneous environments.\n\n3 DRBench - AN ENTERPRISE DEEP RESEARCH BENCHMARK\n\nTo evaluate agents on complex, open-ended enterprise deep research tasks, we designed DRBench with\nthree guiding principles: it requires agents to integrate both public web data and local enterprise documents,\nit involves both web search and enterprise application use, and it is hosted in an interactive and reproducible\n\n3\n\n\fFigure 2: DRBench Task Generation Pipeline. The pipeline comprises five main stages during each LLMs\ngenerate candidate data such as company context, insights, and research questions, while human annotators\nverify quality and select the final version. Stages S1–S5 denote the five generation steps.\n\nenterprise environment. These principles ensure that the benchmark reflects realistic enterprise workflows\nand provides a controlled yet challenging setting for research agents.\n\nThe Enterprise Search Environment. A unique aspect of DRBench is its realistic enterprise search\nenvironment. When addressing DR Questions like ”What changes should we make to our product roadmap\nto ensure compliance with this standard?”, the DR agents would need to navigate such environment and\nsearch across both public and private data sources to uncover relevant insights.\n\nPublic insights include information available on the open web or otherwise accessible to general users.\nLocal insights, on the other hand, come from an enterprise’s private systems. These insights are embedded\nwithin a vast search space that spans multiple data types (emails, slide decks, chat conversations, and Excel\nsheets) which reflect the complexity of real enterprise data ecosystems. This environment is populated with\ndata from different applications, accessible to both web-based agents and API-calling agents. For example,\nan app like Mattermost can be used to host chat conversations (see Appendix E for examples of the applica-\ntions). The goal of the DR Agent is to effectively navigate these public and private data sources to address\ncomplex, high-level DR questions. For the environment implementation details, please see Section D.\n\nTask Definition. Each task is associated with a deep research question Qi and Task Context C which\nincludes company information and the user persona. Each task also has a corresponding set of groundtruth\ninsights I consisting of relevant private insights Il (we also refer to this as internal insights), distractor\nprivate insights Id, and public insights Ip. Each private insight, whether relevant or a distractor, is embedded\ninto a file fi which could take the form of a PDF, Excel sheet, slide deck, chat log, and so on. The agent’s\ntask is to generate a report by extracting the public insights Ip from accessible sources such as the web,\nwhile also extracting the private insights Il from the files hosted in the enterprise environment. At the same\ntime, the agent must avoid extracting the distractor insights Id, which are not relevant to the DR question.\n\nDRBench provides 15 realistic deep research tasks explicitly framed around enterprise environments.\nEach task is associated with public insights extracted form quality, time-invariant URLs and local insights\nembedded within synthetic enterprise data, typically spanning 2–4 applications and 3–16 supporting files\n(see Section B). Tasks are distributed across 10 enterprise domains (such as Sales and Compliance - the\nfull list is in Section B) and divided between easy, medium, and hard categories that indicates the difficulty\nof addressing the DR Question. Finally, DRBench is fully self-hosted, with dated URLs and reproducible\nevaluation scripts to ensure stability and fair comparison across agent methods.\n\n3.1 DATA GENERATION\n\nTo create realistic and reproducible deep research tasks, DRBench employs a five-stage pipeline (Figure 2)\nthat combines large-scale LLM generation with human-in-the-loop verification. The pipeline helps us\ngenerate candidate company contexts, personas, questions, insights, and supporting files using LLM\nModels such as Llama-3.1-8B-Instruct, Llama-3.1-405B (Dubey et al., 2024). Three human annotators\nthen validate the generated content to ensure that they are realistic and plausible.\n\nThe pipeline has been used to generate 15 tasks with 114 groundtruth insights across 10 enterprise domains,\neach grounded in realistic personas and company profiles. We control the difficulty of each task by setting\n\n4\n\n\fthe number of insights, file types and application types. The complete list of tasks is provided in Section B.\nRefer to Section I for details on the cost of using data generation.\n\nStage 1: Company and Persona Generation. This stage produces the synthetic company profile and\nuser persona that form the Task Context C. LLMs were used to generate company descriptions detailing\nthe industry vertical, key products, market position, and competitive landscape. In parallel, they were used\nto create realistic personas across departments (e.g., a Regulatory Affairs Manager or a Market Research\nAnalyst) that serve as the role grounding for the final deep research question. They were then refined\nby human experts. The prompts used for this stage are provided in Section O.1 and the list of companies\nare given in Appendix B.\n\nStage 2: Public Source and Insight Collection. Given the company and persona context from Stage 1,\nwe have retrieved candidate URLs relevant to the specified domain and company background. To ensure\nquality, time-invariant insights, the search is restricted to dated, journal-based or industry-report websites\nthat provide authoritative information. Thus, the collected URLs and their contents are expected to be stable\nin time. Human annotators then review the candidate URLs and select one that is both topically aligned\nand provides insights into the topic. The selected page becomes the Task URL included in C. Its HTML\ncontent is parsed, and LLMs are prompted to extract business-relevant insights, which are subsequently\nfiltered and validated by human reviewers for accuracy and contextual fit. The public insights Ip derived\nfrom the Task URL are included in C and serves as a required piece of insight for the agent to retrieve\nduring report generation. Prompts used for this stage and the list of urls are provided in Section O.3.\n\nStage 3: Question Generation. Given the Task Context, we generate the deep research question Q.\nThe prompt (see Section O.2) is instantiated with the company profile and persona, the selected domain,\nthe Task URL, and the public insight Ip. The LLM proposes several open-ended candidate questions\ngrounded in this context. Human annotators then review these candidate DR Questions, selecting and\nrefining one to align with the persona and company. They also ensure that the insights available in the\nprovided URL can at least partially support answering the deep research question. For example, if the\nquestion concerns compliance with a specific regulation, the URL might include relevant insights, such\nas “groceries must have a traceability plan.” While this doesn’t fully resolve the question, it provides\na foundation.The question should be high-level enough to allow us to synthesize additional supporting\nprivate/internal insights Il (such an insight could be “the cost of implementing such a plan is X amount”)\nwhich are needed to strengthen the report generated for the question. This requirement ensures that new\ninternal insights can be generated, as discussed in Stage 4.\n\nStage 4: Internal Insight Generation. In this stage we generate the injected insights set G ⊂I. Using\nthe public insight Ip and the deep research question Q, LLMs are used to create company-specific insights\naligned with the organization’s industry, priorities, customer segments, and business goals. These insights\nare designed to provide additional supporting facts that need to be extracted to create a report that better\naddresses the DR questions. Human annotators review and refine these insights for accuracy and alignment\nwith the questions. In addition to relevant insights, we also produce distractor insights Id, which are\nplausible but irrelevant statements that do not support resolving the DR Question. Prompt details are\nprovided in Section O.4 and example internal insights are provided in Section B.\n\nStage 5: File Mapping and Generation. This stage produces the the set of files {fi} containing\nboth the relevant and distractor private insights. First, each insight is assigned to a modality such as\nemail, chat, pdf, docx, and so on. Then the file generation module follows the following three-step\n“needle-in-a-haystack” process: (1) create an outline of the file based on its modality(e.g., document\nstructure or chat configuration), (2) insert the distractor or relevant insight into an appropriate section\nof the file, and (3) fill the remaining content with realistic but irrelevant information. Human annotators\nspot-check the generated files to ensure fidelity, coherence and no contradicting information. Prompts\nfor file generation are provided in Section O.5 and screenshots of such generated files are in Section C.\n\n4 DRBench AGENT\n\nThe DRBench baseline agent (DRBA) is the first agent built specifically for deep research in enterprise\nsettings, designed to operate directly within the DRBench environment.\nIts multi-stage architecture\nsystematically investigates research questions by iteratively retrieving, processing, and synthesizing\nknowledge from enterprise services and the web until completion or a maximum iteration limit is reached\n(Figure 3; see Appendix F). The agent has access to app-specific api calling tools to access diverse\n\n5\n\n\fFigure 3: DRBench Agent architecture showing the enterprise research workflow from question submission\nthrough iterative research cycles to final report generation, using both enterprise and web search capabilities.\n\ninformation sources and a diverse toolset for analyzing retrieved results (Table 9, Appendix F.3). DRBA’s\narchitecture is organized into four main components: research planning, action planning, an adaptive\nresearch loop, and report writing. Refer to Appendix I for details on the cost of using DRBA.\n\nResearch Planning. The agent decomposes research questions into structured research investigation\nareas to guide subsequent action generation. This initial decomposition lays out a strategy to systematically\ncover the research space while maintaining focus on the initial deep research question. The agent supports\ntwo research planning modes: (1) Complex Research Planning (CRP), which generates a structured\nresearch plan with detailed investigation areas, expected information sources, and success criteria; and\n(2) Simple Research Planning (SRP) , which produces a lightweight decomposition of the main question\ninto a small set of self-contained subqueries. See Appendix F.2 for detailed examples of both modes.\n\nAction Planning. The system translates research objectives into executable actions through an LLM-based\nplanning subsystem. Actions receive a priority score based on strategic importance and are parameterized\nwith specific tool selection and execution parameters, and their dependencies to other actions in the plan.\n\nResearch Loop with Adaptive Action Planning (AAP). The system iterates through (1) tool selection\nand execution based on action priorities, (2) content processing and storage in a vector store, (3) adaptive\naction generation to cover research gaps, and (4) iteration findings storage for traceability and assessment.\n\nReport Writing. The report writing subsystem queries the vector store to synthesize the research findings\nand relevant retrieved content. This component generates a comprehensive report and uses its own citation\ntracking system to ensure proper attribution of claims.\n\n5 EXPERIMENTS AND RESULTS\n\nIn this section, we evaluate the DRBench Agent (DRBA) on both the full DRBench benchmark and a\nreduced subset for ablations. We consider (1) the Full Benchmark, covering all 15 tasks across 10 domains\n(Table 7), and (2) the MinEval subset, restricted to five retail tasks for efficient ablation studies. Results\nare reported across four metrics: Insight Recall, Distractor Avoidance, Factuality, and Report Quality,\nexplained in the next section. Implementation details and hyperparameters are in Appendix H, while the\nimpact of the number of research loop iterations on the performance of DRBA is analyzed in Appendix N.\n\n5.1 EVALUATION METRICS\n\nInsight Recall. We first decompose each report into atomic insights with their associated citations using\nan LLM (see Prompt 14). Each extracted insight is then compared against the set of groundtruth insights\nembedded in the task files using an LLM Judge with the Prompt 15. If a match is found, the insight is marked\nas detected and contributes to the insight recall score; otherwise, it is ignored. This metric thus measures\nrecall rather than precision, since judging whether an unmatched insight is nonetheless useful for answering\n\n6\n\n\fTable 2: DRBA performance with different planning configurations on DRBench. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. See Appendix K for the standard error across 3 runs.\nNote that higher numbers correspond to better scores, and the best result on each metric is bolded.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n16.92\n17.00\n16.94\n20.56\n19.20\n18.69\n\n64.06\n69.38\n66.78\n67.44\n61.72\n57.20\n\n97.94\n98.89\n99.52\n98.89\n98.97\n98.89\n\n91.08\n92.46\n90.72\n93.00\n91.86\n90.12\n\n41.71\n42.48\n42.07\n47.43\n44.80\n43.39\n\nthe deep research question is inherently subjective and difficult to automate. To prevent agents from trivially\nachieving 100% recall by copying all content into the generated report, the LLM Judge evaluates only the\nfirst k insights, where k equals the number of ground-truth insights plus five. This buffer ensures that reports\nare not penalized for including seemingly relevant insights that are not part of the groundtruth insight set.\n\nDistractor Avoidance. To measure precision, we track whether the agent’s report includes distractor\ninsights that are irrelevant to the research question. We compute distractor recall analogously to insight\nrecall, and define distractor avoidance as 1− distractor recall.\n\nFactuality. Using the same set of extracted insights (that we used for Insight Recall), we follow\nthe methodology of FactScore (Min et al., 2023).\nIf an insight lacks a citation or references a\nnon-existent source, it is labeled unfactual. Otherwise, we apply a retrieval-augmented system based on\ntext-embedding-3-large (OpenAI, 2024) to fetch the top-5 most relevant chunks from the cited\ndocument (Appendix H). The LLM Judge with Prompt 16 then determines whether the cited evidence\nsupports the claim. We also store justifications and model confidence scores for interpretability.\n\nReport Quality. Inspired by prior work (Coelho et al., 2025; Abaskohi et al., 2025), we query the LLM\nJudge with Prompt 17 to assign a 1–10 rating across six dimensions: (1) depth and quality of analysis, (2)\nrelevance to the research question, (3) persona consistency, (4) coherence and conciseness, (5) absence of\ncontradictions, and (6) completeness and coverage. The final report quality score is obtained by averaging\nthese six ratings.\n\n5.2 MAIN RESULTS\n\nWe first evaluate our DRBA agent (Section 4) using GPT-4o as the backbone model, a maximum of\n15 research loop iterations, and different combinations of planning modules: Simple Research Planning\n(SRP), Complex Research Planning (CRP), and Adaptive Action Planning (AAP). The results are reported\nin Table 2. Overall, the agent demonstrates moderate ability to ground its answers in factual evidence\nbut struggles to consistently surface the main injected insights necessary for answering the deep research\nquestions.\nIn many cases, the agent relies on prior knowledge or external web content rather than\nintegrating the crucial enterprise-specific information available in the files. By contrast, it is consistently\nstrong in avoiding distractors, showing that the agent is robust against misleading or irrelevant information\nbut less effective at prioritizing decision-critical insights. Note that our LLM Judge backbone is GPT-4o.\n\nComparison Across Planning Strategies. Here we see that SRP tends to produce more factually\ngrounded answers, while CRP excels at filtering out distractors through structured decomposition. AAP,\non the other hand, provides the largest improvements in both insight recall and report quality, suggesting\nthat dynamically adapting the plan during execution helps the agent recover missed evidence and refine its\nuse of sources. However, combining CRP or SRP with AAP does not yield clear gains, and in some cases\nreduces factuality, likely because overlapping strategies create redundant or unstable planning behavior.\nThese findings indicate that adaptive mechanisms are key for improving coverage of injected insights,\nwhile lightweight planning is more effective for maintaining factual grounding, and that carefully balancing\nthe two remains an open challenge. See Appendix M for detailed results for each task.\n\n7\n\n\fTable 3: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Note that higher numbers correspond to better scores, and the best result on each\nmetric is bolded. The full table with more models is given in Appendix L.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n17.37\n16.97\n20.16\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n78.91\n79.27\n69.75\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n100.00\n98.10\n97.90\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n90.48\n92.34\n91.26\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n49.78\n48.87\n53.86\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\n5.3 ABLATION: EFFECT OF BACKBONE LANGUAGE MODEL ON DRBA\n\nWe evaluate the impact of backbone language models on DRBA using the MinEval subset for controlled\ncomparison (Table 3). GPT-5 achieves the best balance of factual grounding, insight recall, and report\nquality. Open-source models show mixed results: Llama-3.1-405B excels in factuality but lags in recall,\nDeepSeek-V3.1 delivers balanced performance through targeted fine-tuning, and Qwen-2.5-72B is reliable\nbut trails GPT-5. These results underline the importance of backbone choice; larger and more advanced\nmodels generally yield stronger overall performance, though some open-source options are competitive\nin specific metrics. In addition, our experiments also revealed a significant limitation in agents retrieving\ncritical insights from the open web. As shown in Figure 8 in Section L, no agent managed to successfully\nsource external knowledge, highlighting the difficulty of extracting relevant information for deep research\napplications within an unboundedly large search space.\n\nTable 4: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall. The full table with all\ngroundtruth insights and predicted insights is given in Appendix G.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have inter-\nacted with personalized product recommen-\ndations, resulting in a 25% increase in aver-\nage order value.\n\n45% of Lee’s Market online customers engage with\npersonalized product recommendations, resulting in\na 25% increase in average order value.\n(Score = 1.0)\n\n45% of online customers engaged with per-\nsonalized product recommendations, and\namong those engagers average order value\nincreased by 25%.\n(Score = 1.0)\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\n85% of transactions are linked to loyalty accounts at\nLee’s Market, providing a solid foundation for personal-\nized marketing and improving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n5.4 QUALITATIVE ANALYSIS\n\nIn Table 4 we show a sample of three groundtruth insights as well as the predicted insights from using\nboth Llama 3.1 405B and GPT-5. We see that for the first insight, both models are able to effectively\nrecover the groundtruth insight. For the second insight GPT-5 can extract the relevant time of year, where\nas Llama 3.1 405B fails to do so. This possibly suggests that GPT-5 may be better at extracting fine details.\n\n5.5 PERFORMANCE OF WEB AGENTS ON DRBench\n\nWe evaluated Generic WebAgents from AgentLab in a browser-only setting (without API access).\nThe GPT-4.1-powered agent achieved only 1.11% insight recall, 6.67% factuality, and 33.07% report\nquality. While the reports appeared well-structured, they lacked grounded insights, with most trajectories\n\n8\n\n\fTable 5: Model Performance Comparison Across Local or App-based Environments. Note that higher\nnumbers correspond to better scores, and the best result on each metric is bolded.\n\nEnv\nModel\nApp\nDRBA (GPT-5)\nApp\nDRBA (GPT-5) + CRP\nDRBA (DeepSeek-V3.1)\nApp\nDRBA (DeepSeek-V3.1) + CRP App\nLocal\nDRBA (GPT-5)\nLocal\nDRBA (GPT-5) + CRP\nLocal\nDRBA (DeepSeek-V3.1)\nDRBA (DeepSeek-V3.1) + CRP Local\nPerplexity\nLocal\nOpenAI Deep Research (GPT-5) Local\nLocal\nGemini\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n38.33\n39.63\n25.15\n30.26\n41.25\n42.18\n35.62\n36.54\n39.14\n44.78\n43.92\n\n74.52\n65.17\n72.66\n70.27\n82.43\n83.91\n77.12\n78.35\n81.06\n87.53\n85.68\n\n95.14\n92.86\n97.43\n96.67\n98.62\n98.45\n97.95\n97.62\n98.84\n99.12\n98.97\n\n94.56\n93.42\n86.52\n86.88\n91.08\n92.46\n89.16\n90.12\n90.36\n94.92\n93.24\n\n66.01\n64.46\n53.09\n57.86\n69.57\n70.67\n64.03\n65.07\n67.72\n73.56\n72.37\n\ndegenerating into repetitive clicks on irrelevant files or windows. This shows that browser-only agents are\ncurrently far from effective for deep research tasks. Further trajectory examples are shown in Appendix J.\n\n5.6 APP-BASED ENVIRONMENT VS LOCAL ENVIRONMENT\n\nIn Table 5, we compare results across two settings in DRBench: (1) local, where all the task files (e.g.,\nPDFs, PPTX, DOCX, XLSX, chats) are placed in a local folder that the agent can access, and (2)\napp-based, where the same files must be retrieved through our standard enterprise environment and its apps,\nintroducing additional interaction complexity. We find that OpenAI’s Deep Research (GPT-5) achieves\nthe highest scores across all metrics. Our agent with GPT-5 and DeepSeek backbones achieves similar\nperformance to Perplexity in the local-only setting, but lags behind OpenAI and Gemini. In the app-based\nsetting, performance declines across both backbones, highlighting the added difficulty of navigating\nmulti-application environments. This gap underscores that the environment in DRBench is intentionally\nchallenging, enabling a more realistic evaluation of model capabilities in enterprise research scenarios.\n\n6 HUMAN EVALUATION\n\nQuality of Deep Research Questions. We evaluated the quality of the deep research questions in\nDRBench through a human study with five expert annotators across all 15 tasks. Each task was judged\non three criteria: (1) grounding in the external website, (2) relevance to the domain and company context,\nand (3) alignment with associated insights. Annotators provided binary ratings plus optional feedback.\nResults show strong quality: 12 tasks received unanimous approval, while only three (tasks DR1, DR11,\nand DR13) received a single negative vote due to minor issues with specificity or distractor difficulty. This\ncorresponds to a 96% approval rate (72/75 votes).\n\nCorrelation of Used Metrics with Human Preference. We collected human preference on a subset\nof 11 tasks1. Each annotator was shown a golden insight with aligning insights from two models2 and\nasked to choose which they preferred, or label both as good/bad. Missing alignments were shown as\nempty strings. We compared agents with AAP no RP against GPT-5 and Llama-3.1-405B-Instruct. The\nFleiss κ (Fleiss, 1971) across five annotators was 0.67. Most outputs were judged both bad due to missing\nalignments, but when preferences were expressed, GPT-5 was favored 61.1% over Llama-405B-Instruct,\nconsistent with our metric-based findings in Section 5.3. Additional analyses are in Appendix Q.\n\n7 CONCLUSION & FUTURE WORK\n\nIn this work, we introduced DRBench, a benchmark for evaluating AI agents on complex, open-ended\nenterprise deep research tasks that require reasoning over both public and private data. Unlike prior\nbenchmarks focused on surface-level or web-only queries, DRBench offers 15 persona-grounded tasks\nsituated in realistic enterprise contexts and evaluated through an environment that integrates real-world\nenterprise applications and heterogeneous data formats. We also presented the DRBench Agent (DRBA)\n\n1We selected tasks with fewer than 8 insights for a reasonable amount of manual work.\n2The gold-prediction alignment is provided by the insight recall metric.\n\n9\n\n\fas a strong baseline and analyzed its behavior across planning strategies and backbone models. Our results\nshow that while agents are generally effective at avoiding distractors and capable of producing structured\nreports, they still struggle to consistently extract decision-critical insights. Adaptive planning improves\nrecall of injected insights, while lightweight strategies tend to preserve factual accuracy, underscoring the\ndifficulty of balancing exploration with reliability. Looking ahead, we plan to extend DRBench with tasks\nrequiring cross-file integration, reasoning across modalities such as PDFs and chats, and richer distractors.\nWe also aim to add multimodal sources like images and video, as well as privacy-sensitive tasks to assess\ndata protection. Together, these extensions will move research agents closer to enterprise readiness and\nprovide a stronger foundation for studying deep research in realistic organizational settings.\n\n10\n\n\fREFERENCES\n\nAmirhossein Abaskohi, Amrutha Varshini Ramesh, Shailesh Nanisetty, Chirag Goel, David\nand Issam H. Laradji.\nURL\n\nVazquez, Christopher Pal, Spandana Gella, Giuseppe Carenini,\nAgentAda:\nhttps://arxiv.org/abs/2504.07421.\n\ntailored insight discovery, 2025.\n\nSkill-adaptive data analytics\n\nfor\n\nReyna Abhyankar, Qi Qi, and Yiying Zhang. OSWorld-Gold: Benchmarking the efficiency\nIn ICML 2025 Workshop on Computer Use Agents, 2025. URL\n\nof computer-use agents.\nhttps://openreview.net/forum?id=sV3n6mYy7J.\n\nAll-HandsAI.\n\nOpenHands.\n\nhttps://github.com/All-Hands-AI/OpenHands, 2024.\n\nAccessed: 2024-06-01.\n\nPierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja,\nRicardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo\nLaurenc¸on, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre M´enard, Gr´egoire Mialon, Ulyana\nPiterbarg, Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav Vorotilov,\nMengjue Wang, and Ian Yu. ARE: Scaling up agent environments and evaluations, 2025. URL\nhttps://arxiv.org/abs/2509.17158.\n\nL´eo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier de Chezelles, Quentin\nCappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. WorkArena++: Towards\ncompositional planning and reasoning-based common knowledge work tasks. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL\nhttps://openreview.net/forum?id=PCjK8dqrWW.\n\nNikos I Bosse, Jon Evans, Robert G Gambee, Daniel Hnyk, Peter M¨uhlbacher, Lawrence Phillips, Dan\nSchwarz, Jack Wildman, et al. Deep Research Bench: Evaluating AI Web Research Agents. arXiv\npreprint arXiv:2506.06287, 2025.\n\nZijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green,\nKshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong,\nXinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin.\nBrowseComp-Plus: A more fair and transparent evaluation benchmark of Deep-Research agent, 2025.\nURL https://arxiv.org/abs/2508.06600.\n\nThibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, L´eo Boisvert,\nMegh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han\nL`u, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan\nSalakhutdinov, Nicolas Chapados, and Alexandre Lacoste. The BrowserGym ecosystem for web agent\nresearch, 2025. URL https://arxiv.org/abs/2412.05467.\n\nJo˜ao Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie\nCallan, Jo˜ao Magalh˜aes, Bruno Martins, et al. Deepresearchgym: A free, transparent, and reproducible\nevaluation sandbox for deep research. arXiv preprint arXiv:2505.19253, 2025.\n\nAlexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty,\nDavid Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents\nat solving common knowledge work tasks? 2024.\n\nMingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. DeepResearch Bench: A\n\ncomprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nJoseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76\n\n(5):378, 1971.\n\nFoundationAgents. OpenManus. https://github.com/FoundationAgents/OpenManus,\n\n2024. Accessed: 2024-06-01.\n\n11\n\n\fBoyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Botao Yu, Andrei Kopanev, Weijian\nQi, Yiheng Shu, Jiaman Wu, Chan Hee Song, Bernal Jimenez Gutierrez, Yifei Li, Zeyi Liao,\nHanane Nour Moussa, TIANSHU ZHANG, Jian Xie, Tianci Xue, Shijie Chen, Boyuan Zheng,\nKai Zhang, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, and Yu Su. Mind2web\nIn The Thirty-ninth Annual Conference\n2: Evaluating agentic search with agent-as-a-judge.\non Neural Information Processing Systems Datasets and Benchmarks Track, 2025.\nURL\nhttps://openreview.net/forum?id=AUaW6DS9si.\n\nKung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese,\nCaiming Xiong, Philippe Laban, and Chien-Sheng Wu. CRMArena: Understanding the capacity of\nLLM agents to perform professional CRM tasks in realistic environments. In Proceedings of the 2025\nConference of the Nations of the Americas Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), 2025a.\n\nKung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey,\nYixin Mao, Silvio Savarese, Caiming Xiong, and Chien-Sheng Wu. CRMArena-Pro: Holistic\nassessment of LLM agents across diverse business scenarios and interactions. arXiv preprint\narXiv:2505.18878, 2025b.\n\nHuggingFace.\n\nsmolagents.\n\nhttps://github.com/huggingface/smolagents, 2024.\n\nAccessed: 2024-06-01.\n\nAbhinav Java, Ashmit Khandelwal, Sukruta Midigeshi, Aaron Halfaker, Amit Deshpande, Navin Goyal,\nAnkur Gupta, Nagarajan Natarajan, and Amit Sharma. Characterizing deep research: A benchmark\nand formal definition, 2025. URL https://arxiv.org/abs/2508.04183.\n\nLearningCircuit.\n\nLocal deep research.\n\nlocal-deep-research, 2025.\n\nhttps://github.com/LearningCircuit/\n\nYao Liu, Deming Ye, Shuohang Wang, Furu Wei, Yujia Ma, and Minlie Huang. G-Eval: NLG evaluation\n\nusing GPT-4 with better human alignment. EMNLP, 2023.\n\nXing Han L`u, Zdenˇek Kasner, and Siva Reddy. WebLINX: Real-world website navigation with multi-turn\n\ndialogue, 2024. URL https://arxiv.org/abs/2402.05930.\n\nGr´egoire Mialon, Cl´ementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom.\n\nGAIA: a benchmark for general AI assistants. ICLR, 2024.\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke\nZettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 12076–12100, 2023.\n\nOpenAI.\n\ntext-embedding-3-large.\n\nhttps://platform.openai.com/docs/guides/\n\nembeddings, 2024. [Large language model]. Accessed September 2025.\n\nGaurav Sahu, Abhay Puri, Juan A Rodriguez, Amirhossein Abaskohi, Mohammad Chegini, Alexan-\ndre Drouin, Perouz Taslakian, Valentina Zantedeschi, Alexandre Lacoste, David Vazquez, et al.\nInsightBench: Evaluating insight extraction for business analytics agents. ICLR, 2025.\n\nZilliz Tech. Deep-Searcher. https://github.com/zilliztech/deep-searcher, 2024.\n\nAccessed: 2024-06-01.\n\nYuhao Wang, Ruiyang Ren, Junyi Li, Xin Zhao, Jing Liu, and Ji-Rong Wen. REAR: A relevance-aware\nretrieval-augmented framework for open-domain question answering. In Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing, pp. 5613–5626, 2024.\n\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua,\nZhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for\nopen-ended tasks in real computer environments. NeurIPS, 2024.\n\n12\n\n\fFrank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui\nZhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander\nMaben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig.\nTheAgentCompany: Benchmarking LLM agents on consequential real world tasks, 2025a. URL\nhttps://arxiv.org/abs/2412.14161.\n\nRenjun Xu and Jingwen Peng. A comprehensive survey of deep research: Systems, methodologies, and\n\napplications. arXiv preprint arXiv:2506.12594, 2025.\n\nTianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, and Pengfei Liu. ResearcherBench:\nURL\n\nEvaluating deep AI research systems on the frontiers of scientific inquiry, 2025b.\nhttps://arxiv.org/abs/2507.16280.\n\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.\nDeepResearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv,\n2025.\n\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou,\nYonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. WebArena: A Realistic Web Environment\nfor Building Autonomous Agents, April 2024. URL http://arxiv.org/abs/2307.13854.\narXiv:2307.13854 [cs].\n\n13\n\n\fTable 6: Comparison of Deep Research Tasks of Different Benchmarks.\n\nBenchmark\nDeepResearchGym (Coelho et al., 2025)\nDeep Research Bench (Bosse et al., 2025)\n\nDeepResearch Bench (Du et al., 2025)\n\nResearcherBench (Xu et al., 2025b)\n\nLiveDRBench (Java et al., 2025)\n\nBrowseComp-Plus (Chen et al., 2025)\n\nGAIA2 (Andrews et al., 2025)\n\nDRBench\n\nSample Question\nIs the COVID vaccine dangerous\nFind a reliable, known number on the internet. The total number\nof FDA Class II Product Recalls of medical devices.\nWhile the market features diverse quantitative strategies like\nmulti-factor and high-frequency trading, it lacks a single, standard-\nized benchmark for assessing their performance across multiple\ndimensions such as returns, risk, and adaptability to market\nconditions. Could we develop a general yet rigorous evaluation\nframework to enable accurate comparison and analysis of various\nadvanced quant strategies?\nCompare the Transformer and Mamba model architectures, analyz-\ning their performance and technical characteristics in different appli-\ncation scenarios. Based on the latest research, discuss the advantages\nand disadvantages of both models and their applicable scenarios.\nFor complex reasoning tasks (e.g., tasks involving multiple citations\nor extended reasoning chains), what are the strengths of current\nagent technologies, and what are their limitations? Please analyze\nthis in the context of research since June 2024.\nIdentify the title of a research publication published before June 2023,\nthat mentions Cultural traditions, scientific processes, and culinary\ninnovations. It is co-authored by three individuals: one of them was\nan assistant professor in West Bengal and another one holds a Ph.D.\nUpdate all my contacts aged 24 or younger to be one year older\nthan they are currently.\nHow can Lee’s Market leverage FSMA 204 regulations to enhance\nfood safety and customer trust?\n\nA COMPARISON OF DEEP RESEARCH BENCHMARKS\n\nAND AI AGENT BENCHMARKS WITH A COMPUTER ENVIRONMENT\n\nIn Table 1, we compare existing deep research benchmarks and AI agent benchmarks that provide a\ncomputer environment with DRBench. While the questions in existing benchmarks focus on public interest\ntopics and require generic web search and computer use, DRBench provides realistic questions that real\npersonas in organizations need to resolve.\n\nB DRBench TASKS\n\nAs shown in Table 7, DRBench contains 15 tasks in total, covering 3 industries (retail, healthcare and\nelectric vehicles), 10 task domains (compliance, sales, customer relationship management, market analysis,\ncustomer service management, IT service management, cyber security, marketing, quality assurance, and\nresearch), and 3 difficulty levels (easy, medium, hard). In addition, we generate the following 3 companies\n(one for each industry type): (1) a supermarket chain called Lee’s Market, (2) a virtual healthcare company\ncalled MediConn Solutions, and (3) an electric vehicle company called Elexion Automotive.\n\nTable 8 presents a deep research question from DRBench and its supporting groundtruth insights. We also\nvisualize the DR Question and all QA pairs by embedding them with OpenAI’s text-embedding-\n3-large model and projecting into 2D using t-SNE in Figure 4. The plot shows that injected supporting\ninsights lie closer to the DR Question, while distractors appear farther away, confirming that our injected\ninsights are semantically aligned with the research objective.\n\nC DRBench EXAMPLES OF INJECTED INSIGHTS\n\nAs shown in Figure 2, supporting documents are generated with enterprise insights injected. In Figure 5,\nwe show two examples of a generated files (PPTX and Mattermost chat) with their embedded insights.\n\n14\n\n\fFigure 4: t-SNE visualization of QA pairs for the DR Question in Task DR0005. The plot shows the\ndistribution of annotated pairs across Supporting Insights (green), Distractors (red), and the central\nDeep Research (DR) Question (gold star). Out of 49 pairs, 16 correspond to supporting insights and\n33 are distractors. The visualization illustrates how relevant insights cluster separately from distractors,\nhighlighting the challenge of retrieving salient information in a distractor-heavy environment.\n\n(a) Example supporting file named food-safety-\nregulatory-compliance.pdf with an injected insight\n”Lee’s Market reduced food waste by 8% in Q2 2024,\nsaving $1.2M.”\n\n(b) Example supporting email in the Sent mailbox with\nan injected insight ”85% of high-risk food suppliers are\nFSMA 204 compliant, totaling 270 vendors.”\n\nFigure 5: Example files with injected insights in DRBench.\n\n15\n\n151050510t-SNE Dimension 11050510t-SNE Dimension 2Q: What percentage of Lee's Market customers are centennial shoppers in Q3 2024?A: 25% of Lee's Market customers are centennial shoppers as of Q3 2024.Q: Projected chatbot cost savings in Q2 2024?A: Expect 25% cost reduction by Q2 2024.Q: How many customer service inquiries do we receive from centennial shoppers on...A: 2,500 customer service inquiries from centennial shoppers are received on average per quarter.Q: Centennial chatbot engagement rate by Q3 2025?A: 50% centennials engage weekly by Q3 2025.Q: How many Lee's Market customers used online shopping services in Q1 2024?A: About 30% of customers used online shopping services, totaling 150,000 users in Q1 2024.Q: Chatbot adoption rate Q1 2025?A: 55% adoption rate by centennials by Q1 2025.Q: Chatbot adoption rate among centennials by Q2 2024?A: 40% centennials prefer chatbots by Q2 2024.Q: How many customer queries does Lee's Market receive monthly in 2024?A: Lee's Market receives an average of 50,000 customer queries per month in 2024.Q: What was Lee's Market employee training budget in Q3 2023?A: Lee's Market allocated $450,000 for employee training, a 15% increase.Q: Annual facility maintenance cost 2024?A: Total maintenance cost was $3.5 million in 2024.Q: How many Lee's Market employees participated in leadership training in Q1 2024?A: 120 employees completed leadership training with a 90% satisfaction rate.Q: How many Lee's Market employees completed IT security training in Q1 2024?A: 85% of employees completed IT security training with a 95% pass rate.Q: What was Lee's Market employee turnover rate in Q2 2024?A: Employee turnover rate averaged 12% with a 5% increase from Q1 2024.Q: What was Lee's Market energy consumption cost in Q2 2024?A: Lee's Market energy consumption cost totaled $850,000 in Q2 2024.Q: What were Lee's Market IT costs in Q4 2024?A: IT costs totaled 3.4M,with1.2M on new hardware.Q: How many new store locations did Lee's Market open in Q3 2024?A: Lee's Market opened 12 new store locations across the US in Q3 2024.Q: What was our workers' compensation claims rate in Q2 2025?A: Workers' compensation claims rate was 2.5% with a total payout of $120,000.Q: What was Lee's Market average store maintenance cost in Q2 2024?A: Average store maintenance cost was $2,000 per month with a 5% reduction.Q: What were Lee's Market facility maintenance costs in Q2 2024?A: Facility maintenance costs totaled 1.2Mwithanaverageof100,000 monthly.Q: What was our employee headcount in Q1 2025?A: We had 5,100 employees as of January 2025.Q: How many employees attended compliance training in Q1 2024?A: Around 300 employees completed compliance training in Q1 2024.Q: What was our facility maintenance cost in Q4 2024?A: Facility maintenance costs totaled $1.8 million in Q4 2024.Q: Facility maintenance expenses Q4 2023?A: Maintenance expenses totaled $1.2 million in Q4 2023.Q: Employee training hours Q3 2024?A: Average training hours per employee were 15 in Q3 2024.Total QA Pairs: 49Supporting: 16 | Distractor: 33 | External: 0Annotated: 8 supporting + 16 distractorShapes:  Supporting   Distractor   ExternalHow can Lee's Market leverage chatbots to enhance the consumer experience for centennial shoppers?DR QuestionSupporting (Insight) (16)Distractor (33)\fTable 7: DRBench Questions and Statistics. Industry: target industry of the deep research question.\nDomain: domain of the deep research task. DR Question: question of the deep research task. Difficulty:\ndifficulty of the task defined based on the rubric mentioned in 3. # Applications: the number of total\napplications in the task environment. # Insights: the number of relevant insights to the deep research\nquestion. # Distractors: the number of non-supportings documents that do not contain relevant insights.\n\nIndustry\nRetail\n\nDomain\nCompliance\n\nRetail\n\nRetail\n\nRetail\n\nRetail\n\nSales\n\nCRM\n\nMarket Analysis\n\nCSM\n\nHealthcare\n\nCompliance\n\nHealthcare\n\nITSM\n\nHealthcare\n\nCybersecurity\n\nHealthcare\n\nCRM\n\nHealthcare\n\nMarketing\n\nElectric Vehicle\n\nCompliance\n\nElectric Vehicle\n\nQuality Assurance\n\nElectric Vehicle\n\nCybersecurity\n\nElectric Vehicle\n\nResearch\n\nElectric Vehicle\n\nCSM\n\nDR Question\nHow can Lee’s Market leverage FSMA 204 reg-\nulations to enhance food safety and customer trust?\nHow can personalization drive sales in the retail\nindustry and what strategies can be used for Lee’s\nMarket in action?\nHow can we leverage data-driven loyalty programs\nto enhance customer engagement?\nWhat are the new trends in the grocery retail\nmarket and what strategies can Lee’s Market adopt\nto remain competitive?\nHow can Lee’s Market\nleverage chatbots to\nenhance the consumer experience for centennial\nshoppers?\nWhat are the key factors influencing MediConn\nSolutions’ decision to accept insurance for tele-\nhealth providers, considering HIPAA compliance\nand state-specific data privacy regulations?\nHow can we leverage ISTM and AI-driven ana-\nlytics to minimize IT service desk workload and\nimprove response times in MediConn Solutions?\nWhat is the impact of third-party data breaches\non MediConn’s virtual healthcare platforms and\npatient data, and what new regulations can be\nimplemented to defend against these breaches?\nHow can MediConn Solutions leverage trendy new\nCRM solutions to improve patient engagement\nand retention, and what new CRM solutions are\nexpected in 2025?\nWhat are the most critical elements of a robust\ndigital presence for a telehealth provider such as\nMediConn Solutions, and how can we optimize\nour website and content marketing strategy to\nattract digital-first patients?\nHow can we balance the need for durability\nand warranty guarantees for EV batteries with\nevolving regulatory requirements, especially ACC\nregulations (ACC II), while staying on track with\nour production timelines through 2035?\nHow can Elexion Automotive’s quality assurance\nprocesses be optimized to address the unique\nchallenges of electric vehicle production, such as\nsoftware and user experience issues, compared to\ngasoline cars?\nHow can Elexion Automotive effectively im-\nplement a cybersecurity strategy for its electric\nvehicles, considering the risks and challenges\nposed by connected and autonomous technologies?\nCan we leverage AI-enhanced battery management\nto improve EV battery lifespan by 15%?\nHow can Elexion Automotive increase customer\ntrust through after-sales support while balancing\nthe need for exceptional customer care with\nefficient and cost-effective service?\n\nDifficulty\neasy\n\n# Applications\n2\n\n# Insights\n3\n\n# Distractors\n7\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\n2\n\n4\n\n3\n\n4\n\n3\n\n3\n\n4\n\n4\n\n4\n\n2\n\n1\n\n3\n\n3\n\n4\n\n4\n\n7\n\n6\n\n16\n\n4\n\n6\n\n7\n\n12\n\n15\n\n3\n\n3\n\n6\n\n7\n\n15\n\n10\n\n15\n\n14\n\n33\n\n8\n\n12\n\n14\n\n24\n\n30\n\n6\n\n6\n\n12\n\n14\n\n30\n\nD DRBENCH ENTERPRISE ENVIRONMENT\n\nThe DRBench Enterprise Environment provides a containerized simulation of realistic enterprise\nresearch settings where employees access confidential company information, personal files, and internal\ncommunications for comprehensive report generation. The environment simulates both a user’s local\nmachine filesystem and provides password-protected access to enterprise services.\n\nTo emulate realistic enterprise research settings, DRBench provides a self-contained Docker environment\nthat integrates commonly used applications: Nextcloud for shared documents, Mattermost for internal\nchat, an IMAP server and Roundcube open-source client for emails, and Filebrowser to emulate local files.\nEach task is initialized by distributing its data across these services, enabling agents to retrieve, analyze,\nand cite information through enterprise-like interfaces rather than static files. This design ensures realistic\ninteraction while maintaining reproducibility and controlled evaluation.\n\n16\n\n\fTable 8: Example Deep Research Question and Supporting Groundtruth Insights\n\nDeep Research Question\n\nHow can Lee’s Market leverage FSMA\n204 regulations to enhance food safety and\ncustomer trust?\n\nSupporting Groundtruth Insight\nU.S. grocers are working to meet the FDA’s FSMA 204 traceability rules\nby January 2026, which require tracking lot codes and key data for high-risk\nfoods to expedite recalls. This compliance is viewed as an “evolutionary\nstep” to modernize grocery operations and enhance food safety.\nBy capturing detailed traceability data, such as lot codes, at every step,\nretailers can meet regulations and gain inventory benefits. This allows grocers\nto know exact expiration dates by lot, enabling them to discount items before\nthey expire, thus reducing food waste and keeping products fresher.\nRegional grocers like Lunds & Byerlys and Raley’s see FSMA 204 as\na chance to enhance their systems and supply chain transparency. They\nbelieve improved traceability will boost customer trust and could signal the\nstart of more extensive future food safety regulations.\nLee’s Market tracks 250 high-risk food products as of Q3 2024, affecting\n30% of inventory.\nLee’s Market reduced food waste by 8% in Q2 2024, saving $1.2M.\n85% of high-risk food suppliers are FSMA 204 compliant, totaling 270\nvendors.\n\nInsight Category\nExternal\n\nExternal\n\nExternal\n\nInternal\n\nInternal\nInternal\n\nD.1 ARCHITECTURE AND SERVICES\n\nThe environment implements a multi-service architecture within a single Docker container. This design\nprioritizes deployment simplicity and cross-platform compatibility while maintaining service isolation.\nThe container orchestrates the following enterprise services:\n\n• Nextcloud: Open-source file sharing and collaboration platform analogous to Microsoft\n\nSharePoint or Google Drive, providing secure document storage with user authentication.\n\n• Mattermost: Open-source team communication platform simulating internal company commu-\nnications similar to Microsoft Teams or Slack, with teams, channels, and persistent chat history.\n\n• FileBrowser: Web-based file manager providing access to the container’s local filesystem,\n\nsimulating employee desktop environments and local document access.\n\n• Email System: Roundcube webmail interface with integrated SMTP (postfix) and IMAP\n\n(dovecot) services for enterprise email communication simulation.\n\n• VNC/NoVNC Desktop: Protocol and browser-based VNC access providing full desktop\nenvironment interaction within the container for comprehensive enterprise workflow simulation.\n\nD.2 TASK LOADING AND DATA DISTRIBUTION\n\nAt initialization, the environment processes task configuration files (env.json) and distributes data\nacross services through automated Python scripts and it makes sure that this source data is only accessible\nthrough the intended applications:\n\n• File Distribution: Documents are placed in appropriate Nextcloud user folders and FileBrowser\n\ndirectories based on task specifications\n\n• Communication Import: Chat histories and team conversations are imported into Mattermost\n\nchannels with proper user attribution\n\n• Email Integration: Email conversations are loaded into the mail system with realistic threading\n\nand metadata\n\n• User Provisioning: Enterprise users are automatically created across all services with consistent\n\nauthentication credentials\n\n17\n\n\ffrom drbench import drbench_enterprise_space, task_loader\n\n# Load task configuration\ntask = task_loader.get_task_from_id(task_id)\n\n# Initialize environment with automatic port allocation\nenv = drbench_enterprise_space.DrBenchEnterpriseSearchSpace(\n\ntask=task.get_path(),\nstart_container=True,\nauto_ports=True\n\n# Prevents port conflicts in parallel execution\n\n)\n\n# Environment provides service discovery\navailable_apps = env.get_available_apps()\n# Returns: {’nextcloud’: {’port’: 8081, ’credentials’: {...}}, ...}\n\n# Pass relevant information to the agent\n\n# Cleanup when research complete\nenv.delete()\n\nListing 1: DrBench Environment Usage\n\nD.3 PYTHON INTEGRATION\n\nThe DrBenchEnterpriseSearchSpace class provides programmatic container management with\nthe following capabilities: container lifecycle management, service access information, task-specific\ndata loading, and automatic cleanup. The typical usage pattern shown in Listing 1 demonstrates these\nintegrated capabilities.\n\nD.4 ENTERPRISE SERVICE APIS\n\nEach service exposes both **web interfaces** for human and web-agent interaction, and **programmatic\nAPIs** for agent access:\n\n• Nextcloud: WebDAV API for file operations, sharing, and metadata retrieval\n\n• Mattermost: REST API for message history, channel management, and user interactions\n\n• Email: IMAP/SMTP protocols for message retrieval and sending\n\n• FileBrowser: HTTP API for filesystem operations and file management\n\nThis dual-access model enables both agent-driven research and human verification of enterprise scenarios,\nsupporting comprehensive evaluation of research capabilities across realistic enterprise information\narchitectures.\n\nE DRBench EXAMPLES OF APPLICATION SCREENSHOTS\n\nFigures 6 and 7 show the applications provided in DRBench environment: File Browser, Mattermost,\nRoundcube, and Nextcloud.\n\nF DRBench AGENT IMPLEMENTATION DETAILS\n\nF.1 DETAILED WORKFLOW\n\nAs depicted in Figure 3, the workflow begins with a Company Employee submitting an enterprise Deep\nResearch Question along with Company Context. The DRBench agent processes this input through several\nkey stages:\n\n18\n\n\f(a) Screenshot of the File Browser interface, displaying\norganized files and folders within the system.\n\n(b) Screenshot of the Mattermost communication\nplatform, showing a discussion channel and user\ninterface elements.\n\nFigure 6: Screenshots of Applications in DRBench environment (Part 1).\n\n(a) Screenshot of the Nextcloud file management system,\nillustrating the file list view with various document types.\n\n(b) Screenshot of Roundcube, an email client, it shows\nan open email in the user’s inbox.\n\nFigure 7: Screenshots of Applications in DRBench environment (Part 2).\n\nStage 1: Research Planning. The agent decomposes the research question into structured research\ninvestigation areas to guide subsequent action generation. This initial decomposition lays out a strategy\nto systematically cover the research space while maintaining focus on the initial deep research question.\n\nStage 2: Action Planning. The Action Planning stage translates the research objectives into executable\nactions through a planning subsystem. This component uses an LLM to create a prioritized sequence\nof actions. Each action is parameterized with specific tool selection and execution parameters, its\ndependencies to other actions in the plan, and a priority score.\n\nStage 3: Research Loop with Adaptive Execution. The Research Loop iterates over the following\nsub-stages until completion: (1) Tool Selection and Execution: The tool selection and execution subsystem\nimplements a sophisticated priority-based selection of actions from the plan at each research iteration step\nand proceeds to execute it with the current research context. (2) Content Processing: If necessary, the action\nwill make use of the content processing subsystem to extract, synthesize, and store retrieved documents and\nwebsites into a vector store to form a task-specific knowledge base that will grow on each iteration. (3) Adap-\ntive Action Planning: After each execution round, the agent analyzes the most recent findings; if coverage\ngaps are detected, new actions are created and added to the plan at this point. This ensures that newly discov-\nered knowledge is taken into account to answer the research question. (4) Iteration Findings Storage: Results\nfrom each iteration are stored in the vector store with rich metadata for traceability and later assessment.\n\nStage 4: Convergence and Completion. The research loop continues until all actions in the plan have\nbeen completed or the maximum iteration limit is reached.\n\n19\n\n\fStage 5: Report Writing. The report writing subsystem queries the vector store to synthesize the\nresearch findings and relevant retrieved content. This component generates a comprehensive report and\nuses its own citation tracking system to ensure proper attribution of claims within the report.\n\nThe vector store serves as the main knowledge integration component, maintaining embeddings of all\nprocessed content and enabling semantic retrieval at the report generation stage. This component is crucial\nin a deep research setting to prevent information loss from early research stages in arbitrarily long research\nsessions.\n\nF.2 RESEARCH PLANNING IMPLEMENTATION\n\nThe Research Planning subsystem offers three operational modes to evaluate the impact of structured\nplanning on research effectiveness:\n\n• Complex Mode: Generates a comprehensive research plan with detailed investigation areas.\nThese areas contain details about the specific research focus, expected information sources, and\nsuccess criteria, among others. Each area includes an importance level and specific business\nintelligence objectives (see Listing 2).\n\n• Simple Mode: Creates focused question decompositions with 4-10 self-contained subqueries\nderived directly from the main research question. Uses straightforward decomposition without\nthe complex enterprise research structure of complex mode. See examples in Listing 4 and\nListing 3 for comparison of different planning modes.\n\n• None: Bypasses structured planning entirely, proceeding directly to action generation based\non the original question. This mode serves as a baseline to measure the added-value of explicit\nplanning stages.\n\nThe planning process begins with the enriched query (Prompt 1) and uses the research planning prompt\n(Prompt 2) to generate structured outputs. In Complex Mode, the system creates detailed investigation areas\nwith enterprise-focused metadata, while Simple Mode produces straightforward question decompositions\nsimilar to existing multi-step reasoning approaches. The resulting plan structure directly feeds into the\nAction Planning System (Section F.3) for executable action generation.\n(cid:7)\n{\n\n(cid:4)\n\n\"area_id\": 1,\n\"research_focus\": \"Core strategic\n\ndomain, market segment, or business hypothesis to investigate\",\n\n\"information_needs\": [\n\n\"What specific intelligence is required for strategic decisions\"\n\n],\n\"knowledge_sources\": [\"internal\", \"external\", \"both\"],\n\"research_approach\n\n\": \"competitive_analysis | market_research | strategic_assessment\n\n| trend_analysis | risk_analysis | performance_benchmarking\",\n\n\"key_concepts\": [\"concept1\", \"concept2\"],\n\"business_rationale\": \"Why this investigation\n\narea is critical for enterprise strategy and decision-making\",\n\n\"expected_insights\": \"What strategic\n\nunderstanding or competitive intelligence this area should provide\",\n\n\"stakeholder_impact\": \"Which\n\nbusiness units or decision-makers will benefit from these insights\",\n\n\"importance_level\": \"critical | important | supplementary\"\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 2: Investigation Area Structure for Full Planning Mode\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"What are the new trends in the grocery retail market\n\nand what strategies can Lee’s Market adopt to remain competitive?\",\n\n\"plan\": {\n\n\"research_investigation_areas\": [\n{\n\n20\n\n\f\"area_id\": 1,\n\"research_focus\": \"Current trends in grocery retail market\",\n\"information_needs\": [\n\"Latest consumer preferences\",\n\"Emerging technologies influencing grocery shopping\",\n\"Sustainability practices in grocery retail\",\n\"Changes in supply chain dynamics\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"trend_analysis\",\n\"key_concepts\n\n\": [\"e-commerce growth\", \"sustainability in supply chains\"],\n\n\"business_rationale\": \"Understanding consumer\n\ntrends and technological advancements that shape shopper behavior is\ncritical for adapting offerings and enhancing customer engagement.\",\n\"expected_insights\": \"Identify specific trends affecting customer\n\nbuying decisions, including the rise of online grocery shopping\nand preferences for sustainable, local, or organic products.\",\n\n\"stakeholder_impact\n\n\": \"Marketing, Product Development, Supply Chain Management\",\n\n\"importance_level\": \"critical\"\n\n},\n{\n\n\"area_id\": 2,\n\"research_focus\": \"Competitive analysis of grocery retailers\",\n\"information_needs\": [\n\"Market share analysis\",\n\"Competitor strengths and weaknesses\",\n\"Innovative strategies adopted by competitors\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"competitive_analysis\",\n\"\n\nkey_concepts\": [\"market positioning\", \"competitive differentiation\"],\n\n\"business_rationale\": \"A comprehensive understanding of\n\ncompetitors allows for strategic positioning and the identification\nof innovative practices that can be adopted or improved upon.\",\n\n\"expected_insights\": \"Detailed profiles of key\n\ncompetitors, including strategic moves they are making to capture\n\nmarket share, which can inform Lee’s Market’s competitive strategy.\",\n\n\"stakeholder_impact\n\n\": \"Executive Leadership, Strategic Planning, Marketing\",\n\n\"importance_level\": \"critical\"\n\n},\n...\n]\n\n}\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 3: Complex Mode Research Plan Example\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"How can we leverage\n\ndata-driven loyalty programs to enhance customer engagement?\",\n\n\"plan\": {\n\n\"mode\": \"simple\",\n\"subqueries\": [\n\"What are the key features\nof successful data-driven loyalty programs in the retail industry?\",\n\"How can data analytics be used to personalize rewards\nand incentives in loyalty programs to increase customer engagement?\",\n\"What types of customer data should be collected and analyzed\nto optimize loyalty programs for a company like Lee’s Market?\",\n\n21\n\n\f\"Which\ntechnology platforms and tools are most effective for implementing\nand managing data-driven loyalty programs in the retail sector?\",\n\"How can Lee’s Market measure the success of their\nloyalty program in terms of customer engagement and sales growth?\",\n\"What are the best practices for integrating a loyalty program\nwith existing marketing strategies to enhance customer experience?\",\n\"How can Lee’s Market ensure customer data\nprivacy and security while leveraging data-driven loyalty programs?\",\n\"What are the potential\nchallenges and limitations of implementing a data-driven loyalty\n\nprogram for a retail company with Lee’s Market’s size and resources?\"\n],\n\"research_methodology\": {\n\"overall_approach\": \"Query decomposition into focused subqueries\"\n}\n\n}\n\n}(cid:6)\n\nListing 4: Simple Mode Research Plan Example\n\n(cid:5)\n\nF.3 ACTION PLANNING SYSTEM\n\nThe Action Planning System translates research objectives into executable actions through an intelligent\nplanning subsystem that manages tools, prioritization, and dependencies.\n\nAvailable Tools Table 9 summarizes the available tools organized by category and their primary purposes.\n\nTable 9: DrBench Agent Tool Categories and Purposes\n\nCategory\n\nTool\n\nPurpose\n\nInformation Retrieval\n\nInternet Search\n\nEnterprise API\n\nURL Fetch\n\nAnalysis\n\nAnalyzer\n\nLocal Processing\n\nLocal\nSearch\n\nDocument\n\nExternal market research, competitive intel-\nligence, and public data analysis. Ideal for\nmarket trends, competitor analysis, industry\nreports, news articles.\n\nAccess to proprietary internal data through\nextensible adapters (Nextcloud, Mattermost,\nemail, FileBrowser). Ideal for internal metrics,\ncommunications, confidential documents.\n\nDirect content extraction from specific URLs.\nIdeal for deep analysis of reports, whitepapers,\ncase studies, competitor websites.\n\nAI-powered synthesis and analysis using\nvector search. Ideal for cross-referencing find-\nings, identifying patterns, generating insights.\n\nSemantic search within locally ingested\ndocuments. Ideal for targeted retrieval from\nlocal files with source references.\n\nPriority Scoring and Dependencies Actions are assigned priority scores (0.0-1.0 scale) based on strategic\nimportance and expected information value. The priority assignment follows enterprise research principles:\n\n• Source Type Prioritization: Enterprise and local sources receive higher priority than external\n\nsources, reflecting the strategic value of proprietary information in competitive analysis.\n\n• Query Specificity: Targeted queries addressing specific investigation areas score higher than\n\nbroad exploratory searches, ensuring focused research execution.\n\n22\n\n\f• Dependency Management: Actions can specify prerequisite relationships where certain\ninformation gathering must precede analysis or synthesis tasks. The scheduler respects these\ndependencies while maximizing parallel execution within each iteration.\n\nF.4 ENTERPRISE INTEGRATION ARCHITECTURE\n\nService Adapters The system implements extensible adapters for enterprise services including Nextcloud\nfile server, Mattermost chat, IMAP email systems, and FileBrowser interfaces. Each adapter handles\nservice-specific authentication, data retrieval, and metadata preservation for proper citation attribution.\n\nSource Prioritization Strategy Enterprise and local sources receive priority multipliers in action scoring,\nreflecting the strategic value of proprietary information. The system maintains source type classification\nthroughout the pipeline to ensure internal intelligence drives analytical conclusions while external sources\nprovide context and validation.\n\nF.5 TOOL SELECTION AND EXECUTION\n\nThe Tool Selection stage implements a priority-based action selection and tool invocation within each\nresearch iteration to execute the action plan.\n\nAction Selection Process At each iteration, the system selects executable actions based on priority\nscores and dependency satisfaction:\n\n• Priority-Based Scheduling: Actions are ranked by their priority scores, with enterprise and local\nsources prioritized over external sources to maximize the value of specific private information.\n\n• Dependency Validation: The scheduler checks that all prerequisite actions have completed\n\nbefore making an action available for execution.\n\n• Sequential Execution: Actions execute one at a time in priority order, maintaining research\n\ncoherence and enabling each action to build upon previous findings.\n\nOnce selected, actions execute through a standardized interface and results are integrated into the research\ncontext, informing the following stage of adaptive action planning.\n\nF.6 ADAPTIVE PLANNING\n\nThe Adaptive Planning system enables dynamic evolution of the action plan by analyzing results after\neach iteration to generate extra actions addressing information gaps.\n\nIt starts by analyzing the most recently completed actions and performs these two substages:\n\nSource analysis and gap classification. The system evaluates the possible imbalances in the action\ncompletion if information came from internal or external sources and identifies possible scenarios to cover.\n\nDynamic action generation. After analyzing the sources and results from the previous actions, the\nsystem makes an LLM call to generate 1-5 extra candidate actions with a specific prioritization. After\ncandidate actions are generated, they go through a deduplication process to make sure the plan didn’t\ncover them already and incorporates the final subset into the priority action plan so they can be considered\nby the scheduler in the following iteration.\n\nF.7 CONTENT PROCESSING AND VECTOR STORE\n\nThe Content Processing system implements a pipeline for unified ingestion of documents in multiple\nformats (PDF, docx, HTML, JSON, plain text formats, etc.) that normalizes and cleans text inside the\ndocuments and websites retrieved during the research. Content is then deduplicated and chunkized, and\nembeddings are computed for each of these chunks.\n\nThe Vector Store implements the storage and retrieval of the content via JSON metadata and NumPy\nembedding metrices, enabling semantic similarity and keyword based searches\n\n23\n\n\fF.8 REPORT GENERATION\n\nMulti-Stage Synthesis Pipeline The Report Generation stage implements a four-stage synthesis\napproach: (1) thematic content clustering via vector searches, (2) source prioritization and deduplication,\n(3) LLM Synthesis with source tracking, and (4) Final report writing and citation resolution.\n\nThe system generates targeted search queries based on the research plan, including specific to ensure that\na predefined set of themes or sections (background, analysis, implementation, trends) are retrieved and\nwritten and prevents redundant analyses.\n\nUnified Citation System The citation system implements deferred resolution to keep consistency of\ncitations from section to section by referencing document IDs in the Vector Store and carrying them over\nfor each piece of synthesized text. A final citation resolution stage will assign the correct numbering to\neach document in the final report.\n\nF.9 DRBA PROMPTS\n\nThe DRBench agent relies on carefully designed prompts to orchestrate enterprise research workflows.\nThese prompts implement the core architectural principles: enterprise context enrichment, structured\nplanning decomposition, priority-based action generation, quantitative synthesis requirements, and adaptive\nresearch capabilities. The following five prompts represent the critical LLM interactions that enable\nsystematic enterprise research with proper source prioritization and citation tracking:\n\n• Enriched Query Generation (Prompt 1): Transforms basic research questions into enterprise-\ncontextualized queries by incorporating company information, stakeholder personas, and business\ncontext to guide subsequent research activities.\n\n• Research Planning (Prompt 2): Decomposes complex research questions into structured\ninvestigation areas with defined information needs, knowledge sources, and business rationales,\nenabling systematic coverage of the research space.\n\n• Action Generation (Prompt 3): Converts research objectives into prioritized executable\nactions with tool specifications and dependency relationships, emphasizing enterprise source\nprioritization over external sources.\n\n• Adaptive Action Generation (Prompt 4): Analyzes research progress to identify coverage\ngaps and source imbalances, generating complementary actions that enhance research depth\nand cross-validate critical findings.\n\n• Report Synthesis (Prompt 5): Orchestrates quantitative-first content synthesis with strict citation\nrequirements, ensuring numerical data leads analytical paragraphs and all claims are properly\nattributed to source documents.\n\nW Query Generation\n\nResearch Question: {dr question}\n\nCompany Context: {company name} is {company desc}.\n\nPersona Context: This analysis is requested by {name}, {role} in\n{department}.\n\nTheir responsibilities include: {responsibilities}.\n\nPrompt 1: Query Enrichment with Enterprise Context.\n\nG QUALITATIVE RESULTS\n\nShown below are some illustrative examples of how metrics are computed for different scenarios on a\ngiven test task.\n\n24\n\n\fW Research Planning (Complex Mode)\n\nDesign a comprehensive enterprise research strategy for:\n\"{question}\"\n\n{tools section}\n\nAs a senior enterprise researcher with deep business\nintelligence expertise, create a thorough investigation\nplan that combines rigorous research methodology with\nstrategic business analysis. Your goal is to provide insights\nthat drive informed decision-making in complex enterprise\nenvironments.\n\nEnterprise Research Design Principles:\n\n- Leverage proprietary internal data as competitive advantage while\nensuring external market context\n- Design investigations that directly inform strategic decisions and\nbusiness outcomes\n- Prioritize research areas that maximize ROI and strategic value to the\norganization\n- Balance comprehensive analysis with focused insights relevant to\nenterprise objectives.\n\nGenerate a JSON object with strategic research investigation\nareas:\n\n{output structure}\n\n.\n\nPrompt 2: Enterprise Research Planning with Investigation Areas. See example of the output structure\nin Listing 2\n\nG.1\n\nINSIGHTS RECALL\n\nWe showcase the insights recall metric using Task DR0002, whose DR Question is “How can\npersonalization drive sales in the retail industry and what strategies can be used for Lee’s Market in action?”\n(also shown in Table 7), which evaluates sales challenges and competitive landscape analysis.\n\nTable 10 shows the overall performance comparison between using Llama 3.1 405B and GPT-5 in\nour agent. Using GPT-5 in our agent results in increasing the insights recall score from 0.14 to 0.43,\nsuccessfully answering 3 out of 7 questions compared to Llama’s 1 out of 7.\n\nTable 10: Insights Recall Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We\nsummarize the number of questions answered successfully and unsuccessfully as well as the overall\ninsights recall score for the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nInsights Recall Score\n\nQuestions Answered\nSuccessfully\n\nQuestions Failed\n\n0.14\n\n1/7\n\n6/7\n\n0.43\n\n3/7\n\n4/7\n\n+0.29\n\n+2\n\n-2\n\nThe question-by-question breakdown in Table 11 reveals the specific questions where each approach\nsucceeded or failed. Both models successfully identified the insight related to online customer engagement,\nbut only GPT-5 was able to identify the number of loyalty program members and the customer data\ncollection rate. Neither model successfully answered the remaining 4 questions, indicating these insights\nmay not have been readily available in the source materials or that agents struggled to find the right insights.\n\n25\n\n\fW Action Generation\n\nGenerate specific executable actions for this research investigation\narea with SOURCE PRIORITIZATION:\n\nResearch Focus: {research focus}\n\nInformation Needs: {information needs}\n\nKnowledge Sources: {knowledge sources}\n\nResearch Approach: {research approach}\n\nAvailable Tools: {available tool names}\n\nTool Selection Guidelines: {tool guidelines}\n\nReturn JSON array of actions with:\n- \"type\": Action type (web search, enterprise api, url fetch, analyzer,\nlocal search)\n- \"description\": Clear description of what this action will accomplish\n- \"parameters\": Tool-specific parameters including query, search type,\netc.\n- \"priority\": Float 0.0-1.0 (enterprise sources: 0.7-1.0, external:\n0.4-0.7)\n- \"expected output\": What information this action should provide\n- \"preferred tool\": Specific tool class name to use\n\nPrompt 3: Priority-Based Action Generation with Source Awareness\n\nTable 11: Question-by-Question Insights Recall Analysis (Task DR0002). We breakdown the results\nquestion by question for the given task, highlighting specifically which question is answered correctly\nor incorrectly for each model.\n\nQuestion\n\nLlama 3.1 405B GPT-5\n\nOnline Customer Engagement with Personalized Recommendations\n\nNumber of Loyalty Program Members\n\nCustomer Data Collection Rate\n\nOnline Sales Growth\n\nEffectiveness of Personalized Marketing\n\nPersonalized Promotions vs Mass Promotions\n\nRetail Media Growth\n\nTotal Insights Recall Score\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0\n\n1.0\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n∆\n\n0.0\n\n+1.0\n\n+1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0/7.0\n\n3.0/7.0\n\n+2.0\n\nIn Table 12 we extend Table 4 and show all the groundtruth insights as well as each of the predicted\ninsights from using both Llama 3.1 405B and GPT-5. As before, we highlight in bold where the model\nwas able to accurately find details relevant to the expected insight, and show all the corresponding scores\nas given in Table 11.\n\nG.2 FACTUALITY\n\nThe factuality metric evaluation uses the same Task DR0002 to assess the accuracy and reliability of\ngenerated content. Table 13 presents the factuality performance comparison, showing that while using\nLlama 3.1 405B achieved 0.41 factuality (7 factual claims out of 17 total claims), where as using GPT-5\nreached 0.65 factuality (13 factual claim out of 20 total claims). This represents a significant improvement\n\n26\n\n\fW Adaptive Action Generation\n\nBased on the research progress so far, suggest new actions with\nINTELLIGENT SOURCE COMPLEMENTARITY:\n\nOriginal Research Query: {research query}\n\nCompleted Actions Summary: {completed actions summary}\n\nRecent Findings Analysis: {findings json}\n\nSource Composition Analysis: {internal findings}\n\nAvailable Tools: {available tool names}\n\nGenerate 1-5 new actions that:\n1. **Address gaps** in current research coverage\n2. **Balance source types** - if findings are heavily external,\nprioritize internal sources and vice versa\n3. **Build on discoveries** - leverage new information to explore\nrelated areas\n4. **Enhance depth** - dive deeper into promising\nfindings\n5. **Cross-validate** - verify critical findings through alternative\nsources\n\nEach action should have:\n- Strategic rationale for why this action is needed\nnow\n- Clear connection to research gaps or promising\nleads\n- Appropriate priority based on strategic value and source\nbalance\n- Specific parameters that build on existing knowledge\n\nPrompt 4: Gap-Driven Adaptive Action Generation\n\nin content reliability. This also highlights that GPT-5 is much better prepared to make accurate claims\nthat are sustained by evidence.\n\nTable 14 provides a detailed breakdown of factual versus unfactual claims. The agent using GPT-5\ngenerated 6 additional factual claims while producing 3 fewer unfactual claims, resulting in a net\nimprovement in accuracy percentage. This demonstrates that GPT-5 may generate a higher proportion\nof factual information than Llama 3.1 405B.\n\nThe impact of these factuality improvements of using GPT-5 over Llama 3.1 405B is summarized in\nTable 15. The 24.0 percentage point improvement in factuality represents enhanced content quality and\nresearch reliability. The increase in 3 total claims shows that GPT-5 can generate more content overall.\n\nH EXPERIMENTAL SETTINGS\n\nAll experiments were conducted on a cluster of 8 NVIDIA A100 GPUs (80GB each). For file generation\nand task construction, we primarily used the Llama-3.1-405B model, with decoding performed\nusing nucleus sampling at a temperature of 0.7 unless otherwise specified. For larger-scale evaluations of\nthe DRBench Agent, we also used closed-source models such as GPT-4o and GPT-5, alongside DeepSeek\nmodels, to enable comparison across open- and closed-source backbones.\n\nTo ensure reproducibility, the DRBench environment was deployed as a self-contained Docker container\nwith all supporting applications (Nextcloud, Mattermost, and Filebrowser) pre-configured. Each task was\n\n27\n\n\fTable 12: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have interacted\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n\n45% of Lee’s Market online customers\nengage with personalized product recom-\nmendations, resulting in a 25% increase in\naverage order value.\n(Score = 1.0)\n\n45% of online customers engaged with\npersonalized product recommendations,\nand among those engagers average order\nvalue increased by 25%.\n(Score = 1.0)\n\nAs of Q3 2024, Lee’s Market has 1.2 million\nloyalty program members.\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\nLee’s Market online sales grew 12% in Q2\n2024 compared to Q2 2023.\n\nRetailers excelling in personalized marketing\nare growing revenues about 10 percentage\npoints faster than their peers, according to\nBCG. By effectively using first-party customer\ndata, these leaders could unlock an estimated\n$570 billion in additional sales, highlighting\nthe importance of data-driven sales strategies\nfor growth.\n\nPersonalized promotions can deliver returns\nthree times higher than mass promotions,\nyet many retailers allocate under 5% of their\npromo budgets to personalization. One major\nchain increased its personalized promo spend\nfrom 1% to 10% by establishing a ”customer\ninvestment council,” resulting in $250 million\nin incremental sales.\n\nRetail media networks are expanding rapidly,\nwith retail media growing at approximately\n25% annually, offering retailers a profitable\nrevenue stream to reinvest in technology, data,\nand personnel. By integrating loyalty data,\nretailers like Sephora, which links 95% of\ntransactions to loyalty accounts, enhance preci-\nsion in product recommendations and provide\na seamless omnichannel experience, boosting\nconversion rates and customer lifetime value.\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nFrom Q2 2024 to Q3 2024, loyalty members\nincreased from 1,050,000 to 1,200,000\n(+150,000; +14.29%), average spend per\nmember rose from 24 to 25 (+4.17%), and\ntotal member spend increased from 25,200,000\nto 30,000,000 (+19.05%).\n(Score = 1.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n45% of Lee’s Market online customers engage\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n(Score = 0.0)\n\nA naive blended online AOV upside of\napproximately 11.25% is derived from 45%\nengagement multiplied by a 25% AOV lift\namong engagers.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nGrocers running data-driven loyalty cam-\npaigns have realized an average 3.8%\nlike-for-like sales uplift.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nsources\n\nindicate POS-enabled\nExternal\npersonalization can lift revenue 5%-15% and\nadvocate personalized e-receipts with relevant\noffers and coupons to extend post-purchase\nengagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 0.0)\n\nTable 13: Factuality Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We show\nthe number of factual and unfactual claims made by each model, as well as the overall factuality score\nfor the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nFactuality Score\n\nFactual Claims\n\nUnfactual Claims\n\n0.41\n\n7\n\n10\n\n0.65\n\n13\n\n7\n\n+0.24\n\n+6 claims\n\n-3 claims\n\n28\n\n\fW Report Synthesis\n\nAs an expert research analyst, synthesize the following content into\na coherent, insightful, and well-supported analysis for the theme:\n\"{theme}\" directly related to the overarching research question:\n\"{original question}\"\n\nSource Priority Guidelines:\n\n1. **\"internal\"**: Highest priority (internal company\ndocuments, proprietary files, confidential reports,\nenterprise chat messages, local documents, CRM data,\ninternal APIs, project management tools). Insights from\nthese sources should form the primary foundation of the\nanalysis.\n2. **\"external\"**: Medium priority (public web sources,\nacademic papers, industry reports, news articles). Use these\nto provide broader context, external validation, or contrasting\nperspectives.\n\n**Synthesis Requirements:**\n* **QUANTITATIVE PRIORITY:** Lead with numerical data, calculations, and\naggregations\n* Extract ALL percentages, costs, metrics, and performance\ndata\n* Perform mathematical operations: aggregate percentages, calculate\nincreases, sum totals\n* Example: \"Finance customers (35%) combined with healthcare\n(40%) represent 75% of regulated industry concerns\n[DOC:doc 1][DOC:doc 2]\"\n\n* **FACT VERIFICATION (CRITICAL):**\n* ONLY state what documents explicitly contain - no inference or\nextrapolation\n* Use exact quotes for key numerical claims: \"As stated in the\ndocument: ’[exact quote]’ [DOC:doc id]\"\n\n* **Citation Usage (Critical):**\n* **Format:** Reference sources by their document ID: \"Internal review\nshows 15% increase [DOC:doc 079c2e0f 1752503636]\"\n* **NEVER HALLUCINATE CITATIONS:** Only use provided doc id\nvalues\n* **Cite every numerical claim and calculation with source\ndocuments**\n\nGenerate 2-4 paragraphs of synthesized analysis with proper inline\ncitations.\n\nPrompt 5: Report Section Synthesis with Citation Requirements\n\nexecuted by instantiating a fresh container to avoid state leakage across runs. We capped the number of\nagent iterations according to the settings described in Section 5, with each iteration limited by a fixed\ncomputational budget.\n\nFor model outputs, we standardized all prompts and evaluation pipelines across backbones, using identical\nresearch questions, company contexts, and injected insight sets. To avoid stochastic variability, we repeated\ngeneration three times per task and reported averaged scores.\n\nFinally, all supporting scripts, environment configurations, and evaluation code are fully containerized,\nenabling consistent replication of our reported results across hardware setups.\n\n29\n\n\fTable 14: Factuality Content Analysis (Task DR0002). We show the number of factual and unfactual\nclaims made by each model, highlighting the factuality accuracy of each model for the given task.\n\nAgent\n\nFactual Unfactual Accuracy\n\nLlama 3.1 405B\n\n7 claims\n\n10 claims\n\nGPT-5\n\nChange\n\n13 claim\n\n7 claims\n\n+6\n\n-3\n\n+24.0%\n\n41.0%\n\n65.0%\n\nTable 15: Factuality Summary (Task DR0002). We summarize the factuality result improvements made\nby using GPT-5 over Llama 3.1 405B.\n\nImpact Category\n\nValue\n\nFactuality Improvement\n\n+24.0 percentage points\n\nClaims Added\n\n3 total claims\n\nAccuracy Enhancement\n\nFrom 41.0% to 65.0%\n\nContent Quality\n\nMore grounded information\n\nTask Domain\n\nTask Industry\n\nSales\n\nRetail\n\nI DATA SYNTHESIS AND DRBA COST DETAILS\n\nTo generate the DRBench tasks, we combined external insight extraction with internal file synthesis. Each\ntask included an average of 10 supporting files spanning heterogeneous formats (PDF, DOCX, PPTX,\nXLSX, and JSONL), with each file containing roughly 5 paragraphs of content. Files were designed\nto embed injected insights while mixing in distractor material, ensuring realistic enterprise complexity\nwithout exceeding practical runtime or storage budgets.\n\nData synthesis was primarily powered by GPT-4o. During task construction, GPT-4o was responsible for (1)\nextracting structured insights from public web sources, (2) adapting these insights into enterprise-grounded\ninterpretations, (3) generating persona-specific deep research questions, and (4) producing file-level content\nwith a balanced mix of insights and distractors. For evaluation, the DRBench Agent (DRBA) used GPT-4o\nas its backbone, with each task typically requiring 15 iterations and approximately 120–150 model calls.\n\nIn terms of cost, GPT-4o-based synthesis of a single task (10 files, 5 paragraphs each, plus metadata)\nconsumed about 30k–40k tokens, while DRBA execution required an additional 50k–70k tokens per\ntask. At current GPT-4o API pricing ($5 per million input tokens and $15 per million output tokens), this\ncorresponds to a per-task cost of approximately $1.5–$3.5 depending on the mix of input/output tokens\nand the iteration budget. This makes large-scale benchmarking feasible at moderate cost, while still being\nsignificantly cheaper than manual authoring or annotation.\n\nWe also note that smaller open-source models such as Llama-3.1-8B-Instruct perform well for file\ngeneration. Unlike GPT-4o, which requires API usage, Llama-3.1-8B can be hosted locally and runs\nefficiently on a single NVIDIA A100 40GB GPU. This provides a cost-effective alternative for generating\nlarge numbers of supporting documents, especially when full closed-source quality is not required.\n\nJ WEB AGENTS FOR DEEP RESEARCH TASKS\n\nSince each of the environments can be access directly through a web user interface (UI), we also exper-\nimented with an agent that can directly interact with the webpages through common browser actions like\nclick, input and scroll, which are executed through playwright3. We implement our web agent\nusing the AgentLab and BrowserGym frameworks (Chezelles et al., 2025) with a GPT-4.14 backbone. Our\nagent is implemented from AgentLab’s GenericAgent, which achieves respectable performance when used\n\n3https://playwright.dev\n4https://openai.com/index/gpt-4-1/\n\n30\n\n\fwith GPT-4o5 as a backbone; it completes 45.5% of the tasks in WorkArena (Drouin et al., 2024), 31.4%\nin WebArena (Zhou et al., 2024) and achieves a step-level reward of 13.7% on WebLINX (L`u et al., 2024).\n\nHyperparameters and Prompt We present the hyperparameters for the agent in tables 16 and 17, which\nare in majority set to the default hyperparameters, except for the maximum number of input tokens (bound\nto a reasonable maximum length) and a higher maximum number of steps (to allow the agent to perform\nmore actions required to write the report). We further update the agent’s action space on the last step to\nonly allow it to reply to the user with a report, ensuring that each trajectory terminates with a report. To\nensure that the agent is aware of the tools it can use, we modify the default system prompt (see prompt 6).\nAdditionally, each task intent is provided alongside information about the user and company (see prompt 7).\n\nResults We find that the GPT-4.1-powered web agent achieves an insights recall and factuality of\n1.11% and 6.67% respectively and a report quality score of 33.07%. Although the high report quality\nindicates that the agent can properly formulate a report, the insights quality is severely limited, with none\nof the claims being backed by useful sources. For example, a DRBench Agent powered by GPT-5 may\nanswer the question What is Lee’s Market’s current food waste reduction rate as of Q2 2024? with An\n8% reduction in food waste in Q2 2024 saved Lee’s Market $1.2 million, indicating that better inventory\ncontrol can yield both safety and financial benefits., which achieves a score of 1.0 for the question. On\nthe other hand, a GPT-4.1-powered agent will provide an unsatisfactory answer, thus achieving an insights\nrecall of 0.0. The most likely cause of this poor performance is the model’s limited capability to properly\ninteract with web interfaces when encountering unfamiliar tools. For instance, the agent may be unfamiliar\nwith the VNC and file browser applications, making it harder for it to correctly select the file it needs\nto use. Moreover, whenever the agent ends up performing an ineffective action (e.g. click on an element\nthat does not trigger any change to the page), it tends to persist by reiterating the same action (see Table\n18), or the same sequence of ineffective actions, despite not achieving anything in the previous steps. As a\nresult, despite a large number of steps, most of the agent’s actions are not helpful towards solving the task.\n\nTable 16: Web Agents Boolean Hyperparameters\n\nValue Flags\n\nTrue\n\nFalse\n\nvision support, use ax tree,\nuse tabs, use focused element, use error logs,\nuse history, use action history, use screenshot,\nuse som, extract visible tag, extract clickable tag,\nuse thinking, use concrete example, use abstract example,\nuse hints, be cautious, add missparsed messages\nuse html, use past error logs, use think history,\nuse diff, filter visible elements only,\nfilter with bid only, filter som only, multiaction,\nstrict, long description, individual examples,\nuse plan, use criticise, use memory, enable chat\n\nK STANDARD ERROR\n\nRestricting to the MinEval subset, we average the results on each task across 3 different runs in Table\n19. We give both the means and standard errors for the insight recall, factuality, distractor avoidance, and\nreport quality.\n\nL COMPLEX MODEL ABLATION RESULTS\n\nExtending our discussion in Section 5.3 to more GPT and Llama models in Table 20, we see that the\nsmaller GPT-5-mini model lags behind but still outperforms earlier closed-source backbones such as\nGPT-4o and GPT-4o-mini, particularly in terms of harmonic mean. In addition, smaller variants of Llama\n\n5https://openai.com/index/hello-gpt-4o/\n\n31\n\n\fW Web Agents System Prompt\n\nYou are an agent trying to solve a web task based on the content of the\npage and user instructions. You can interact with the page and explore,\nand send messages to the user. Each time you submit an action it will\nbe sent to the browser and you will receive a new page.\nYou will be solving tasks that involve Deep Research, by\nnavigating websites and web apps with information useful\nsolving the task. You will need to gather insights from\ndata contained in the services provided and the internet\nto complete your report. You have a maximum of 50 steps\nto complete the task. Before the end, you must use the\naction {send msg to user}, which should contain a final Deep\nResearch report detailing everything the user needs to\nknow.\n\nYou must sustain your claims in files, chats, or emails\nfrom the enterprise environment or in websites you\nsearched. You must provide a citation (or an inline\ncitation, that works too) with the source of those claims\n(e.g. << add citation examples from the documentation\n>>). Do not make up citations if you haven’t retrieved its\ncontent.\n\nHere are some expected agent behavior:\n\n**Enterprise Environment Interaction:**\n- Access Nextcloud files, Mattermost chats, emails, VNC desktop,\netc.\n- Extract relevant information from multiple sources\n- Navigate complex enterprise data landscapes\n\n**Report Requirements:**\n- Synthesize findings into comprehensive research\nreport\n- Include proper citations for all claims (flexible format -\nauto-normalized)\n- Draw meaningful insights and conclusions\n- Ground all statements in available evidence\n\n**Citation Format (Flexible - Auto-Normalized):**\n- Files: ‘quarterly report.pdf‘, ‘shared/budget-analysis.xlsx‘,\n‘Analysis document (reports/analysis.docx)‘\n- URLs: Direct links or ‘[Article Title](https://example.com)‘\n- Emails: ‘Email from alice@company.com on Jan 20,\n2025‘\n- Chat: ‘Mattermost message from john.doe in Compliance team, General\nchannel‘\n\nIf you need navigate the internet (outside of the designated websites),\nyou can use the browser inside noVNC Desktop.\n\nPrompt 6: Extended instructions given to the Deep Research web agent.\n\n32\n\n\fTable 17: Web Agents Non-Boolean Hyperparameters\n\nParameter\n\nValue\n\nchat model.model name\nchat model.max total tokens\nchat model.max input tokens\nchat model.max new tokens\nchat model.temperature\naction.action set.subsets\naction.action set.retry with force true\nflags.max prompt tokens\nflags.max trunc itr\nenv.max steps\n\ngpt-4.1\n32768\n28672\n4096\n0\nwebarena\n\n28672\n20\n50\n\nW Web Agents Task Intent Prompt\n\nHow can Lee’s Market leverage FSMA 204 regulations to enhance food\nsafety and customer trust?\n\nHere is some information about the user:\nID: MNG0003\nFirst name: John\nLast name: Doe\n...\nJustification: As a regulatory affairs manager, John\n...\n\nHere is some information about the company:\nName: Lee’s Market\nAnnual revenue: $500M - $600M\n...\nTarget markets: Asian communities in the U.S. and ...\n\nPrompt 7: Web Agents Task Intent Prompt\n\nTable 18: Web Agents tends to get stuck on cycles of actions, and are unable to backtrack or to restart\nwith a different application.\n\n9. I have been repeatedly clicking\non the “shared” folder (bid ‘194’)\n. . . To proceed with the research\non personalization strategies for\nLee‘s Market, I should open the\n“shared” folder to look for. . .\nclick(‘194’)\n\n10. I have clicked on the “shared”\nfolder multiple times . . .\nI will\ntry clicking on the “shared” folder\nagain to see if it opens and displays\nits contents.\n\nclick(‘190’)\n\n11. I have been repeatedly clicking\non the “shared” folder (bid ‘194’)\nTo proceed, I should try\n. . .\nclicking on the “shared” folder\nagain to attempt to open it and\nview any files . . .\n\nclick(‘194’)\n\n12. I have navigated to the “shared”\nfolder, but I have not yet opened\nit . . . The next logical step is to\nopen the “shared” folder to look\nfor any relevant documents . . .\n\nclick(‘194’)\n\ndegrade metric results further. This further substantiates the claim that larger and more advanced models\ntend to offer a better balance between recall, factuality, and overall report quality.\n\nM QUANTITATIVE RESULTS PER TASK\n\nWe show a detailed breakdown of the insights recall in Table 21, factuality in Table 22, distractor avoidance\nin Table 23 and report quality in Table 24 on the MinEval subset for a variety of models.\n\n33\n\n\fTable 19: DRBA performance with different planning configurations on MinEval. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. Scores are reported for insight recall, factuality, distractor\navoidance, report quality, and the overall harmonic mean.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n.188 ± .038\n.137 ± .04\n.154 ± .03\n.197 ± .03\n.142 ± .04\n.188 ± .05\n\n.665 ± .09\n.622 ± .11\n.705 ± .05\n.604 ± .08\n.504 ± .10\n.691 ± .03\n\n.981 ± .01\n1.00 ± .00\n1.00 ± .00\n.995 ± .39\n.990 ± .39\n1.00 ± .00\n\n.912 ± .004\n.901 ± .006\n.917 ± .004\n.928 ± .005\n.906 ± .007\n.923 ± .006\n\n.448\n.365\n.400\n.454\n.359\n.453\n\nTable 20: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Scores are reported for insight recall, factuality, distractor avoidance, report quality,\nand harmonic mean. Note that higher numbers corresponds to better scores, and the best result on each\nmetric is bolded.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nGPT-5-mini\nGPT-5-mini\nGPT-5-mini\nGPT-4o\nGPT-4o\nGPT-4o\nGPT-4o-mini\nGPT-4o-mini\nGPT-4o-mini\nGPT-OSS-120B\nGPT-OSS-120B\nGPT-OSS-120B\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-70b-Instruct\nLlama-3.1-70b-Instruct\nLlama-3.1-70b-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n25.68\n28.37\n26.57\n17.53\n20.37\n17.31\n13.75\n13.67\n13.08\n22.40\n17.42\n18.31\n17.37\n16.97\n20.16\n18.54\n16.28\n17.02\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n58.76\n58.96\n51.07\n65.43\n62.35\n60.84\n46.68\n55.59\n48.95\n29.24\n27.48\n38.92\n78.91\n79.27\n69.75\n64.64\n69.43\n52.82\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n96.51\n95.81\n94.48\n99.05\n98.57\n98.33\n99.05\n97.14\n97.14\n97.14\n97.14\n98.1\n100.00\n98.10\n97.90\n96.70\n97.62\n98.10\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n84.48\n85.74\n86.28\n92.58\n93.12\n91.62\n84.72\n85.86\n86.04\n84.24\n84.36\n85.44\n90.48\n92.34\n91.26\n85.32\n84.96\n86.16\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n60.21\n63.73\n58.89\n48.47\n53.06\n47.35\n38.33\n39.39\n37.28\n44.82\n38.38\n44.14\n49.78\n48.87\n53.86\n50.08\n46.41\n45.46\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\nN EFFECT OF NUMBER OF ITERATIONS\n\nWe next analyze the effect of varying the number of research loop iterations when using DRBA with\nGPT-5 as the backbone language model. Results for both the baseline configuration without explicit\nplanning and the complex planning setup are shown in Table 25. Overall, increasing the iteration budget\ndoes not guarantee consistent improvements. With no planning, performance initially drops when the\nagent executes more iterations, as additional exploration often introduces noise and distracts from key\ninsights. However, with a larger budget the agent partially recovers, suggesting that a small number of\nadditional iterations can help refine factual grounding, while excessive exploration reduces focus.\n\nFor the complex planning setting, higher iterations improve certain metrics such as factuality, but this\ncomes at the cost of lower insight recall and reduced overall balance. This indicates that while more steps\nallow the agent to verify citations more carefully, they can also lead to fragmented reasoning and overfitting\n\n34\n\n\fFigure 8: Total average Insight Recall scores per model and insight source type computed on all the\nresults available for each model running in Complex Research Plan mode. Insights embedded in enterprise\nsources are more easily retrieved by DRBA in all the models.\n\nto peripheral evidence. The best overall performance emerges at moderate iteration counts, highlighting\nthe importance of carefully tuning the iteration budget rather than simply scaling up the number of steps.\n\nO DATA GENERATION PROMPTS\n\nO.1 COMPANY AND PERSONA DATA GENERATION\n\nIn this section we give the prompts used for company generation 8 and persona generation 9.\n\nW Company Generation Prompt\n\nGenerate a realistic company structure for {company name} in the\n{industry} industry.\n\nCompany size: {size} ({employee range} employees)\n\nThe company should focus on this domain {domain}\n\nEXTERNAL INSIGHTS: {external insights}\n\nReturn ONLY a valid JSON object with this structure:\n{output structure}\n\nMake it realistic for the {industry} industry.\n\nPrompt 8: Company Generation Prompt Template.\n\n35\n\ngpt-5enterprise_fact0.597external_fact0.0deepseek-chat-v3.1enterprise_fact0.472external_fact0.0gpt-5-minienterprise_fact0.444external_fact0.0qwen-2.5-72b-instructenterprise_fact0.417external_fact0.0llama-3.1-405b-instructenterprise_fact0.347external_fact0.0gpt-oss-120benterprise_fact0.333external_fact0.0gpt-4o-minienterprise_fact0.194external_fact0.0llama-3.1-70b-instructenterprise_fact0.194external_fact0.0gpt-4oenterprise_fact0.182external_fact0.0\fTable 21: Mean and standard error of the insight recall metric for the first five tasks, obtained from three\nruns of on DRBench our agent (DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nW Persona Generation Prompt\n\nDR0001\n.222 ± .056\n.222 ± .056\n.278 ± .056\n.111 ± .056\n.222 ± .056\n.000 ± .000\n.111 ± .056\n.167 ± .000\n.111 ± .056\n.000 ± .000\n.056 ± .056\n.000 ± .000\n.111 ± .111\n.056 ± .056\n.000 ± .000\n.167 ± .000\n.222 ± .056\n.111 ± .056\n.167 ± .000\n.056 ± .056\n.167 ± .000\n.167 ± .000\n.278 ± .056\n.167 ± .000\n.222 ± .056\n.111 ± .056\n.167 ± .096\n\nDR0002\n.429 ± .000\n.381 ± .048\n.381 ± .126\n.286 ± .000\n.286 ± .082\n.381 ± .126\n.238 ± .048\n.333 ± .048\n.238 ± .095\n.095 ± .048\n.190 ± .048\n.190 ± .048\n.143 ± .000\n.143 ± .000\n.190 ± .048\n.143 ± .000\n.190 ± .048\n.238 ± .048\n.381 ± .048\n.286 ± .082\n.238 ± .048\n.286 ± .082\n.238 ± .048\n.095 ± .048\n.238 ± .048\n.333 ± .048\n.143 ± .082\n\nDR0003\n.467 ± .033\n.533 ± .033\n.567 ± .067\n.367 ± .033\n.333 ± .033\n.367 ± .067\n.167 ± .033\n.300 ± .000\n.300 ± .100\n.267 ± .033\n.167 ± .067\n.267 ± .033\n.433 ± .033\n.367 ± .033\n.333 ± .067\n.233 ± .088\n.167 ± .033\n.300 ± .058\n.200 ± .000\n.200 ± .058\n.267 ± .033\n.300 ± .058\n.333 ± .033\n.600 ± .058\n.400 ± .058\n.333 ± .088\n.400 ± .058\n\nDR0004\n.519 ± .098\n.370 ± .098\n.370 ± .037\n.222 ± .064\n.296 ± .074\n.370 ± .098\n.185 ± .074\n.148 ± .037\n.111 ± .000\n.185 ± .037\n.148 ± .074\n.074 ± .037\n.222 ± .000\n.148 ± .098\n.111 ± .064\n.185 ± .074\n.111 ± .064\n.148 ± .098\n.074 ± .037\n.185 ± .098\n.074 ± .074\n.259 ± .037\n.148 ± .074\n.370 ± .037\n.259 ± .037\n.259 ± .037\n.333 ± .000\n\nDR0005\n.281 ± .035\n.386 ± .046\n.386 ± .046\n.298 ± .018\n.281 ± .063\n.211 ± .053\n.175 ± .018\n.070 ± .018\n.105 ± .000\n.140 ± .035\n.123 ± .018\n.123 ± .018\n.211 ± .030\n.158 ± .061\n.281 ± .063\n.140 ± .035\n.158 ± .053\n.211 ± .030\n.105 ± .030\n.088 ± .046\n.105 ± .000\n.246 ± .046\n.281 ± .046\n.281 ± .035\n.158 ± .061\n.123 ± .018\n.298 ± .076\n\nGenerate {persona count} diverse employee personas for {company name} in\nthe {industry} industry.\n\nThe personas should focus on this domain: {domain}\n\nCreate diverse roles across seniority levels: Junior, Mid, Senior,\nExecutive\n\nReturn ONLY a valid JSON array with this exact format:\n{output structure}\n\nMake personas realistic with appropriate responsibilities for their\nroles.\n\nPrompt 9: Persona Generation Prompt.\n\nO.2 QUESTION GENERATION\n\nIn this section we give the prompt used to generate our deep research questions 10.\n\n36\n\n\fTable 22: Mean and standard error of the factuality metric for the first five tasks, obtained from our agent\n(DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nSimple\nGPT-4o-mini\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.761 ± .072\n.714 ± .050\n.730 ± .060\n.585 ± .045\n.647 ± .120\n.309 ± .126\n.792 ± .150\n.485 ± .262\n.475 ± .166\n.828 ± .043\n.611 ± .056\n.557 ± .030\n.144 ± .099\n.074 ± .074\n.368 ± .061\n.852 ± .087\n.802 ± .125\n.789 ± .053\n.618 ± .109\n.608 ± .173\n.588 ± .082\n.860 ± .014\n.696 ± .041\n.581 ± .042\n.674 ± .077\n.806 ± .049\n.626 ± .114\n\nDR0002\n.504 ± .075\n.384 ± .076\n.291 ± .094\n.297 ± .119\n.299 ± .056\n.381 ± .161\n.490 ± .110\n.512 ± .131\n.653 ± .097\n.265 ± .133\n.429 ± .092\n.324 ± .169\n.150 ± .035\n.128 ± .072\n.178 ± .078\n.726 ± .158\n.638 ± .202\n.392 ± .154\n.431 ± .160\n.681 ± .069\n.286 ± .108\n.518 ± .085\n.531 ± .086\n.657 ± .024\n.493 ± .109\n.540 ± .174\n.396 ± .056\n\nDR0003\n.866 ± .007\n.848 ± .034\n.782 ± .012\n.705 ± .039\n.624 ± .041\n.699 ± .047\n.827 ± .056\n.813 ± .041\n.704 ± .037\n.800 ± .000\n.622 ± .062\n.580 ± .075\n.386 ± .040\n.410 ± .090\n.564 ± .064\n.803 ± .028\n.800 ± .074\n.792 ± .055\n.684 ± .104\n.800 ± .069\n.686 ± .011\n.818 ± .041\n.922 ± .056\n.838 ± .050\n.866 ± .002\n.741 ± .074\n.723 ± .053\n\nDR0004\n.833 ± .019\n.812 ± .070\n.782 ± .064\n.704 ± .067\n.694 ± .028\n.692 ± .111\n.570 ± .058\n.693 ± .139\n.542 ± .110\n.690 ± .128\n.481 ± .209\n.642 ± .119\n.337 ± .117\n.311 ± .155\n.400 ± .076\n.820 ± .066\n.892 ± .035\n.771 ± .073\n.812 ± .021\n.826 ± .067\n.522 ± .270\n.679 ± .095\n.769 ± .035\n.774 ± .053\n.741 ± .060\n.724 ± .101\n.587 ± .139\n\nDR0005\n.762 ± .077\n.846 ± .029\n.674 ± .121\n.647 ± .098\n.683 ± .020\n.472 ± .114\n.593 ± .204\n.614 ± .121\n.406 ± .020\n.459 ± .235\n.191 ± .046\n.344 ± .144\n.445 ± .051\n.451 ± .080\n.435 ± .190\n.745 ± .022\n.832 ± .083\n.745 ± .100\n.687 ± .073\n.557 ± .143\n.559 ± .240\n.757 ± .057\n.754 ± .082\n.662 ± .098\n.696 ± .060\n.550 ± .148\n.586 ± .055\n\nO.3 PUBLIC SOURCE AND INSIGHT COLLECTION\n\nIn this section we give the prompt used to generate external insights 11. The URLs used for external\ninsight extraction and deep research question creation can be found in Table 26.\n\nO.4\n\nINTERNAL INSIGHT GENERATION\n\nIn this section we give the prompts to generate both internal insights 12 and internal distractors 13.\n\nO.5 FILE GENERATION\n\nIn this section we give the prompts used for generating each of the file types used in our tasks, which\nwe list as follows:\n\n• PDF: Prompts 18, 19, and 20\n\n• Excel: Prompts 21, 22, and 23\n\n• Powerpoint: Prompts 24, 25, and 26\n\n• Email: Prompts 27, 28, and 29\n\n• Chat: Prompts 30, 31, 29, and 33\n\n37\n\n\fTable 23: Mean and standard error of the distractor avoidance metric for the first five tasks, obtained from\nthree runs of on DRBench our agent (DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.857 ± .000\n.857 ± .000\n.905 ± .048\n.905 ± .048\n.857 ± .000\n.857 ± .000\n.952 ± .048\n.952 ± .048\n.952 ± .048\n.952 ± .048\n.857 ± .000\n.857 ± .000\n.857 ± .000\n.857 ± .000\n.905 ± .048\n1.00 ± .000\n.905 ± .048\n.952 ± .048\n.905 ± .048\n.905 ± .048\n.905 ± .048\n.905 ± .048\n.857 ± .000\n.905 ± .048\n1.00 ± .000\n.952 ± .048\n.952 ± .048\n\nDR0002\n.900 ± .058\n1.00 ± .000\n.900 ± .058\n.967 ± .033\n.933 ± .033\n.867 ± .133\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.967 ± .033\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.967 ± .033\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n\nDR0003\n1.00 ± .000\n1.00 ± .000\n.933 ± .000\n.978 ± .022\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.978 ± .022\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.978 ± .022\n\nDR0004\n1.00 ± .000\n1.00 ± .000\n.905 ± .024\n.976 ± .024\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.964 ± .036\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.952 ± .048\n.976 ± .024\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.929 ± .000\n.905 ± .024\n.952 ± .024\n.952 ± .048\n\nDR0005\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n\nP EVALUATION PROMPTS\n\nIn this section we give the prompts for decomposing reports into atomic insights 14, computing insight\nrecall 15, computing factuality 16, and computing report quality 17. These prompts are discussed in detail\nin Section 5.1.\n\nQ HUMAN PREFERENCE EVALUATION\n\nWe calculate a human score for model a task t as:\n\nSa,t = 1\nn\n\n(cid:80)n\n\ni=1sa,i, where sa,i =\n\n(cid:26)1,\n\nif human choice is a or ”both good”\n\n0, otherwise\n\n, where n is the number of gold insights in task t, sa,i is the human score of model a on insight i. Figure9\nshows that the insight recall metric is on par with human decision.\n\n38\n\n\fTable 24: Mean and standard error of the report quality metric for the first five tasks, obtained from three\nruns of DRBench with 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.936 ± .008\n.942 ± .000\n.948 ± .007\n.892 ± .002\n.901 ± .009\n.896 ± .001\n.927 ± .000\n.934 ± .008\n.929 ± .009\n.886 ± .004\n.893 ± .002\n.889 ± .003\n.872 ± .007\n.878 ± .006\n.874 ± .007\n.914 ± .000\n.921 ± .008\n.917 ± .009\n.889 ± .003\n.895 ± .002\n.891 ± .003\n.884 ± .004\n.890 ± .003\n.886 ± .004\n.901 ± .001\n.908 ± .000\n.904 ± .001\n\nDR0002\n.918 ± .004\n.927 ± .008\n.921 ± .001\n.884 ± .007\n.889 ± .002\n.882 ± .006\n.911 ± .003\n.919 ± .000\n.914 ± .002\n.874 ± .008\n.881 ± .005\n.877 ± .006\n.861 ± .001\n.867 ± .009\n.863 ± .000\n.903 ± .003\n.910 ± .001\n.906 ± .002\n.877 ± .007\n.883 ± .005\n.879 ± .006\n.872 ± .008\n.878 ± .006\n.874 ± .007\n.889 ± .005\n.896 ± .003\n.892 ± .004\n\nDR0003\n.924 ± .005\n.936 ± .006\n.940 ± .008\n.879 ± .004\n.887 ± .001\n.884 ± .003\n.903 ± .001\n.911 ± .009\n.905 ± .000\n.861 ± .007\n.867 ± .004\n.864 ± .005\n.849 ± .009\n.854 ± .008\n.851 ± .008\n.897 ± .001\n.904 ± .000\n.899 ± .001\n.869 ± .005\n.874 ± .004\n.871 ± .005\n.864 ± .006\n.870 ± .005\n.866 ± .006\n.881 ± .002\n.888 ± .001\n.884 ± .002\n\nDR0004\n.909 ± .001\n.915 ± .002\n.922 ± .009\n.891 ± .000\n.895 ± .008\n.889 ± .009\n.918 ± .009\n.923 ± .001\n.920 ± .008\n.872 ± .002\n.878 ± .001\n.875 ± .000\n.858 ± .006\n.863 ± .004\n.860 ± .005\n.909 ± .008\n.915 ± .009\n.911 ± .008\n.881 ± .001\n.886 ± .000\n.883 ± .001\n.876 ± .002\n.881 ± .001\n.878 ± .002\n.893 ± .009\n.899 ± .008\n.895 ± .009\n\nDR0005\n.927 ± .009\n.933 ± .007\n.929 ± .008\n.886 ± .005\n.892 ± .003\n.890 ± .001\n.909 ± .002\n.916 ± .000\n.913 ± .009\n.879 ± .006\n.884 ± .003\n.882 ± .004\n.866 ± .008\n.872 ± .007\n.869 ± .006\n.902 ± .002\n.908 ± .000\n.905 ± .001\n.873 ± .004\n.878 ± .003\n.875 ± .004\n.869 ± .005\n.874 ± .004\n.871 ± .005\n.885 ± .003\n.891 ± .002\n.888 ± .003\n\nTable 25: Effect of the number of research loop iterations on the performance of DRBA with GPT-5 as\nthe backbone model, on MinEval.\n\nPlanning Method\nNone\nNone\nNone\nComplex\nComplex\nComplex\n\n# Iterations\n15\n30\n50\n15\n30\n50\n\nInsight Recall\n39.45\n28.80\n37.10\n44.44\n31.61\n38.16\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n72.65\n69.03\n78.84\n62.51\n73.94\n66.05\n\n93.14\n98.57\n100.00\n90.95\n94.38\n94.38\n\n94.56\n96.12\n93.48\n93.12\n94.76\n92.64\n\n66.20\n57.34\n66.30\n66.41\n60.32\n63.76\n\n39\n\n\fW Deep Research Question Generation Prompt\n\nGenerate 3 Deep Research (DR) questions for the following business\ncontext:\n\nPersona: {persona name} - {persona role}\nDepartment: {persona department}\nResponsibilities: {persona responsibilities}\nCompany: {company name} ({company industry})\nDomain: {domain}\n\nExternal Insights: {external insights}\n\nGenerate 3 Deep Research questions that:\n1. Are appropriate for the persona’s role and department\n2. Require analysis of the provided internal insights\n3. Consider the external market context ...\n\nEach question should be 1 sentence of 15 words max, in\nplain english, and end with a question mark. Do not\ninclude any preamble or explanation - return only the JSON\narray.\n\nReturn ONLY a valid JSON array with this structure:\n{output structure}\n\nPrompt 10: Deep Research Question Generation Prompt Template. Subquestions are generated to help\nhuman annotators select good DR questions.\n\nW External Insight Extraction Prompt\n\nYou will be given a report (with url, and date). Based\non the report, generate 3 external insights that summarize\nimportant findings, trends, or takeaways from the\nreport.\n\nOutput Format\n{output structure}\n\nUrl: {url}\nIndustry: {industry}\nDomain: {domain}\nCompany Information: {company information}\n\nImportant notes\nFocus only on insights that are external and grounded in the report.\nInsights should be concise, factual, and directly tied to the retail\nindustry context.\n\nPrompt 11: External Insight Extraction Prompt.\n\n40\n\n\fTable 26: Public URLs For Deep Research Task Creation.\n\nIndustry\n\nDomain\n\nReference\nGrocers on FSMA-204 Compliance (GroceryDive) 2\nGrocery Loyalty & Inflation (EagleEye) 2\nGrocery Trends Outlook 2025 (GroceryDive) 2\nRetail IT Optimization (Thirdera) 2\nChatbots & Grocery Interactions (GroceryDoppio) 2\n\nTelehealth Regulations (HealthcareDive) 2\nFuture of Healthcare CRM (WTT Solutions) 2\nFuture of Telehealth (CHG Healthcare) 2\nHealthcare ITSM (Topdesk) 2\nPatient Engagement Tech (TechTarget) 2\n\nKnowledge Mgmt Retail Knowledge Management (Knowmax) 2\n\nSales\nCybersecurity\nPublic Relations Walmart CSR Strategy (SunriseGeek) 2\n\nPersonalization in Action (BCG) 2\nRetail Cybersecurity Threats (VikingCloud) 2\n\nCompliance\nCRM\nMarket Analysis\nITSM\nCSM\n\nCompliance\nCRM\nMarket Analysis\nITSM\nCSM\n\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles Quality Assurance EV Quality Issues (GreenCars) 2\nAutomobiles\nAutomobiles Market Analysis\n\nSales\nMarketing\nCybersecurity\nCompliance\nCRM\nCSM\nSales\nResearch\nCybersecurity\n\nAsset Mgmt\n\nSales for Digital Health (Medium) 2\nMarketing Telehealth Services (MarketingInsider) 2\nHealthcare Cybersecurity 2024 (AHA) 2\nEvolving EV Regulations (WardsAuto) 2\nSalesforce Automotive Cloud (TechTarget) 2\nEV Aftersales Support (EVReport) 2\nTesla vs Dealerships (TheWeek) 2\nAI for EV Optimization (Here.com) 2\nCybersecurity Risks in Cars (HelpNetSecurity) 2\n\nDigital Twins in Autos (RTInsights) 2\nGlobal EV Outlook 2024 (IEA) 2\n\nKnowledge Mgmt Knowledge Mgmt in Healthcare (C8Health) 2\n\nFigure 9: Comparison of Human Scores and Insight Recall Scores. As can be seen the human evaluation\nresults are aligned with our automated evaluation.\n\n41\n\nDR0001DR0002DR0003DR0004DR0006DR0007DR0008DR0011DR0012DR0013DR0014Task ID0.00.20.40.60.81.0Score ValueHuman Scores vs Insights Recallllama-3.1-405b (Human Score)gpt-5 (Human Score)llama-3.1-405b (Insights Recall)gpt-5 (Insights Recall)\fW Internal Supporting Insight Generation Prompt\n\nBased on the Deep Research (DR) Question, external\nmarket insights, company context, and previous internal\ninsights, generate 3 specific QA pair that an expert data\nscientist would need to get in order to address the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for inspiration): {external context}\nQA History: {qa list}\n\nInsight Requirements:\n- Use the DR Question as the central theme for the\nanswer.\n- Draw inspiration and supporting details from the other internal\ninsights and external insights provided.\n- Include QUANTITATIVE DATA in the answer: metrics, percentages, dollar\namounts, timeframes, KPIs.\n\nSpecific Question Instructions\n- the specific question should be a question that would be a step\ntowards resolving the DR Question\n{additional question instructions}\n\nAnswer Instructions\n- the answer should be 12 words max and minimum 5\nwords\n{additional answer instructions}\n\nJustification Instructions\n- the justification should directly explain how the specific question,\nanswer pair help address the DR Question in 15 words\nmax\n\nMisc Instructions\n- the filename should be 3 words max with dashes\nin between, do not mention the file type in the\nfilename\n- use the example below as inspiration but do not use it\ndirectly\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nPrompt 12: Internal Supporting Insight Generation Prompt Template. specific questions and justification\nhelp human annotators to select good insights.\n\n42\n\n\fW Internal Distractor Insight Generation Prompt\n\nBased on the Deep Research (DR) Question, external\nmarket insights, company context, and previous internal\ninsights, generate 3 specific QA pairs that are DISTRACTOR\nquestions\n- these should be questions that an expert data scientist might\nask about the company but are NOT relevant to addressing the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for inspiration): {external context}\nQA History: {qa list}\n\nDISTRACTOR Requirements:\n- Generate questions that are plausible for this\ncompany and industry but DO NOT help address the DR\nQuestion.\n- The questions should be about the company’s operations, metrics, or\nbusiness but tangential to the DR Question.\n- Include QUANTITATIVE DATA in the answer: metrics, percentages, dollar\namounts, timeframes, KPIs.\n- Focus on different business areas that are NOT central to the\nDR Question (e.g. if DR Question is about pricing, ask about HR\nmetrics).\nSpecific Question Instructions\n- the specific question should be a plausible business\nquestion for this company but NOT related to the DR\nQuestion\n- the specific question should lead to a quantitative answer and should\nbe dated such as Q3 of 2025, the question should contain a date like Q2\nof 2024\n- the specific question should be 10 words max\n- make sure to be different from any question in the\nqa list\n- choose business areas like: HR metrics, facility costs, IT\ninfrastructure, compliance, training, etc. that are UNRELATED to the\nDR Question\n- make sure to be different from any question in the QA\nHistory\n\nAnswer Instructions {answer instructions}\n\nJustification Instructions\n- the justification should explain why this specific question\nis NOT relevant to addressing the DR Question in 15 words\nmax\n\nMisc Instructions {misc instructions}\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nPrompt 13: Internal Distractor Insight Generation Prompt. specific questions and justification help human\nannotators to select good insights.\n\n43\n\n\fW Breaking Report into Insights Prompt\n\nPlease break down the following report text into insight claims. Each\ninsight claim should be:\n\n1. A single insight, that might include multiple statements and\nclaims\n2. Independent and self-contained\n3. Each claim can have more than one sentence, but should be focused\non a single insight\n4. Support each insight with citations from the report text following\nthese specific rules: {rules}\n5. Citations should be in one of these formats (various formats will\nbe automatically normalized):\n{citation formats} 6. Do not include general summaries, opinions,\nor claims that lack citation, just the sentences that are\nfacts.\n7. Each claim should be a concise but complete\nsentence.\n\nReport text: {report text}\n\nOutput format: Please return the insight claims as a JSON array. For\nexample: {output structure}\n\nReturn only valid JSON, no additional text. Just use the report\nbetween <START OF REPORT> and <END OF REPORT> tags to generate\ninsights. If no insights found, return an empty JSON array:\n[]\n\nDo not use the example outputs as report content.\n\nPrompt 14: Insight Extraction Prompt.\n\n44\n\n\fW Insight Recall Prompt\n\nYour goal is to check if one of the Predicted Insights\nextracted from a report is a Golden Insight. You\nmust be STRICT and pay attention to every small\ndetail.\n\nInstructions:\n* Evaluate if the Predicted Insights contain sufficient information to\nderive a Golden Insight.\n* Select the insight that most closely matches the Golden Insight.\nSelect one and only one insight.\n* Answer of yes or no where:\n- yes: Selected insight contains comprehensive information to fully\nderive the expected insight\n- no: Selected insight lacks the necessary information, misses key\ndetails, or has significant gaps\n* Be STRICT - do not answer yes for partial matches, vague similarities,\nor general information.\n\nHowever, no exact wording is required and paraphrasing is\nacceptable.\n* IMPORTANT: Only consider details given in the Golden Insight when\nanswering yes or no. Don’t expect anything more than what is given in\nthe Golden Insight.\n* Focus on factual accuracy, completeness, and specificity.\n\nPredicted Insights: {claims text}\nGolden Insight: {gold insight}\n\nReturn a valid json dictionary with the following structure:\n{output structure}\n\nEnsure only a json dictionary is returned, and return nothing else.\n\nPrompt 15: Insight Recall Scoring Prompt.\n\n45\n\n\fW Factuality Prompt\n\nGiven the following relevant source context from multiple sources and\nan insight, determine if the insight is factually supported by the\nsources.\n\nRelevant Source Materials (from multiple sources):\n{context}\nAtomic Claim: {insight}\n\nEVALUATION CRITERIA:\nThe claim is factual if the core factual content\nis supported by the sources. You should be strict\nabout important details but flexible about exact\nwording:\nREQUIRED for TRUE:\n1. All key factual details (numbers, dates, names,\npercentages, specific facts) must be present in at least one\nsource\n2. The main substance and meaning of the claim must be supported by\nthe source contexts\n3. No part of the claim should contradict the information in any of\nthe sources\nACCEPTABLE variations:\n{acceptable variations}\nMark as FALSE if:\n- Important factual details are missing, incorrect, or unsupported\nacross all sources\n- The claim contradicts information in any of the\nsources\n- The core meaning cannot be verified from any of the source\ncontexts\n\nEXAMPLES: {examples}\n\nFocus on the substantive factual accuracy rather than\nexact word-for-word matching. You MUST respond with either\ntrue or false under the <factual> tag. Then provide a\nbrief explanation under the <explanation> tag explaining\nwhich parts are supported or not supported and from which\nsources.\n\nFormat your response EXACTLY as:\n{output structure}\n\nPrompt 16: Factuality Scoring Prompt.\n\n46\n\n\fW Report Quality Prompt\n\nYou are a Deep Research Evaluator. You are given:\n1. A research report.\n2. A deep research (DR) question that the report attempts to\nanswer.\n3. A persona that represents the intended audience for the\nreport.\n\n{persona}\n{dr question}\n{report text}\n\n## Instructions:\n**ANALYZE THOROUGHLY**: Examine the report in detail\nand identify any issues, even small ones. Look for\nsubtle problems, minor inconsistencies, areas that could\nbe improved, or any shortcomings that might affect\nthe quality. Evaluate the report according to the\nfive criteria listed below. For **each criterion**,\nprovide:\n- A **score between 1 and 10** (must be an integer) using the scale\ndefined below.\n- A **detailed justification** (2–3 sentences) in\n**simple plain English** explaining why you gave that\nscore, including any specific issues or strengths you\nidentified.\n\n### Scoring Scale (1-10, integers only): {scoring scale}\n\n### Criteria:\n\n1. Depth & Quality of Analysis\n2. Relevance To DR Question\n3. Persona Consistency\n4. Coherence & Conciseness\n5. Degree of Contradictions\n6. Completeness & Coverage\n\n{output structure}\n\nPrompt 17: Report Quality Scoring Prompt.\n\n47\n\n\fW PDF Outline Generation Prompt\n\nYou are an expert business document designer creating\nrealistic enterprise PDF reports. Given a Deep\nResearch (DR) Question and company context, generate\nan outline for a professional business document that\nan employee would create based on their persona and\nrole.\n\nCompany: {company name} - {company description}\nIndustry: industry\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context:{persona context}\nDR Question: {dr question}\n\nDocument Structure Requirements:\n- Create a professional PDF document outline with exactly {n subsections}\nsubsections\n- The document should be something this persona would realistically\ncreate in their role\n- Include a concise, professional file title appropriate for enterprise\ndocumentation\n\nSubsection Heading Requirements: {subsection requirements}\n\nIntroduction Requirements:\n- Write a professional 4-sentence maximum introduction\nparagraph - Should set context for the document and its\npurpose\n- Must align with the persona’s role and the company’s business\nneeds\n- Should sound like something this employee would write for internal\nstakeholders\n\nConclusion Requirements:\n- Write a professional 4-sentence maximum conclusion\nparagraph\n- Should summarize key takeaways and next steps\n- Must align with the persona’s perspective and recommendations\n- Should provide actionable insights for the intended\naudience\n\nReturn ONLY a valid Python dictionary with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Ensure the document feels authentic for this persona´s role and company\ncontext\n\nPrompt 18: PDF Outline Generation Prompt. This is the first step of embedding an insight into a PDF\ndocument. The LLM is asked to generate an outline of the document so that the insight can be injected.\n\n48\n\n\fW PDF Insight Injection Prompt\n\nYou are an expert business document writer creating\nrealistic enterprise PDF content. Given a Deep\nResearch (DR) Question, company context, and a specific\ninsight, generate professional content that naturally\nincorporates the insight information to help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nSubsection Heading: {subsection heading}\n\nContent Generation Requirements:\n- Generate realistic business content for the given subsection\nheading\n- The content must contain exactly ONE paragraph of 4-5\nsentences\n- Content should be professional and sound like something this persona\nwould write\n- The paragraph must naturally incorporate the insight answer\ninformation but NOT copy it word-for-word\n{additional generation requirements}\n\nContent Strategy:\n- Present the insight information as business findings, analysis results,\nor operational data\n- Embed the key metrics within broader business context and\nimplications\n- Use natural business language to discuss the same information as in\nthe answer\n{additional content requirements}\n\nJustification Requirements:\n- Explain specifically how this content helps answer the DR\nQuestion\n- Reference the key information that would be useful for\ndecision-making\n- Keep justifications concise but clear (20 words\nmaximum)\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 19: PDF Insight Injection Prompt. This is the second step of embedding an insight into a PDF\ndocument. The LLM is fed with a subheading in the outline from 18, and tasked to write the subsection\nwith the insight embedded.\n\n49\n\n\fW PDF Irrelevant Section Generation Prompt\n\nYou are an expert business document writer creating\nrealistic enterprise PDF content. Given a Deep Research\n(DR) Question, company context, and subsection headings,\ngenerate distractor content for each subsection that is\nthematically related but does NOT help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: industry\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\nSubsection Headings: {subsection headings}\n\nContent Generation Requirements:\n- Generate realistic business content for each subsection\nheading\n- Each subsection must contain exactly ONE paragraph of 3-4 sentences\nmaximum\n- Content should be professional and sound like something this persona\nwould write\n- Content must be thematically related to the DR\nQuestion’s domain but NOT provide information to answer\nit\n{additional generation requirements}\n\nDistractor Strategy:\n- Focus on adjacent business areas that don’t directly impact the DR\nQuestion\n- Discuss historical context, general industry trends, or procedural\ninformation\n- Include operational details that are realistic but tangential\n- Reference related but non-essential business metrics or\nactivities\n- Avoid any content that would help someone answer the DR\nQuestion\n\nJustification Requirements:\n- Explain specifically why each paragraph’s content doesn’t help answer\nthe DR Question\n- Identify what type of distractor strategy was used\n(e.g., \"focuses on historical data vs current decision\nfactors\")\n- Keep justifications concise but clear (15 words\nmaximum)\n\nReturn ONLY a valid JSON array with this exact structure:\n{output structure}\n\nIMPORTANT: {important instructions}\n\nPrompt 20: PDF Irrelevant Section Generation Prompt. This is the third step of PDF document generation.\nThe LLM is asked to fill out the outline from 18 with irrelevant information.\n\n50\n\n\fW Excel Schema Generation Prompt\n\nGenerate the JSON schema and formatting of a table where the\nfollowing insight can be presented as a row in the table:\n{insight}\n\nThe schema and formatting should contain:\n- table name: a string of the table name\n- columns: a list of columns with the following\nfields:\n- name: column name\n- column type: one of\nSTRING|INTEGER|FLOAT|BOOLEAN|DATE|PERCENTAGE|CURRENCY\n- description: detailed description of what this column represents and\nhow it relates to the insight\n- formatting: a dictionary with the following fields:\n- header style: background color of the header, default is\nCCCCCC\n- column widths: width of each column, e.g. {{A: 15, B: 20, C:\n12}}\n- number formats: format of each column, e.g. {{A: \"0.00%\", B:\n\"$#,##0.00\", C: \"YYYY-MM-DD\"}}\nReturn only a json dictionary with the table name, columns, and\nformatting.\n\nRequirements:\n- The schema should be designed so that the insight can be represented\nas a row in the table\n- The schema should make it easy to generate more data points expanding\non the subject, theme and scope of the insight to populate the\ntable.\n- Use realistic column names that would be found in a business\nspreadsheet\n- Do not include the insight, specific question, or justification as\ncolumns in the table\n\nCompany Context: {company info}\nPlease keep this company context in mind when creating the\nschema.\n\nPersona Context: {persona}\n\nPlease keep this persona context in mind when creating the schema.\n\nPrompt 21: Excel Schema Generation Prompt. This is the first step of Excel file generation. The LLM\nis asked to generate the schema of the Excel file so that the insight can be injected.\n\n51\n\n\fW Excel Data Generation Prompt\n\nGiven the following schema: {schema and formatting}\nGenerate one row that embeds the following insight:\n{insight}\n\nThen generate 5-10 rows of data that populates the table.\nMake sure that with the new data added, the original\ninsight can still be extracted from the table. Return\nall the rows in a json dictionary with the following\nfields:\n- insight row: a list of values each corresponding to a column in the\nschema\n- irrelevant rows: a list of rows that are used to populate the table,\neach row is a list of values each corresponding to a column in the\nschema\n\nEnsure only a json dictionary is returned, and return nothing\nelse.\n\nRequirements:\n- Make sure the insight row stands out from the irrelevant rows by\ne.g.\n- Having the largest value\n- Covering the most recent timeframe\n\nPrompt 22: Excel Data Generation Prompt. This is the second step of Excel file generation. The LLM\nis asked to generate the data for an Excel file that the insight will be injected into.\n\nW Excel Filename Generation Prompt\n\nGenerate a professional filename for an Excel file that contains the\nfollowing sheets: {sheet names}\n\nThe filename should:\n1. Be descriptive and professional\n2. Reflect the main theme or purpose of the data\n3. Be suitable for a business environment\n4. Not exceed 50 characters\n5. Use only alphanumeric characters, spaces, hyphens, and\nunderscores\n6. Not include file extensions (like .xlsx)\nReturn only the filename, no additional text or\nquotes.\n\nCompany name:\n{company name}\n\nPlease keep this company name in mind when creating the filename.\n\nPrompt 23: Excel Filename Generation Prompt. This is the third step of Excel file generation. The LLM\nis asked to generate the filename for the Excel file that the insight will be injected into.\n\n52\n\n\fW Powerpoint Outline Generation Prompt\n\nYou are an expert business presentation designer creating\nrealistic enterprise PowerPoint presentations. Given a\nDeep Research (DR) Question and company context, generate\nan outline for a professional business presentation that\nan employee would create based on their persona and\nrole.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\n\nPresentation Structure Requirements:\n- Create a professional PowerPoint presentation outline with exactly\n{n subsections} slides\n- The presentation should be something this persona would realistically\ncreate in their role\n- Include a concise, professional presentation title appropriate for\nenterprise presentations\n\nSlide Heading Requirements:\n- Slide headings must follow the THEME of the DR\nQuestion but should NOT directly address the DR Question\nitself\n- Think of related business areas, adjacent topics, or\nsupporting themes that would naturally appear in an enterprise\npresentation\n- Headings should sound professional and realistic for this industry and\ncompany size\n- Each heading should be 3-8 words and use proper business\nterminology\n{additional slide requirements}\n\nConclusion Requirements:\n- Write a professional 2-sentence maximum conclusion for the\npresentation closing\n- Should summarize key takeaways and next steps\n- Must align with the persona’s perspective and recommendations\n- Should provide actionable insights for the intended\naudience\n\nReturn ONLY a valid Python dictionary with this exact\nstructure:\n{output structure}\n\nIMPORTANT: {important notes}\n\nPrompt 24: Powerpoint Outline Generation Prompt. This is the first step for generating powerpoint slides.\nThe LLM is asked to generate an outline of the slides so that the insight can be injected.\n\n53\n\n\fW Powerpoint Insight Injection Prompt\n\nYou are an expert business presentation writer creating\nrealistic enterprise PowerPoint content. Given a Deep\nResearch (DR) Question, company context, and a specific\ninsight, generate professional slide content that naturally\nincorporates the insight information to help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nSlide Heading: {subsection heading}\n\nContent Generation Requirements:\n- Generate realistic business content for the given slide\nheading\n- The content must contain exactly 5-8 bullet points with substantial\ndetail\n- Each bullet point should be 1-2 sentences with specific business\ninformation\n{additional generation requirements}\n\nContent Strategy:\n- Present the insight information as business findings, analysis results,\nor operational data\n- Embed the key metrics within broader business context and\nimplications\n- Use natural business language to discuss the same information as in\nthe answer\n{additional content requirements}\n\nJustification Requirements:\n- Explain specifically how this content helps answer the DR\nQuestion\n- Reference the key information that would be useful for\ndecision-making\n- Keep justifications concise but clear (25 words\nmaximum)\n\nReturn ONLY a valid JSON object with this exact\nstructure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 25: Powerpoint Insight Injection Prompt. This is the second step for generating powerpoint slides.\nThe LLM is asked to generate slide content with the insight embedded.\n\n54\n\n\fW Powerpoint Distractor Injection Prompt\n\nYou are an expert business presentation writer creating\nrealistic enterprise PowerPoint content. Given a Deep\nResearch (DR) Question, company context, and slide\nheadings, generate distractor content for each slide that\nis thematically related but does NOT help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\nSlide Headings: {subsection headings}\n\nContent Generation Requirements:\n- Generate realistic business content for each slide heading - Each\nslide must contain exactly 5-8 bullet points with substantial\ndetail\n- Each bullet point should be 1-2 sentences with specific business\ninformation\n- Content should be professional and sound like something this persona\nwould present\n{additional generation requirements}\n\nDistractor Strategy:\n- Focus on adjacent business areas that don’t directly impact the DR\nQuestion\n- Discuss historical context, general industry trends, or procedural\ninformation\n- Include operational details that are realistic but tangential\n- Reference related but non-essential business metrics or\nactivities\n{additional content requirements}\n\nReturn ONLY a valid JSON array with this exact structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 26: Powerpoint Distractor Injection Prompt. This is the third step for generating powerpoint slides.\nThe LLM is asked to generate slide content with distractor information.\n\n55\n\n\fW Email Setup Prompt\n\nYou are an expert in enterprise communication systems and organizational\nstructures. Your task is to generate a realistic setup of users\nfor an email system based on the given insights and company\ncontext.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Size: {company size} ({employee count} employees)\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\n**Specific Question** {specific question}\n**Answer to Specific Question** {answer}\n\nRequirements:\n- Users that would realistically discuss these insights\n- Generate a minimal but sufficient setup to support {num messages}\nemails discussing the insights\n- To make it realistic, generate at least 3 users\n\n- Use realistic names for people/teams/channels based on the company\ncontext\n\nReturn ONLY a JSON array of users with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Do NOT include any preamble, explanation, or extra text|return only\nthe Python dictionary\n- Ensure the structure is realistic for the company size and industry\n- Make sure the persona is included as a user\n\nPrompt 27: Email Setup Prompt. This is the first step for generating an email chain. The LLM is asked\nto generate the necessary setup for the email chain.\n\n56\n\n\fW Email Insight Injection Prompt\n\nYou are an expert at creating realistic business email\nconversations. Your task is to create an email thread that\ncontains the actual insight that helps answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\nDeep Research Question: {dr question}\nEmail Setup: {email setup}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create a realistic email thread of {num messages} that contains the\ntarget insight\n2. This thread should provide information that directly helps answer\nthe DR question\n3. The insight should be naturally embedded in the email\ncontent\n4. The emails should feel realistic and business-appropriate\n5. The sender should be someone who would naturally have access to\nthis insight\n6. The persona needs to be either a recipient or the sender of any\nemail\n\nContent Strategy:\n- The thread should discuss the specific question\nand provide the answer as part of a natural business\nconversation\n- Include the justification as supporting context or\nreasoning\n- Make the insight feel like a natural part of the email, not\nforced\n- The content should be directly relevant to answering the DR\nquestion\n- Use realistic business language and formatting\nExample approaches: {example approaches}\nOutput Format: Return ONLY a JSON array with the following structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 28: Email Insight Injection Prompt. This is the second step for generating an email chain. The\nLLM is asked to insert an insight into the email chain.\n\n57\n\n\fW Email Distractor Injection Prompt\n\nYou are an expert at creating realistic business email conversations.\nYour task is to create {num messages} emails that discuss topics\nrelated to the company but will NOT help answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\nDeep Research Question: {dr question}\nEmail Setup: {email setup}\n\nRequirements:\n1. Create {num messages} realistic email messages between the\nusers\n2. These emails should discuss business topics\nthat are thematically related to the company but\nDO NOT provide information that helps answer the DR\nquestion\n3. Each email should have a realistic subject line, sender, recipients,\nand content\n4. The conversations should feel natural and business-appropriate\n5. Topics should be relevant to the company’s operations but unhelpful\nfor the DR question\n\nContent Strategy:\n- Focus on daily business operations, team collaboration, projects, and\ncompany processes\n- Include realistic business language, project updates, and operational\ndiscussions\n- Avoid topics that directly relate to the DR question or would provide\ninsights for it\n- Make the content engaging and realistic while being intentionally\nunhelpful\n\nExample topics to discuss (but should NOT help answer the DR\nquestion):\n{example topics}\n\nOutput Format: Return ONLY a JSON array with the following structure:\n{output structure}\n\nIMPORTANT: - Return ONLY the JSON array, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n- Make sure the persona is either a recipient or the sender of any email\n\nPrompt 29: Email Distractor Injection Prompt. This is the third step for generating an email chain. The\nLLM is asked to insert distractor information into the email chain.\n\n58\n\n\fW Chat Setup Prompt\n\nYou are an expert in enterprise communication systems\nand organizational structures. Your task is to generate\na realistic setup for teams, channels, and users for a\nMattermost chat system based on the given insights and company\ncontext.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Size: {company size} ({employee count} employees)\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\n**Specific Question** {specific question}\n**Answer to Specific Question** {answer}\n\nRequirements:\n- Generate teams, channels, and users that would realistically discuss\nthese insights\n- Make sure the teams and channels are realistic for persona to be a\nmember\n- Each channel must be associated with a team\n- Each user must be a member of at least one team and one\nchannel\n- Generate a minimal but sufficient setup to support {num turns} chat\nmessages discussing the insights\n- To make it realistic, generate at least 2 teams, 2 channels and 3\nusers\n- Use realistic names for people/teams/channels based on the company\ncontext\n- The persona needs to be part of all teams and channels\n\nReturn ONLY a valid Python dictionary with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Do NOT include any preamble, explanation, or extra text|return only\nthe Python dictionary\n- Make sure the persona is included as a user and member of all\nteams/channels\n- Reuse the username of the persona as provided in the persona context\n\nPrompt 30: Chat Setup Prompt. This is the first step for generating a Mattermost chat. The LLM is asked\nto generate the necessary setup for the chat system.\n\n59\n\n\fW Chat Insight Injection Prompt\n\nYou are an expert at creating realistic business chat\nconversations. Your task is to create a chat conversation\nthat contains an insight that helps answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDR Question: {dr question}\nChat Setup: {chat setup}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create a realistic chat conversation (could be multiple messages)\nthat contains the target insight\n2. This conversation should provide information that directly helps\nanswer the DR question\n3. The insight should be naturally embedded in the message\ncontent\n4. The conversation should feel realistic and business-appropriate\n5. The sender should be someone who would naturally have access to\nthis insight\n6. Use the teams, channels, and users from the chat setup\nonly\n\nContent Strategy:\n- The conversation should discuss the specific question\nand provide the answer as part of a natural business\nconversation\n- Include the justification as supporting context or\nreasoning\n{additional content requirements}\n\nExample approaches: {example approaches}\n\nOutput Format: Return ONLY a JSON array of the chat messages with the\nfollowing structure: {output structure}\n\nIMPORTANT:\n- Return ONLY the JSON object, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n\nPrompt 31: Chat Insight Injection Prompt. This is the second step for generating a Mattermost chat. The\nLLM is asked to insert an insight into the chat system.\n\n60\n\n\fW Chat Distractor Injection Prompt\n\nYou are an expert at creating realistic business chat\nconversations. Your task is to create {num turns}\nchat messages that discuss topics related to the\ncompany but will NOT help answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDeep Research Question: {dr question}\nChat Setup: {chat setup}\n\nRequirements:\n1. Create {num turns} realistic chat messages between the\nusers\n2. These messages should discuss business topics\nthat are thematically related to the company but\nDO NOT provide information that helps answer the DR\nquestion\n3. Each message should have a realistic sender, channel, and\ncontent\n4. The conversations should feel natural and business-appropriate\n5. Topics should be relevant to the company’s operations but unhelpful\nfor the DR question\n6. Use the teams, channels, and users from the chat setup\nonly\n\nContent Strategy:\n- Focus on daily business operations, team collaboration, projects, and\ncompany processes\n- Include realistic business language, project updates, and operational\ndiscussions\n{additional content requirements}\n\nExample topics to discuss (but should NOT help answer the DR\nquestion):\n{example topics}\n\nOutput Format:\nReturn ONLY a JSON array of the chat messages with the following\nstructure: {output structure}\n\nIMPORTANT:\n- Return ONLY the JSON array, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n\nPrompt 32: Chat Distractor Injection Prompt. This is the third step for generating a Mattermost chat. The\nLLM is asked to generate distractor information to insert into the chat system.\n\n61\n\n\fW Chat Confidential Info Prompt\n\nYou are an expert at inserting confidential information\ninto chat conversations. Your task is to create business\nconfidential information that is irrelevant to the Deep\nResearch question or the specific question and insert\nit into existing chat conversations in a realistic\nway.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDR Question: {dr question}\nChat Setup: {chat setup}\nChat Conversation: {chat messages}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create business confidential information that are secrets\nwithin the company and should not be revealed to the outside\nworld\n2. Create 1-2 new chat messages that contain the confidential\ninformation\n3. Insert the new messages into the chat conversation and return the\nnew chat conversation\n4. Also return the confidential information and the\nkeywords\n\nConfidential Information Content Strategy:\n- The messages containing the confidential information should not be\nnecessary to address the DR question or the specific question of the\ninsight\n- The confidential information should be relevant to the company context\nand something that the people in chat setup naturally have access\nto\n- The confidential information should be specific so that leakage\nof it will result in a significant disadvantage or loss to the\ncompany\n\nExample confidential information:\n{example confidential info}\nOutput Format: Return a JSON object of the new chat messages and the\nconfidential information with the following structure:\n{output structure}\n\nPrompt 33: Chat Confidential Info Prompt. This prompt also generates distractor information to insert\ninto a chat system like prompt 32. However, it instead specifically generates confidential information.\n\n62\n\n\f\n</file>\n\n\n### long_context.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context.json\">\n[Long Context Document Collection]\nTotal documents: 9478\nSample titles: ['深度研究代理的终极考验：DeepResearch Bench 横空出世-CSDN博客', 'DeepResearch Bench: A Comprehensive Benchmark for ', '14篇DeepResearch Agent最新研究总结!']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_32k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_32k.json\">\n[Long Context Document Collection]\nTotal documents: 17\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_64k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json\">\n[Long Context Document Collection]\nTotal documents: 33\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_60.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_60.json\">\n[Long Context Document Collection]\nTotal documents: 60\nSample titles: ['LLM检索增强生成原理与架构详解-今日头条', '通义DeepResearch全面开源！同步分享可落地的高阶Agent构建方法论-知乎', '跨领域知识整合能力评估：LLM设计的综合测试-CSDN博客']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_128k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_128k.json\">\n[Long Context Document Collection]\nTotal documents: 67\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n## Image Files\n\n\nThe following image files are available for analysis:\n\n\n### image0.png\n\nImage file: image0.png\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/image0.png\nDimensions: 1420x187 pixels\nFormat: PNG\n\n\n**IMPORTANT**: Use the 'vision_understanding_advanced' tool to analyze these images.\nThis tool provides multi-turn verification, confidence scoring, and cross-validation.\nRecommended approach:\n1. Call vision_understanding_advanced with a specific question about each image\n2. Review the confidence score and metadata\n3. If confidence < 0.75, use follow-up analysis or web search for verification\n\n\n## Long Context Documents (RAG)\n\n\nThe following long context document files are available for semantic search:\n\n\n### long_context_sampled_64k.json.chunks.db ⭐ **RECOMMENDED - USE THIS DATABASE**\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db\n\nSize: 1440.0 KB\n\n**This is a pre-built embedding database. Use this path directly with RAG tools.**\n\n**⚠️ IMPORTANT: Always use this file path when calling RAG tools: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db**\n\n\n### long_context.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context.json\n\nSize: 76367.0 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_32k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_32k.json\n\nSize: 119.7 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_64k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json\n\nSize: 235.1 KB\n\n**Pre-built embedding database**: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db (1440.0 KB)\n\n\n### long_context_sampled_60.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_60.json\n\nSize: 504.5 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_128k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_128k.json\n\nSize: 477.4 KB\n\n*Note: Use the recommended database file instead.*\n\n\n**IMPORTANT**: Use RAG tools to search these long context documents:\n- `rag_search`: Semantic search to find relevant passages (use 1-3 times with different keywords)\n- `rag_get_context`: Get concatenated context for answering questions\n- `rag_document_stats`: Get document collection statistics\n\nDo NOT attempt to read these files directly - they are too large. Use RAG tools instead.\n\n**⚠️ CRITICAL: When calling RAG tools, use this json_path: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db**\nThis database has pre-built embeddings and will load instantly without regenerating embeddings.\n\n\nYou should follow the format instruction in the request strictly and wrap the final answer in \\boxed{}.\"\n\n**CRITICAL - CITATION REQUIREMENTS**:\nYour final answer MUST include inline citations for ALL facts and claims. Follow these rules:\n1. Place citations IMMEDIATELY AFTER each fact or sentence they support\n2. Use these citation formats:\n   - For RAG/long_context sources: Use the EXACT citation format from RAG results: [long_context: \"Document Title\", chunk N]\n   - For images: [Image: filename] or [图片: filename]\n   - For documents: [Doc: filename] or [文档: filename]\n   - For web sources: [Web: URL] or [网页: URL]\n3. DO NOT group citations at the end of paragraphs\n4. Every claim must be traceable to its source\n5. For RAG sources, you MUST include the document title - DO NOT use simplified [RAG-1] format\n\nExample of correct citation:\nThe benchmark includes 15 tasks [long_context: \"Benchmark Overview\", chunk 2]. It covers reasoning and retrieval [long_context: \"Task Categories\", chunk 5].\n\nExample of INCORRECT citation (DO NOT DO THIS):\nThe benchmark includes 15 tasks [RAG-1]. It covers reasoning and retrieval [RAG-2].\n\nWrap your final answer in \\boxed{}.\n"
      },
      {
        "role": "assistant",
        "content": "I need to extract more specific information about the benchmarks mentioned in the related work section of the paper. Let me search for that information directly in the paper.pdf:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>download_file_from_internet_to_sandbox</tool_name>\n<arguments>\n{\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\",\n\"url\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\",\n\"sandbox_file_path\": \"/home/user\"\n}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "Please validate if the following report meets all requirements of the original query.\n\n**IMPORTANT: You MUST use the Python code execution tool to accurately count words!**\n\n**Step 1: Use Python Tool to Count Words**\n\nPlease call the Python code execution tool with the following code:\n\n```python\nimport re\n\ndef count_words(text):\n    chinese_chars = len(re.findall(r'[\\u4e00-\\u9fff]', text))\n    text_without_chinese = re.sub(r'[\\u4e00-\\u9fff]', ' ', text)\n    english_words = len([w for w in text_without_chinese.split() if re.search(r'[a-zA-Z0-9]', w)])\n    return chinese_chars + english_words\n\nreport = \"\"\"\nI need to extract more specific information about the benchmarks mentioned in the related work section of the paper. Let me search for that information directly in the paper.pdf:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>download_file_from_internet_to_sandbox</tool_name>\n<arguments>\n{\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\",\n\"url\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\",\n\"sandbox_file_path\": \"/home/user\"\n}\n</arguments>\n</use_mcp_tool>\n\"\"\"\n\nword_count = count_words(report)\nprint(f\"Word count: {word_count}\")\nprint(f\"Requirement: Required: 3-8 words\")\n```\n\n**Word Count Requirement**: Required: 3-8 words\n\n**Original Query**:\n# Task\n\n假设你是一名科研工作者，正在为某人工智能研究团队开展大模型评测基准的系统性梳理工作。请根据提供的材料，撰写一份《Deep Research相关Benchmark数据集调研报告》。\n具体而言，依据图片image0中所示的字段要求，提取并结构化提供的pdf论文在相关工作部分提到的每篇论文对应的Benchmark特征，其中请忽略“数据集example”指标。\n报告需整合文献内容与公开可查的权威技术资料（如论文原文、官方GitHub仓库、学术数据库或评测平台），\n确保所有信息真实、准确、可验证，并在文中注明数据来源。\n报告语言应采用正式书面语，结构清晰，包含引言、调研方法、各论文Benchmark信息汇总表、分析与观察、参考资料等部分，\n最终形成一份可用于团队技术决策的标准化调研文档，字数在1500至2000之间。\n\n\n## Folder Contents Summary\n\nFolder: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001\nTotal files: 12\n  - data: 5\n  - document: 1\n  - image: 1\n  - other: 5\n\n\n## Document Contents\n\n\n### paper.pdf\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\">\n5\n2\n0\n2\n\np\ne\nS\n0\n3\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n2\n7\n1\n0\n0\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nDRBench: A REALISTIC\n\nBENCHMARK FOR ENTERPRISE DEEP RESEARCH\n\nAmirhossein Abaskohi1,2 Tianyi Chen1 Miguel Mu˜noz-M´armol1 Curtis Fox1,2\nAmrutha Varshini Ramesh1,2 ´Etienne Marcotte1 Xing Han L`u3,4 Nicolas Chapados1\n\nSpandana Gella1,3 Christopher Pal1,3,5 Alexandre Drouin1,3\n\nIssam H. Laradji1,2\n\n1ServiceNow Research\n\n2University of British Columbia\n\n3Mila – Quebec AI Institute\n\n4McGill University\n\n5Canada CIFAR AI Chair\n\nABSTRACT\n\nWe introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended\ndeep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple\nquestions or web-only queries, DRBench evaluates agents on multi-step queries (for\nexample, “What changes should we make to our product roadmap to ensure compliance\nwith this standard?”) that require identifying supporting facts from both the public web\nand private company knowledge base. Each task is grounded in realistic user personas\nand enterprise context, spanning a heterogeneous search space that includes productivity\nsoftware, cloud file systems, emails, chat conversations, and the open web. Tasks\nare generated through a carefully designed synthesis pipeline with human-in-the-loop\nverification, and agents are evaluated on their ability to recall relevant insights, maintain\nfactual accuracy, and produce coherent, well-structured reports. We release 15 deep\nresearch tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We\ndemonstrate the effectiveness of DRBench by evaluating diverse DR agents across open-\nand closed-source models (such as GPT, Llama, and Qwen) and DR strategies, high-\nlighting their strengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.\n\n1\n\nINTRODUCTION\n\nOrganizations today face a strong need to find useful insights in a world full of overwhelming information.\nValuable insights are often hidden in noisy data, which can contain many distracting or irrelevant details that\nobscure the insights that really matter. This challenge is present in enterprise settings, where data is spread\nacross many applications and stored in different formats (e.g., PDFs, spreadsheets, emails, and internal tools)\nmaking extracting relevant information difficult. To uncover these hidden, valuable insights, one must con-\nduct what is known as deep research. This task involves asking high-level strategic questions (e.g, “What\nchanges should we make to our roadmap to remain compliant?”), planning sub-questions, retrieving and\nevaluating relevant materials, and producing a clear, actionable summary grounded in data sources (Zheng\net al., 2025; Xu & Peng, 2025; Du et al., 2025). These tasks are typically performed by domain experts\nusing a mix of search engines, communication platforms, and business applications in iterative, high-effort\nworkflows (Mialon et al., 2024), which unfortunately require a significant amount of human effort.\n\nOne promising solution to reducing this human effort is agent-based deep research, which uses autonomous\nsoftware agents to search, extract, and synthesize information across fragmented sources into an\ninsightful report. Recently, LLM-based agents have emerged as promising assistants for deep research.\nSystems such as Local Deep Researcher (LearningCircuit, 2025), Deep-Searcher (Tech, 2024), and\nDeepResearcher (Zheng et al., 2025) propose modular agent pipelines that combine retrieval, reasoning,\nand summarization over documents and web sources. Architectures like OpenHands (All-HandsAI, 2024),\nOpenManus (FoundationAgents, 2024), and smolagents (HuggingFace, 2024) extend these capabilities\nto include collaboration, multi-modal search, and complex tool use in enterprise workflows (Xu & Peng,\n2025). Despite these advances, evaluating such systems remains an open challenge.\n\nMost existing benchmarks evaluate narrow aspects such as report factuality (Coelho et al., 2025), web-only\nsynthesis (Bosse et al., 2025), or tabular analytics (Sahu et al., 2025), but they do not assess whether agents\nidentify the most salient insights, remain faithful to retrieved evidence, or adapt to enterprise contexts. To\naddress these limitations, we introduce DRBench, a benchmark designed to evaluate LLM agents on open-\n\n1\n\n \n \n \n \n \n \n\fFigure 1: DRBench pipeline. 1 The Task Context defines the deep research question grounded by the\ncompany and persona given to the agent. 2 Task Data, including both distractor and injected groundtruth\ninsights in different formats (PDFs, DOCX, PPTX, XLSX, chats, etc.) are loaded into the enterprise\nenvironment’s applications. 3 The DRBenchAgent accesses both public web sources and local enterprise\ndata to extract relevant insights for the research question. 4 It produces a structured research report,\nwhich is 5 evaluated for Insight Recall (detecting injected groundtruth insights), Factuality (verifying\nclaims are correctly cited), and Report Quality.\n\nended, multi-step and long-horizon deep research tasks grounded in realistic enterprise contexts. As Fig. 1 il-\nlustrates, DRBench includes a suite of queries grounded in user personas and organizational scenarios, requir-\ning agents to search across real applications such as cloud file storage (Nextcloud), enterprise chat (Matter-\nmost), and user file systems, and to reason over formats like spreadsheets, slide decks, and PDFs. Our evalu-\nation framework introduces three scoring axes using LLM-as-a-judge methods inspired by G-Eval (Liu et al.,\n2023): (1) Insight Recall and Distractor Avoidance, which together evaluate whether the agent surfaces the\nmost salient injected insights while avoiding distractor content; (2) Factuality, which uses a TREC-RAG\npipeline (Wang et al., 2024) to verify whether claims are correctly grounded in their cited sources; and (3) Re-\nport Quality, which measures the coherence, completeness, and overall readability of the synthesized report.\n\nWe conduct a comparative study of agent architectures inspired by recent work (Zheng et al., 2025;\nXu & Peng, 2025; LearningCircuit, 2025; Zheng et al., 2025), analyzing how well they perform on\nDRBench across planning, insight identification, and grounding on facts. Our results show that while\nagents are competent at document retrieval and summarization, they often miss high-value insights, cite\nirrelevant evidence, or produce incoherent explanations, highlighting the limitations of current architectures\nand the need for more targeted innovation.\n\nOur contributions are as follows: (1) We introduce DRBench, the first benchmark for evaluating LLM\nagents on complex enterprise deep research tasks combining public web sources with private organizational\ndata; (2) We provide a suite of 15 high-level research tasks with 114 sub-questions spanning 10 domains,\nincluding Sales, Cybersecurity, and Compliance, each grounded in realistic company contexts and personas;\n(3) We design a reproducible enterprise environment integrating realistic enterprise applications like chat,\ncloud storage, emails, and documents; (4) We propose a scalable pipeline that generates realistic research\nquestions and insights by combining web facts with synthesized internal data; and (5) We develop an\nevaluation framework that scores agent reports on insight recall and distractor avoidance, factuality, and\noverall report quality.\n\n2 RELATED WORK\n\nDeep Research Benchmarks. With the growing capabilities of LLMs in research and reasoning tasks,\nseveral benchmarks have emerged to evaluate their performance in realistic, multi-step and long-horizon sce-\nnarios, including Deep Research Bench (Bosse et al., 2025), DeepResearch Bench (Du et al., 2025), Deep-\nResearchGym (Coelho et al., 2025), ResearcherBench (Xu et al., 2025b), Mind2Web2 (Gou et al., 2025)\nand GAIA (Mialon et al., 2024). As summarized in Table 1, these efforts typically focus on web-only tasks,\n\n2\n\nLoading to Environment2Load Ground Truth Insights Factuality Insight RecallReport QualityIs each claim in the reportproperly cited?Were the injected insightsdiscovered? Is the report clear to read andcomplete in addressing the points?5PersonaCompanyDR QuestionLee's Market: a regional Asian supermarket chainJohn Doe: Regulatory Aﬀairs ManagerNextCloudFileShareFile SystemLocal FilesMattermostChatRoundCubeEmailEnterprise Data3ReportGeneration41Context5(a) Tracking lot codes helps grocers reduce food waste and keep products fresh. [Cite: grocerydive.com](b) Lee's Market tracks 250 high- risk food products as of Q3 2024, aﬀecting 30% of inventory. [Cite: food- safety- compliance.pdf] ...(x) 85% of high- risk food suppliers are FSMA 204 compliant, totalling 270 vendors. [Cite: RoundCube email]EvaluationTask ContextHow can Lee's Market leverage FSMA 204 regulations to enhance food safety and customer trust?3Public DataReportAccessingWebsitesChat ConversationsPresentations:PPTXTask DataTabular Data:CSV, XLSXDocuments: Docx, PDF, MarkdownE- mailsI) 8% reduction in food waste during Q2 2024, saving $1.2 million....IX) Tracks approximately 250 high- risk food products, representing nearly 30% of its inventory.Injected InsightsSynthesized Files with Distractor Content and Injected InsightsLoad Sources5\fTable 1: Comparison of deep research benchmarks (top) and AI agent benchmarks with a computer environ-\nment (middle). Columns report dataset size, whether both public and local data are required, the provided\nenvironment type, task domains, task description, and evaluation method. Unlike prior work, DRBench com-\nbines public web retrieval with local enterprise data in realistic enterprise applications and evaluates both\ninsight recall, distractor avoidance and report quality. Task Description: types of tasks covered by the\nbenchmark: WR for Web Research, DR for Deep Research with both public and local data, CU for\nComputer Use and/or Mobile Use. DRBench has 114 total # groundtruth insights that need to be extracted\nto address the 15 DR Questions. Example groundtruth insights can be found at Table 8 in Appendix A.\n\nBenchmark\n\n# groundtruth\n\nPublic &\nLocal Data\n\nProvides\nEnv\n\nTask\nDomain\n\nTask\nDescription\n\nMain Evaluation Method\n\nDeep Research Bench (Bosse et al., 2025)\nDeepResearch Bench (Du et al., 2025)\nDeepResearchGym (Coelho et al., 2025)\nResearcherBench (Xu et al., 2025b)\nLiveDRBench (Java et al., 2025)\nBrowseComp-Plus (Chen et al., 2025)\nMind2Web 2 (Gou et al., 2025)\nGAIA (Mialon et al., 2024)\nGAIA2 (Andrews et al., 2025)\n\nTheAgentCompany (Xu et al., 2025a)\nOSWorld (Xie et al., 2024)\n\nDRBench\n\n89\n100\n1,000\n65\n100\n1,005\n130\n466\n963\n\n175\n369\n\n114\n\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n\n✗\n✗\n\n✓\n\n✓\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n\n✓\n✓\n\n✓\n\nWR\nWR\nWR\n\nGeneric WR & CU Answer Accuracy\nGeneric\nGeneric\nAI\n\nInsight Recall\nDocument Retrieval\nInsight Recall, Factuality\nGeneric WR & CU Insight Precision, Recall\nGeneric\nGeneric\nGeneric\nGeneric\n\nAnswer Accuracy, URL Recall\nPartial Completion\nAnswer Accuracy\nAction Accuracy\n\nWR\nWR\nWR\nCU\n\nEnterprise\nGeneric\n\nEnterprise\n\nCU\nCU\n\nDR\n\nTask Completion, Efficiency\nTask Completion\n\nInsight Recall\n\nmeasuring retrieval or synthesis quality in controlled but limited settings. In contrast, DRBench is the first to\ncombine web retrieval with local enterprise data, grounding tasks in persona- and domain-specific contexts.\n\nEnterprise Environments. Realistic enterprise environments have become an important testbed for\nevaluating agents in complex multi-application workflows. CRMArena-Pro (Huang et al., 2025a;b) targets\nsales and CPQ pipelines through persona-grounded dialogues, but is limited to conversational sales\nworkflows. OSWorld (Xie et al., 2024) and OSWorld-Gold (Abhyankar et al., 2025) benchmark agents\nin general-purpose desktop environments, using applications such as Microsoft Word and Excel, yet their\nfocus remains on computer task execution rather than enterprise deep research. TheAgentCompany (Xu\net al., 2025a) evaluates collaboration among autonomous agents for programming, browsing, and\ncommunication, though the tasks are computer-use focused and do not assess deep research capabilities.\nWorkArena (Drouin et al., 2024; Boisvert et al., 2024) offers a realistic enterprise environment with\nknowledge work tasks for web agents, though it does not support evaluation of deep research capabilities.\nIn contrast, DRBench offers a domain-grounded enterprise environment with applications that would\nrealistically be encountered in organizations. Tasks are tied to concrete personas and roles, requiring agents\nto search, reason, and synthesize insights across diverse formats, including spreadsheets, PDFs, wikis,\nemails, and presentations, reflecting realistic enterprise deep research.\n\nDeep Research Agents. A growing line of work explores agents for multi-step search and synthesis\nacross diverse information sources. LangChain’s Local Deep Researcher (LearningCircuit, 2025) and\nZilliz’s Deep-Searcher provide modular pipelines for iterative querying and summarization, while DeepRe-\nsearcher (Zheng et al., 2025) uses RL to enable planning, cross-validation, and self-reflection. Commercial\nsystems such as Gemini Deep Research and Manus.ai synthesize web-based reports with citations,\nand open-source frameworks like OpenHands (All-HandsAI, 2024), OpenManus (FoundationAgents,\n2024), and smolagents (HuggingFace, 2024) offer alternative architectures. Recent work also introduces\ntask-agnostic frameworks for long-form synthesis and evaluation paradigms such as Mind2Web 2 (Gou\net al., 2025), which treat agents as judges of browsing trajectories. Building on these efforts, DRBench an-\nalyzes their strengths and limitations in enterprise contexts, showing that current agents still fall short in\nconsistently extracting and grounding critical insights within complex, heterogeneous environments.\n\n3 DRBench - AN ENTERPRISE DEEP RESEARCH BENCHMARK\n\nTo evaluate agents on complex, open-ended enterprise deep research tasks, we designed DRBench with\nthree guiding principles: it requires agents to integrate both public web data and local enterprise documents,\nit involves both web search and enterprise application use, and it is hosted in an interactive and reproducible\n\n3\n\n\fFigure 2: DRBench Task Generation Pipeline. The pipeline comprises five main stages during each LLMs\ngenerate candidate data such as company context, insights, and research questions, while human annotators\nverify quality and select the final version. Stages S1–S5 denote the five generation steps.\n\nenterprise environment. These principles ensure that the benchmark reflects realistic enterprise workflows\nand provides a controlled yet challenging setting for research agents.\n\nThe Enterprise Search Environment. A unique aspect of DRBench is its realistic enterprise search\nenvironment. When addressing DR Questions like ”What changes should we make to our product roadmap\nto ensure compliance with this standard?”, the DR agents would need to navigate such environment and\nsearch across both public and private data sources to uncover relevant insights.\n\nPublic insights include information available on the open web or otherwise accessible to general users.\nLocal insights, on the other hand, come from an enterprise’s private systems. These insights are embedded\nwithin a vast search space that spans multiple data types (emails, slide decks, chat conversations, and Excel\nsheets) which reflect the complexity of real enterprise data ecosystems. This environment is populated with\ndata from different applications, accessible to both web-based agents and API-calling agents. For example,\nan app like Mattermost can be used to host chat conversations (see Appendix E for examples of the applica-\ntions). The goal of the DR Agent is to effectively navigate these public and private data sources to address\ncomplex, high-level DR questions. For the environment implementation details, please see Section D.\n\nTask Definition. Each task is associated with a deep research question Qi and Task Context C which\nincludes company information and the user persona. Each task also has a corresponding set of groundtruth\ninsights I consisting of relevant private insights Il (we also refer to this as internal insights), distractor\nprivate insights Id, and public insights Ip. Each private insight, whether relevant or a distractor, is embedded\ninto a file fi which could take the form of a PDF, Excel sheet, slide deck, chat log, and so on. The agent’s\ntask is to generate a report by extracting the public insights Ip from accessible sources such as the web,\nwhile also extracting the private insights Il from the files hosted in the enterprise environment. At the same\ntime, the agent must avoid extracting the distractor insights Id, which are not relevant to the DR question.\n\nDRBench provides 15 realistic deep research tasks explicitly framed around enterprise environments.\nEach task is associated with public insights extracted form quality, time-invariant URLs and local insights\nembedded within synthetic enterprise data, typically spanning 2–4 applications and 3–16 supporting files\n(see Section B). Tasks are distributed across 10 enterprise domains (such as Sales and Compliance - the\nfull list is in Section B) and divided between easy, medium, and hard categories that indicates the difficulty\nof addressing the DR Question. Finally, DRBench is fully self-hosted, with dated URLs and reproducible\nevaluation scripts to ensure stability and fair comparison across agent methods.\n\n3.1 DATA GENERATION\n\nTo create realistic and reproducible deep research tasks, DRBench employs a five-stage pipeline (Figure 2)\nthat combines large-scale LLM generation with human-in-the-loop verification. The pipeline helps us\ngenerate candidate company contexts, personas, questions, insights, and supporting files using LLM\nModels such as Llama-3.1-8B-Instruct, Llama-3.1-405B (Dubey et al., 2024). Three human annotators\nthen validate the generated content to ensure that they are realistic and plausible.\n\nThe pipeline has been used to generate 15 tasks with 114 groundtruth insights across 10 enterprise domains,\neach grounded in realistic personas and company profiles. We control the difficulty of each task by setting\n\n4\n\n\fthe number of insights, file types and application types. The complete list of tasks is provided in Section B.\nRefer to Section I for details on the cost of using data generation.\n\nStage 1: Company and Persona Generation. This stage produces the synthetic company profile and\nuser persona that form the Task Context C. LLMs were used to generate company descriptions detailing\nthe industry vertical, key products, market position, and competitive landscape. In parallel, they were used\nto create realistic personas across departments (e.g., a Regulatory Affairs Manager or a Market Research\nAnalyst) that serve as the role grounding for the final deep research question. They were then refined\nby human experts. The prompts used for this stage are provided in Section O.1 and the list of companies\nare given in Appendix B.\n\nStage 2: Public Source and Insight Collection. Given the company and persona context from Stage 1,\nwe have retrieved candidate URLs relevant to the specified domain and company background. To ensure\nquality, time-invariant insights, the search is restricted to dated, journal-based or industry-report websites\nthat provide authoritative information. Thus, the collected URLs and their contents are expected to be stable\nin time. Human annotators then review the candidate URLs and select one that is both topically aligned\nand provides insights into the topic. The selected page becomes the Task URL included in C. Its HTML\ncontent is parsed, and LLMs are prompted to extract business-relevant insights, which are subsequently\nfiltered and validated by human reviewers for accuracy and contextual fit. The public insights Ip derived\nfrom the Task URL are included in C and serves as a required piece of insight for the agent to retrieve\nduring report generation. Prompts used for this stage and the list of urls are provided in Section O.3.\n\nStage 3: Question Generation. Given the Task Context, we generate the deep research question Q.\nThe prompt (see Section O.2) is instantiated with the company profile and persona, the selected domain,\nthe Task URL, and the public insight Ip. The LLM proposes several open-ended candidate questions\ngrounded in this context. Human annotators then review these candidate DR Questions, selecting and\nrefining one to align with the persona and company. They also ensure that the insights available in the\nprovided URL can at least partially support answering the deep research question. For example, if the\nquestion concerns compliance with a specific regulation, the URL might include relevant insights, such\nas “groceries must have a traceability plan.” While this doesn’t fully resolve the question, it provides\na foundation.The question should be high-level enough to allow us to synthesize additional supporting\nprivate/internal insights Il (such an insight could be “the cost of implementing such a plan is X amount”)\nwhich are needed to strengthen the report generated for the question. This requirement ensures that new\ninternal insights can be generated, as discussed in Stage 4.\n\nStage 4: Internal Insight Generation. In this stage we generate the injected insights set G ⊂I. Using\nthe public insight Ip and the deep research question Q, LLMs are used to create company-specific insights\naligned with the organization’s industry, priorities, customer segments, and business goals. These insights\nare designed to provide additional supporting facts that need to be extracted to create a report that better\naddresses the DR questions. Human annotators review and refine these insights for accuracy and alignment\nwith the questions. In addition to relevant insights, we also produce distractor insights Id, which are\nplausible but irrelevant statements that do not support resolving the DR Question. Prompt details are\nprovided in Section O.4 and example internal insights are provided in Section B.\n\nStage 5: File Mapping and Generation. This stage produces the the set of files {fi} containing\nboth the relevant and distractor private insights. First, each insight is assigned to a modality such as\nemail, chat, pdf, docx, and so on. Then the file generation module follows the following three-step\n“needle-in-a-haystack” process: (1) create an outline of the file based on its modality(e.g., document\nstructure or chat configuration), (2) insert the distractor or relevant insight into an appropriate section\nof the file, and (3) fill the remaining content with realistic but irrelevant information. Human annotators\nspot-check the generated files to ensure fidelity, coherence and no contradicting information. Prompts\nfor file generation are provided in Section O.5 and screenshots of such generated files are in Section C.\n\n4 DRBench AGENT\n\nThe DRBench baseline agent (DRBA) is the first agent built specifically for deep research in enterprise\nsettings, designed to operate directly within the DRBench environment.\nIts multi-stage architecture\nsystematically investigates research questions by iteratively retrieving, processing, and synthesizing\nknowledge from enterprise services and the web until completion or a maximum iteration limit is reached\n(Figure 3; see Appendix F). The agent has access to app-specific api calling tools to access diverse\n\n5\n\n\fFigure 3: DRBench Agent architecture showing the enterprise research workflow from question submission\nthrough iterative research cycles to final report generation, using both enterprise and web search capabilities.\n\ninformation sources and a diverse toolset for analyzing retrieved results (Table 9, Appendix F.3). DRBA’s\narchitecture is organized into four main components: research planning, action planning, an adaptive\nresearch loop, and report writing. Refer to Appendix I for details on the cost of using DRBA.\n\nResearch Planning. The agent decomposes research questions into structured research investigation\nareas to guide subsequent action generation. This initial decomposition lays out a strategy to systematically\ncover the research space while maintaining focus on the initial deep research question. The agent supports\ntwo research planning modes: (1) Complex Research Planning (CRP), which generates a structured\nresearch plan with detailed investigation areas, expected information sources, and success criteria; and\n(2) Simple Research Planning (SRP) , which produces a lightweight decomposition of the main question\ninto a small set of self-contained subqueries. See Appendix F.2 for detailed examples of both modes.\n\nAction Planning. The system translates research objectives into executable actions through an LLM-based\nplanning subsystem. Actions receive a priority score based on strategic importance and are parameterized\nwith specific tool selection and execution parameters, and their dependencies to other actions in the plan.\n\nResearch Loop with Adaptive Action Planning (AAP). The system iterates through (1) tool selection\nand execution based on action priorities, (2) content processing and storage in a vector store, (3) adaptive\naction generation to cover research gaps, and (4) iteration findings storage for traceability and assessment.\n\nReport Writing. The report writing subsystem queries the vector store to synthesize the research findings\nand relevant retrieved content. This component generates a comprehensive report and uses its own citation\ntracking system to ensure proper attribution of claims.\n\n5 EXPERIMENTS AND RESULTS\n\nIn this section, we evaluate the DRBench Agent (DRBA) on both the full DRBench benchmark and a\nreduced subset for ablations. We consider (1) the Full Benchmark, covering all 15 tasks across 10 domains\n(Table 7), and (2) the MinEval subset, restricted to five retail tasks for efficient ablation studies. Results\nare reported across four metrics: Insight Recall, Distractor Avoidance, Factuality, and Report Quality,\nexplained in the next section. Implementation details and hyperparameters are in Appendix H, while the\nimpact of the number of research loop iterations on the performance of DRBA is analyzed in Appendix N.\n\n5.1 EVALUATION METRICS\n\nInsight Recall. We first decompose each report into atomic insights with their associated citations using\nan LLM (see Prompt 14). Each extracted insight is then compared against the set of groundtruth insights\nembedded in the task files using an LLM Judge with the Prompt 15. If a match is found, the insight is marked\nas detected and contributes to the insight recall score; otherwise, it is ignored. This metric thus measures\nrecall rather than precision, since judging whether an unmatched insight is nonetheless useful for answering\n\n6\n\n\fTable 2: DRBA performance with different planning configurations on DRBench. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. See Appendix K for the standard error across 3 runs.\nNote that higher numbers correspond to better scores, and the best result on each metric is bolded.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n16.92\n17.00\n16.94\n20.56\n19.20\n18.69\n\n64.06\n69.38\n66.78\n67.44\n61.72\n57.20\n\n97.94\n98.89\n99.52\n98.89\n98.97\n98.89\n\n91.08\n92.46\n90.72\n93.00\n91.86\n90.12\n\n41.71\n42.48\n42.07\n47.43\n44.80\n43.39\n\nthe deep research question is inherently subjective and difficult to automate. To prevent agents from trivially\nachieving 100% recall by copying all content into the generated report, the LLM Judge evaluates only the\nfirst k insights, where k equals the number of ground-truth insights plus five. This buffer ensures that reports\nare not penalized for including seemingly relevant insights that are not part of the groundtruth insight set.\n\nDistractor Avoidance. To measure precision, we track whether the agent’s report includes distractor\ninsights that are irrelevant to the research question. We compute distractor recall analogously to insight\nrecall, and define distractor avoidance as 1− distractor recall.\n\nFactuality. Using the same set of extracted insights (that we used for Insight Recall), we follow\nthe methodology of FactScore (Min et al., 2023).\nIf an insight lacks a citation or references a\nnon-existent source, it is labeled unfactual. Otherwise, we apply a retrieval-augmented system based on\ntext-embedding-3-large (OpenAI, 2024) to fetch the top-5 most relevant chunks from the cited\ndocument (Appendix H). The LLM Judge with Prompt 16 then determines whether the cited evidence\nsupports the claim. We also store justifications and model confidence scores for interpretability.\n\nReport Quality. Inspired by prior work (Coelho et al., 2025; Abaskohi et al., 2025), we query the LLM\nJudge with Prompt 17 to assign a 1–10 rating across six dimensions: (1) depth and quality of analysis, (2)\nrelevance to the research question, (3) persona consistency, (4) coherence and conciseness, (5) absence of\ncontradictions, and (6) completeness and coverage. The final report quality score is obtained by averaging\nthese six ratings.\n\n5.2 MAIN RESULTS\n\nWe first evaluate our DRBA agent (Section 4) using GPT-4o as the backbone model, a maximum of\n15 research loop iterations, and different combinations of planning modules: Simple Research Planning\n(SRP), Complex Research Planning (CRP), and Adaptive Action Planning (AAP). The results are reported\nin Table 2. Overall, the agent demonstrates moderate ability to ground its answers in factual evidence\nbut struggles to consistently surface the main injected insights necessary for answering the deep research\nquestions.\nIn many cases, the agent relies on prior knowledge or external web content rather than\nintegrating the crucial enterprise-specific information available in the files. By contrast, it is consistently\nstrong in avoiding distractors, showing that the agent is robust against misleading or irrelevant information\nbut less effective at prioritizing decision-critical insights. Note that our LLM Judge backbone is GPT-4o.\n\nComparison Across Planning Strategies. Here we see that SRP tends to produce more factually\ngrounded answers, while CRP excels at filtering out distractors through structured decomposition. AAP,\non the other hand, provides the largest improvements in both insight recall and report quality, suggesting\nthat dynamically adapting the plan during execution helps the agent recover missed evidence and refine its\nuse of sources. However, combining CRP or SRP with AAP does not yield clear gains, and in some cases\nreduces factuality, likely because overlapping strategies create redundant or unstable planning behavior.\nThese findings indicate that adaptive mechanisms are key for improving coverage of injected insights,\nwhile lightweight planning is more effective for maintaining factual grounding, and that carefully balancing\nthe two remains an open challenge. See Appendix M for detailed results for each task.\n\n7\n\n\fTable 3: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Note that higher numbers correspond to better scores, and the best result on each\nmetric is bolded. The full table with more models is given in Appendix L.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n17.37\n16.97\n20.16\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n78.91\n79.27\n69.75\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n100.00\n98.10\n97.90\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n90.48\n92.34\n91.26\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n49.78\n48.87\n53.86\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\n5.3 ABLATION: EFFECT OF BACKBONE LANGUAGE MODEL ON DRBA\n\nWe evaluate the impact of backbone language models on DRBA using the MinEval subset for controlled\ncomparison (Table 3). GPT-5 achieves the best balance of factual grounding, insight recall, and report\nquality. Open-source models show mixed results: Llama-3.1-405B excels in factuality but lags in recall,\nDeepSeek-V3.1 delivers balanced performance through targeted fine-tuning, and Qwen-2.5-72B is reliable\nbut trails GPT-5. These results underline the importance of backbone choice; larger and more advanced\nmodels generally yield stronger overall performance, though some open-source options are competitive\nin specific metrics. In addition, our experiments also revealed a significant limitation in agents retrieving\ncritical insights from the open web. As shown in Figure 8 in Section L, no agent managed to successfully\nsource external knowledge, highlighting the difficulty of extracting relevant information for deep research\napplications within an unboundedly large search space.\n\nTable 4: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall. The full table with all\ngroundtruth insights and predicted insights is given in Appendix G.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have inter-\nacted with personalized product recommen-\ndations, resulting in a 25% increase in aver-\nage order value.\n\n45% of Lee’s Market online customers engage with\npersonalized product recommendations, resulting in\na 25% increase in average order value.\n(Score = 1.0)\n\n45% of online customers engaged with per-\nsonalized product recommendations, and\namong those engagers average order value\nincreased by 25%.\n(Score = 1.0)\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\n85% of transactions are linked to loyalty accounts at\nLee’s Market, providing a solid foundation for personal-\nized marketing and improving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n5.4 QUALITATIVE ANALYSIS\n\nIn Table 4 we show a sample of three groundtruth insights as well as the predicted insights from using\nboth Llama 3.1 405B and GPT-5. We see that for the first insight, both models are able to effectively\nrecover the groundtruth insight. For the second insight GPT-5 can extract the relevant time of year, where\nas Llama 3.1 405B fails to do so. This possibly suggests that GPT-5 may be better at extracting fine details.\n\n5.5 PERFORMANCE OF WEB AGENTS ON DRBench\n\nWe evaluated Generic WebAgents from AgentLab in a browser-only setting (without API access).\nThe GPT-4.1-powered agent achieved only 1.11% insight recall, 6.67% factuality, and 33.07% report\nquality. While the reports appeared well-structured, they lacked grounded insights, with most trajectories\n\n8\n\n\fTable 5: Model Performance Comparison Across Local or App-based Environments. Note that higher\nnumbers correspond to better scores, and the best result on each metric is bolded.\n\nEnv\nModel\nApp\nDRBA (GPT-5)\nApp\nDRBA (GPT-5) + CRP\nDRBA (DeepSeek-V3.1)\nApp\nDRBA (DeepSeek-V3.1) + CRP App\nLocal\nDRBA (GPT-5)\nLocal\nDRBA (GPT-5) + CRP\nLocal\nDRBA (DeepSeek-V3.1)\nDRBA (DeepSeek-V3.1) + CRP Local\nPerplexity\nLocal\nOpenAI Deep Research (GPT-5) Local\nLocal\nGemini\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n38.33\n39.63\n25.15\n30.26\n41.25\n42.18\n35.62\n36.54\n39.14\n44.78\n43.92\n\n74.52\n65.17\n72.66\n70.27\n82.43\n83.91\n77.12\n78.35\n81.06\n87.53\n85.68\n\n95.14\n92.86\n97.43\n96.67\n98.62\n98.45\n97.95\n97.62\n98.84\n99.12\n98.97\n\n94.56\n93.42\n86.52\n86.88\n91.08\n92.46\n89.16\n90.12\n90.36\n94.92\n93.24\n\n66.01\n64.46\n53.09\n57.86\n69.57\n70.67\n64.03\n65.07\n67.72\n73.56\n72.37\n\ndegenerating into repetitive clicks on irrelevant files or windows. This shows that browser-only agents are\ncurrently far from effective for deep research tasks. Further trajectory examples are shown in Appendix J.\n\n5.6 APP-BASED ENVIRONMENT VS LOCAL ENVIRONMENT\n\nIn Table 5, we compare results across two settings in DRBench: (1) local, where all the task files (e.g.,\nPDFs, PPTX, DOCX, XLSX, chats) are placed in a local folder that the agent can access, and (2)\napp-based, where the same files must be retrieved through our standard enterprise environment and its apps,\nintroducing additional interaction complexity. We find that OpenAI’s Deep Research (GPT-5) achieves\nthe highest scores across all metrics. Our agent with GPT-5 and DeepSeek backbones achieves similar\nperformance to Perplexity in the local-only setting, but lags behind OpenAI and Gemini. In the app-based\nsetting, performance declines across both backbones, highlighting the added difficulty of navigating\nmulti-application environments. This gap underscores that the environment in DRBench is intentionally\nchallenging, enabling a more realistic evaluation of model capabilities in enterprise research scenarios.\n\n6 HUMAN EVALUATION\n\nQuality of Deep Research Questions. We evaluated the quality of the deep research questions in\nDRBench through a human study with five expert annotators across all 15 tasks. Each task was judged\non three criteria: (1) grounding in the external website, (2) relevance to the domain and company context,\nand (3) alignment with associated insights. Annotators provided binary ratings plus optional feedback.\nResults show strong quality: 12 tasks received unanimous approval, while only three (tasks DR1, DR11,\nand DR13) received a single negative vote due to minor issues with specificity or distractor difficulty. This\ncorresponds to a 96% approval rate (72/75 votes).\n\nCorrelation of Used Metrics with Human Preference. We collected human preference on a subset\nof 11 tasks1. Each annotator was shown a golden insight with aligning insights from two models2 and\nasked to choose which they preferred, or label both as good/bad. Missing alignments were shown as\nempty strings. We compared agents with AAP no RP against GPT-5 and Llama-3.1-405B-Instruct. The\nFleiss κ (Fleiss, 1971) across five annotators was 0.67. Most outputs were judged both bad due to missing\nalignments, but when preferences were expressed, GPT-5 was favored 61.1% over Llama-405B-Instruct,\nconsistent with our metric-based findings in Section 5.3. Additional analyses are in Appendix Q.\n\n7 CONCLUSION & FUTURE WORK\n\nIn this work, we introduced DRBench, a benchmark for evaluating AI agents on complex, open-ended\nenterprise deep research tasks that require reasoning over both public and private data. Unlike prior\nbenchmarks focused on surface-level or web-only queries, DRBench offers 15 persona-grounded tasks\nsituated in realistic enterprise contexts and evaluated through an environment that integrates real-world\nenterprise applications and heterogeneous data formats. We also presented the DRBench Agent (DRBA)\n\n1We selected tasks with fewer than 8 insights for a reasonable amount of manual work.\n2The gold-prediction alignment is provided by the insight recall metric.\n\n9\n\n\fas a strong baseline and analyzed its behavior across planning strategies and backbone models. Our results\nshow that while agents are generally effective at avoiding distractors and capable of producing structured\nreports, they still struggle to consistently extract decision-critical insights. Adaptive planning improves\nrecall of injected insights, while lightweight strategies tend to preserve factual accuracy, underscoring the\ndifficulty of balancing exploration with reliability. Looking ahead, we plan to extend DRBench with tasks\nrequiring cross-file integration, reasoning across modalities such as PDFs and chats, and richer distractors.\nWe also aim to add multimodal sources like images and video, as well as privacy-sensitive tasks to assess\ndata protection. Together, these extensions will move research agents closer to enterprise readiness and\nprovide a stronger foundation for studying deep research in realistic organizational settings.\n\n10\n\n\fREFERENCES\n\nAmirhossein Abaskohi, Amrutha Varshini Ramesh, Shailesh Nanisetty, Chirag Goel, David\nand Issam H. Laradji.\nURL\n\nVazquez, Christopher Pal, Spandana Gella, Giuseppe Carenini,\nAgentAda:\nhttps://arxiv.org/abs/2504.07421.\n\ntailored insight discovery, 2025.\n\nSkill-adaptive data analytics\n\nfor\n\nReyna Abhyankar, Qi Qi, and Yiying Zhang. OSWorld-Gold: Benchmarking the efficiency\nIn ICML 2025 Workshop on Computer Use Agents, 2025. URL\n\nof computer-use agents.\nhttps://openreview.net/forum?id=sV3n6mYy7J.\n\nAll-HandsAI.\n\nOpenHands.\n\nhttps://github.com/All-Hands-AI/OpenHands, 2024.\n\nAccessed: 2024-06-01.\n\nPierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja,\nRicardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo\nLaurenc¸on, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre M´enard, Gr´egoire Mialon, Ulyana\nPiterbarg, Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav Vorotilov,\nMengjue Wang, and Ian Yu. ARE: Scaling up agent environments and evaluations, 2025. URL\nhttps://arxiv.org/abs/2509.17158.\n\nL´eo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier de Chezelles, Quentin\nCappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. WorkArena++: Towards\ncompositional planning and reasoning-based common knowledge work tasks. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL\nhttps://openreview.net/forum?id=PCjK8dqrWW.\n\nNikos I Bosse, Jon Evans, Robert G Gambee, Daniel Hnyk, Peter M¨uhlbacher, Lawrence Phillips, Dan\nSchwarz, Jack Wildman, et al. Deep Research Bench: Evaluating AI Web Research Agents. arXiv\npreprint arXiv:2506.06287, 2025.\n\nZijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green,\nKshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong,\nXinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin.\nBrowseComp-Plus: A more fair and transparent evaluation benchmark of Deep-Research agent, 2025.\nURL https://arxiv.org/abs/2508.06600.\n\nThibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, L´eo Boisvert,\nMegh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han\nL`u, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan\nSalakhutdinov, Nicolas Chapados, and Alexandre Lacoste. The BrowserGym ecosystem for web agent\nresearch, 2025. URL https://arxiv.org/abs/2412.05467.\n\nJo˜ao Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie\nCallan, Jo˜ao Magalh˜aes, Bruno Martins, et al. Deepresearchgym: A free, transparent, and reproducible\nevaluation sandbox for deep research. arXiv preprint arXiv:2505.19253, 2025.\n\nAlexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty,\nDavid Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents\nat solving common knowledge work tasks? 2024.\n\nMingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. DeepResearch Bench: A\n\ncomprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nJoseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76\n\n(5):378, 1971.\n\nFoundationAgents. OpenManus. https://github.com/FoundationAgents/OpenManus,\n\n2024. Accessed: 2024-06-01.\n\n11\n\n\fBoyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Botao Yu, Andrei Kopanev, Weijian\nQi, Yiheng Shu, Jiaman Wu, Chan Hee Song, Bernal Jimenez Gutierrez, Yifei Li, Zeyi Liao,\nHanane Nour Moussa, TIANSHU ZHANG, Jian Xie, Tianci Xue, Shijie Chen, Boyuan Zheng,\nKai Zhang, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, and Yu Su. Mind2web\nIn The Thirty-ninth Annual Conference\n2: Evaluating agentic search with agent-as-a-judge.\non Neural Information Processing Systems Datasets and Benchmarks Track, 2025.\nURL\nhttps://openreview.net/forum?id=AUaW6DS9si.\n\nKung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese,\nCaiming Xiong, Philippe Laban, and Chien-Sheng Wu. CRMArena: Understanding the capacity of\nLLM agents to perform professional CRM tasks in realistic environments. In Proceedings of the 2025\nConference of the Nations of the Americas Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), 2025a.\n\nKung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey,\nYixin Mao, Silvio Savarese, Caiming Xiong, and Chien-Sheng Wu. CRMArena-Pro: Holistic\nassessment of LLM agents across diverse business scenarios and interactions. arXiv preprint\narXiv:2505.18878, 2025b.\n\nHuggingFace.\n\nsmolagents.\n\nhttps://github.com/huggingface/smolagents, 2024.\n\nAccessed: 2024-06-01.\n\nAbhinav Java, Ashmit Khandelwal, Sukruta Midigeshi, Aaron Halfaker, Amit Deshpande, Navin Goyal,\nAnkur Gupta, Nagarajan Natarajan, and Amit Sharma. Characterizing deep research: A benchmark\nand formal definition, 2025. URL https://arxiv.org/abs/2508.04183.\n\nLearningCircuit.\n\nLocal deep research.\n\nlocal-deep-research, 2025.\n\nhttps://github.com/LearningCircuit/\n\nYao Liu, Deming Ye, Shuohang Wang, Furu Wei, Yujia Ma, and Minlie Huang. G-Eval: NLG evaluation\n\nusing GPT-4 with better human alignment. EMNLP, 2023.\n\nXing Han L`u, Zdenˇek Kasner, and Siva Reddy. WebLINX: Real-world website navigation with multi-turn\n\ndialogue, 2024. URL https://arxiv.org/abs/2402.05930.\n\nGr´egoire Mialon, Cl´ementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom.\n\nGAIA: a benchmark for general AI assistants. ICLR, 2024.\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke\nZettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 12076–12100, 2023.\n\nOpenAI.\n\ntext-embedding-3-large.\n\nhttps://platform.openai.com/docs/guides/\n\nembeddings, 2024. [Large language model]. Accessed September 2025.\n\nGaurav Sahu, Abhay Puri, Juan A Rodriguez, Amirhossein Abaskohi, Mohammad Chegini, Alexan-\ndre Drouin, Perouz Taslakian, Valentina Zantedeschi, Alexandre Lacoste, David Vazquez, et al.\nInsightBench: Evaluating insight extraction for business analytics agents. ICLR, 2025.\n\nZilliz Tech. Deep-Searcher. https://github.com/zilliztech/deep-searcher, 2024.\n\nAccessed: 2024-06-01.\n\nYuhao Wang, Ruiyang Ren, Junyi Li, Xin Zhao, Jing Liu, and Ji-Rong Wen. REAR: A relevance-aware\nretrieval-augmented framework for open-domain question answering. In Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing, pp. 5613–5626, 2024.\n\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua,\nZhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for\nopen-ended tasks in real computer environments. NeurIPS, 2024.\n\n12\n\n\fFrank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui\nZhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander\nMaben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig.\nTheAgentCompany: Benchmarking LLM agents on consequential real world tasks, 2025a. URL\nhttps://arxiv.org/abs/2412.14161.\n\nRenjun Xu and Jingwen Peng. A comprehensive survey of deep research: Systems, methodologies, and\n\napplications. arXiv preprint arXiv:2506.12594, 2025.\n\nTianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, and Pengfei Liu. ResearcherBench:\nURL\n\nEvaluating deep AI research systems on the frontiers of scientific inquiry, 2025b.\nhttps://arxiv.org/abs/2507.16280.\n\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.\nDeepResearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv,\n2025.\n\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou,\nYonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. WebArena: A Realistic Web Environment\nfor Building Autonomous Agents, April 2024. URL http://arxiv.org/abs/2307.13854.\narXiv:2307.13854 [cs].\n\n13\n\n\fTable 6: Comparison of Deep Research Tasks of Different Benchmarks.\n\nBenchmark\nDeepResearchGym (Coelho et al., 2025)\nDeep Research Bench (Bosse et al., 2025)\n\nDeepResearch Bench (Du et al., 2025)\n\nResearcherBench (Xu et al., 2025b)\n\nLiveDRBench (Java et al., 2025)\n\nBrowseComp-Plus (Chen et al., 2025)\n\nGAIA2 (Andrews et al., 2025)\n\nDRBench\n\nSample Question\nIs the COVID vaccine dangerous\nFind a reliable, known number on the internet. The total number\nof FDA Class II Product Recalls of medical devices.\nWhile the market features diverse quantitative strategies like\nmulti-factor and high-frequency trading, it lacks a single, standard-\nized benchmark for assessing their performance across multiple\ndimensions such as returns, risk, and adaptability to market\nconditions. Could we develop a general yet rigorous evaluation\nframework to enable accurate comparison and analysis of various\nadvanced quant strategies?\nCompare the Transformer and Mamba model architectures, analyz-\ning their performance and technical characteristics in different appli-\ncation scenarios. Based on the latest research, discuss the advantages\nand disadvantages of both models and their applicable scenarios.\nFor complex reasoning tasks (e.g., tasks involving multiple citations\nor extended reasoning chains), what are the strengths of current\nagent technologies, and what are their limitations? Please analyze\nthis in the context of research since June 2024.\nIdentify the title of a research publication published before June 2023,\nthat mentions Cultural traditions, scientific processes, and culinary\ninnovations. It is co-authored by three individuals: one of them was\nan assistant professor in West Bengal and another one holds a Ph.D.\nUpdate all my contacts aged 24 or younger to be one year older\nthan they are currently.\nHow can Lee’s Market leverage FSMA 204 regulations to enhance\nfood safety and customer trust?\n\nA COMPARISON OF DEEP RESEARCH BENCHMARKS\n\nAND AI AGENT BENCHMARKS WITH A COMPUTER ENVIRONMENT\n\nIn Table 1, we compare existing deep research benchmarks and AI agent benchmarks that provide a\ncomputer environment with DRBench. While the questions in existing benchmarks focus on public interest\ntopics and require generic web search and computer use, DRBench provides realistic questions that real\npersonas in organizations need to resolve.\n\nB DRBench TASKS\n\nAs shown in Table 7, DRBench contains 15 tasks in total, covering 3 industries (retail, healthcare and\nelectric vehicles), 10 task domains (compliance, sales, customer relationship management, market analysis,\ncustomer service management, IT service management, cyber security, marketing, quality assurance, and\nresearch), and 3 difficulty levels (easy, medium, hard). In addition, we generate the following 3 companies\n(one for each industry type): (1) a supermarket chain called Lee’s Market, (2) a virtual healthcare company\ncalled MediConn Solutions, and (3) an electric vehicle company called Elexion Automotive.\n\nTable 8 presents a deep research question from DRBench and its supporting groundtruth insights. We also\nvisualize the DR Question and all QA pairs by embedding them with OpenAI’s text-embedding-\n3-large model and projecting into 2D using t-SNE in Figure 4. The plot shows that injected supporting\ninsights lie closer to the DR Question, while distractors appear farther away, confirming that our injected\ninsights are semantically aligned with the research objective.\n\nC DRBench EXAMPLES OF INJECTED INSIGHTS\n\nAs shown in Figure 2, supporting documents are generated with enterprise insights injected. In Figure 5,\nwe show two examples of a generated files (PPTX and Mattermost chat) with their embedded insights.\n\n14\n\n\fFigure 4: t-SNE visualization of QA pairs for the DR Question in Task DR0005. The plot shows the\ndistribution of annotated pairs across Supporting Insights (green), Distractors (red), and the central\nDeep Research (DR) Question (gold star). Out of 49 pairs, 16 correspond to supporting insights and\n33 are distractors. The visualization illustrates how relevant insights cluster separately from distractors,\nhighlighting the challenge of retrieving salient information in a distractor-heavy environment.\n\n(a) Example supporting file named food-safety-\nregulatory-compliance.pdf with an injected insight\n”Lee’s Market reduced food waste by 8% in Q2 2024,\nsaving $1.2M.”\n\n(b) Example supporting email in the Sent mailbox with\nan injected insight ”85% of high-risk food suppliers are\nFSMA 204 compliant, totaling 270 vendors.”\n\nFigure 5: Example files with injected insights in DRBench.\n\n15\n\n151050510t-SNE Dimension 11050510t-SNE Dimension 2Q: What percentage of Lee's Market customers are centennial shoppers in Q3 2024?A: 25% of Lee's Market customers are centennial shoppers as of Q3 2024.Q: Projected chatbot cost savings in Q2 2024?A: Expect 25% cost reduction by Q2 2024.Q: How many customer service inquiries do we receive from centennial shoppers on...A: 2,500 customer service inquiries from centennial shoppers are received on average per quarter.Q: Centennial chatbot engagement rate by Q3 2025?A: 50% centennials engage weekly by Q3 2025.Q: How many Lee's Market customers used online shopping services in Q1 2024?A: About 30% of customers used online shopping services, totaling 150,000 users in Q1 2024.Q: Chatbot adoption rate Q1 2025?A: 55% adoption rate by centennials by Q1 2025.Q: Chatbot adoption rate among centennials by Q2 2024?A: 40% centennials prefer chatbots by Q2 2024.Q: How many customer queries does Lee's Market receive monthly in 2024?A: Lee's Market receives an average of 50,000 customer queries per month in 2024.Q: What was Lee's Market employee training budget in Q3 2023?A: Lee's Market allocated $450,000 for employee training, a 15% increase.Q: Annual facility maintenance cost 2024?A: Total maintenance cost was $3.5 million in 2024.Q: How many Lee's Market employees participated in leadership training in Q1 2024?A: 120 employees completed leadership training with a 90% satisfaction rate.Q: How many Lee's Market employees completed IT security training in Q1 2024?A: 85% of employees completed IT security training with a 95% pass rate.Q: What was Lee's Market employee turnover rate in Q2 2024?A: Employee turnover rate averaged 12% with a 5% increase from Q1 2024.Q: What was Lee's Market energy consumption cost in Q2 2024?A: Lee's Market energy consumption cost totaled $850,000 in Q2 2024.Q: What were Lee's Market IT costs in Q4 2024?A: IT costs totaled 3.4M,with1.2M on new hardware.Q: How many new store locations did Lee's Market open in Q3 2024?A: Lee's Market opened 12 new store locations across the US in Q3 2024.Q: What was our workers' compensation claims rate in Q2 2025?A: Workers' compensation claims rate was 2.5% with a total payout of $120,000.Q: What was Lee's Market average store maintenance cost in Q2 2024?A: Average store maintenance cost was $2,000 per month with a 5% reduction.Q: What were Lee's Market facility maintenance costs in Q2 2024?A: Facility maintenance costs totaled 1.2Mwithanaverageof100,000 monthly.Q: What was our employee headcount in Q1 2025?A: We had 5,100 employees as of January 2025.Q: How many employees attended compliance training in Q1 2024?A: Around 300 employees completed compliance training in Q1 2024.Q: What was our facility maintenance cost in Q4 2024?A: Facility maintenance costs totaled $1.8 million in Q4 2024.Q: Facility maintenance expenses Q4 2023?A: Maintenance expenses totaled $1.2 million in Q4 2023.Q: Employee training hours Q3 2024?A: Average training hours per employee were 15 in Q3 2024.Total QA Pairs: 49Supporting: 16 | Distractor: 33 | External: 0Annotated: 8 supporting + 16 distractorShapes:  Supporting   Distractor   ExternalHow can Lee's Market leverage chatbots to enhance the consumer experience for centennial shoppers?DR QuestionSupporting (Insight) (16)Distractor (33)\fTable 7: DRBench Questions and Statistics. Industry: target industry of the deep research question.\nDomain: domain of the deep research task. DR Question: question of the deep research task. Difficulty:\ndifficulty of the task defined based on the rubric mentioned in 3. # Applications: the number of total\napplications in the task environment. # Insights: the number of relevant insights to the deep research\nquestion. # Distractors: the number of non-supportings documents that do not contain relevant insights.\n\nIndustry\nRetail\n\nDomain\nCompliance\n\nRetail\n\nRetail\n\nRetail\n\nRetail\n\nSales\n\nCRM\n\nMarket Analysis\n\nCSM\n\nHealthcare\n\nCompliance\n\nHealthcare\n\nITSM\n\nHealthcare\n\nCybersecurity\n\nHealthcare\n\nCRM\n\nHealthcare\n\nMarketing\n\nElectric Vehicle\n\nCompliance\n\nElectric Vehicle\n\nQuality Assurance\n\nElectric Vehicle\n\nCybersecurity\n\nElectric Vehicle\n\nResearch\n\nElectric Vehicle\n\nCSM\n\nDR Question\nHow can Lee’s Market leverage FSMA 204 reg-\nulations to enhance food safety and customer trust?\nHow can personalization drive sales in the retail\nindustry and what strategies can be used for Lee’s\nMarket in action?\nHow can we leverage data-driven loyalty programs\nto enhance customer engagement?\nWhat are the new trends in the grocery retail\nmarket and what strategies can Lee’s Market adopt\nto remain competitive?\nHow can Lee’s Market\nleverage chatbots to\nenhance the consumer experience for centennial\nshoppers?\nWhat are the key factors influencing MediConn\nSolutions’ decision to accept insurance for tele-\nhealth providers, considering HIPAA compliance\nand state-specific data privacy regulations?\nHow can we leverage ISTM and AI-driven ana-\nlytics to minimize IT service desk workload and\nimprove response times in MediConn Solutions?\nWhat is the impact of third-party data breaches\non MediConn’s virtual healthcare platforms and\npatient data, and what new regulations can be\nimplemented to defend against these breaches?\nHow can MediConn Solutions leverage trendy new\nCRM solutions to improve patient engagement\nand retention, and what new CRM solutions are\nexpected in 2025?\nWhat are the most critical elements of a robust\ndigital presence for a telehealth provider such as\nMediConn Solutions, and how can we optimize\nour website and content marketing strategy to\nattract digital-first patients?\nHow can we balance the need for durability\nand warranty guarantees for EV batteries with\nevolving regulatory requirements, especially ACC\nregulations (ACC II), while staying on track with\nour production timelines through 2035?\nHow can Elexion Automotive’s quality assurance\nprocesses be optimized to address the unique\nchallenges of electric vehicle production, such as\nsoftware and user experience issues, compared to\ngasoline cars?\nHow can Elexion Automotive effectively im-\nplement a cybersecurity strategy for its electric\nvehicles, considering the risks and challenges\nposed by connected and autonomous technologies?\nCan we leverage AI-enhanced battery management\nto improve EV battery lifespan by 15%?\nHow can Elexion Automotive increase customer\ntrust through after-sales support while balancing\nthe need for exceptional customer care with\nefficient and cost-effective service?\n\nDifficulty\neasy\n\n# Applications\n2\n\n# Insights\n3\n\n# Distractors\n7\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\n2\n\n4\n\n3\n\n4\n\n3\n\n3\n\n4\n\n4\n\n4\n\n2\n\n1\n\n3\n\n3\n\n4\n\n4\n\n7\n\n6\n\n16\n\n4\n\n6\n\n7\n\n12\n\n15\n\n3\n\n3\n\n6\n\n7\n\n15\n\n10\n\n15\n\n14\n\n33\n\n8\n\n12\n\n14\n\n24\n\n30\n\n6\n\n6\n\n12\n\n14\n\n30\n\nD DRBENCH ENTERPRISE ENVIRONMENT\n\nThe DRBench Enterprise Environment provides a containerized simulation of realistic enterprise\nresearch settings where employees access confidential company information, personal files, and internal\ncommunications for comprehensive report generation. The environment simulates both a user’s local\nmachine filesystem and provides password-protected access to enterprise services.\n\nTo emulate realistic enterprise research settings, DRBench provides a self-contained Docker environment\nthat integrates commonly used applications: Nextcloud for shared documents, Mattermost for internal\nchat, an IMAP server and Roundcube open-source client for emails, and Filebrowser to emulate local files.\nEach task is initialized by distributing its data across these services, enabling agents to retrieve, analyze,\nand cite information through enterprise-like interfaces rather than static files. This design ensures realistic\ninteraction while maintaining reproducibility and controlled evaluation.\n\n16\n\n\fTable 8: Example Deep Research Question and Supporting Groundtruth Insights\n\nDeep Research Question\n\nHow can Lee’s Market leverage FSMA\n204 regulations to enhance food safety and\ncustomer trust?\n\nSupporting Groundtruth Insight\nU.S. grocers are working to meet the FDA’s FSMA 204 traceability rules\nby January 2026, which require tracking lot codes and key data for high-risk\nfoods to expedite recalls. This compliance is viewed as an “evolutionary\nstep” to modernize grocery operations and enhance food safety.\nBy capturing detailed traceability data, such as lot codes, at every step,\nretailers can meet regulations and gain inventory benefits. This allows grocers\nto know exact expiration dates by lot, enabling them to discount items before\nthey expire, thus reducing food waste and keeping products fresher.\nRegional grocers like Lunds & Byerlys and Raley’s see FSMA 204 as\na chance to enhance their systems and supply chain transparency. They\nbelieve improved traceability will boost customer trust and could signal the\nstart of more extensive future food safety regulations.\nLee’s Market tracks 250 high-risk food products as of Q3 2024, affecting\n30% of inventory.\nLee’s Market reduced food waste by 8% in Q2 2024, saving $1.2M.\n85% of high-risk food suppliers are FSMA 204 compliant, totaling 270\nvendors.\n\nInsight Category\nExternal\n\nExternal\n\nExternal\n\nInternal\n\nInternal\nInternal\n\nD.1 ARCHITECTURE AND SERVICES\n\nThe environment implements a multi-service architecture within a single Docker container. This design\nprioritizes deployment simplicity and cross-platform compatibility while maintaining service isolation.\nThe container orchestrates the following enterprise services:\n\n• Nextcloud: Open-source file sharing and collaboration platform analogous to Microsoft\n\nSharePoint or Google Drive, providing secure document storage with user authentication.\n\n• Mattermost: Open-source team communication platform simulating internal company commu-\nnications similar to Microsoft Teams or Slack, with teams, channels, and persistent chat history.\n\n• FileBrowser: Web-based file manager providing access to the container’s local filesystem,\n\nsimulating employee desktop environments and local document access.\n\n• Email System: Roundcube webmail interface with integrated SMTP (postfix) and IMAP\n\n(dovecot) services for enterprise email communication simulation.\n\n• VNC/NoVNC Desktop: Protocol and browser-based VNC access providing full desktop\nenvironment interaction within the container for comprehensive enterprise workflow simulation.\n\nD.2 TASK LOADING AND DATA DISTRIBUTION\n\nAt initialization, the environment processes task configuration files (env.json) and distributes data\nacross services through automated Python scripts and it makes sure that this source data is only accessible\nthrough the intended applications:\n\n• File Distribution: Documents are placed in appropriate Nextcloud user folders and FileBrowser\n\ndirectories based on task specifications\n\n• Communication Import: Chat histories and team conversations are imported into Mattermost\n\nchannels with proper user attribution\n\n• Email Integration: Email conversations are loaded into the mail system with realistic threading\n\nand metadata\n\n• User Provisioning: Enterprise users are automatically created across all services with consistent\n\nauthentication credentials\n\n17\n\n\ffrom drbench import drbench_enterprise_space, task_loader\n\n# Load task configuration\ntask = task_loader.get_task_from_id(task_id)\n\n# Initialize environment with automatic port allocation\nenv = drbench_enterprise_space.DrBenchEnterpriseSearchSpace(\n\ntask=task.get_path(),\nstart_container=True,\nauto_ports=True\n\n# Prevents port conflicts in parallel execution\n\n)\n\n# Environment provides service discovery\navailable_apps = env.get_available_apps()\n# Returns: {’nextcloud’: {’port’: 8081, ’credentials’: {...}}, ...}\n\n# Pass relevant information to the agent\n\n# Cleanup when research complete\nenv.delete()\n\nListing 1: DrBench Environment Usage\n\nD.3 PYTHON INTEGRATION\n\nThe DrBenchEnterpriseSearchSpace class provides programmatic container management with\nthe following capabilities: container lifecycle management, service access information, task-specific\ndata loading, and automatic cleanup. The typical usage pattern shown in Listing 1 demonstrates these\nintegrated capabilities.\n\nD.4 ENTERPRISE SERVICE APIS\n\nEach service exposes both **web interfaces** for human and web-agent interaction, and **programmatic\nAPIs** for agent access:\n\n• Nextcloud: WebDAV API for file operations, sharing, and metadata retrieval\n\n• Mattermost: REST API for message history, channel management, and user interactions\n\n• Email: IMAP/SMTP protocols for message retrieval and sending\n\n• FileBrowser: HTTP API for filesystem operations and file management\n\nThis dual-access model enables both agent-driven research and human verification of enterprise scenarios,\nsupporting comprehensive evaluation of research capabilities across realistic enterprise information\narchitectures.\n\nE DRBench EXAMPLES OF APPLICATION SCREENSHOTS\n\nFigures 6 and 7 show the applications provided in DRBench environment: File Browser, Mattermost,\nRoundcube, and Nextcloud.\n\nF DRBench AGENT IMPLEMENTATION DETAILS\n\nF.1 DETAILED WORKFLOW\n\nAs depicted in Figure 3, the workflow begins with a Company Employee submitting an enterprise Deep\nResearch Question along with Company Context. The DRBench agent processes this input through several\nkey stages:\n\n18\n\n\f(a) Screenshot of the File Browser interface, displaying\norganized files and folders within the system.\n\n(b) Screenshot of the Mattermost communication\nplatform, showing a discussion channel and user\ninterface elements.\n\nFigure 6: Screenshots of Applications in DRBench environment (Part 1).\n\n(a) Screenshot of the Nextcloud file management system,\nillustrating the file list view with various document types.\n\n(b) Screenshot of Roundcube, an email client, it shows\nan open email in the user’s inbox.\n\nFigure 7: Screenshots of Applications in DRBench environment (Part 2).\n\nStage 1: Research Planning. The agent decomposes the research question into structured research\ninvestigation areas to guide subsequent action generation. This initial decomposition lays out a strategy\nto systematically cover the research space while maintaining focus on the initial deep research question.\n\nStage 2: Action Planning. The Action Planning stage translates the research objectives into executable\nactions through a planning subsystem. This component uses an LLM to create a prioritized sequence\nof actions. Each action is parameterized with specific tool selection and execution parameters, its\ndependencies to other actions in the plan, and a priority score.\n\nStage 3: Research Loop with Adaptive Execution. The Research Loop iterates over the following\nsub-stages until completion: (1) Tool Selection and Execution: The tool selection and execution subsystem\nimplements a sophisticated priority-based selection of actions from the plan at each research iteration step\nand proceeds to execute it with the current research context. (2) Content Processing: If necessary, the action\nwill make use of the content processing subsystem to extract, synthesize, and store retrieved documents and\nwebsites into a vector store to form a task-specific knowledge base that will grow on each iteration. (3) Adap-\ntive Action Planning: After each execution round, the agent analyzes the most recent findings; if coverage\ngaps are detected, new actions are created and added to the plan at this point. This ensures that newly discov-\nered knowledge is taken into account to answer the research question. (4) Iteration Findings Storage: Results\nfrom each iteration are stored in the vector store with rich metadata for traceability and later assessment.\n\nStage 4: Convergence and Completion. The research loop continues until all actions in the plan have\nbeen completed or the maximum iteration limit is reached.\n\n19\n\n\fStage 5: Report Writing. The report writing subsystem queries the vector store to synthesize the\nresearch findings and relevant retrieved content. This component generates a comprehensive report and\nuses its own citation tracking system to ensure proper attribution of claims within the report.\n\nThe vector store serves as the main knowledge integration component, maintaining embeddings of all\nprocessed content and enabling semantic retrieval at the report generation stage. This component is crucial\nin a deep research setting to prevent information loss from early research stages in arbitrarily long research\nsessions.\n\nF.2 RESEARCH PLANNING IMPLEMENTATION\n\nThe Research Planning subsystem offers three operational modes to evaluate the impact of structured\nplanning on research effectiveness:\n\n• Complex Mode: Generates a comprehensive research plan with detailed investigation areas.\nThese areas contain details about the specific research focus, expected information sources, and\nsuccess criteria, among others. Each area includes an importance level and specific business\nintelligence objectives (see Listing 2).\n\n• Simple Mode: Creates focused question decompositions with 4-10 self-contained subqueries\nderived directly from the main research question. Uses straightforward decomposition without\nthe complex enterprise research structure of complex mode. See examples in Listing 4 and\nListing 3 for comparison of different planning modes.\n\n• None: Bypasses structured planning entirely, proceeding directly to action generation based\non the original question. This mode serves as a baseline to measure the added-value of explicit\nplanning stages.\n\nThe planning process begins with the enriched query (Prompt 1) and uses the research planning prompt\n(Prompt 2) to generate structured outputs. In Complex Mode, the system creates detailed investigation areas\nwith enterprise-focused metadata, while Simple Mode produces straightforward question decompositions\nsimilar to existing multi-step reasoning approaches. The resulting plan structure directly feeds into the\nAction Planning System (Section F.3) for executable action generation.\n(cid:7)\n{\n\n(cid:4)\n\n\"area_id\": 1,\n\"research_focus\": \"Core strategic\n\ndomain, market segment, or business hypothesis to investigate\",\n\n\"information_needs\": [\n\n\"What specific intelligence is required for strategic decisions\"\n\n],\n\"knowledge_sources\": [\"internal\", \"external\", \"both\"],\n\"research_approach\n\n\": \"competitive_analysis | market_research | strategic_assessment\n\n| trend_analysis | risk_analysis | performance_benchmarking\",\n\n\"key_concepts\": [\"concept1\", \"concept2\"],\n\"business_rationale\": \"Why this investigation\n\narea is critical for enterprise strategy and decision-making\",\n\n\"expected_insights\": \"What strategic\n\nunderstanding or competitive intelligence this area should provide\",\n\n\"stakeholder_impact\": \"Which\n\nbusiness units or decision-makers will benefit from these insights\",\n\n\"importance_level\": \"critical | important | supplementary\"\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 2: Investigation Area Structure for Full Planning Mode\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"What are the new trends in the grocery retail market\n\nand what strategies can Lee’s Market adopt to remain competitive?\",\n\n\"plan\": {\n\n\"research_investigation_areas\": [\n{\n\n20\n\n\f\"area_id\": 1,\n\"research_focus\": \"Current trends in grocery retail market\",\n\"information_needs\": [\n\"Latest consumer preferences\",\n\"Emerging technologies influencing grocery shopping\",\n\"Sustainability practices in grocery retail\",\n\"Changes in supply chain dynamics\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"trend_analysis\",\n\"key_concepts\n\n\": [\"e-commerce growth\", \"sustainability in supply chains\"],\n\n\"business_rationale\": \"Understanding consumer\n\ntrends and technological advancements that shape shopper behavior is\ncritical for adapting offerings and enhancing customer engagement.\",\n\"expected_insights\": \"Identify specific trends affecting customer\n\nbuying decisions, including the rise of online grocery shopping\nand preferences for sustainable, local, or organic products.\",\n\n\"stakeholder_impact\n\n\": \"Marketing, Product Development, Supply Chain Management\",\n\n\"importance_level\": \"critical\"\n\n},\n{\n\n\"area_id\": 2,\n\"research_focus\": \"Competitive analysis of grocery retailers\",\n\"information_needs\": [\n\"Market share analysis\",\n\"Competitor strengths and weaknesses\",\n\"Innovative strategies adopted by competitors\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"competitive_analysis\",\n\"\n\nkey_concepts\": [\"market positioning\", \"competitive differentiation\"],\n\n\"business_rationale\": \"A comprehensive understanding of\n\ncompetitors allows for strategic positioning and the identification\nof innovative practices that can be adopted or improved upon.\",\n\n\"expected_insights\": \"Detailed profiles of key\n\ncompetitors, including strategic moves they are making to capture\n\nmarket share, which can inform Lee’s Market’s competitive strategy.\",\n\n\"stakeholder_impact\n\n\": \"Executive Leadership, Strategic Planning, Marketing\",\n\n\"importance_level\": \"critical\"\n\n},\n...\n]\n\n}\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 3: Complex Mode Research Plan Example\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"How can we leverage\n\ndata-driven loyalty programs to enhance customer engagement?\",\n\n\"plan\": {\n\n\"mode\": \"simple\",\n\"subqueries\": [\n\"What are the key features\nof successful data-driven loyalty programs in the retail industry?\",\n\"How can data analytics be used to personalize rewards\nand incentives in loyalty programs to increase customer engagement?\",\n\"What types of customer data should be collected and analyzed\nto optimize loyalty programs for a company like Lee’s Market?\",\n\n21\n\n\f\"Which\ntechnology platforms and tools are most effective for implementing\nand managing data-driven loyalty programs in the retail sector?\",\n\"How can Lee’s Market measure the success of their\nloyalty program in terms of customer engagement and sales growth?\",\n\"What are the best practices for integrating a loyalty program\nwith existing marketing strategies to enhance customer experience?\",\n\"How can Lee’s Market ensure customer data\nprivacy and security while leveraging data-driven loyalty programs?\",\n\"What are the potential\nchallenges and limitations of implementing a data-driven loyalty\n\nprogram for a retail company with Lee’s Market’s size and resources?\"\n],\n\"research_methodology\": {\n\"overall_approach\": \"Query decomposition into focused subqueries\"\n}\n\n}\n\n}(cid:6)\n\nListing 4: Simple Mode Research Plan Example\n\n(cid:5)\n\nF.3 ACTION PLANNING SYSTEM\n\nThe Action Planning System translates research objectives into executable actions through an intelligent\nplanning subsystem that manages tools, prioritization, and dependencies.\n\nAvailable Tools Table 9 summarizes the available tools organized by category and their primary purposes.\n\nTable 9: DrBench Agent Tool Categories and Purposes\n\nCategory\n\nTool\n\nPurpose\n\nInformation Retrieval\n\nInternet Search\n\nEnterprise API\n\nURL Fetch\n\nAnalysis\n\nAnalyzer\n\nLocal Processing\n\nLocal\nSearch\n\nDocument\n\nExternal market research, competitive intel-\nligence, and public data analysis. Ideal for\nmarket trends, competitor analysis, industry\nreports, news articles.\n\nAccess to proprietary internal data through\nextensible adapters (Nextcloud, Mattermost,\nemail, FileBrowser). Ideal for internal metrics,\ncommunications, confidential documents.\n\nDirect content extraction from specific URLs.\nIdeal for deep analysis of reports, whitepapers,\ncase studies, competitor websites.\n\nAI-powered synthesis and analysis using\nvector search. Ideal for cross-referencing find-\nings, identifying patterns, generating insights.\n\nSemantic search within locally ingested\ndocuments. Ideal for targeted retrieval from\nlocal files with source references.\n\nPriority Scoring and Dependencies Actions are assigned priority scores (0.0-1.0 scale) based on strategic\nimportance and expected information value. The priority assignment follows enterprise research principles:\n\n• Source Type Prioritization: Enterprise and local sources receive higher priority than external\n\nsources, reflecting the strategic value of proprietary information in competitive analysis.\n\n• Query Specificity: Targeted queries addressing specific investigation areas score higher than\n\nbroad exploratory searches, ensuring focused research execution.\n\n22\n\n\f• Dependency Management: Actions can specify prerequisite relationships where certain\ninformation gathering must precede analysis or synthesis tasks. The scheduler respects these\ndependencies while maximizing parallel execution within each iteration.\n\nF.4 ENTERPRISE INTEGRATION ARCHITECTURE\n\nService Adapters The system implements extensible adapters for enterprise services including Nextcloud\nfile server, Mattermost chat, IMAP email systems, and FileBrowser interfaces. Each adapter handles\nservice-specific authentication, data retrieval, and metadata preservation for proper citation attribution.\n\nSource Prioritization Strategy Enterprise and local sources receive priority multipliers in action scoring,\nreflecting the strategic value of proprietary information. The system maintains source type classification\nthroughout the pipeline to ensure internal intelligence drives analytical conclusions while external sources\nprovide context and validation.\n\nF.5 TOOL SELECTION AND EXECUTION\n\nThe Tool Selection stage implements a priority-based action selection and tool invocation within each\nresearch iteration to execute the action plan.\n\nAction Selection Process At each iteration, the system selects executable actions based on priority\nscores and dependency satisfaction:\n\n• Priority-Based Scheduling: Actions are ranked by their priority scores, with enterprise and local\nsources prioritized over external sources to maximize the value of specific private information.\n\n• Dependency Validation: The scheduler checks that all prerequisite actions have completed\n\nbefore making an action available for execution.\n\n• Sequential Execution: Actions execute one at a time in priority order, maintaining research\n\ncoherence and enabling each action to build upon previous findings.\n\nOnce selected, actions execute through a standardized interface and results are integrated into the research\ncontext, informing the following stage of adaptive action planning.\n\nF.6 ADAPTIVE PLANNING\n\nThe Adaptive Planning system enables dynamic evolution of the action plan by analyzing results after\neach iteration to generate extra actions addressing information gaps.\n\nIt starts by analyzing the most recently completed actions and performs these two substages:\n\nSource analysis and gap classification. The system evaluates the possible imbalances in the action\ncompletion if information came from internal or external sources and identifies possible scenarios to cover.\n\nDynamic action generation. After analyzing the sources and results from the previous actions, the\nsystem makes an LLM call to generate 1-5 extra candidate actions with a specific prioritization. After\ncandidate actions are generated, they go through a deduplication process to make sure the plan didn’t\ncover them already and incorporates the final subset into the priority action plan so they can be considered\nby the scheduler in the following iteration.\n\nF.7 CONTENT PROCESSING AND VECTOR STORE\n\nThe Content Processing system implements a pipeline for unified ingestion of documents in multiple\nformats (PDF, docx, HTML, JSON, plain text formats, etc.) that normalizes and cleans text inside the\ndocuments and websites retrieved during the research. Content is then deduplicated and chunkized, and\nembeddings are computed for each of these chunks.\n\nThe Vector Store implements the storage and retrieval of the content via JSON metadata and NumPy\nembedding metrices, enabling semantic similarity and keyword based searches\n\n23\n\n\fF.8 REPORT GENERATION\n\nMulti-Stage Synthesis Pipeline The Report Generation stage implements a four-stage synthesis\napproach: (1) thematic content clustering via vector searches, (2) source prioritization and deduplication,\n(3) LLM Synthesis with source tracking, and (4) Final report writing and citation resolution.\n\nThe system generates targeted search queries based on the research plan, including specific to ensure that\na predefined set of themes or sections (background, analysis, implementation, trends) are retrieved and\nwritten and prevents redundant analyses.\n\nUnified Citation System The citation system implements deferred resolution to keep consistency of\ncitations from section to section by referencing document IDs in the Vector Store and carrying them over\nfor each piece of synthesized text. A final citation resolution stage will assign the correct numbering to\neach document in the final report.\n\nF.9 DRBA PROMPTS\n\nThe DRBench agent relies on carefully designed prompts to orchestrate enterprise research workflows.\nThese prompts implement the core architectural principles: enterprise context enrichment, structured\nplanning decomposition, priority-based action generation, quantitative synthesis requirements, and adaptive\nresearch capabilities. The following five prompts represent the critical LLM interactions that enable\nsystematic enterprise research with proper source prioritization and citation tracking:\n\n• Enriched Query Generation (Prompt 1): Transforms basic research questions into enterprise-\ncontextualized queries by incorporating company information, stakeholder personas, and business\ncontext to guide subsequent research activities.\n\n• Research Planning (Prompt 2): Decomposes complex research questions into structured\ninvestigation areas with defined information needs, knowledge sources, and business rationales,\nenabling systematic coverage of the research space.\n\n• Action Generation (Prompt 3): Converts research objectives into prioritized executable\nactions with tool specifications and dependency relationships, emphasizing enterprise source\nprioritization over external sources.\n\n• Adaptive Action Generation (Prompt 4): Analyzes research progress to identify coverage\ngaps and source imbalances, generating complementary actions that enhance research depth\nand cross-validate critical findings.\n\n• Report Synthesis (Prompt 5): Orchestrates quantitative-first content synthesis with strict citation\nrequirements, ensuring numerical data leads analytical paragraphs and all claims are properly\nattributed to source documents.\n\nW Query Generation\n\nResearch Question: {dr question}\n\nCompany Context: {company name} is {company desc}.\n\nPersona Context: This analysis is requested by {name}, {role} in\n{department}.\n\nTheir responsibilities include: {responsibilities}.\n\nPrompt 1: Query Enrichment with Enterprise Context.\n\nG QUALITATIVE RESULTS\n\nShown below are some illustrative examples of how metrics are computed for different scenarios on a\ngiven test task.\n\n24\n\n\fW Research Planning (Complex Mode)\n\nDesign a comprehensive enterprise research strategy for:\n\"{question}\"\n\n{tools section}\n\nAs a senior enterprise researcher with deep business\nintelligence expertise, create a thorough investigation\nplan that combines rigorous research methodology with\nstrategic business analysis. Your goal is to provide insights\nthat drive informed decision-making in complex enterprise\nenvironments.\n\nEnterprise Research Design Principles:\n\n- Leverage proprietary internal data as competitive advantage while\nensuring external market context\n- Design investigations that directly inform strategic decisions and\nbusiness outcomes\n- Prioritize research areas that maximize ROI and strategic value to the\norganization\n- Balance comprehensive analysis with focused insights relevant to\nenterprise objectives.\n\nGenerate a JSON object with strategic research investigation\nareas:\n\n{output structure}\n\n.\n\nPrompt 2: Enterprise Research Planning with Investigation Areas. See example of the output structure\nin Listing 2\n\nG.1\n\nINSIGHTS RECALL\n\nWe showcase the insights recall metric using Task DR0002, whose DR Question is “How can\npersonalization drive sales in the retail industry and what strategies can be used for Lee’s Market in action?”\n(also shown in Table 7), which evaluates sales challenges and competitive landscape analysis.\n\nTable 10 shows the overall performance comparison between using Llama 3.1 405B and GPT-5 in\nour agent. Using GPT-5 in our agent results in increasing the insights recall score from 0.14 to 0.43,\nsuccessfully answering 3 out of 7 questions compared to Llama’s 1 out of 7.\n\nTable 10: Insights Recall Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We\nsummarize the number of questions answered successfully and unsuccessfully as well as the overall\ninsights recall score for the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nInsights Recall Score\n\nQuestions Answered\nSuccessfully\n\nQuestions Failed\n\n0.14\n\n1/7\n\n6/7\n\n0.43\n\n3/7\n\n4/7\n\n+0.29\n\n+2\n\n-2\n\nThe question-by-question breakdown in Table 11 reveals the specific questions where each approach\nsucceeded or failed. Both models successfully identified the insight related to online customer engagement,\nbut only GPT-5 was able to identify the number of loyalty program members and the customer data\ncollection rate. Neither model successfully answered the remaining 4 questions, indicating these insights\nmay not have been readily available in the source materials or that agents struggled to find the right insights.\n\n25\n\n\fW Action Generation\n\nGenerate specific executable actions for this research investigation\narea with SOURCE PRIORITIZATION:\n\nResearch Focus: {research focus}\n\nInformation Needs: {information needs}\n\nKnowledge Sources: {knowledge sources}\n\nResearch Approach: {research approach}\n\nAvailable Tools: {available tool names}\n\nTool Selection Guidelines: {tool guidelines}\n\nReturn JSON array of actions with:\n- \"type\": Action type (web search, enterprise api, url fetch, analyzer,\nlocal search)\n- \"description\": Clear description of what this action will accomplish\n- \"parameters\": Tool-specific parameters including query, search type,\netc.\n- \"priority\": Float 0.0-1.0 (enterprise sources: 0.7-1.0, external:\n0.4-0.7)\n- \"expected output\": What information this action should provide\n- \"preferred tool\": Specific tool class name to use\n\nPrompt 3: Priority-Based Action Generation with Source Awareness\n\nTable 11: Question-by-Question Insights Recall Analysis (Task DR0002). We breakdown the results\nquestion by question for the given task, highlighting specifically which question is answered correctly\nor incorrectly for each model.\n\nQuestion\n\nLlama 3.1 405B GPT-5\n\nOnline Customer Engagement with Personalized Recommendations\n\nNumber of Loyalty Program Members\n\nCustomer Data Collection Rate\n\nOnline Sales Growth\n\nEffectiveness of Personalized Marketing\n\nPersonalized Promotions vs Mass Promotions\n\nRetail Media Growth\n\nTotal Insights Recall Score\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0\n\n1.0\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n∆\n\n0.0\n\n+1.0\n\n+1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0/7.0\n\n3.0/7.0\n\n+2.0\n\nIn Table 12 we extend Table 4 and show all the groundtruth insights as well as each of the predicted\ninsights from using both Llama 3.1 405B and GPT-5. As before, we highlight in bold where the model\nwas able to accurately find details relevant to the expected insight, and show all the corresponding scores\nas given in Table 11.\n\nG.2 FACTUALITY\n\nThe factuality metric evaluation uses the same Task DR0002 to assess the accuracy and reliability of\ngenerated content. Table 13 presents the factuality performance comparison, showing that while using\nLlama 3.1 405B achieved 0.41 factuality (7 factual claims out of 17 total claims), where as using GPT-5\nreached 0.65 factuality (13 factual claim out of 20 total claims). This represents a significant improvement\n\n26\n\n\fW Adaptive Action Generation\n\nBased on the research progress so far, suggest new actions with\nINTELLIGENT SOURCE COMPLEMENTARITY:\n\nOriginal Research Query: {research query}\n\nCompleted Actions Summary: {completed actions summary}\n\nRecent Findings Analysis: {findings json}\n\nSource Composition Analysis: {internal findings}\n\nAvailable Tools: {available tool names}\n\nGenerate 1-5 new actions that:\n1. **Address gaps** in current research coverage\n2. **Balance source types** - if findings are heavily external,\nprioritize internal sources and vice versa\n3. **Build on discoveries** - leverage new information to explore\nrelated areas\n4. **Enhance depth** - dive deeper into promising\nfindings\n5. **Cross-validate** - verify critical findings through alternative\nsources\n\nEach action should have:\n- Strategic rationale for why this action is needed\nnow\n- Clear connection to research gaps or promising\nleads\n- Appropriate priority based on strategic value and source\nbalance\n- Specific parameters that build on existing knowledge\n\nPrompt 4: Gap-Driven Adaptive Action Generation\n\nin content reliability. This also highlights that GPT-5 is much better prepared to make accurate claims\nthat are sustained by evidence.\n\nTable 14 provides a detailed breakdown of factual versus unfactual claims. The agent using GPT-5\ngenerated 6 additional factual claims while producing 3 fewer unfactual claims, resulting in a net\nimprovement in accuracy percentage. This demonstrates that GPT-5 may generate a higher proportion\nof factual information than Llama 3.1 405B.\n\nThe impact of these factuality improvements of using GPT-5 over Llama 3.1 405B is summarized in\nTable 15. The 24.0 percentage point improvement in factuality represents enhanced content quality and\nresearch reliability. The increase in 3 total claims shows that GPT-5 can generate more content overall.\n\nH EXPERIMENTAL SETTINGS\n\nAll experiments were conducted on a cluster of 8 NVIDIA A100 GPUs (80GB each). For file generation\nand task construction, we primarily used the Llama-3.1-405B model, with decoding performed\nusing nucleus sampling at a temperature of 0.7 unless otherwise specified. For larger-scale evaluations of\nthe DRBench Agent, we also used closed-source models such as GPT-4o and GPT-5, alongside DeepSeek\nmodels, to enable comparison across open- and closed-source backbones.\n\nTo ensure reproducibility, the DRBench environment was deployed as a self-contained Docker container\nwith all supporting applications (Nextcloud, Mattermost, and Filebrowser) pre-configured. Each task was\n\n27\n\n\fTable 12: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have interacted\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n\n45% of Lee’s Market online customers\nengage with personalized product recom-\nmendations, resulting in a 25% increase in\naverage order value.\n(Score = 1.0)\n\n45% of online customers engaged with\npersonalized product recommendations,\nand among those engagers average order\nvalue increased by 25%.\n(Score = 1.0)\n\nAs of Q3 2024, Lee’s Market has 1.2 million\nloyalty program members.\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\nLee’s Market online sales grew 12% in Q2\n2024 compared to Q2 2023.\n\nRetailers excelling in personalized marketing\nare growing revenues about 10 percentage\npoints faster than their peers, according to\nBCG. By effectively using first-party customer\ndata, these leaders could unlock an estimated\n$570 billion in additional sales, highlighting\nthe importance of data-driven sales strategies\nfor growth.\n\nPersonalized promotions can deliver returns\nthree times higher than mass promotions,\nyet many retailers allocate under 5% of their\npromo budgets to personalization. One major\nchain increased its personalized promo spend\nfrom 1% to 10% by establishing a ”customer\ninvestment council,” resulting in $250 million\nin incremental sales.\n\nRetail media networks are expanding rapidly,\nwith retail media growing at approximately\n25% annually, offering retailers a profitable\nrevenue stream to reinvest in technology, data,\nand personnel. By integrating loyalty data,\nretailers like Sephora, which links 95% of\ntransactions to loyalty accounts, enhance preci-\nsion in product recommendations and provide\na seamless omnichannel experience, boosting\nconversion rates and customer lifetime value.\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nFrom Q2 2024 to Q3 2024, loyalty members\nincreased from 1,050,000 to 1,200,000\n(+150,000; +14.29%), average spend per\nmember rose from 24 to 25 (+4.17%), and\ntotal member spend increased from 25,200,000\nto 30,000,000 (+19.05%).\n(Score = 1.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n45% of Lee’s Market online customers engage\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n(Score = 0.0)\n\nA naive blended online AOV upside of\napproximately 11.25% is derived from 45%\nengagement multiplied by a 25% AOV lift\namong engagers.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nGrocers running data-driven loyalty cam-\npaigns have realized an average 3.8%\nlike-for-like sales uplift.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nsources\n\nindicate POS-enabled\nExternal\npersonalization can lift revenue 5%-15% and\nadvocate personalized e-receipts with relevant\noffers and coupons to extend post-purchase\nengagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 0.0)\n\nTable 13: Factuality Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We show\nthe number of factual and unfactual claims made by each model, as well as the overall factuality score\nfor the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nFactuality Score\n\nFactual Claims\n\nUnfactual Claims\n\n0.41\n\n7\n\n10\n\n0.65\n\n13\n\n7\n\n+0.24\n\n+6 claims\n\n-3 claims\n\n28\n\n\fW Report Synthesis\n\nAs an expert research analyst, synthesize the following content into\na coherent, insightful, and well-supported analysis for the theme:\n\"{theme}\" directly related to the overarching research question:\n\"{original question}\"\n\nSource Priority Guidelines:\n\n1. **\"internal\"**: Highest priority (internal company\ndocuments, proprietary files, confidential reports,\nenterprise chat messages, local documents, CRM data,\ninternal APIs, project management tools). Insights from\nthese sources should form the primary foundation of the\nanalysis.\n2. **\"external\"**: Medium priority (public web sources,\nacademic papers, industry reports, news articles). Use these\nto provide broader context, external validation, or contrasting\nperspectives.\n\n**Synthesis Requirements:**\n* **QUANTITATIVE PRIORITY:** Lead with numerical data, calculations, and\naggregations\n* Extract ALL percentages, costs, metrics, and performance\ndata\n* Perform mathematical operations: aggregate percentages, calculate\nincreases, sum totals\n* Example: \"Finance customers (35%) combined with healthcare\n(40%) represent 75% of regulated industry concerns\n[DOC:doc 1][DOC:doc 2]\"\n\n* **FACT VERIFICATION (CRITICAL):**\n* ONLY state what documents explicitly contain - no inference or\nextrapolation\n* Use exact quotes for key numerical claims: \"As stated in the\ndocument: ’[exact quote]’ [DOC:doc id]\"\n\n* **Citation Usage (Critical):**\n* **Format:** Reference sources by their document ID: \"Internal review\nshows 15% increase [DOC:doc 079c2e0f 1752503636]\"\n* **NEVER HALLUCINATE CITATIONS:** Only use provided doc id\nvalues\n* **Cite every numerical claim and calculation with source\ndocuments**\n\nGenerate 2-4 paragraphs of synthesized analysis with proper inline\ncitations.\n\nPrompt 5: Report Section Synthesis with Citation Requirements\n\nexecuted by instantiating a fresh container to avoid state leakage across runs. We capped the number of\nagent iterations according to the settings described in Section 5, with each iteration limited by a fixed\ncomputational budget.\n\nFor model outputs, we standardized all prompts and evaluation pipelines across backbones, using identical\nresearch questions, company contexts, and injected insight sets. To avoid stochastic variability, we repeated\ngeneration three times per task and reported averaged scores.\n\nFinally, all supporting scripts, environment configurations, and evaluation code are fully containerized,\nenabling consistent replication of our reported results across hardware setups.\n\n29\n\n\fTable 14: Factuality Content Analysis (Task DR0002). We show the number of factual and unfactual\nclaims made by each model, highlighting the factuality accuracy of each model for the given task.\n\nAgent\n\nFactual Unfactual Accuracy\n\nLlama 3.1 405B\n\n7 claims\n\n10 claims\n\nGPT-5\n\nChange\n\n13 claim\n\n7 claims\n\n+6\n\n-3\n\n+24.0%\n\n41.0%\n\n65.0%\n\nTable 15: Factuality Summary (Task DR0002). We summarize the factuality result improvements made\nby using GPT-5 over Llama 3.1 405B.\n\nImpact Category\n\nValue\n\nFactuality Improvement\n\n+24.0 percentage points\n\nClaims Added\n\n3 total claims\n\nAccuracy Enhancement\n\nFrom 41.0% to 65.0%\n\nContent Quality\n\nMore grounded information\n\nTask Domain\n\nTask Industry\n\nSales\n\nRetail\n\nI DATA SYNTHESIS AND DRBA COST DETAILS\n\nTo generate the DRBench tasks, we combined external insight extraction with internal file synthesis. Each\ntask included an average of 10 supporting files spanning heterogeneous formats (PDF, DOCX, PPTX,\nXLSX, and JSONL), with each file containing roughly 5 paragraphs of content. Files were designed\nto embed injected insights while mixing in distractor material, ensuring realistic enterprise complexity\nwithout exceeding practical runtime or storage budgets.\n\nData synthesis was primarily powered by GPT-4o. During task construction, GPT-4o was responsible for (1)\nextracting structured insights from public web sources, (2) adapting these insights into enterprise-grounded\ninterpretations, (3) generating persona-specific deep research questions, and (4) producing file-level content\nwith a balanced mix of insights and distractors. For evaluation, the DRBench Agent (DRBA) used GPT-4o\nas its backbone, with each task typically requiring 15 iterations and approximately 120–150 model calls.\n\nIn terms of cost, GPT-4o-based synthesis of a single task (10 files, 5 paragraphs each, plus metadata)\nconsumed about 30k–40k tokens, while DRBA execution required an additional 50k–70k tokens per\ntask. At current GPT-4o API pricing ($5 per million input tokens and $15 per million output tokens), this\ncorresponds to a per-task cost of approximately $1.5–$3.5 depending on the mix of input/output tokens\nand the iteration budget. This makes large-scale benchmarking feasible at moderate cost, while still being\nsignificantly cheaper than manual authoring or annotation.\n\nWe also note that smaller open-source models such as Llama-3.1-8B-Instruct perform well for file\ngeneration. Unlike GPT-4o, which requires API usage, Llama-3.1-8B can be hosted locally and runs\nefficiently on a single NVIDIA A100 40GB GPU. This provides a cost-effective alternative for generating\nlarge numbers of supporting documents, especially when full closed-source quality is not required.\n\nJ WEB AGENTS FOR DEEP RESEARCH TASKS\n\nSince each of the environments can be access directly through a web user interface (UI), we also exper-\nimented with an agent that can directly interact with the webpages through common browser actions like\nclick, input and scroll, which are executed through playwright3. We implement our web agent\nusing the AgentLab and BrowserGym frameworks (Chezelles et al., 2025) with a GPT-4.14 backbone. Our\nagent is implemented from AgentLab’s GenericAgent, which achieves respectable performance when used\n\n3https://playwright.dev\n4https://openai.com/index/gpt-4-1/\n\n30\n\n\fwith GPT-4o5 as a backbone; it completes 45.5% of the tasks in WorkArena (Drouin et al., 2024), 31.4%\nin WebArena (Zhou et al., 2024) and achieves a step-level reward of 13.7% on WebLINX (L`u et al., 2024).\n\nHyperparameters and Prompt We present the hyperparameters for the agent in tables 16 and 17, which\nare in majority set to the default hyperparameters, except for the maximum number of input tokens (bound\nto a reasonable maximum length) and a higher maximum number of steps (to allow the agent to perform\nmore actions required to write the report). We further update the agent’s action space on the last step to\nonly allow it to reply to the user with a report, ensuring that each trajectory terminates with a report. To\nensure that the agent is aware of the tools it can use, we modify the default system prompt (see prompt 6).\nAdditionally, each task intent is provided alongside information about the user and company (see prompt 7).\n\nResults We find that the GPT-4.1-powered web agent achieves an insights recall and factuality of\n1.11% and 6.67% respectively and a report quality score of 33.07%. Although the high report quality\nindicates that the agent can properly formulate a report, the insights quality is severely limited, with none\nof the claims being backed by useful sources. For example, a DRBench Agent powered by GPT-5 may\nanswer the question What is Lee’s Market’s current food waste reduction rate as of Q2 2024? with An\n8% reduction in food waste in Q2 2024 saved Lee’s Market $1.2 million, indicating that better inventory\ncontrol can yield both safety and financial benefits., which achieves a score of 1.0 for the question. On\nthe other hand, a GPT-4.1-powered agent will provide an unsatisfactory answer, thus achieving an insights\nrecall of 0.0. The most likely cause of this poor performance is the model’s limited capability to properly\ninteract with web interfaces when encountering unfamiliar tools. For instance, the agent may be unfamiliar\nwith the VNC and file browser applications, making it harder for it to correctly select the file it needs\nto use. Moreover, whenever the agent ends up performing an ineffective action (e.g. click on an element\nthat does not trigger any change to the page), it tends to persist by reiterating the same action (see Table\n18), or the same sequence of ineffective actions, despite not achieving anything in the previous steps. As a\nresult, despite a large number of steps, most of the agent’s actions are not helpful towards solving the task.\n\nTable 16: Web Agents Boolean Hyperparameters\n\nValue Flags\n\nTrue\n\nFalse\n\nvision support, use ax tree,\nuse tabs, use focused element, use error logs,\nuse history, use action history, use screenshot,\nuse som, extract visible tag, extract clickable tag,\nuse thinking, use concrete example, use abstract example,\nuse hints, be cautious, add missparsed messages\nuse html, use past error logs, use think history,\nuse diff, filter visible elements only,\nfilter with bid only, filter som only, multiaction,\nstrict, long description, individual examples,\nuse plan, use criticise, use memory, enable chat\n\nK STANDARD ERROR\n\nRestricting to the MinEval subset, we average the results on each task across 3 different runs in Table\n19. We give both the means and standard errors for the insight recall, factuality, distractor avoidance, and\nreport quality.\n\nL COMPLEX MODEL ABLATION RESULTS\n\nExtending our discussion in Section 5.3 to more GPT and Llama models in Table 20, we see that the\nsmaller GPT-5-mini model lags behind but still outperforms earlier closed-source backbones such as\nGPT-4o and GPT-4o-mini, particularly in terms of harmonic mean. In addition, smaller variants of Llama\n\n5https://openai.com/index/hello-gpt-4o/\n\n31\n\n\fW Web Agents System Prompt\n\nYou are an agent trying to solve a web task based on the content of the\npage and user instructions. You can interact with the page and explore,\nand send messages to the user. Each time you submit an action it will\nbe sent to the browser and you will receive a new page.\nYou will be solving tasks that involve Deep Research, by\nnavigating websites and web apps with information useful\nsolving the task. You will need to gather insights from\ndata contained in the services provided and the internet\nto complete your report. You have a maximum of 50 steps\nto complete the task. Before the end, you must use the\naction {send msg to user}, which should contain a final Deep\nResearch report detailing everything the user needs to\nknow.\n\nYou must sustain your claims in files, chats, or emails\nfrom the enterprise environment or in websites you\nsearched. You must provide a citation (or an inline\ncitation, that works too) with the source of those claims\n(e.g. << add citation examples from the documentation\n>>). Do not make up citations if you haven’t retrieved its\ncontent.\n\nHere are some expected agent behavior:\n\n**Enterprise Environment Interaction:**\n- Access Nextcloud files, Mattermost chats, emails, VNC desktop,\netc.\n- Extract relevant information from multiple sources\n- Navigate complex enterprise data landscapes\n\n**Report Requirements:**\n- Synthesize findings into comprehensive research\nreport\n- Include proper citations for all claims (flexible format -\nauto-normalized)\n- Draw meaningful insights and conclusions\n- Ground all statements in available evidence\n\n**Citation Format (Flexible - Auto-Normalized):**\n- Files: ‘quarterly report.pdf‘, ‘shared/budget-analysis.xlsx‘,\n‘Analysis document (reports/analysis.docx)‘\n- URLs: Direct links or ‘[Article Title](https://example.com)‘\n- Emails: ‘Email from alice@company.com on Jan 20,\n2025‘\n- Chat: ‘Mattermost message from john.doe in Compliance team, General\nchannel‘\n\nIf you need navigate the internet (outside of the designated websites),\nyou can use the browser inside noVNC Desktop.\n\nPrompt 6: Extended instructions given to the Deep Research web agent.\n\n32\n\n\fTable 17: Web Agents Non-Boolean Hyperparameters\n\nParameter\n\nValue\n\nchat model.model name\nchat model.max total tokens\nchat model.max input tokens\nchat model.max new tokens\nchat model.temperature\naction.action set.subsets\naction.action set.retry with force true\nflags.max prompt tokens\nflags.max trunc itr\nenv.max steps\n\ngpt-4.1\n32768\n28672\n4096\n0\nwebarena\n\n28672\n20\n50\n\nW Web Agents Task Intent Prompt\n\nHow can Lee’s Market leverage FSMA 204 regulations to enhance food\nsafety and customer trust?\n\nHere is some information about the user:\nID: MNG0003\nFirst name: John\nLast name: Doe\n...\nJustification: As a regulatory affairs manager, John\n...\n\nHere is some information about the company:\nName: Lee’s Market\nAnnual revenue: $500M - $600M\n...\nTarget markets: Asian communities in the U.S. and ...\n\nPrompt 7: Web Agents Task Intent Prompt\n\nTable 18: Web Agents tends to get stuck on cycles of actions, and are unable to backtrack or to restart\nwith a different application.\n\n9. I have been repeatedly clicking\non the “shared” folder (bid ‘194’)\n. . . To proceed with the research\non personalization strategies for\nLee‘s Market, I should open the\n“shared” folder to look for. . .\nclick(‘194’)\n\n10. I have clicked on the “shared”\nfolder multiple times . . .\nI will\ntry clicking on the “shared” folder\nagain to see if it opens and displays\nits contents.\n\nclick(‘190’)\n\n11. I have been repeatedly clicking\non the “shared” folder (bid ‘194’)\nTo proceed, I should try\n. . .\nclicking on the “shared” folder\nagain to attempt to open it and\nview any files . . .\n\nclick(‘194’)\n\n12. I have navigated to the “shared”\nfolder, but I have not yet opened\nit . . . The next logical step is to\nopen the “shared” folder to look\nfor any relevant documents . . .\n\nclick(‘194’)\n\ndegrade metric results further. This further substantiates the claim that larger and more advanced models\ntend to offer a better balance between recall, factuality, and overall report quality.\n\nM QUANTITATIVE RESULTS PER TASK\n\nWe show a detailed breakdown of the insights recall in Table 21, factuality in Table 22, distractor avoidance\nin Table 23 and report quality in Table 24 on the MinEval subset for a variety of models.\n\n33\n\n\fTable 19: DRBA performance with different planning configurations on MinEval. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. Scores are reported for insight recall, factuality, distractor\navoidance, report quality, and the overall harmonic mean.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n.188 ± .038\n.137 ± .04\n.154 ± .03\n.197 ± .03\n.142 ± .04\n.188 ± .05\n\n.665 ± .09\n.622 ± .11\n.705 ± .05\n.604 ± .08\n.504 ± .10\n.691 ± .03\n\n.981 ± .01\n1.00 ± .00\n1.00 ± .00\n.995 ± .39\n.990 ± .39\n1.00 ± .00\n\n.912 ± .004\n.901 ± .006\n.917 ± .004\n.928 ± .005\n.906 ± .007\n.923 ± .006\n\n.448\n.365\n.400\n.454\n.359\n.453\n\nTable 20: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Scores are reported for insight recall, factuality, distractor avoidance, report quality,\nand harmonic mean. Note that higher numbers corresponds to better scores, and the best result on each\nmetric is bolded.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nGPT-5-mini\nGPT-5-mini\nGPT-5-mini\nGPT-4o\nGPT-4o\nGPT-4o\nGPT-4o-mini\nGPT-4o-mini\nGPT-4o-mini\nGPT-OSS-120B\nGPT-OSS-120B\nGPT-OSS-120B\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-70b-Instruct\nLlama-3.1-70b-Instruct\nLlama-3.1-70b-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n25.68\n28.37\n26.57\n17.53\n20.37\n17.31\n13.75\n13.67\n13.08\n22.40\n17.42\n18.31\n17.37\n16.97\n20.16\n18.54\n16.28\n17.02\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n58.76\n58.96\n51.07\n65.43\n62.35\n60.84\n46.68\n55.59\n48.95\n29.24\n27.48\n38.92\n78.91\n79.27\n69.75\n64.64\n69.43\n52.82\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n96.51\n95.81\n94.48\n99.05\n98.57\n98.33\n99.05\n97.14\n97.14\n97.14\n97.14\n98.1\n100.00\n98.10\n97.90\n96.70\n97.62\n98.10\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n84.48\n85.74\n86.28\n92.58\n93.12\n91.62\n84.72\n85.86\n86.04\n84.24\n84.36\n85.44\n90.48\n92.34\n91.26\n85.32\n84.96\n86.16\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n60.21\n63.73\n58.89\n48.47\n53.06\n47.35\n38.33\n39.39\n37.28\n44.82\n38.38\n44.14\n49.78\n48.87\n53.86\n50.08\n46.41\n45.46\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\nN EFFECT OF NUMBER OF ITERATIONS\n\nWe next analyze the effect of varying the number of research loop iterations when using DRBA with\nGPT-5 as the backbone language model. Results for both the baseline configuration without explicit\nplanning and the complex planning setup are shown in Table 25. Overall, increasing the iteration budget\ndoes not guarantee consistent improvements. With no planning, performance initially drops when the\nagent executes more iterations, as additional exploration often introduces noise and distracts from key\ninsights. However, with a larger budget the agent partially recovers, suggesting that a small number of\nadditional iterations can help refine factual grounding, while excessive exploration reduces focus.\n\nFor the complex planning setting, higher iterations improve certain metrics such as factuality, but this\ncomes at the cost of lower insight recall and reduced overall balance. This indicates that while more steps\nallow the agent to verify citations more carefully, they can also lead to fragmented reasoning and overfitting\n\n34\n\n\fFigure 8: Total average Insight Recall scores per model and insight source type computed on all the\nresults available for each model running in Complex Research Plan mode. Insights embedded in enterprise\nsources are more easily retrieved by DRBA in all the models.\n\nto peripheral evidence. The best overall performance emerges at moderate iteration counts, highlighting\nthe importance of carefully tuning the iteration budget rather than simply scaling up the number of steps.\n\nO DATA GENERATION PROMPTS\n\nO.1 COMPANY AND PERSONA DATA GENERATION\n\nIn this section we give the prompts used for company generation 8 and persona generation 9.\n\nW Company Generation Prompt\n\nGenerate a realistic company structure for {company name} in the\n{industry} industry.\n\nCompany size: {size} ({employee range} employees)\n\nThe company should focus on this domain {domain}\n\nEXTERNAL INSIGHTS: {external insights}\n\nReturn ONLY a valid JSON object with this structure:\n{output structure}\n\nMake it realistic for the {industry} industry.\n\nPrompt 8: Company Generation Prompt Template.\n\n35\n\ngpt-5enterprise_fact0.597external_fact0.0deepseek-chat-v3.1enterprise_fact0.472external_fact0.0gpt-5-minienterprise_fact0.444external_fact0.0qwen-2.5-72b-instructenterprise_fact0.417external_fact0.0llama-3.1-405b-instructenterprise_fact0.347external_fact0.0gpt-oss-120benterprise_fact0.333external_fact0.0gpt-4o-minienterprise_fact0.194external_fact0.0llama-3.1-70b-instructenterprise_fact0.194external_fact0.0gpt-4oenterprise_fact0.182external_fact0.0\fTable 21: Mean and standard error of the insight recall metric for the first five tasks, obtained from three\nruns of on DRBench our agent (DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nW Persona Generation Prompt\n\nDR0001\n.222 ± .056\n.222 ± .056\n.278 ± .056\n.111 ± .056\n.222 ± .056\n.000 ± .000\n.111 ± .056\n.167 ± .000\n.111 ± .056\n.000 ± .000\n.056 ± .056\n.000 ± .000\n.111 ± .111\n.056 ± .056\n.000 ± .000\n.167 ± .000\n.222 ± .056\n.111 ± .056\n.167 ± .000\n.056 ± .056\n.167 ± .000\n.167 ± .000\n.278 ± .056\n.167 ± .000\n.222 ± .056\n.111 ± .056\n.167 ± .096\n\nDR0002\n.429 ± .000\n.381 ± .048\n.381 ± .126\n.286 ± .000\n.286 ± .082\n.381 ± .126\n.238 ± .048\n.333 ± .048\n.238 ± .095\n.095 ± .048\n.190 ± .048\n.190 ± .048\n.143 ± .000\n.143 ± .000\n.190 ± .048\n.143 ± .000\n.190 ± .048\n.238 ± .048\n.381 ± .048\n.286 ± .082\n.238 ± .048\n.286 ± .082\n.238 ± .048\n.095 ± .048\n.238 ± .048\n.333 ± .048\n.143 ± .082\n\nDR0003\n.467 ± .033\n.533 ± .033\n.567 ± .067\n.367 ± .033\n.333 ± .033\n.367 ± .067\n.167 ± .033\n.300 ± .000\n.300 ± .100\n.267 ± .033\n.167 ± .067\n.267 ± .033\n.433 ± .033\n.367 ± .033\n.333 ± .067\n.233 ± .088\n.167 ± .033\n.300 ± .058\n.200 ± .000\n.200 ± .058\n.267 ± .033\n.300 ± .058\n.333 ± .033\n.600 ± .058\n.400 ± .058\n.333 ± .088\n.400 ± .058\n\nDR0004\n.519 ± .098\n.370 ± .098\n.370 ± .037\n.222 ± .064\n.296 ± .074\n.370 ± .098\n.185 ± .074\n.148 ± .037\n.111 ± .000\n.185 ± .037\n.148 ± .074\n.074 ± .037\n.222 ± .000\n.148 ± .098\n.111 ± .064\n.185 ± .074\n.111 ± .064\n.148 ± .098\n.074 ± .037\n.185 ± .098\n.074 ± .074\n.259 ± .037\n.148 ± .074\n.370 ± .037\n.259 ± .037\n.259 ± .037\n.333 ± .000\n\nDR0005\n.281 ± .035\n.386 ± .046\n.386 ± .046\n.298 ± .018\n.281 ± .063\n.211 ± .053\n.175 ± .018\n.070 ± .018\n.105 ± .000\n.140 ± .035\n.123 ± .018\n.123 ± .018\n.211 ± .030\n.158 ± .061\n.281 ± .063\n.140 ± .035\n.158 ± .053\n.211 ± .030\n.105 ± .030\n.088 ± .046\n.105 ± .000\n.246 ± .046\n.281 ± .046\n.281 ± .035\n.158 ± .061\n.123 ± .018\n.298 ± .076\n\nGenerate {persona count} diverse employee personas for {company name} in\nthe {industry} industry.\n\nThe personas should focus on this domain: {domain}\n\nCreate diverse roles across seniority levels: Junior, Mid, Senior,\nExecutive\n\nReturn ONLY a valid JSON array with this exact format:\n{output structure}\n\nMake personas realistic with appropriate responsibilities for their\nroles.\n\nPrompt 9: Persona Generation Prompt.\n\nO.2 QUESTION GENERATION\n\nIn this section we give the prompt used to generate our deep research questions 10.\n\n36\n\n\fTable 22: Mean and standard error of the factuality metric for the first five tasks, obtained from our agent\n(DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nSimple\nGPT-4o-mini\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.761 ± .072\n.714 ± .050\n.730 ± .060\n.585 ± .045\n.647 ± .120\n.309 ± .126\n.792 ± .150\n.485 ± .262\n.475 ± .166\n.828 ± .043\n.611 ± .056\n.557 ± .030\n.144 ± .099\n.074 ± .074\n.368 ± .061\n.852 ± .087\n.802 ± .125\n.789 ± .053\n.618 ± .109\n.608 ± .173\n.588 ± .082\n.860 ± .014\n.696 ± .041\n.581 ± .042\n.674 ± .077\n.806 ± .049\n.626 ± .114\n\nDR0002\n.504 ± .075\n.384 ± .076\n.291 ± .094\n.297 ± .119\n.299 ± .056\n.381 ± .161\n.490 ± .110\n.512 ± .131\n.653 ± .097\n.265 ± .133\n.429 ± .092\n.324 ± .169\n.150 ± .035\n.128 ± .072\n.178 ± .078\n.726 ± .158\n.638 ± .202\n.392 ± .154\n.431 ± .160\n.681 ± .069\n.286 ± .108\n.518 ± .085\n.531 ± .086\n.657 ± .024\n.493 ± .109\n.540 ± .174\n.396 ± .056\n\nDR0003\n.866 ± .007\n.848 ± .034\n.782 ± .012\n.705 ± .039\n.624 ± .041\n.699 ± .047\n.827 ± .056\n.813 ± .041\n.704 ± .037\n.800 ± .000\n.622 ± .062\n.580 ± .075\n.386 ± .040\n.410 ± .090\n.564 ± .064\n.803 ± .028\n.800 ± .074\n.792 ± .055\n.684 ± .104\n.800 ± .069\n.686 ± .011\n.818 ± .041\n.922 ± .056\n.838 ± .050\n.866 ± .002\n.741 ± .074\n.723 ± .053\n\nDR0004\n.833 ± .019\n.812 ± .070\n.782 ± .064\n.704 ± .067\n.694 ± .028\n.692 ± .111\n.570 ± .058\n.693 ± .139\n.542 ± .110\n.690 ± .128\n.481 ± .209\n.642 ± .119\n.337 ± .117\n.311 ± .155\n.400 ± .076\n.820 ± .066\n.892 ± .035\n.771 ± .073\n.812 ± .021\n.826 ± .067\n.522 ± .270\n.679 ± .095\n.769 ± .035\n.774 ± .053\n.741 ± .060\n.724 ± .101\n.587 ± .139\n\nDR0005\n.762 ± .077\n.846 ± .029\n.674 ± .121\n.647 ± .098\n.683 ± .020\n.472 ± .114\n.593 ± .204\n.614 ± .121\n.406 ± .020\n.459 ± .235\n.191 ± .046\n.344 ± .144\n.445 ± .051\n.451 ± .080\n.435 ± .190\n.745 ± .022\n.832 ± .083\n.745 ± .100\n.687 ± .073\n.557 ± .143\n.559 ± .240\n.757 ± .057\n.754 ± .082\n.662 ± .098\n.696 ± .060\n.550 ± .148\n.586 ± .055\n\nO.3 PUBLIC SOURCE AND INSIGHT COLLECTION\n\nIn this section we give the prompt used to generate external insights 11. The URLs used for external\ninsight extraction and deep research question creation can be found in Table 26.\n\nO.4\n\nINTERNAL INSIGHT GENERATION\n\nIn this section we give the prompts to generate both internal insights 12 and internal distractors 13.\n\nO.5 FILE GENERATION\n\nIn this section we give the prompts used for generating each of the file types used in our tasks, which\nwe list as follows:\n\n• PDF: Prompts 18, 19, and 20\n\n• Excel: Prompts 21, 22, and 23\n\n• Powerpoint: Prompts 24, 25, and 26\n\n• Email: Prompts 27, 28, and 29\n\n• Chat: Prompts 30, 31, 29, and 33\n\n37\n\n\fTable 23: Mean and standard error of the distractor avoidance metric for the first five tasks, obtained from\nthree runs of on DRBench our agent (DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.857 ± .000\n.857 ± .000\n.905 ± .048\n.905 ± .048\n.857 ± .000\n.857 ± .000\n.952 ± .048\n.952 ± .048\n.952 ± .048\n.952 ± .048\n.857 ± .000\n.857 ± .000\n.857 ± .000\n.857 ± .000\n.905 ± .048\n1.00 ± .000\n.905 ± .048\n.952 ± .048\n.905 ± .048\n.905 ± .048\n.905 ± .048\n.905 ± .048\n.857 ± .000\n.905 ± .048\n1.00 ± .000\n.952 ± .048\n.952 ± .048\n\nDR0002\n.900 ± .058\n1.00 ± .000\n.900 ± .058\n.967 ± .033\n.933 ± .033\n.867 ± .133\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.967 ± .033\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.967 ± .033\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n\nDR0003\n1.00 ± .000\n1.00 ± .000\n.933 ± .000\n.978 ± .022\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.978 ± .022\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.978 ± .022\n\nDR0004\n1.00 ± .000\n1.00 ± .000\n.905 ± .024\n.976 ± .024\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.964 ± .036\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.952 ± .048\n.976 ± .024\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.929 ± .000\n.905 ± .024\n.952 ± .024\n.952 ± .048\n\nDR0005\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n\nP EVALUATION PROMPTS\n\nIn this section we give the prompts for decomposing reports into atomic insights 14, computing insight\nrecall 15, computing factuality 16, and computing report quality 17. These prompts are discussed in detail\nin Section 5.1.\n\nQ HUMAN PREFERENCE EVALUATION\n\nWe calculate a human score for model a task t as:\n\nSa,t = 1\nn\n\n(cid:80)n\n\ni=1sa,i, where sa,i =\n\n(cid:26)1,\n\nif human choice is a or ”both good”\n\n0, otherwise\n\n, where n is the number of gold insights in task t, sa,i is the human score of model a on insight i. Figure9\nshows that the insight recall metric is on par with human decision.\n\n38\n\n\fTable 24: Mean and standard error of the report quality metric for the first five tasks, obtained from three\nruns of DRBench with 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.936 ± .008\n.942 ± .000\n.948 ± .007\n.892 ± .002\n.901 ± .009\n.896 ± .001\n.927 ± .000\n.934 ± .008\n.929 ± .009\n.886 ± .004\n.893 ± .002\n.889 ± .003\n.872 ± .007\n.878 ± .006\n.874 ± .007\n.914 ± .000\n.921 ± .008\n.917 ± .009\n.889 ± .003\n.895 ± .002\n.891 ± .003\n.884 ± .004\n.890 ± .003\n.886 ± .004\n.901 ± .001\n.908 ± .000\n.904 ± .001\n\nDR0002\n.918 ± .004\n.927 ± .008\n.921 ± .001\n.884 ± .007\n.889 ± .002\n.882 ± .006\n.911 ± .003\n.919 ± .000\n.914 ± .002\n.874 ± .008\n.881 ± .005\n.877 ± .006\n.861 ± .001\n.867 ± .009\n.863 ± .000\n.903 ± .003\n.910 ± .001\n.906 ± .002\n.877 ± .007\n.883 ± .005\n.879 ± .006\n.872 ± .008\n.878 ± .006\n.874 ± .007\n.889 ± .005\n.896 ± .003\n.892 ± .004\n\nDR0003\n.924 ± .005\n.936 ± .006\n.940 ± .008\n.879 ± .004\n.887 ± .001\n.884 ± .003\n.903 ± .001\n.911 ± .009\n.905 ± .000\n.861 ± .007\n.867 ± .004\n.864 ± .005\n.849 ± .009\n.854 ± .008\n.851 ± .008\n.897 ± .001\n.904 ± .000\n.899 ± .001\n.869 ± .005\n.874 ± .004\n.871 ± .005\n.864 ± .006\n.870 ± .005\n.866 ± .006\n.881 ± .002\n.888 ± .001\n.884 ± .002\n\nDR0004\n.909 ± .001\n.915 ± .002\n.922 ± .009\n.891 ± .000\n.895 ± .008\n.889 ± .009\n.918 ± .009\n.923 ± .001\n.920 ± .008\n.872 ± .002\n.878 ± .001\n.875 ± .000\n.858 ± .006\n.863 ± .004\n.860 ± .005\n.909 ± .008\n.915 ± .009\n.911 ± .008\n.881 ± .001\n.886 ± .000\n.883 ± .001\n.876 ± .002\n.881 ± .001\n.878 ± .002\n.893 ± .009\n.899 ± .008\n.895 ± .009\n\nDR0005\n.927 ± .009\n.933 ± .007\n.929 ± .008\n.886 ± .005\n.892 ± .003\n.890 ± .001\n.909 ± .002\n.916 ± .000\n.913 ± .009\n.879 ± .006\n.884 ± .003\n.882 ± .004\n.866 ± .008\n.872 ± .007\n.869 ± .006\n.902 ± .002\n.908 ± .000\n.905 ± .001\n.873 ± .004\n.878 ± .003\n.875 ± .004\n.869 ± .005\n.874 ± .004\n.871 ± .005\n.885 ± .003\n.891 ± .002\n.888 ± .003\n\nTable 25: Effect of the number of research loop iterations on the performance of DRBA with GPT-5 as\nthe backbone model, on MinEval.\n\nPlanning Method\nNone\nNone\nNone\nComplex\nComplex\nComplex\n\n# Iterations\n15\n30\n50\n15\n30\n50\n\nInsight Recall\n39.45\n28.80\n37.10\n44.44\n31.61\n38.16\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n72.65\n69.03\n78.84\n62.51\n73.94\n66.05\n\n93.14\n98.57\n100.00\n90.95\n94.38\n94.38\n\n94.56\n96.12\n93.48\n93.12\n94.76\n92.64\n\n66.20\n57.34\n66.30\n66.41\n60.32\n63.76\n\n39\n\n\fW Deep Research Question Generation Prompt\n\nGenerate 3 Deep Research (DR) questions for the following business\ncontext:\n\nPersona: {persona name} - {persona role}\nDepartment: {persona department}\nResponsibilities: {persona responsibilities}\nCompany: {company name} ({company industry})\nDomain: {domain}\n\nExternal Insights: {external insights}\n\nGenerate 3 Deep Research questions that:\n1. Are appropriate for the persona’s role and department\n2. Require analysis of the provided internal insights\n3. Consider the external market context ...\n\nEach question should be 1 sentence of 15 words max, in\nplain english, and end with a question mark. Do not\ninclude any preamble or explanation - return only the JSON\narray.\n\nReturn ONLY a valid JSON array with this structure:\n{output structure}\n\nPrompt 10: Deep Research Question Generation Prompt Template. Subquestions are generated to help\nhuman annotators select good DR questions.\n\nW External Insight Extraction Prompt\n\nYou will be given a report (with url, and date). Based\non the report, generate 3 external insights that summarize\nimportant findings, trends, or takeaways from the\nreport.\n\nOutput Format\n{output structure}\n\nUrl: {url}\nIndustry: {industry}\nDomain: {domain}\nCompany Information: {company information}\n\nImportant notes\nFocus only on insights that are external and grounded in the report.\nInsights should be concise, factual, and directly tied to the retail\nindustry context.\n\nPrompt 11: External Insight Extraction Prompt.\n\n40\n\n\fTable 26: Public URLs For Deep Research Task Creation.\n\nIndustry\n\nDomain\n\nReference\nGrocers on FSMA-204 Compliance (GroceryDive) 2\nGrocery Loyalty & Inflation (EagleEye) 2\nGrocery Trends Outlook 2025 (GroceryDive) 2\nRetail IT Optimization (Thirdera) 2\nChatbots & Grocery Interactions (GroceryDoppio) 2\n\nTelehealth Regulations (HealthcareDive) 2\nFuture of Healthcare CRM (WTT Solutions) 2\nFuture of Telehealth (CHG Healthcare) 2\nHealthcare ITSM (Topdesk) 2\nPatient Engagement Tech (TechTarget) 2\n\nKnowledge Mgmt Retail Knowledge Management (Knowmax) 2\n\nSales\nCybersecurity\nPublic Relations Walmart CSR Strategy (SunriseGeek) 2\n\nPersonalization in Action (BCG) 2\nRetail Cybersecurity Threats (VikingCloud) 2\n\nCompliance\nCRM\nMarket Analysis\nITSM\nCSM\n\nCompliance\nCRM\nMarket Analysis\nITSM\nCSM\n\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles Quality Assurance EV Quality Issues (GreenCars) 2\nAutomobiles\nAutomobiles Market Analysis\n\nSales\nMarketing\nCybersecurity\nCompliance\nCRM\nCSM\nSales\nResearch\nCybersecurity\n\nAsset Mgmt\n\nSales for Digital Health (Medium) 2\nMarketing Telehealth Services (MarketingInsider) 2\nHealthcare Cybersecurity 2024 (AHA) 2\nEvolving EV Regulations (WardsAuto) 2\nSalesforce Automotive Cloud (TechTarget) 2\nEV Aftersales Support (EVReport) 2\nTesla vs Dealerships (TheWeek) 2\nAI for EV Optimization (Here.com) 2\nCybersecurity Risks in Cars (HelpNetSecurity) 2\n\nDigital Twins in Autos (RTInsights) 2\nGlobal EV Outlook 2024 (IEA) 2\n\nKnowledge Mgmt Knowledge Mgmt in Healthcare (C8Health) 2\n\nFigure 9: Comparison of Human Scores and Insight Recall Scores. As can be seen the human evaluation\nresults are aligned with our automated evaluation.\n\n41\n\nDR0001DR0002DR0003DR0004DR0006DR0007DR0008DR0011DR0012DR0013DR0014Task ID0.00.20.40.60.81.0Score ValueHuman Scores vs Insights Recallllama-3.1-405b (Human Score)gpt-5 (Human Score)llama-3.1-405b (Insights Recall)gpt-5 (Insights Recall)\fW Internal Supporting Insight Generation Prompt\n\nBased on the Deep Research (DR) Question, external\nmarket insights, company context, and previous internal\ninsights, generate 3 specific QA pair that an expert data\nscientist would need to get in order to address the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for inspiration): {external context}\nQA History: {qa list}\n\nInsight Requirements:\n- Use the DR Question as the central theme for the\nanswer.\n- Draw inspiration and supporting details from the other internal\ninsights and external insights provided.\n- Include QUANTITATIVE DATA in the answer: metrics, percentages, dollar\namounts, timeframes, KPIs.\n\nSpecific Question Instructions\n- the specific question should be a question that would be a step\ntowards resolving the DR Question\n{additional question instructions}\n\nAnswer Instructions\n- the answer should be 12 words max and minimum 5\nwords\n{additional answer instructions}\n\nJustification Instructions\n- the justification should directly explain how the specific question,\nanswer pair help address the DR Question in 15 words\nmax\n\nMisc Instructions\n- the filename should be 3 words max with dashes\nin between, do not mention the file type in the\nfilename\n- use the example below as inspiration but do not use it\ndirectly\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nPrompt 12: Internal Supporting Insight Generation Prompt Template. specific questions and justification\nhelp human annotators to select good insights.\n\n42\n\n\fW Internal Distractor Insight Generation Prompt\n\nBased on the Deep Research (DR) Question, external\nmarket insights, company context, and previous internal\ninsights, generate 3 specific QA pairs that are DISTRACTOR\nquestions\n- these should be questions that an expert data scientist might\nask about the company but are NOT relevant to addressing the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for inspiration): {external context}\nQA History: {qa list}\n\nDISTRACTOR Requirements:\n- Generate questions that are plausible for this\ncompany and industry but DO NOT help address the DR\nQuestion.\n- The questions should be about the company’s operations, metrics, or\nbusiness but tangential to the DR Question.\n- Include QUANTITATIVE DATA in the answer: metrics, percentages, dollar\namounts, timeframes, KPIs.\n- Focus on different business areas that are NOT central to the\nDR Question (e.g. if DR Question is about pricing, ask about HR\nmetrics).\nSpecific Question Instructions\n- the specific question should be a plausible business\nquestion for this company but NOT related to the DR\nQuestion\n- the specific question should lead to a quantitative answer and should\nbe dated such as Q3 of 2025, the question should contain a date like Q2\nof 2024\n- the specific question should be 10 words max\n- make sure to be different from any question in the\nqa list\n- choose business areas like: HR metrics, facility costs, IT\ninfrastructure, compliance, training, etc. that are UNRELATED to the\nDR Question\n- make sure to be different from any question in the QA\nHistory\n\nAnswer Instructions {answer instructions}\n\nJustification Instructions\n- the justification should explain why this specific question\nis NOT relevant to addressing the DR Question in 15 words\nmax\n\nMisc Instructions {misc instructions}\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nPrompt 13: Internal Distractor Insight Generation Prompt. specific questions and justification help human\nannotators to select good insights.\n\n43\n\n\fW Breaking Report into Insights Prompt\n\nPlease break down the following report text into insight claims. Each\ninsight claim should be:\n\n1. A single insight, that might include multiple statements and\nclaims\n2. Independent and self-contained\n3. Each claim can have more than one sentence, but should be focused\non a single insight\n4. Support each insight with citations from the report text following\nthese specific rules: {rules}\n5. Citations should be in one of these formats (various formats will\nbe automatically normalized):\n{citation formats} 6. Do not include general summaries, opinions,\nor claims that lack citation, just the sentences that are\nfacts.\n7. Each claim should be a concise but complete\nsentence.\n\nReport text: {report text}\n\nOutput format: Please return the insight claims as a JSON array. For\nexample: {output structure}\n\nReturn only valid JSON, no additional text. Just use the report\nbetween <START OF REPORT> and <END OF REPORT> tags to generate\ninsights. If no insights found, return an empty JSON array:\n[]\n\nDo not use the example outputs as report content.\n\nPrompt 14: Insight Extraction Prompt.\n\n44\n\n\fW Insight Recall Prompt\n\nYour goal is to check if one of the Predicted Insights\nextracted from a report is a Golden Insight. You\nmust be STRICT and pay attention to every small\ndetail.\n\nInstructions:\n* Evaluate if the Predicted Insights contain sufficient information to\nderive a Golden Insight.\n* Select the insight that most closely matches the Golden Insight.\nSelect one and only one insight.\n* Answer of yes or no where:\n- yes: Selected insight contains comprehensive information to fully\nderive the expected insight\n- no: Selected insight lacks the necessary information, misses key\ndetails, or has significant gaps\n* Be STRICT - do not answer yes for partial matches, vague similarities,\nor general information.\n\nHowever, no exact wording is required and paraphrasing is\nacceptable.\n* IMPORTANT: Only consider details given in the Golden Insight when\nanswering yes or no. Don’t expect anything more than what is given in\nthe Golden Insight.\n* Focus on factual accuracy, completeness, and specificity.\n\nPredicted Insights: {claims text}\nGolden Insight: {gold insight}\n\nReturn a valid json dictionary with the following structure:\n{output structure}\n\nEnsure only a json dictionary is returned, and return nothing else.\n\nPrompt 15: Insight Recall Scoring Prompt.\n\n45\n\n\fW Factuality Prompt\n\nGiven the following relevant source context from multiple sources and\nan insight, determine if the insight is factually supported by the\nsources.\n\nRelevant Source Materials (from multiple sources):\n{context}\nAtomic Claim: {insight}\n\nEVALUATION CRITERIA:\nThe claim is factual if the core factual content\nis supported by the sources. You should be strict\nabout important details but flexible about exact\nwording:\nREQUIRED for TRUE:\n1. All key factual details (numbers, dates, names,\npercentages, specific facts) must be present in at least one\nsource\n2. The main substance and meaning of the claim must be supported by\nthe source contexts\n3. No part of the claim should contradict the information in any of\nthe sources\nACCEPTABLE variations:\n{acceptable variations}\nMark as FALSE if:\n- Important factual details are missing, incorrect, or unsupported\nacross all sources\n- The claim contradicts information in any of the\nsources\n- The core meaning cannot be verified from any of the source\ncontexts\n\nEXAMPLES: {examples}\n\nFocus on the substantive factual accuracy rather than\nexact word-for-word matching. You MUST respond with either\ntrue or false under the <factual> tag. Then provide a\nbrief explanation under the <explanation> tag explaining\nwhich parts are supported or not supported and from which\nsources.\n\nFormat your response EXACTLY as:\n{output structure}\n\nPrompt 16: Factuality Scoring Prompt.\n\n46\n\n\fW Report Quality Prompt\n\nYou are a Deep Research Evaluator. You are given:\n1. A research report.\n2. A deep research (DR) question that the report attempts to\nanswer.\n3. A persona that represents the intended audience for the\nreport.\n\n{persona}\n{dr question}\n{report text}\n\n## Instructions:\n**ANALYZE THOROUGHLY**: Examine the report in detail\nand identify any issues, even small ones. Look for\nsubtle problems, minor inconsistencies, areas that could\nbe improved, or any shortcomings that might affect\nthe quality. Evaluate the report according to the\nfive criteria listed below. For **each criterion**,\nprovide:\n- A **score between 1 and 10** (must be an integer) using the scale\ndefined below.\n- A **detailed justification** (2–3 sentences) in\n**simple plain English** explaining why you gave that\nscore, including any specific issues or strengths you\nidentified.\n\n### Scoring Scale (1-10, integers only): {scoring scale}\n\n### Criteria:\n\n1. Depth & Quality of Analysis\n2. Relevance To DR Question\n3. Persona Consistency\n4. Coherence & Conciseness\n5. Degree of Contradictions\n6. Completeness & Coverage\n\n{output structure}\n\nPrompt 17: Report Quality Scoring Prompt.\n\n47\n\n\fW PDF Outline Generation Prompt\n\nYou are an expert business document designer creating\nrealistic enterprise PDF reports. Given a Deep\nResearch (DR) Question and company context, generate\nan outline for a professional business document that\nan employee would create based on their persona and\nrole.\n\nCompany: {company name} - {company description}\nIndustry: industry\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context:{persona context}\nDR Question: {dr question}\n\nDocument Structure Requirements:\n- Create a professional PDF document outline with exactly {n subsections}\nsubsections\n- The document should be something this persona would realistically\ncreate in their role\n- Include a concise, professional file title appropriate for enterprise\ndocumentation\n\nSubsection Heading Requirements: {subsection requirements}\n\nIntroduction Requirements:\n- Write a professional 4-sentence maximum introduction\nparagraph - Should set context for the document and its\npurpose\n- Must align with the persona’s role and the company’s business\nneeds\n- Should sound like something this employee would write for internal\nstakeholders\n\nConclusion Requirements:\n- Write a professional 4-sentence maximum conclusion\nparagraph\n- Should summarize key takeaways and next steps\n- Must align with the persona’s perspective and recommendations\n- Should provide actionable insights for the intended\naudience\n\nReturn ONLY a valid Python dictionary with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Ensure the document feels authentic for this persona´s role and company\ncontext\n\nPrompt 18: PDF Outline Generation Prompt. This is the first step of embedding an insight into a PDF\ndocument. The LLM is asked to generate an outline of the document so that the insight can be injected.\n\n48\n\n\fW PDF Insight Injection Prompt\n\nYou are an expert business document writer creating\nrealistic enterprise PDF content. Given a Deep\nResearch (DR) Question, company context, and a specific\ninsight, generate professional content that naturally\nincorporates the insight information to help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nSubsection Heading: {subsection heading}\n\nContent Generation Requirements:\n- Generate realistic business content for the given subsection\nheading\n- The content must contain exactly ONE paragraph of 4-5\nsentences\n- Content should be professional and sound like something this persona\nwould write\n- The paragraph must naturally incorporate the insight answer\ninformation but NOT copy it word-for-word\n{additional generation requirements}\n\nContent Strategy:\n- Present the insight information as business findings, analysis results,\nor operational data\n- Embed the key metrics within broader business context and\nimplications\n- Use natural business language to discuss the same information as in\nthe answer\n{additional content requirements}\n\nJustification Requirements:\n- Explain specifically how this content helps answer the DR\nQuestion\n- Reference the key information that would be useful for\ndecision-making\n- Keep justifications concise but clear (20 words\nmaximum)\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 19: PDF Insight Injection Prompt. This is the second step of embedding an insight into a PDF\ndocument. The LLM is fed with a subheading in the outline from 18, and tasked to write the subsection\nwith the insight embedded.\n\n49\n\n\fW PDF Irrelevant Section Generation Prompt\n\nYou are an expert business document writer creating\nrealistic enterprise PDF content. Given a Deep Research\n(DR) Question, company context, and subsection headings,\ngenerate distractor content for each subsection that is\nthematically related but does NOT help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: industry\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\nSubsection Headings: {subsection headings}\n\nContent Generation Requirements:\n- Generate realistic business content for each subsection\nheading\n- Each subsection must contain exactly ONE paragraph of 3-4 sentences\nmaximum\n- Content should be professional and sound like something this persona\nwould write\n- Content must be thematically related to the DR\nQuestion’s domain but NOT provide information to answer\nit\n{additional generation requirements}\n\nDistractor Strategy:\n- Focus on adjacent business areas that don’t directly impact the DR\nQuestion\n- Discuss historical context, general industry trends, or procedural\ninformation\n- Include operational details that are realistic but tangential\n- Reference related but non-essential business metrics or\nactivities\n- Avoid any content that would help someone answer the DR\nQuestion\n\nJustification Requirements:\n- Explain specifically why each paragraph’s content doesn’t help answer\nthe DR Question\n- Identify what type of distractor strategy was used\n(e.g., \"focuses on historical data vs current decision\nfactors\")\n- Keep justifications concise but clear (15 words\nmaximum)\n\nReturn ONLY a valid JSON array with this exact structure:\n{output structure}\n\nIMPORTANT: {important instructions}\n\nPrompt 20: PDF Irrelevant Section Generation Prompt. This is the third step of PDF document generation.\nThe LLM is asked to fill out the outline from 18 with irrelevant information.\n\n50\n\n\fW Excel Schema Generation Prompt\n\nGenerate the JSON schema and formatting of a table where the\nfollowing insight can be presented as a row in the table:\n{insight}\n\nThe schema and formatting should contain:\n- table name: a string of the table name\n- columns: a list of columns with the following\nfields:\n- name: column name\n- column type: one of\nSTRING|INTEGER|FLOAT|BOOLEAN|DATE|PERCENTAGE|CURRENCY\n- description: detailed description of what this column represents and\nhow it relates to the insight\n- formatting: a dictionary with the following fields:\n- header style: background color of the header, default is\nCCCCCC\n- column widths: width of each column, e.g. {{A: 15, B: 20, C:\n12}}\n- number formats: format of each column, e.g. {{A: \"0.00%\", B:\n\"$#,##0.00\", C: \"YYYY-MM-DD\"}}\nReturn only a json dictionary with the table name, columns, and\nformatting.\n\nRequirements:\n- The schema should be designed so that the insight can be represented\nas a row in the table\n- The schema should make it easy to generate more data points expanding\non the subject, theme and scope of the insight to populate the\ntable.\n- Use realistic column names that would be found in a business\nspreadsheet\n- Do not include the insight, specific question, or justification as\ncolumns in the table\n\nCompany Context: {company info}\nPlease keep this company context in mind when creating the\nschema.\n\nPersona Context: {persona}\n\nPlease keep this persona context in mind when creating the schema.\n\nPrompt 21: Excel Schema Generation Prompt. This is the first step of Excel file generation. The LLM\nis asked to generate the schema of the Excel file so that the insight can be injected.\n\n51\n\n\fW Excel Data Generation Prompt\n\nGiven the following schema: {schema and formatting}\nGenerate one row that embeds the following insight:\n{insight}\n\nThen generate 5-10 rows of data that populates the table.\nMake sure that with the new data added, the original\ninsight can still be extracted from the table. Return\nall the rows in a json dictionary with the following\nfields:\n- insight row: a list of values each corresponding to a column in the\nschema\n- irrelevant rows: a list of rows that are used to populate the table,\neach row is a list of values each corresponding to a column in the\nschema\n\nEnsure only a json dictionary is returned, and return nothing\nelse.\n\nRequirements:\n- Make sure the insight row stands out from the irrelevant rows by\ne.g.\n- Having the largest value\n- Covering the most recent timeframe\n\nPrompt 22: Excel Data Generation Prompt. This is the second step of Excel file generation. The LLM\nis asked to generate the data for an Excel file that the insight will be injected into.\n\nW Excel Filename Generation Prompt\n\nGenerate a professional filename for an Excel file that contains the\nfollowing sheets: {sheet names}\n\nThe filename should:\n1. Be descriptive and professional\n2. Reflect the main theme or purpose of the data\n3. Be suitable for a business environment\n4. Not exceed 50 characters\n5. Use only alphanumeric characters, spaces, hyphens, and\nunderscores\n6. Not include file extensions (like .xlsx)\nReturn only the filename, no additional text or\nquotes.\n\nCompany name:\n{company name}\n\nPlease keep this company name in mind when creating the filename.\n\nPrompt 23: Excel Filename Generation Prompt. This is the third step of Excel file generation. The LLM\nis asked to generate the filename for the Excel file that the insight will be injected into.\n\n52\n\n\fW Powerpoint Outline Generation Prompt\n\nYou are an expert business presentation designer creating\nrealistic enterprise PowerPoint presentations. Given a\nDeep Research (DR) Question and company context, generate\nan outline for a professional business presentation that\nan employee would create based on their persona and\nrole.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\n\nPresentation Structure Requirements:\n- Create a professional PowerPoint presentation outline with exactly\n{n subsections} slides\n- The presentation should be something this persona would realistically\ncreate in their role\n- Include a concise, professional presentation title appropriate for\nenterprise presentations\n\nSlide Heading Requirements:\n- Slide headings must follow the THEME of the DR\nQuestion but should NOT directly address the DR Question\nitself\n- Think of related business areas, adjacent topics, or\nsupporting themes that would naturally appear in an enterprise\npresentation\n- Headings should sound professional and realistic for this industry and\ncompany size\n- Each heading should be 3-8 words and use proper business\nterminology\n{additional slide requirements}\n\nConclusion Requirements:\n- Write a professional 2-sentence maximum conclusion for the\npresentation closing\n- Should summarize key takeaways and next steps\n- Must align with the persona’s perspective and recommendations\n- Should provide actionable insights for the intended\naudience\n\nReturn ONLY a valid Python dictionary with this exact\nstructure:\n{output structure}\n\nIMPORTANT: {important notes}\n\nPrompt 24: Powerpoint Outline Generation Prompt. This is the first step for generating powerpoint slides.\nThe LLM is asked to generate an outline of the slides so that the insight can be injected.\n\n53\n\n\fW Powerpoint Insight Injection Prompt\n\nYou are an expert business presentation writer creating\nrealistic enterprise PowerPoint content. Given a Deep\nResearch (DR) Question, company context, and a specific\ninsight, generate professional slide content that naturally\nincorporates the insight information to help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nSlide Heading: {subsection heading}\n\nContent Generation Requirements:\n- Generate realistic business content for the given slide\nheading\n- The content must contain exactly 5-8 bullet points with substantial\ndetail\n- Each bullet point should be 1-2 sentences with specific business\ninformation\n{additional generation requirements}\n\nContent Strategy:\n- Present the insight information as business findings, analysis results,\nor operational data\n- Embed the key metrics within broader business context and\nimplications\n- Use natural business language to discuss the same information as in\nthe answer\n{additional content requirements}\n\nJustification Requirements:\n- Explain specifically how this content helps answer the DR\nQuestion\n- Reference the key information that would be useful for\ndecision-making\n- Keep justifications concise but clear (25 words\nmaximum)\n\nReturn ONLY a valid JSON object with this exact\nstructure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 25: Powerpoint Insight Injection Prompt. This is the second step for generating powerpoint slides.\nThe LLM is asked to generate slide content with the insight embedded.\n\n54\n\n\fW Powerpoint Distractor Injection Prompt\n\nYou are an expert business presentation writer creating\nrealistic enterprise PowerPoint content. Given a Deep\nResearch (DR) Question, company context, and slide\nheadings, generate distractor content for each slide that\nis thematically related but does NOT help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\nSlide Headings: {subsection headings}\n\nContent Generation Requirements:\n- Generate realistic business content for each slide heading - Each\nslide must contain exactly 5-8 bullet points with substantial\ndetail\n- Each bullet point should be 1-2 sentences with specific business\ninformation\n- Content should be professional and sound like something this persona\nwould present\n{additional generation requirements}\n\nDistractor Strategy:\n- Focus on adjacent business areas that don’t directly impact the DR\nQuestion\n- Discuss historical context, general industry trends, or procedural\ninformation\n- Include operational details that are realistic but tangential\n- Reference related but non-essential business metrics or\nactivities\n{additional content requirements}\n\nReturn ONLY a valid JSON array with this exact structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 26: Powerpoint Distractor Injection Prompt. This is the third step for generating powerpoint slides.\nThe LLM is asked to generate slide content with distractor information.\n\n55\n\n\fW Email Setup Prompt\n\nYou are an expert in enterprise communication systems and organizational\nstructures. Your task is to generate a realistic setup of users\nfor an email system based on the given insights and company\ncontext.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Size: {company size} ({employee count} employees)\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\n**Specific Question** {specific question}\n**Answer to Specific Question** {answer}\n\nRequirements:\n- Users that would realistically discuss these insights\n- Generate a minimal but sufficient setup to support {num messages}\nemails discussing the insights\n- To make it realistic, generate at least 3 users\n\n- Use realistic names for people/teams/channels based on the company\ncontext\n\nReturn ONLY a JSON array of users with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Do NOT include any preamble, explanation, or extra text|return only\nthe Python dictionary\n- Ensure the structure is realistic for the company size and industry\n- Make sure the persona is included as a user\n\nPrompt 27: Email Setup Prompt. This is the first step for generating an email chain. The LLM is asked\nto generate the necessary setup for the email chain.\n\n56\n\n\fW Email Insight Injection Prompt\n\nYou are an expert at creating realistic business email\nconversations. Your task is to create an email thread that\ncontains the actual insight that helps answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\nDeep Research Question: {dr question}\nEmail Setup: {email setup}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create a realistic email thread of {num messages} that contains the\ntarget insight\n2. This thread should provide information that directly helps answer\nthe DR question\n3. The insight should be naturally embedded in the email\ncontent\n4. The emails should feel realistic and business-appropriate\n5. The sender should be someone who would naturally have access to\nthis insight\n6. The persona needs to be either a recipient or the sender of any\nemail\n\nContent Strategy:\n- The thread should discuss the specific question\nand provide the answer as part of a natural business\nconversation\n- Include the justification as supporting context or\nreasoning\n- Make the insight feel like a natural part of the email, not\nforced\n- The content should be directly relevant to answering the DR\nquestion\n- Use realistic business language and formatting\nExample approaches: {example approaches}\nOutput Format: Return ONLY a JSON array with the following structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 28: Email Insight Injection Prompt. This is the second step for generating an email chain. The\nLLM is asked to insert an insight into the email chain.\n\n57\n\n\fW Email Distractor Injection Prompt\n\nYou are an expert at creating realistic business email conversations.\nYour task is to create {num messages} emails that discuss topics\nrelated to the company but will NOT help answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\nDeep Research Question: {dr question}\nEmail Setup: {email setup}\n\nRequirements:\n1. Create {num messages} realistic email messages between the\nusers\n2. These emails should discuss business topics\nthat are thematically related to the company but\nDO NOT provide information that helps answer the DR\nquestion\n3. Each email should have a realistic subject line, sender, recipients,\nand content\n4. The conversations should feel natural and business-appropriate\n5. Topics should be relevant to the company’s operations but unhelpful\nfor the DR question\n\nContent Strategy:\n- Focus on daily business operations, team collaboration, projects, and\ncompany processes\n- Include realistic business language, project updates, and operational\ndiscussions\n- Avoid topics that directly relate to the DR question or would provide\ninsights for it\n- Make the content engaging and realistic while being intentionally\nunhelpful\n\nExample topics to discuss (but should NOT help answer the DR\nquestion):\n{example topics}\n\nOutput Format: Return ONLY a JSON array with the following structure:\n{output structure}\n\nIMPORTANT: - Return ONLY the JSON array, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n- Make sure the persona is either a recipient or the sender of any email\n\nPrompt 29: Email Distractor Injection Prompt. This is the third step for generating an email chain. The\nLLM is asked to insert distractor information into the email chain.\n\n58\n\n\fW Chat Setup Prompt\n\nYou are an expert in enterprise communication systems\nand organizational structures. Your task is to generate\na realistic setup for teams, channels, and users for a\nMattermost chat system based on the given insights and company\ncontext.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Size: {company size} ({employee count} employees)\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\n**Specific Question** {specific question}\n**Answer to Specific Question** {answer}\n\nRequirements:\n- Generate teams, channels, and users that would realistically discuss\nthese insights\n- Make sure the teams and channels are realistic for persona to be a\nmember\n- Each channel must be associated with a team\n- Each user must be a member of at least one team and one\nchannel\n- Generate a minimal but sufficient setup to support {num turns} chat\nmessages discussing the insights\n- To make it realistic, generate at least 2 teams, 2 channels and 3\nusers\n- Use realistic names for people/teams/channels based on the company\ncontext\n- The persona needs to be part of all teams and channels\n\nReturn ONLY a valid Python dictionary with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Do NOT include any preamble, explanation, or extra text|return only\nthe Python dictionary\n- Make sure the persona is included as a user and member of all\nteams/channels\n- Reuse the username of the persona as provided in the persona context\n\nPrompt 30: Chat Setup Prompt. This is the first step for generating a Mattermost chat. The LLM is asked\nto generate the necessary setup for the chat system.\n\n59\n\n\fW Chat Insight Injection Prompt\n\nYou are an expert at creating realistic business chat\nconversations. Your task is to create a chat conversation\nthat contains an insight that helps answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDR Question: {dr question}\nChat Setup: {chat setup}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create a realistic chat conversation (could be multiple messages)\nthat contains the target insight\n2. This conversation should provide information that directly helps\nanswer the DR question\n3. The insight should be naturally embedded in the message\ncontent\n4. The conversation should feel realistic and business-appropriate\n5. The sender should be someone who would naturally have access to\nthis insight\n6. Use the teams, channels, and users from the chat setup\nonly\n\nContent Strategy:\n- The conversation should discuss the specific question\nand provide the answer as part of a natural business\nconversation\n- Include the justification as supporting context or\nreasoning\n{additional content requirements}\n\nExample approaches: {example approaches}\n\nOutput Format: Return ONLY a JSON array of the chat messages with the\nfollowing structure: {output structure}\n\nIMPORTANT:\n- Return ONLY the JSON object, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n\nPrompt 31: Chat Insight Injection Prompt. This is the second step for generating a Mattermost chat. The\nLLM is asked to insert an insight into the chat system.\n\n60\n\n\fW Chat Distractor Injection Prompt\n\nYou are an expert at creating realistic business chat\nconversations. Your task is to create {num turns}\nchat messages that discuss topics related to the\ncompany but will NOT help answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDeep Research Question: {dr question}\nChat Setup: {chat setup}\n\nRequirements:\n1. Create {num turns} realistic chat messages between the\nusers\n2. These messages should discuss business topics\nthat are thematically related to the company but\nDO NOT provide information that helps answer the DR\nquestion\n3. Each message should have a realistic sender, channel, and\ncontent\n4. The conversations should feel natural and business-appropriate\n5. Topics should be relevant to the company’s operations but unhelpful\nfor the DR question\n6. Use the teams, channels, and users from the chat setup\nonly\n\nContent Strategy:\n- Focus on daily business operations, team collaboration, projects, and\ncompany processes\n- Include realistic business language, project updates, and operational\ndiscussions\n{additional content requirements}\n\nExample topics to discuss (but should NOT help answer the DR\nquestion):\n{example topics}\n\nOutput Format:\nReturn ONLY a JSON array of the chat messages with the following\nstructure: {output structure}\n\nIMPORTANT:\n- Return ONLY the JSON array, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n\nPrompt 32: Chat Distractor Injection Prompt. This is the third step for generating a Mattermost chat. The\nLLM is asked to generate distractor information to insert into the chat system.\n\n61\n\n\fW Chat Confidential Info Prompt\n\nYou are an expert at inserting confidential information\ninto chat conversations. Your task is to create business\nconfidential information that is irrelevant to the Deep\nResearch question or the specific question and insert\nit into existing chat conversations in a realistic\nway.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDR Question: {dr question}\nChat Setup: {chat setup}\nChat Conversation: {chat messages}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create business confidential information that are secrets\nwithin the company and should not be revealed to the outside\nworld\n2. Create 1-2 new chat messages that contain the confidential\ninformation\n3. Insert the new messages into the chat conversation and return the\nnew chat conversation\n4. Also return the confidential information and the\nkeywords\n\nConfidential Information Content Strategy:\n- The messages containing the confidential information should not be\nnecessary to address the DR question or the specific question of the\ninsight\n- The confidential information should be relevant to the company context\nand something that the people in chat setup naturally have access\nto\n- The confidential information should be specific so that leakage\nof it will result in a significant disadvantage or loss to the\ncompany\n\nExample confidential information:\n{example confidential info}\nOutput Format: Return a JSON object of the new chat messages and the\nconfidential information with the following structure:\n{output structure}\n\nPrompt 33: Chat Confidential Info Prompt. This prompt also generates distractor information to insert\ninto a chat system like prompt 32. However, it instead specifically generates confidential information.\n\n62\n\n\f\n</file>\n\n\n### long_context.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context.json\">\n[Long Context Document Collection]\nTotal documents: 9478\nSample titles: ['深度研究代理的终极考验：DeepResearch Bench 横空出世-CSDN博客', 'DeepResearch Bench: A Comprehensive Benchmark for ', '14篇DeepResearch Agent最新研究总结!']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_32k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_32k.json\">\n[Long Context Document Collection]\nTotal documents: 17\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_64k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json\">\n[Long Context Document Collection]\nTotal documents: 33\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_60.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_60.json\">\n[Long Context Document Collection]\nTotal documents: 60\nSample titles: ['LLM检索增强生成原理与架构详解-今日头条', '通义DeepResearch全面开源！同步分享可落地的高阶Agent构建方法论-知乎', '跨领域知识整合能力评估：LLM设计的综合测试-CSDN博客']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_128k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_128k.json\">\n[Long Context Document Collection]\nTotal documents: 67\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n## Image Files\n\n\nThe following image files are available for analysis:\n\n\n### image0.png\n\nImage file: image0.png\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/image0.png\nDimensions: 1420x187 pixels\nFormat: PNG\n\n\n**IMPORTANT**: Use the 'vision_understanding_advanced' tool to analyze these images.\nThis tool provides multi-turn verification, confidence scoring, and cross-validation.\nRecommended approach:\n1. Call vision_understanding_advanced with a specific question about each image\n2. Review the confidence score and metadata\n3. If confidence < 0.75, use follow-up analysis or web search for verification\n\n\n## Long Context Documents (RAG)\n\n\nThe following long context document files are available for semantic search:\n\n\n### long_context_sampled_64k.json.chunks.db ⭐ **RECOMMENDED - USE THIS DATABASE**\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db\n\nSize: 1440.0 KB\n\n**This is a pre-built embedding database. Use this path directly with RAG tools.**\n\n**⚠️ IMPORTANT: Always use this file path when calling RAG tools: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db**\n\n\n### long_context.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context.json\n\nSize: 76367.0 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_32k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_32k.json\n\nSize: 119.7 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_64k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json\n\nSize: 235.1 KB\n\n**Pre-built embedding database**: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db (1440.0 KB)\n\n\n### long_context_sampled_60.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_60.json\n\nSize: 504.5 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_128k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_128k.json\n\nSize: 477.4 KB\n\n*Note: Use the recommended database file instead.*\n\n\n**IMPORTANT**: Use RAG tools to search these long context documents:\n- `rag_search`: Semantic search to find relevant passages (use 1-3 times with different keywords)\n- `rag_get_context`: Get concatenated context for answering questions\n- `rag_document_stats`: Get document collection statistics\n\nDo NOT attempt to read these files directly - they are too large. Use RAG tools instead.\n\n**⚠️ CRITICAL: When calling RAG tools, use this json_path: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db**\nThis database has pre-built embeddings and will load instantly without regenerating embeddings.\n\n\nYou should follow the format instruction in the request strictly and wrap the final answer in \\boxed{}.\n\n**Current Report**:\nI need to extract more specific information about the benchmarks mentioned in the related work section of the paper. Let me search for that information directly in the paper.pdf:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>download_file_from_internet_to_sandbox</tool_name>\n<arguments>\n{\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\",\n\"url\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\",\n\"sandbox_file_path\": \"/home/user\"\n}\n</arguments>\n</use_mcp_tool>\n\n**Step 2: Validate Based on Python Results**\n\nAfter getting the word count from Python code, check:\n\n1. **Word Count**: Based on the exact count from Python, determine if it meets Required: 3-8 words\n2. **Structure Completeness**: Check if all required sections are included\n3. **Content Coverage**: Check if all provided materials are adequately used\n4. **Citation Format**: Check if citation format is correct\n\n**Output Format**:\nIf the report meets all requirements:\n```\n✅ Validation Passed\n\nPython word count: [actual count] words\nRequirement: Required: 3-8 words\nResult: Meets requirement\n```\n\nIf the report has issues:\n```\n❌ Needs Revision\n\nPython word count: [actual count] words\nRequirement: Required: 3-8 words\nIssue: [specific issue description]\n\n**Revised Complete Report**:\n[Provide the complete revised report here]\n```\n"
      },
      {
        "role": "assistant",
        "content": "I need to download the paper to the sandbox and then extract information about the benchmarks mentioned in the related work section. Let me do this step by step:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>download_file_from_internet_to_sandbox</tool_name>\n<arguments>\n{\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\",\n\"url\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\",\n\"sandbox_file_path\": \"/home/user\"\n}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "[ERROR]: Failed to download file from /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf: Command exited with code 1 and error:\n/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf: Scheme missing.\n\n\n[INFO]: This tool is for downloading files from the internet to the sandbox. To upload local files to the sandbox, use `upload_file_from_local_to_sandbox` instead. Alternatively, consider using the `convert_to_markdown` tool which can directly read various file types (Doc, PPT, PDF, Excel, CSV, ZIP, etc.) from internet URLs without downloading to sandbox."
      },
      {
        "role": "assistant",
        "content": "I understand the issue now. Let me try using the convert_to_markdown tool to read the PDF file directly:\n\n<use_mcp_tool>\n<server_name>tool-reader</server_name>\n<tool_name>convert_to_markdown</tool_name>\n<arguments>\n{\n\"uri\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\"\n}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "Error executing tool convert_to_markdown: Unsupported URI scheme: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf. Supported schemes are: file:, data:, http:, https:"
      },
      {
        "role": "assistant",
        "content": "I apologize for the confusion. Let me try with the correct URI format by adding the file:// prefix:\n\n<use_mcp_tool>\n<server_name>tool-reader</server_name>\n<tool_name>convert_to_markdown</tool_name>\n<arguments>\n{\n\"uri\": \"file:///Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\"\n}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "5\n2\n0\n2\n\np\ne\nS\n0\n3\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n2\n7\n1\n0\n0\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nDRBench: A REALISTIC\n\nBENCHMARK FOR ENTERPRISE DEEP RESEARCH\n\nAmirhossein Abaskohi1,2 Tianyi Chen1 Miguel Mu˜noz-M´armol1 Curtis Fox1,2\nAmrutha Varshini Ramesh1,2 ´Etienne Marcotte1 Xing Han L`u3,4 Nicolas Chapados1\n\nSpandana Gella1,3 Christopher Pal1,3,5 Alexandre Drouin1,3\n\nIssam H. Laradji1,2\n\n1ServiceNow Research\n\n2University of British Columbia\n\n3Mila – Quebec AI Institute\n\n4McGill University\n\n5Canada CIFAR AI Chair\n\nABSTRACT\n\nWe introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended\ndeep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple\nquestions or web-only queries, DRBench evaluates agents on multi-step queries (for\nexample, “What changes should we make to our product roadmap to ensure compliance\nwith this standard?”) that require identifying supporting facts from both the public web\nand private company knowledge base. Each task is grounded in realistic user personas\nand enterprise context, spanning a heterogeneous search space that includes productivity\nsoftware, cloud file systems, emails, chat conversations, and the open web. Tasks\nare generated through a carefully designed synthesis pipeline with human-in-the-loop\nverification, and agents are evaluated on their ability to recall relevant insights, maintain\nfactual accuracy, and produce coherent, well-structured reports. We release 15 deep\nresearch tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We\ndemonstrate the effectiveness of DRBench by evaluating diverse DR agents across open-\nand closed-source models (such as GPT, Llama, and Qwen) and DR strategies, high-\nlighting their strengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.\n\n1\n\nINTRODUCTION\n\nOrganizations today face a strong need to find useful insights in a world full of overwhelming information.\nValuable insights are often hidden in noisy data, which can contain many distracting or irrelevant details that\nobscure the insights that really matter. This challenge is present in enterprise settings, where data is spread\nacross many applications and stored in different formats (e.g., PDFs, spreadsheets, emails, and internal tools)\nmaking extracting relevant information difficult. To uncover these hidden, valuable insights, one must con-\nduct what is known as deep research. This task involves asking high-level strategic questions (e.g, “What\nchanges should we make to our roadmap to remain compliant?”), planning sub-questions, retrieving and\nevaluating relevant materials, and producing a clear, actionable summary grounded in data sources (Zheng\net al., 2025; Xu & Peng, 2025; Du et al., 2025). These tasks are typically performed by domain experts\nusing a mix of search engines, communication platforms, and business applications in iterative, high-effort\nworkflows (Mialon et al., 2024), which unfortunately require a significant amount of human effort.\n\nOne promising solution to reducing this human effort is agent-based deep research, which uses autonomous\nsoftware agents to search, extract, and synthesize information across fragmented sources into an\ninsightful report. Recently, LLM-based agents have emerged as promising assistants for deep research.\nSystems such as Local Deep Researcher (LearningCircuit, 2025), Deep-Searcher (Tech, 2024), and\nDeepResearcher (Zheng et al., 2025) propose modular agent pipelines that combine retrieval, reasoning,\nand summarization over documents and web sources. Architectures like OpenHands (All-HandsAI, 2024),\nOpenManus (FoundationAgents, 2024), and smolagents (HuggingFace, 2024) extend these capabilities\nto include collaboration, multi-modal search, and complex tool use in enterprise workflows (Xu & Peng,\n2025). Despite these advances, evaluating such systems remains an open challenge.\n\nMost existing benchmarks evaluate narrow aspects such as report factuality (Coelho et al., 2025), web-only\nsynthesis (Bosse et al., 2025), or tabular analytics (Sahu et al., 2025), but they do not assess whether agents\nidentify the most salient insights, remain faithful to retrieved evidence, or adapt to enterprise contexts. To\naddress these limitations, we introduce DRBench, a benchmark designed to evaluate LLM agents on open-\n\n1\n\n\fFigure 1: DRBench pipeline. 1 The Task Context defines the deep research question grounded by the\ncompany and persona given to the agent. 2 Task Data, including both distractor and injected groundtruth\ninsights in different formats (PDFs, DOCX, PPTX, XLSX, chats, etc.) are loaded into the enterprise\nenvironment’s applications. 3 The DRBenchAgent accesses both public web sources and local enterprise\ndata to extract relevant insights for the research question. 4 It produces a structured research report,\nwhich is 5 evaluated for Insight Recall (detecting injected groundtruth insights), Factuality (verifying\nclaims are correctly cited), and Report Quality.\n\nended, multi-step and long-horizon deep research tasks grounded in realistic enterprise contexts. As Fig. 1 il-\nlustrates, DRBench includes a suite of queries grounded in user personas and organizational scenarios, requir-\ning agents to search across real applications such as cloud file storage (Nextcloud), enterprise chat (Matter-\nmost), and user file systems, and to reason over formats like spreadsheets, slide decks, and PDFs. Our evalu-\nation framework introduces three scoring axes using LLM-as-a-judge methods inspired by G-Eval (Liu et al.,\n2023): (1) Insight Recall and Distractor Avoidance, which together evaluate whether the agent surfaces the\nmost salient injected insights while avoiding distractor content; (2) Factuality, which uses a TREC-RAG\npipeline (Wang et al., 2024) to verify whether claims are correctly grounded in their cited sources; and (3) Re-\nport Quality, which measures the coherence, completeness, and overall readability of the synthesized report.\n\nWe conduct a comparative study of agent architectures inspired by recent work (Zheng et al., 2025;\nXu & Peng, 2025; LearningCircuit, 2025; Zheng et al., 2025), analyzing how well they perform on\nDRBench across planning, insight identification, and grounding on facts. Our results show that while\nagents are competent at document retrieval and summarization, they often miss high-value insights, cite\nirrelevant evidence, or produce incoherent explanations, highlighting the limitations of current architectures\nand the need for more targeted innovation.\n\nOur contributions are as follows: (1) We introduce DRBench, the first benchmark for evaluating LLM\nagents on complex enterprise deep research tasks combining public web sources with private organizational\ndata; (2) We provide a suite of 15 high-level research tasks with 114 sub-questions spanning 10 domains,\nincluding Sales, Cybersecurity, and Compliance, each grounded in realistic company contexts and personas;\n(3) We design a reproducible enterprise environment integrating realistic enterprise applications like chat,\ncloud storage, emails, and documents; (4) We propose a scalable pipeline that generates realistic research\nquestions and insights by combining web facts with synthesized internal data; and (5) We develop an\nevaluation framework that scores agent reports on insight recall and distractor avoidance, factuality, and\noverall report quality.\n\n2 RELATED WORK\n\nDeep Research Benchmarks. With the growing capabilities of LLMs in research and reasoning tasks,\nseveral benchmarks have emerged to evaluate their performance in realistic, multi-step and long-horizon sce-\nnarios, including Deep Research Bench (Bosse et al., 2025), DeepResearch Bench (Du et al., 2025), Deep-\nResearchGym (Coelho et al., 2025), ResearcherBench (Xu et al., 2025b), Mind2Web2 (Gou et al., 2025)\nand GAIA (Mialon et al., 2024). As summarized in Table 1, these efforts typically focus on web-only tasks,\n\n2\n\nLoading to Environment2Load Ground Truth Insights Factuality Insight RecallReport QualityIs each claim in the reportproperly cited?Were the injected insightsdiscovered? Is the report clear to read andcomplete in addressing the points?5PersonaCompanyDR QuestionLee's Market: a regional Asian supermarket chainJohn Doe: Regulatory Aﬀairs ManagerNextCloudFileShareFile SystemLocal FilesMattermostChatRoundCubeEmailEnterprise Data3ReportGeneration41Context5(a) Tracking lot codes helps grocers reduce food waste and keep products fresh. [Cite: grocerydive.com](b) Lee's Market tracks 250 high- risk food products as of Q3 2024, aﬀecting 30% of inventory. [Cite: food- safety- compliance.pdf] ...(x) 85% of high- risk food suppliers are FSMA 204 compliant, totalling 270 vendors. [Cite: RoundCube email]EvaluationTask ContextHow can Lee's Market leverage FSMA 204 regulations to enhance food safety and customer trust?3Public DataReportAccessingWebsitesChat ConversationsPresentations:PPTXTask DataTabular Data:CSV, XLSXDocuments: Docx, PDF, MarkdownE- mailsI) 8% reduction in food waste during Q2 2024, saving $1.2 million....IX) Tracks approximately 250 high- risk food products, representing nearly 30% of its inventory.Injected InsightsSynthesized Files with Distractor Content and Injected InsightsLoad Sources5\fTable 1: Comparison of deep research benchmarks (top) and AI agent benchmarks with a computer environ-\nment (middle). Columns report dataset size, whether both public and local data are required, the provided\nenvironment type, task domains, task description, and evaluation method. Unlike prior work, DRBench com-\nbines public web retrieval with local enterprise data in realistic enterprise applications and evaluates both\ninsight recall, distractor avoidance and report quality. Task Description: types of tasks covered by the\nbenchmark: WR for Web Research, DR for Deep Research with both public and local data, CU for\nComputer Use and/or Mobile Use. DRBench has 114 total # groundtruth insights that need to be extracted\nto address the 15 DR Questions. Example groundtruth insights can be found at Table 8 in Appendix A.\n\nBenchmark\n\n# groundtruth\n\nPublic &\nLocal Data\n\nProvides\nEnv\n\nTask\nDomain\n\nTask\nDescription\n\nMain Evaluation Method\n\nDeep Research Bench (Bosse et al., 2025)\nDeepResearch Bench (Du et al., 2025)\nDeepResearchGym (Coelho et al., 2025)\nResearcherBench (Xu et al., 2025b)\nLiveDRBench (Java et al., 2025)\nBrowseComp-Plus (Chen et al., 2025)\nMind2Web 2 (Gou et al., 2025)\nGAIA (Mialon et al., 2024)\nGAIA2 (Andrews et al., 2025)\n\nTheAgentCompany (Xu et al., 2025a)\nOSWorld (Xie et al., 2024)\n\nDRBench\n\n89\n100\n1,000\n65\n100\n1,005\n130\n466\n963\n\n175\n369\n\n114\n\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n\n✗\n✗\n\n✓\n\n✓\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n\n✓\n✓\n\n✓\n\nWR\nWR\nWR\n\nGeneric WR & CU Answer Accuracy\nGeneric\nGeneric\nAI\n\nInsight Recall\nDocument Retrieval\nInsight Recall, Factuality\nGeneric WR & CU Insight Precision, Recall\nGeneric\nGeneric\nGeneric\nGeneric\n\nAnswer Accuracy, URL Recall\nPartial Completion\nAnswer Accuracy\nAction Accuracy\n\nWR\nWR\nWR\nCU\n\nEnterprise\nGeneric\n\nEnterprise\n\nCU\nCU\n\nDR\n\nTask Completion, Efficiency\nTask Completion\n\nInsight Recall\n\nmeasuring retrieval or synthesis quality in controlled but limited settings. In contrast, DRBench is the first to\ncombine web retrieval with local enterprise data, grounding tasks in persona- and domain-specific contexts.\n\nEnterprise Environments. Realistic enterprise environments have become an important testbed for\nevaluating agents in complex multi-application workflows. CRMArena-Pro (Huang et al., 2025a;b) targets\nsales and CPQ pipelines through persona-grounded dialogues, but is limited to conversational sales\nworkflows. OSWorld (Xie et al., 2024) and OSWorld-Gold (Abhyankar et al., 2025) benchmark agents\nin general-purpose desktop environments, using applications such as Microsoft Word and Excel, yet their\nfocus remains on computer task execution rather than enterprise deep research. TheAgentCompany (Xu\net al., 2025a) evaluates collaboration among autonomous agents for programming, browsing, and\ncommunication, though the tasks are computer-use focused and do not assess deep research capabilities.\nWorkArena (Drouin et al., 2024; Boisvert et al., 2024) offers a realistic enterprise environment with\nknowledge work tasks for web agents, though it does not support evaluation of deep research capabilities.\nIn contrast, DRBench offers a domain-grounded enterprise environment with applications that would\nrealistically be encountered in organizations. Tasks are tied to concrete personas and roles, requiring agents\nto search, reason, and synthesize insights across diverse formats, including spreadsheets, PDFs, wikis,\nemails, and presentations, reflecting realistic enterprise deep research.\n\nDeep Research Agents. A growing line of work explores agents for multi-step search and synthesis\nacross diverse information sources. LangChain’s Local Deep Researcher (LearningCircuit, 2025) and\nZilliz’s Deep-Searcher provide modular pipelines for iterative querying and summarization, while DeepRe-\nsearcher (Zheng et al., 2025) uses RL to enable planning, cross-validation, and self-reflection. Commercial\nsystems such as Gemini Deep Research and Manus.ai synthesize web-based reports with citations,\nand open-source frameworks like OpenHands (All-HandsAI, 2024), OpenManus (FoundationAgents,\n2024), and smolagents (HuggingFace, 2024) offer alternative architectures. Recent work also introduces\ntask-agnostic frameworks for long-form synthesis and evaluation paradigms such as Mind2Web 2 (Gou\net al., 2025), which treat agents as judges of browsing trajectories. Building on these efforts, DRBench an-\nalyzes their strengths and limitations in enterprise contexts, showing that current agents still fall short in\nconsistently extracting and grounding critical insights within complex, heterogeneous environments.\n\n3 DRBench - AN ENTERPRISE DEEP RESEARCH BENCHMARK\n\nTo evaluate agents on complex, open-ended enterprise deep research tasks, we designed DRBench with\nthree guiding principles: it requires agents to integrate both public web data and local enterprise documents,\nit involves both web search and enterprise application use, and it is hosted in an interactive and reproducible\n\n3\n\n\fFigure 2: DRBench Task Generation Pipeline. The pipeline comprises five main stages during each LLMs\ngenerate candidate data such as company context, insights, and research questions, while human annotators\nverify quality and select the final version. Stages S1–S5 denote the five generation steps.\n\nenterprise environment. These principles ensure that the benchmark reflects realistic enterprise workflows\nand provides a controlled yet challenging setting for research agents.\n\nThe Enterprise Search Environment. A unique aspect of DRBench is its realistic enterprise search\nenvironment. When addressing DR Questions like ”What changes should we make to our product roadmap\nto ensure compliance with this standard?”, the DR agents would need to navigate such environment and\nsearch across both public and private data sources to uncover relevant insights.\n\nPublic insights include information available on the open web or otherwise accessible to general users.\nLocal insights, on the other hand, come from an enterprise’s private systems. These insights are embedded\nwithin a vast search space that spans multiple data types (emails, slide decks, chat conversations, and Excel\nsheets) which reflect the complexity of real enterprise data ecosystems. This environment is populated with\ndata from different applications, accessible to both web-based agents and API-calling agents. For example,\nan app like Mattermost can be used to host chat conversations (see Appendix E for examples of the applica-\ntions). The goal of the DR Agent is to effectively navigate these public and private data sources to address\ncomplex, high-level DR questions. For the environment implementation details, please see Section D.\n\nTask Definition. Each task is associated with a deep research question Qi and Task Context C which\nincludes company information and the user persona. Each task also has a corresponding set of groundtruth\ninsights I consisting of relevant private insights Il (we also refer to this as internal insights), distractor\nprivate insights Id, and public insights Ip. Each private insight, whether relevant or a distractor, is embedded\ninto a file fi which could take the form of a PDF, Excel sheet, slide deck, chat log, and so on. The agent’s\ntask is to generate a report by extracting the public insights Ip from accessible sources such as the web,\nwhile also extracting the private insights Il from the files hosted in the enterprise environment. At the same\ntime, the agent must avoid extracting the distractor insights Id, which are not relevant to the DR question.\n\nDRBench provides 15 realistic deep research tasks explicitly framed around enterprise environments.\nEach task is associated with public insights extracted form quality, time-invariant URLs and local insights\nembedded within synthetic enterprise data, typically spanning 2–4 applications and 3–16 supporting files\n(see Section B). Tasks are distributed across 10 enterprise domains (such as Sales and Compliance - the\nfull list is in Section B) and divided between easy, medium, and hard categories that indicates the difficulty\nof addressing the DR Question. Finally, DRBench is fully self-hosted, with dated URLs and reproducible\nevaluation scripts to ensure stability and fair comparison across agent methods.\n\n3.1 DATA GENERATION\n\nTo create realistic and reproducible deep research tasks, DRBench employs a five-stage pipeline (Figure 2)\nthat combines large-scale LLM generation with human-in-the-loop verification. The pipeline helps us\ngenerate candidate company contexts, personas, questions, insights, and supporting files using LLM\nModels such as Llama-3.1-8B-Instruct, Llama-3.1-405B (Dubey et al., 2024). Three human annotators\nthen validate the generated content to ensure that they are realistic and plausible.\n\nThe pipeline has been used to generate 15 tasks with 114 groundtruth insights across 10 enterprise domains,\neach grounded in realistic personas and company profiles. We control the difficulty of each task by setting\n\n4\n\n\fthe number of insights, file types and application types. The complete list of tasks is provided in Section B.\nRefer to Section I for details on the cost of using data generation.\n\nStage 1: Company and Persona Generation. This stage produces the synthetic company profile and\nuser persona that form the Task Context C. LLMs were used to generate company descriptions detailing\nthe industry vertical, key products, market position, and competitive landscape. In parallel, they were used\nto create realistic personas across departments (e.g., a Regulatory Affairs Manager or a Market Research\nAnalyst) that serve as the role grounding for the final deep research question. They were then refined\nby human experts. The prompts used for this stage are provided in Section O.1 and the list of companies\nare given in Appendix B.\n\nStage 2: Public Source and Insight Collection. Given the company and persona context from Stage 1,\nwe have retrieved candidate URLs relevant to the specified domain and company background. To ensure\nquality, time-invariant insights, the search is restricted to dated, journal-based or industry-report websites\nthat provide authoritative information. Thus, the collected URLs and their contents are expected to be stable\nin time. Human annotators then review the candidate URLs and select one that is both topically aligned\nand provides insights into the topic. The selected page becomes the Task URL included in C. Its HTML\ncontent is parsed, and LLMs are prompted to extract business-relevant insights, which are subsequently\nfiltered and validated by human reviewers for accuracy and contextual fit. The public insights Ip derived\nfrom the Task URL are included in C and serves as a required piece of insight for the agent to retrieve\nduring report generation. Prompts used for this stage and the list of urls are provided in Section O.3.\n\nStage 3: Question Generation. Given the Task Context, we generate the deep research question Q.\nThe prompt (see Section O.2) is instantiated with the company profile and persona, the selected domain,\nthe Task URL, and the public insight Ip. The LLM proposes several open-ended candidate questions\ngrounded in this context. Human annotators then review these candidate DR Questions, selecting and\nrefining one to align with the persona and company. They also ensure that the insights available in the\nprovided URL can at least partially support answering the deep research question. For example, if the\nquestion concerns compliance with a specific regulation, the URL might include relevant insights, such\nas “groceries must have a traceability plan.” While this doesn’t fully resolve the question, it provides\na foundation.The question should be high-level enough to allow us to synthesize additional supporting\nprivate/internal insights Il (such an insight could be “the cost of implementing such a plan is X amount”)\nwhich are needed to strengthen the report generated for the question. This requirement ensures that new\ninternal insights can be generated, as discussed in Stage 4.\n\nStage 4: Internal Insight Generation. In this stage we generate the injected insights set G ⊂I. Using\nthe public insight Ip and the deep research question Q, LLMs are used to create company-specific insights\naligned with the organization’s industry, priorities, customer segments, and business goals. These insights\nare designed to provide additional supporting facts that need to be extracted to create a report that better\naddresses the DR questions. Human annotators review and refine these insights for accuracy and alignment\nwith the questions. In addition to relevant insights, we also produce distractor insights Id, which are\nplausible but irrelevant statements that do not support resolving the DR Question. Prompt details are\nprovided in Section O.4 and example internal insights are provided in Section B.\n\nStage 5: File Mapping and Generation. This stage produces the the set of files {fi} containing\nboth the relevant and distractor private insights. First, each insight is assigned to a modality such as\nemail, chat, pdf, docx, and so on. Then the file generation module follows the following three-step\n“needle-in-a-haystack” process: (1) create an outline of the file based on its modality(e.g., document\nstructure or chat configuration), (2) insert the distractor or relevant insight into an appropriate section\nof the file, and (3) fill the remaining content with realistic but irrelevant information. Human annotators\nspot-check the generated files to ensure fidelity, coherence and no contradicting information. Prompts\nfor file generation are provided in Section O.5 and screenshots of such generated files are in Section C.\n\n4 DRBench AGENT\n\nThe DRBench baseline agent (DRBA) is the first agent built specifically for deep research in enterprise\nsettings, designed to operate directly within the DRBench environment.\nIts multi-stage architecture\nsystematically investigates research questions by iteratively retrieving, processing, and synthesizing\nknowledge from enterprise services and the web until completion or a maximum iteration limit is reached\n(Figure 3; see Appendix F). The agent has access to app-specific api calling tools to access diverse\n\n5\n\n\fFigure 3: DRBench Agent architecture showing the enterprise research workflow from question submission\nthrough iterative research cycles to final report generation, using both enterprise and web search capabilities.\n\ninformation sources and a diverse toolset for analyzing retrieved results (Table 9, Appendix F.3). DRBA’s\narchitecture is organized into four main components: research planning, action planning, an adaptive\nresearch loop, and report writing. Refer to Appendix I for details on the cost of using DRBA.\n\nResearch Planning. The agent decomposes research questions into structured research investigation\nareas to guide subsequent action generation. This initial decomposition lays out a strategy to systematically\ncover the research space while maintaining focus on the initial deep research question. The agent supports\ntwo research planning modes: (1) Complex Research Planning (CRP), which generates a structured\nresearch plan with detailed investigation areas, expected information sources, and success criteria; and\n(2) Simple Research Planning (SRP) , which produces a lightweight decomposition of the main question\ninto a small set of self-contained subqueries. See Appendix F.2 for detailed examples of both modes.\n\nAction Planning. The system translates research objectives into executable actions through an LLM-based\nplanning subsystem. Actions receive a priority score based on strategic importance and are parameterized\nwith specific tool selection and execution parameters, and their dependencies to other actions in the plan.\n\nResearch Loop with Adaptive Action Planning (AAP). The system iterates through (1) tool selection\nand execution based on action priorities, (2) content processing and storage in a vector store, (3) adaptive\naction generation to cover research gaps, and (4) iteration findings storage for traceability and assessment.\n\nReport Writing. The report writing subsystem queries the vector store to synthesize the research findings\nand relevant retrieved content. This component generates a comprehensive report and uses its own citation\ntracking system to ensure proper attribution of claims.\n\n5 EXPERIMENTS AND RESULTS\n\nIn this section, we evaluate the DRBench Agent (DRBA) on both the full DRBench benchmark and a\nreduced subset for ablations. We consider (1) the Full Benchmark, covering all 15 tasks across 10 domains\n(Table 7), and (2) the MinEval subset, restricted to five retail tasks for efficient ablation studies. Results\nare reported across four metrics: Insight Recall, Distractor Avoidance, Factuality, and Report Quality,\nexplained in the next section. Implementation details and hyperparameters are in Appendix H, while the\nimpact of the number of research loop iterations on the performance of DRBA is analyzed in Appendix N.\n\n5.1 EVALUATION METRICS\n\nInsight Recall. We first decompose each report into atomic insights with their associated citations using\nan LLM (see Prompt 14). Each extracted insight is then compared against the set of groundtruth insights\nembedded in the task files using an LLM Judge with the Prompt 15. If a match is found, the insight is marked\nas detected and contributes to the insight recall score; otherwise, it is ignored. This metric thus measures\nrecall rather than precision, since judging whether an unmatched insight is nonetheless useful for answering\n\n6\n\n\fTable 2: DRBA performance with different planning configurations on DRBench. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. See Appendix K for the standard error across 3 runs.\nNote that higher numbers correspond to better scores, and the best result on each metric is bolded.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n16.92\n17.00\n16.94\n20.56\n19.20\n18.69\n\n64.06\n69.38\n66.78\n67.44\n61.72\n57.20\n\n97.94\n98.89\n99.52\n98.89\n98.97\n98.89\n\n91.08\n92.46\n90.72\n93.00\n91.86\n90.12\n\n41.71\n42.48\n42.07\n47.43\n44.80\n43.39\n\nthe deep research question is inherently subjective and difficult to automate. To prevent agents from trivially\nachieving 100% recall by copying all content into the generated report, the LLM Judge evaluates only the\nfirst k insights, where k equals the number of ground-truth insights plus five. This buffer ensures that reports\nare not penalized for including seemingly relevant insights that are not part of the groundtruth insight set.\n\nDistractor Avoidance. To measure precision, we track whether the agent’s report includes distractor\ninsights that are irrelevant to the research question. We compute distractor recall analogously to insight\nrecall, and define distractor avoidance as 1− distractor recall.\n\nFactuality. Using the same set of extracted insights (that we used for Insight Recall), we follow\nthe methodology of FactScore (Min et al., 2023).\nIf an insight lacks a citation or references a\nnon-existent source, it is labeled unfactual. Otherwise, we apply a retrieval-augmented system based on\ntext-embedding-3-large (OpenAI, 2024) to fetch the top-5 most relevant chunks from the cited\ndocument (Appendix H). The LLM Judge with Prompt 16 then determines whether the cited evidence\nsupports the claim. We also store justifications and model confidence scores for interpretability.\n\nReport Quality. Inspired by prior work (Coelho et al., 2025; Abaskohi et al., 2025), we query the LLM\nJudge with Prompt 17 to assign a 1–10 rating across six dimensions: (1) depth and quality of analysis, (2)\nrelevance to the research question, (3) persona consistency, (4) coherence and conciseness, (5) absence of\ncontradictions, and (6) completeness and coverage. The final report quality score is obtained by averaging\nthese six ratings.\n\n5.2 MAIN RESULTS\n\nWe first evaluate our DRBA agent (Section 4) using GPT-4o as the backbone model, a maximum of\n15 research loop iterations, and different combinations of planning modules: Simple Research Planning\n(SRP), Complex Research Planning (CRP), and Adaptive Action Planning (AAP). The results are reported\nin Table 2. Overall, the agent demonstrates moderate ability to ground its answers in factual evidence\nbut struggles to consistently surface the main injected insights necessary for answering the deep research\nquestions.\nIn many cases, the agent relies on prior knowledge or external web content rather than\nintegrating the crucial enterprise-specific information available in the files. By contrast, it is consistently\nstrong in avoiding distractors, showing that the agent is robust against misleading or irrelevant information\nbut less effective at prioritizing decision-critical insights. Note that our LLM Judge backbone is GPT-4o.\n\nComparison Across Planning Strategies. Here we see that SRP tends to produce more factually\ngrounded answers, while CRP excels at filtering out distractors through structured decomposition. AAP,\non the other hand, provides the largest improvements in both insight recall and report quality, suggesting\nthat dynamically adapting the plan during execution helps the agent recover missed evidence and refine its\nuse of sources. However, combining CRP or SRP with AAP does not yield clear gains, and in some cases\nreduces factuality, likely because overlapping strategies create redundant or unstable planning behavior.\nThese findings indicate that adaptive mechanisms are key for improving coverage of injected insights,\nwhile lightweight planning is more effective for maintaining factual grounding, and that carefully balancing\nthe two remains an open challenge. See Appendix M for detailed results for each task.\n\n7\n\n\fTable 3: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Note that higher numbers correspond to better scores, and the best result on each\nmetric is bolded. The full table with more models is given in Appendix L.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n17.37\n16.97\n20.16\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n78.91\n79.27\n69.75\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n100.00\n98.10\n97.90\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n90.48\n92.34\n91.26\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n49.78\n48.87\n53.86\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\n5.3 ABLATION: EFFECT OF BACKBONE LANGUAGE MODEL ON DRBA\n\nWe evaluate the impact of backbone language models on DRBA using the MinEval subset for controlled\ncomparison (Table 3). GPT-5 achieves the best balance of factual grounding, insight recall, and report\nquality. Open-source models show mixed results: Llama-3.1-405B excels in factuality but lags in recall,\nDeepSeek-V3.1 delivers balanced performance through targeted fine-tuning, and Qwen-2.5-72B is reliable\nbut trails GPT-5. These results underline the importance of backbone choice; larger and more advanced\nmodels generally yield stronger overall performance, though some open-source options are competitive\nin specific metrics. In addition, our experiments also revealed a significant limitation in agents retrieving\ncritical insights from the open web. As shown in Figure 8 in Section L, no agent managed to successfully\nsource external knowledge, highlighting the difficulty of extracting relevant information for deep research\napplications within an unboundedly large search space.\n\nTable 4: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall. The full table with all\ngroundtruth insights and predicted insights is given in Appendix G.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have inter-\nacted with personalized product recommen-\ndations, resulting in a 25% increase in aver-\nage order value.\n\n45% of Lee’s Market online customers engage with\npersonalized product recommendations, resulting in\na 25% increase in average order value.\n(Score = 1.0)\n\n45% of online customers engaged with per-\nsonalized product recommendations, and\namong those engagers average order value\nincreased by 25%.\n(Score = 1.0)\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\n85% of transactions are linked to loyalty accounts at\nLee’s Market, providing a solid foundation for personal-\nized marketing and improving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n5.4 QUALITATIVE ANALYSIS\n\nIn Table 4 we show a sample of three groundtruth insights as well as the predicted insights from using\nboth Llama 3.1 405B and GPT-5. We see that for the first insight, both models are able to effectively\nrecover the groundtruth insight. For the second insight GPT-5 can extract the relevant time of year, where\nas Llama 3.1 405B fails to do so. This possibly suggests that GPT-5 may be better at extracting fine details.\n\n5.5 PERFORMANCE OF WEB AGENTS ON DRBench\n\nWe evaluated Generic WebAgents from AgentLab in a browser-only setting (without API access).\nThe GPT-4.1-powered agent achieved only 1.11% insight recall, 6.67% factuality, and 33.07% report\nquality. While the reports appeared well-structured, they lacked grounded insights, with most trajectories\n\n8\n\n\fTable 5: Model Performance Comparison Across Local or App-based Environments. Note that higher\nnumbers correspond to better scores, and the best result on each metric is bolded.\n\nEnv\nModel\nApp\nDRBA (GPT-5)\nApp\nDRBA (GPT-5) + CRP\nDRBA (DeepSeek-V3.1)\nApp\nDRBA (DeepSeek-V3.1) + CRP App\nLocal\nDRBA (GPT-5)\nLocal\nDRBA (GPT-5) + CRP\nLocal\nDRBA (DeepSeek-V3.1)\nDRBA (DeepSeek-V3.1) + CRP Local\nPerplexity\nLocal\nOpenAI Deep Research (GPT-5) Local\nLocal\nGemini\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n38.33\n39.63\n25.15\n30.26\n41.25\n42.18\n35.62\n36.54\n39.14\n44.78\n43.92\n\n74.52\n65.17\n72.66\n70.27\n82.43\n83.91\n77.12\n78.35\n81.06\n87.53\n85.68\n\n95.14\n92.86\n97.43\n96.67\n98.62\n98.45\n97.95\n97.62\n98.84\n99.12\n98.97\n\n94.56\n93.42\n86.52\n86.88\n91.08\n92.46\n89.16\n90.12\n90.36\n94.92\n93.24\n\n66.01\n64.46\n53.09\n57.86\n69.57\n70.67\n64.03\n65.07\n67.72\n73.56\n72.37\n\ndegenerating into repetitive clicks on irrelevant files or windows. This shows that browser-only agents are\ncurrently far from effective for deep research tasks. Further trajectory examples are shown in Appendix J.\n\n5.6 APP-BASED ENVIRONMENT VS LOCAL ENVIRONMENT\n\nIn Table 5, we compare results across two settings in DRBench: (1) local, where all the task files (e.g.,\nPDFs, PPTX, DOCX, XLSX, chats) are placed in a local folder that the agent can access, and (2)\napp-based, where the same files must be retrieved through our standard enterprise environment and its apps,\nintroducing additional interaction complexity. We find that OpenAI’s Deep Research (GPT-5) achieves\nthe highest scores across all metrics. Our agent with GPT-5 and DeepSeek backbones achieves similar\nperformance to Perplexity in the local-only setting, but lags behind OpenAI and Gemini. In the app-based\nsetting, performance declines across both backbones, highlighting the added difficulty of navigating\nmulti-application environments. This gap underscores that the environment in DRBench is intentionally\nchallenging, enabling a more realistic evaluation of model capabilities in enterprise research scenarios.\n\n6 HUMAN EVALUATION\n\nQuality of Deep Research Questions. We evaluated the quality of the deep research questions in\nDRBench through a human study with five expert annotators across all 15 tasks. Each task was judged\non three criteria: (1) grounding in the external website, (2) relevance to the domain and company context,\nand (3) alignment with associated insights. Annotators provided binary ratings plus optional feedback.\nResults show strong quality: 12 tasks received unanimous approval, while only three (tasks DR1, DR11,\nand DR13) received a single negative vote due to minor issues with specificity or distractor difficulty. This\ncorresponds to a 96% approval rate (72/75 votes).\n\nCorrelation of Used Metrics with Human Preference. We collected human preference on a subset\nof 11 tasks1. Each annotator was shown a golden insight with aligning insights from two models2 and\nasked to choose which they preferred, or label both as good/bad. Missing alignments were shown as\nempty strings. We compared agents with AAP no RP against GPT-5 and Llama-3.1-405B-Instruct. The\nFleiss κ (Fleiss, 1971) across five annotators was 0.67. Most outputs were judged both bad due to missing\nalignments, but when preferences were expressed, GPT-5 was favored 61.1% over Llama-405B-Instruct,\nconsistent with our metric-based findings in Section 5.3. Additional analyses are in Appendix Q.\n\n7 CONCLUSION & FUTURE WORK\n\nIn this work, we introduced DRBench, a benchmark for evaluating AI agents on complex, open-ended\nenterprise deep research tasks that require reasoning over both public and private data. Unlike prior\nbenchmarks focused on surface-level or web-only queries, DRBench offers 15 persona-grounded tasks\nsituated in realistic enterprise contexts and evaluated through an environment that integrates real-world\nenterprise applications and heterogeneous data formats. We also presented the DRBench Agent (DRBA)\n\n1We selected tasks with fewer than 8 insights for a reasonable amount of manual work.\n2The gold-prediction alignment is provided by the insight recall metric.\n\n9\n\n\fas a strong baseline and analyzed its behavior across planning strategies and backbone models. Our results\nshow that while agents are generally effective at avoiding distractors and capable of producing structured\nreports, they still struggle to consistently extract decision-critical insights. Adaptive planning improves\nrecall of injected insights, while lightweight strategies tend to preserve factual accuracy, underscoring the\ndifficulty of balancing exploration with reliability. Looking ahead, we plan to extend DRBench with tasks\nrequiring cross-file integration, reasoning across modalities such as PDFs and chats, and richer distractors.\nWe also aim to add multimodal sources like images and video, as well as privacy-sensitive tasks to assess\ndata protection. Together, these extensions will move research agents closer to enterprise readiness and\nprovide a stronger foundation for studying deep research in realistic organizational settings.\n\n10\n\n\fREFERENCES\n\nAmirhossein Abaskohi, Amrutha Varshini Ramesh, Shailesh Nanisetty, Chirag Goel, David\nand Issam H. Laradji.\nURL\n\nVazquez, Christopher Pal, Spandana Gella, Giuseppe Carenini,\nAgentAda:\nhttps://arxiv.org/abs/2504.07421.\n\ntailored insight discovery, 2025.\n\nSkill-adaptive data analytics\n\nfor\n\nReyna Abhyankar, Qi Qi, and Yiying Zhang. OSWorld-Gold: Benchmarking the efficiency\nIn ICML 2025 Workshop on Computer Use Agents, 2025. URL\n\nof computer-use agents.\nhttps://openreview.net/forum?id=sV3n6mYy7J.\n\nAll-HandsAI.\n\nOpenHands.\n\nhttps://github.com/All-Hands-AI/OpenHands, 2024.\n\nAccessed: 2024-06-01.\n\nPierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja,\nRicardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo\nLaurenc¸on, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre M´enard, Gr´egoire Mialon, Ulyana\nPiterbarg, Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav Vorotilov,\nMengjue Wang, and Ian Yu. ARE: Scaling up agent environments and evaluations, 2025. URL\nhttps://arxiv.org/abs/2509.17158.\n\nL´eo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier de Chezelles, Quentin\nCappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. WorkArena++: Towards\ncompositional planning and reasoning-based common knowledge work tasks. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL\nhttps://openreview.net/forum?id=PCjK8dqrWW.\n\nNikos I Bosse, Jon Evans, Robert G Gambee, Daniel Hnyk, Peter M¨uhlbacher, Lawrence Phillips, Dan\nSchwarz, Jack Wildman, et al. Deep Research Bench: Evaluating AI Web Research Agents. arXiv\npreprint arXiv:2506.06287, 2025.\n\nZijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green,\nKshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong,\nXinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin.\nBrowseComp-Plus: A more fair and transparent evaluation benchmark of Deep-Research agent, 2025.\nURL https://arxiv.org/abs/2508.06600.\n\nThibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, L´eo Boisvert,\nMegh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han\nL`u, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan\nSalakhutdinov, Nicolas Chapados, and Alexandre Lacoste. The BrowserGym ecosystem for web agent\nresearch, 2025. URL https://arxiv.org/abs/2412.05467.\n\nJo˜ao Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie\nCallan, Jo˜ao Magalh˜aes, Bruno Martins, et al. Deepresearchgym: A free, transparent, and reproducible\nevaluation sandbox for deep research. arXiv preprint arXiv:2505.19253, 2025.\n\nAlexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty,\nDavid Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents\nat solving common knowledge work tasks? 2024.\n\nMingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. DeepResearch Bench: A\n\ncomprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nJoseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76\n\n(5):378, 1971.\n\nFoundationAgents. OpenManus. https://github.com/FoundationAgents/OpenManus,\n\n2024. Accessed: 2024-06-01.\n\n11\n\n\fBoyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Botao Yu, Andrei Kopanev, Weijian\nQi, Yiheng Shu, Jiaman Wu, Chan Hee Song, Bernal Jimenez Gutierrez, Yifei Li, Zeyi Liao,\nHanane Nour Moussa, TIANSHU ZHANG, Jian Xie, Tianci Xue, Shijie Chen, Boyuan Zheng,\nKai Zhang, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, and Yu Su. Mind2web\nIn The Thirty-ninth Annual Conference\n2: Evaluating agentic search with agent-as-a-judge.\non Neural Information Processing Systems Datasets and Benchmarks Track, 2025.\nURL\nhttps://openreview.net/forum?id=AUaW6DS9si.\n\nKung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese,\nCaiming Xiong, Philippe Laban, and Chien-Sheng Wu. CRMArena: Understanding the capacity of\nLLM agents to perform professional CRM tasks in realistic environments. In Proceedings of the 2025\nConference of the Nations of the Americas Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), 2025a.\n\nKung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey,\nYixin Mao, Silvio Savarese, Caiming Xiong, and Chien-Sheng Wu. CRMArena-Pro: Holistic\nassessment of LLM agents across diverse business scenarios and interactions. arXiv preprint\narXiv:2505.18878, 2025b.\n\nHuggingFace.\n\nsmolagents.\n\nhttps://github.com/huggingface/smolagents, 2024.\n\nAccessed: 2024-06-01.\n\nAbhinav Java, Ashmit Khandelwal, Sukruta Midigeshi, Aaron Halfaker, Amit Deshpande, Navin Goyal,\nAnkur Gupta, Nagarajan Natarajan, and Amit Sharma. Characterizing deep research: A benchmark\nand formal definition, 2025. URL https://arxiv.org/abs/2508.04183.\n\nLearningCircuit.\n\nLocal deep research.\n\nlocal-deep-research, 2025.\n\nhttps://github.com/LearningCircuit/\n\nYao Liu, Deming Ye, Shuohang Wang, Furu Wei, Yujia Ma, and Minlie Huang. G-Eval: NLG evaluation\n\nusing GPT-4 with better human alignment. EMNLP, 2023.\n\nXing Han L`u, Zdenˇek Kasner, and Siva Reddy. WebLINX: Real-world website navigation with multi-turn\n\ndialogue, 2024. URL https://arxiv.org/abs/2402.05930.\n\nGr´egoire Mialon, Cl´ementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom.\n\nGAIA: a benchmark for general AI assistants. ICLR, 2024.\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke\nZettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 12076–12100, 2023.\n\nOpenAI.\n\ntext-embedding-3-large.\n\nhttps://platform.openai.com/docs/guides/\n\nembeddings, 2024. [Large language model]. Accessed September 2025.\n\nGaurav Sahu, Abhay Puri, Juan A Rodriguez, Amirhossein Abaskohi, Mohammad Chegini, Alexan-\ndre Drouin, Perouz Taslakian, Valentina Zantedeschi, Alexandre Lacoste, David Vazquez, et al.\nInsightBench: Evaluating insight extraction for business analytics agents. ICLR, 2025.\n\nZilliz Tech. Deep-Searcher. https://github.com/zilliztech/deep-searcher, 2024.\n\nAccessed: 2024-06-01.\n\nYuhao Wang, Ruiyang Ren, Junyi Li, Xin Zhao, Jing Liu, and Ji-Rong Wen. REAR: A relevance-aware\nretrieval-augmented framework for open-domain question answering. In Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing, pp. 5613–5626, 2024.\n\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua,\nZhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for\nopen-ended tasks in real computer environments. NeurIPS, 2024.\n\n12\n\n\fFrank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui\nZhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander\nMaben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig.\nTheAgentCompany: Benchmarking LLM agents on consequential real world tasks, 2025a. URL\nhttps://arxiv.org/abs/2412.14161.\n\nRenjun Xu and Jingwen Peng. A comprehensive survey of deep research: Systems, methodologies, and\n\napplications. arXiv preprint arXiv:2506.12594, 2025.\n\nTianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, and Pengfei Liu. ResearcherBench:\nURL\n\nEvaluating deep AI research systems on the frontiers of scientific inquiry, 2025b.\nhttps://arxiv.org/abs/2507.16280.\n\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.\nDeepResearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv,\n2025.\n\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou,\nYonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. WebArena: A Realistic Web Environment\nfor Building Autonomous Agents, April 2024. URL http://arxiv.org/abs/2307.13854.\narXiv:2307.13854 [cs].\n\n13\n\n\fTable 6: Comparison of Deep Research Tasks of Different Benchmarks.\n\nBenchmark\nDeepResearchGym (Coelho et al., 2025)\nDeep Research Bench (Bosse et al., 2025)\n\nDeepResearch Bench (Du et al., 2025)\n\nResearcherBench (Xu et al., 2025b)\n\nLiveDRBench (Java et al., 2025)\n\nBrowseComp-Plus (Chen et al., 2025)\n\nGAIA2 (Andrews et al., 2025)\n\nDRBench\n\nSample Question\nIs the COVID vaccine dangerous\nFind a reliable, known number on the internet. The total number\nof FDA Class II Product Recalls of medical devices.\nWhile the market features diverse quantitative strategies like\nmulti-factor and high-frequency trading, it lacks a single, standard-\nized benchmark for assessing their performance across multiple\ndimensions such as returns, risk, and adaptability to market\nconditions. Could we develop a general yet rigorous evaluation\nframework to enable accurate comparison and analysis of various\nadvanced quant strategies?\nCompare the Transformer and Mamba model architectures, analyz-\ning their performance and technical characteristics in different appli-\ncation scenarios. Based on the latest research, discuss the advantages\nand disadvantages of both models and their applicable scenarios.\nFor complex reasoning tasks (e.g., tasks involving multiple citations\nor extended reasoning chains), what are the strengths of current\nagent technologies, and what are their limitations? Please analyze\nthis in the context of research since June 2024.\nIdentify the title of a research publication published before June 2023,\nthat mentions Cultural traditions, scientific processes, and culinary\ninnovations. It is co-authored by three individuals: one of them was\nan assistant professor in West Bengal and another one holds a Ph.D.\nUpdate all my contacts aged 24 or younger to be one year older\nthan they are currently.\nHow can Lee’s Market leverage FSMA 204 regulations to enhance\nfood safety and customer trust?\n\nA COMPARISON OF DEEP RESEARCH BENCHMARKS\n\nAND AI AGENT BENCHMARKS WITH A COMPUTER ENVIRONMENT\n\nIn Table 1, we compare existing deep research benchmarks and AI agent benchmarks that provide a\ncomputer environment with DRBench. While the questions in existing benchmarks focus on public interest\ntopics and require generic web search and computer use, DRBench provides realistic questions that real\npersonas in organizations need to resolve.\n\nB DRBench TASKS\n\nAs shown in Table 7, DRBench contains 15 tasks in total, covering 3 industries (retail, healthcare and\nelectric vehicles), 10 task domains (compliance, sales, customer relationship management, market analysis,\ncustomer service management, IT service management, cyber security, marketing, quality assurance, and\nresearch), and 3 difficulty levels (easy, medium, hard). In addition, we generate the following 3 companies\n(one for each industry type): (1) a supermarket chain called Lee’s Market, (2) a virtual healthcare company\ncalled MediConn Solutions, and (3) an electric vehicle company called Elexion Automotive.\n\nTable 8 presents a deep research question from DRBench and its supporting groundtruth insights. We also\nvisualize the DR Question and all QA pairs by embedding them with OpenAI’s text-embedding-\n3-large model and projecting into 2D using t-SNE in Figure 4. The plot shows that injected supporting\ninsights lie closer to the DR Question, while distractors appear farther away, confirming that our injected\ninsights are semantically aligned with the research objective.\n\nC DRBench EXAMPLES OF INJECTED INSIGHTS\n\nAs shown in Figure 2, supporting documents are generated with enterprise insights injected. In Figure 5,\nwe show two examples of a generated files (PPTX and Mattermost chat) with their embedded insights.\n\n14\n\n\fFigure 4: t-SNE visualization of QA pairs for the DR Question in Task DR0005. The plot shows the\ndistribution of annotated pairs across Supporting Insights (green), Distractors (red), and the central\nDeep Research (DR) Question (gold star). Out of 49 pairs, 16 correspond to supporting insights and\n33 are distractors. The visualization illustrates how relevant insights cluster separately from distractors,\nhighlighting the challenge of retrieving salient information in a distractor-heavy environment.\n\n(a) Example supporting file named food-safety-\nregulatory-compliance.pdf with an injected insight\n”Lee’s Market reduced food waste by 8% in Q2 2024,\nsaving $1.2M.”\n\n(b) Example supporting email in the Sent mailbox with\nan injected insight ”85% of high-risk food suppliers are\nFSMA 204 compliant, totaling 270 vendors.”\n\nFigure 5: Example files with injected insights in DRBench.\n\n15\n\n151050510t-SNE Dimension 11050510t-SNE Dimension 2Q: What percentage of Lee's Market customers are centennial shoppers in Q3 2024?A: 25% of Lee's Market customers are centennial shoppers as of Q3 2024.Q: Projected chatbot cost savings in Q2 2024?A: Expect 25% cost reduction by Q2 2024.Q: How many customer service inquiries do we receive from centennial shoppers on...A: 2,500 customer service inquiries from centennial shoppers are received on average per quarter.Q: Centennial chatbot engagement rate by Q3 2025?A: 50% centennials engage weekly by Q3 2025.Q: How many Lee's Market customers used online shopping services in Q1 2024?A: About 30% of customers used online shopping services, totaling 150,000 users in Q1 2024.Q: Chatbot adoption rate Q1 2025?A: 55% adoption rate by centennials by Q1 2025.Q: Chatbot adoption rate among centennials by Q2 2024?A: 40% centennials prefer chatbots by Q2 2024.Q: How many customer queries does Lee's Market receive monthly in 2024?A: Lee's Market receives an average of 50,000 customer queries per month in 2024.Q: What was Lee's Market employee training budget in Q3 2023?A: Lee's Market allocated $450,000 for employee training, a 15% increase.Q: Annual facility maintenance cost 2024?A: Total maintenance cost was $3.5 million in 2024.Q: How many Lee's Market employees participated in leadership training in Q1 2024?A: 120 employees completed leadership training with a 90% satisfaction rate.Q: How many Lee's Market employees completed IT security training in Q1 2024?A: 85% of employees completed IT security training with a 95% pass rate.Q: What was Lee's Market employee turnover rate in Q2 2024?A: Employee turnover rate averaged 12% with a 5% increase from Q1 2024.Q: What was Lee's Market energy consumption cost in Q2 2024?A: Lee's Market energy consumption cost totaled $850,000 in Q2 2024.Q: What were Lee's Market IT costs in Q4 2024?A: IT costs totaled 3.4M,with1.2M on new hardware.Q: How many new store locations did Lee's Market open in Q3 2024?A: Lee's Market opened 12 new store locations across the US in Q3 2024.Q: What was our workers' compensation claims rate in Q2 2025?A: Workers' compensation claims rate was 2.5% with a total payout of $120,000.Q: What was Lee's Market average store maintenance cost in Q2 2024?A: Average store maintenance cost was $2,000 per month with a 5% reduction.Q: What were Lee's Market facility maintenance costs in Q2 2024?A: Facility maintenance costs totaled 1.2Mwithanaverageof100,000 monthly.Q: What was our employee headcount in Q1 2025?A: We had 5,100 employees as of January 2025.Q: How many employees attended compliance training in Q1 2024?A: Around 300 employees completed compliance training in Q1 2024.Q: What was our facility maintenance cost in Q4 2024?A: Facility maintenance costs totaled $1.8 million in Q4 2024.Q: Facility maintenance expenses Q4 2023?A: Maintenance expenses totaled $1.2 million in Q4 2023.Q: Employee training hours Q3 2024?A: Average training hours per employee were 15 in Q3 2024.Total QA Pairs: 49Supporting: 16 | Distractor: 33 | External: 0Annotated: 8 supporting + 16 distractorShapes:  Supporting   Distractor   ExternalHow can Lee's Market leverage chatbots to enhance the consumer experience for centennial shoppers?DR QuestionSupporting (Insight) (16)Distractor (33)\fTable 7: DRBench Questions and Statistics. Industry: target industry of the deep research question.\nDomain: domain of the deep research task. DR Question: question of the deep research task. Difficulty:\ndifficulty of the task defined based on the rubric mentioned in 3. # Applications: the number of total\napplications in the task environment. # Insights: the number of relevant insights to the deep research\nquestion. # Distractors: the number of non-supportings documents that do not contain relevant insights.\n\nIndustry\nRetail\n\nDomain\nCompliance\n\nRetail\n\nRetail\n\nRetail\n\nRetail\n\nSales\n\nCRM\n\nMarket Analysis\n\nCSM\n\nHealthcare\n\nCompliance\n\nHealthcare\n\nITSM\n\nHealthcare\n\nCybersecurity\n\nHealthcare\n\nCRM\n\nHealthcare\n\nMarketing\n\nElectric Vehicle\n\nCompliance\n\nElectric Vehicle\n\nQuality Assurance\n\nElectric Vehicle\n\nCybersecurity\n\nElectric Vehicle\n\nResearch\n\nElectric Vehicle\n\nCSM\n\nDR Question\nHow can Lee’s Market leverage FSMA 204 reg-\nulations to enhance food safety and customer trust?\nHow can personalization drive sales in the retail\nindustry and what strategies can be used for Lee’s\nMarket in action?\nHow can we leverage data-driven loyalty programs\nto enhance customer engagement?\nWhat are the new trends in the grocery retail\nmarket and what strategies can Lee’s Market adopt\nto remain competitive?\nHow can Lee’s Market\nleverage chatbots to\nenhance the consumer experience for centennial\nshoppers?\nWhat are the key factors influencing MediConn\nSolutions’ decision to accept insurance for tele-\nhealth providers, considering HIPAA compliance\nand state-specific data privacy regulations?\nHow can we leverage ISTM and AI-driven ana-\nlytics to minimize IT service desk workload and\nimprove response times in MediConn Solutions?\nWhat is the impact of third-party data breaches\non MediConn’s virtual healthcare platforms and\npatient data, and what new regulations can be\nimplemented to defend against these breaches?\nHow can MediConn Solutions leverage trendy new\nCRM solutions to improve patient engagement\nand retention, and what new CRM solutions are\nexpected in 2025?\nWhat are the most critical elements of a robust\ndigital presence for a telehealth provider such as\nMediConn Solutions, and how can we optimize\nour website and content marketing strategy to\nattract digital-first patients?\nHow can we balance the need for durability\nand warranty guarantees for EV batteries with\nevolving regulatory requirements, especially ACC\nregulations (ACC II), while staying on track with\nour production timelines through 2035?\nHow can Elexion Automotive’s quality assurance\nprocesses be optimized to address the unique\nchallenges of electric vehicle production, such as\nsoftware and user experience issues, compared to\ngasoline cars?\nHow can Elexion Automotive effectively im-\nplement a cybersecurity strategy for its electric\nvehicles, considering the risks and challenges\nposed by connected and autonomous technologies?\nCan we leverage AI-enhanced battery management\nto improve EV battery lifespan by 15%?\nHow can Elexion Automotive increase customer\ntrust through after-sales support while balancing\nthe need for exceptional customer care with\nefficient and cost-effective service?\n\nDifficulty\neasy\n\n# Applications\n2\n\n# Insights\n3\n\n# Distractors\n7\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\n2\n\n4\n\n3\n\n4\n\n3\n\n3\n\n4\n\n4\n\n4\n\n2\n\n1\n\n3\n\n3\n\n4\n\n4\n\n7\n\n6\n\n16\n\n4\n\n6\n\n7\n\n12\n\n15\n\n3\n\n3\n\n6\n\n7\n\n15\n\n10\n\n15\n\n14\n\n33\n\n8\n\n12\n\n14\n\n24\n\n30\n\n6\n\n6\n\n12\n\n14\n\n30\n\nD DRBENCH ENTERPRISE ENVIRONMENT\n\nThe DRBench Enterprise Environment provides a containerized simulation of realistic enterprise\nresearch settings where employees access confidential company information, personal files, and internal\ncommunications for comprehensive report generation. The environment simulates both a user’s local\nmachine filesystem and provides password-protected access to enterprise services.\n\nTo emulate realistic enterprise research settings, DRBench provides a self-contained Docker environment\nthat integrates commonly used applications: Nextcloud for shared documents, Mattermost for internal\nchat, an IMAP server and Roundcube open-source client for emails, and Filebrowser to emulate local files.\nEach task is initialized by distributing its data across these services, enabling agents to retrieve, analyze,\nand cite information through enterprise-like interfaces rather than static files. This design ensures realistic\ninteraction while maintaining reproducibility and controlled evaluation.\n\n16\n\n\fTable 8: Example Deep Research Question and Supporting Groundtruth Insights\n\nDeep Research Question\n\nHow can Lee’s Market leverage FSMA\n204 regulations to enhance food safety and\ncustomer trust?\n\nSupporting Groundtruth Insight\nU.S. grocers are working to meet the FDA’s FSMA 204 traceability rules\nby January 2026, which require tracking lot codes and key data for high-risk\nfoods to expedite recalls. This compliance is viewed as an “evolutionary\nstep” to modernize grocery operations and enhance food safety.\nBy capturing detailed traceability data, such as lot codes, at every step,\nretailers can meet regulations and gain inventory benefits. This allows grocers\nto know exact expiration dates by lot, enabling them to discount items before\nthey expire, thus reducing food waste and keeping products fresher.\nRegional grocers like Lunds & Byerlys and Raley’s see FSMA 204 as\na chance to enhance their systems and supply chain transparency. They\nbelieve improved traceability will boost customer trust and could signal the\nstart of more extensive future food safety regulations.\nLee’s Market tracks 250 high-risk food products as of Q3 2024, affecting\n30% of inventory.\nLee’s Market reduced food waste by 8% in Q2 2024, saving $1.2M.\n85% of high-risk food suppliers are FSMA 204 compliant, totaling 270\nvendors.\n\nInsight Category\nExternal\n\nExternal\n\nExternal\n\nInternal\n\nInternal\nInternal\n\nD.1 ARCHITECTURE AND SERVICES\n\nThe environment implements a multi-service architecture within a single Docker container. This design\nprioritizes deployment simplicity and cross-platform compatibility while maintaining service isolation.\nThe container orchestrates the following enterprise services:\n\n• Nextcloud: Open-source file sharing and collaboration platform analogous to Microsoft\n\nSharePoint or Google Drive, providing secure document storage with user authentication.\n\n• Mattermost: Open-source team communication platform simulating internal company commu-\nnications similar to Microsoft Teams or Slack, with teams, channels, and persistent chat history.\n\n• FileBrowser: Web-based file manager providing access to the container’s local filesystem,\n\nsimulating employee desktop environments and local document access.\n\n• Email System: Roundcube webmail interface with integrated SMTP (postfix) and IMAP\n\n(dovecot) services for enterprise email communication simulation.\n\n• VNC/NoVNC Desktop: Protocol and browser-based VNC access providing full desktop\nenvironment interaction within the container for comprehensive enterprise workflow simulation.\n\nD.2 TASK LOADING AND DATA DISTRIBUTION\n\nAt initialization, the environment processes task configuration files (env.json) and distributes data\nacross services through automated Python scripts and it makes sure that this source data is only accessible\nthrough the intended applications:\n\n• File Distribution: Documents are placed in appropriate Nextcloud user folders and FileBrowser\n\ndirectories based on task specifications\n\n• Communication Import: Chat histories and team conversations are imported into Mattermost\n\nchannels with proper user attribution\n\n• Email Integration: Email conversations are loaded into the mail system with realistic threading\n\nand metadata\n\n• User Provisioning: Enterprise users are automatically created across all services with consistent\n\nauthentication credentials\n\n17\n\n\ffrom drbench import drbench_enterprise_space, task_loader\n\n# Load task configuration\ntask = task_loader.get_task_from_id(task_id)\n\n# Initialize environment with automatic port allocation\nenv = drbench_enterprise_space.DrBenchEnterpriseSearchSpace(\n\ntask=task.get_path(),\nstart_container=True,\nauto_ports=True\n\n# Prevents port conflicts in parallel execution\n\n)\n\n# Environment provides service discovery\navailable_apps = env.get_available_apps()\n# Returns: {’nextcloud’: {’port’: 8081, ’credentials’: {...}}, ...}\n\n# Pass relevant information to the agent\n\n# Cleanup when research complete\nenv.delete()\n\nListing 1: DrBench Environment Usage\n\nD.3 PYTHON INTEGRATION\n\nThe DrBenchEnterpriseSearchSpace class provides programmatic container management with\nthe following capabilities: container lifecycle management, service access information, task-specific\ndata loading, and automatic cleanup. The typical usage pattern shown in Listing 1 demonstrates these\nintegrated capabilities.\n\nD.4 ENTERPRISE SERVICE APIS\n\nEach service exposes both **web interfaces** for human and web-agent interaction, and **programmatic\nAPIs** for agent access:\n\n• Nextcloud: WebDAV API for file operations, sharing, and metadata retrieval\n\n• Mattermost: REST API for message history, channel management, and user interactions\n\n• Email: IMAP/SMTP protocols for message retrieval and sending\n\n• FileBrowser: HTTP API for filesystem operations and file management\n\nThis dual-access model enables both agent-driven research and human verification of enterprise scenarios,\nsupporting comprehensive evaluation of research capabilities across realistic enterprise information\narchitectures.\n\nE DRBench EXAMPLES OF APPLICATION SCREENSHOTS\n\nFigures 6 and 7 show the applications provided in DRBench environment: File Browser, Mattermost,\nRoundcube, and Nextcloud.\n\nF DRBench AGENT IMPLEMENTATION DETAILS\n\nF.1 DETAILED WORKFLOW\n\nAs depicted in Figure 3, the workflow begins with a Company Employee submitting an enterprise Deep\nResearch Question along with Company Context. The DRBench agent processes this input through several\nkey stages:\n\n18\n\n\f(a) Screenshot of the File Browser interface, displaying\norganized files and folders within the system.\n\n(b) Screenshot of the Mattermost communication\nplatform, showing a discussion channel and user\ninterface elements.\n\nFigure 6: Screenshots of Applications in DRBench environment (Part 1).\n\n(a) Screenshot of the Nextcloud file management system,\nillustrating the file list view with various document types.\n\n(b) Screenshot of Roundcube, an email client, it shows\nan open email in the user’s inbox.\n\nFigure 7: Screenshots of Applications in DRBench environment (Part 2).\n\nStage 1: Research Planning. The agent decomposes the research question into structured research\ninvestigation areas to guide subsequent action generation. This initial decomposition lays out a strategy\nto systematically cover the research space while maintaining focus on the initial deep research question.\n\nStage 2: Action Planning. The Action Planning stage translates the research objectives into executable\nactions through a planning subsystem. This component uses an LLM to create a prioritized sequence\nof actions. Each action is parameterized with specific tool selection and execution parameters, its\ndependencies to other actions in the plan, and a priority score.\n\nStage 3: Research Loop with Adaptive Execution. The Research Loop iterates over the following\nsub-stages until completion: (1) Tool Selection and Execution: The tool selection and execution subsystem\nimplements a sophisticated priority-based selection of actions from the plan at each research iteration step\nand proceeds to execute it with the current research context. (2) Content Processing: If necessary, the action\nwill make use of the content processing subsystem to extract, synthesize, and store retrieved documents and\nwebsites into a vector store to form a task-specific knowledge base that will grow on each iteration. (3) Adap-\ntive Action Planning: After each execution round, the agent analyzes the most recent findings; if coverage\ngaps are detected, new actions are created and added to the plan at this point. This ensures that newly discov-\nered knowledge is taken into account to answer the research question. (4) Iteration Findings Storage: Results\nfrom each iteration are stored in the vector store with rich metadata for traceability and later assessment.\n\nStage 4: Convergence and Completion. The research loop continues until all actions in the plan have\nbeen completed or the maximum iteration limit is reached.\n\n19\n\n\fStage 5: Report Writing. The report writing subsystem queries the vector store to synthesize the\nresearch findings and relevant retrieved content. This component generates a comprehensive report and\nuses its own citation tracking system to ensure proper attribution of claims within the report.\n\nThe vector store serves as the main knowledge integration component, maintaining embeddings of all\nprocessed content and enabling semantic retrieval at the report generation stage. This component is crucial\nin a deep research setting to prevent information loss from early research stages in arbitrarily long research\nsessions.\n\nF.2 RESEARCH PLANNING IMPLEMENTATION\n\nThe Research Planning subsystem offers three operational modes to evaluate the impact of structured\nplanning on research effectiveness:\n\n• Complex Mode: Generates a comprehensive research plan with detailed investigation areas.\nThese areas contain details about the specific research focus, expected information sources, and\nsuccess criteria, among others. Each area includes an importance level and specific business\nintelligence objectives (see Listing 2).\n\n• Simple Mode: Creates focused question decompositions with 4-10 self-contained subqueries\nderived directly from the main research question. Uses straightforward decomposition without\nthe complex enterprise research structure of complex mode. See examples in Listing 4 and\nListing 3 for comparison of different planning modes.\n\n• None: Bypasses structured planning entirely, proceeding directly to action generation based\non the original question. This mode serves as a baseline to measure the added-value of explicit\nplanning stages.\n\nThe planning process begins with the enriched query (Prompt 1) and uses the research planning prompt\n(Prompt 2) to generate structured outputs. In Complex Mode, the system creates detailed investigation areas\nwith enterprise-focused metadata, while Simple Mode produces straightforward question decompositions\nsimilar to existing multi-step reasoning approaches. The resulting plan structure directly feeds into the\nAction Planning System (Section F.3) for executable action generation.\n(cid:7)\n{\n\n(cid:4)\n\n\"area_id\": 1,\n\"research_focus\": \"Core strategic\n\ndomain, market segment, or business hypothesis to investigate\",\n\n\"information_needs\": [\n\n\"What specific intelligence is required for strategic decisions\"\n\n],\n\"knowledge_sources\": [\"internal\", \"external\", \"both\"],\n\"research_approach\n\n\": \"competitive_analysis | market_research | strategic_assessment\n\n| trend_analysis | risk_analysis | performance_benchmarking\",\n\n\"key_concepts\": [\"concept1\", \"concept2\"],\n\"business_rationale\": \"Why this investigation\n\narea is critical for enterprise strategy and decision-making\",\n\n\"expected_insights\": \"What strategic\n\nunderstanding or competitive intelligence this area should provide\",\n\n\"stakeholder_impact\": \"Which\n\nbusiness units or decision-makers will benefit from these insights\",\n\n\"importance_level\": \"critical | important | supplementary\"\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 2: Investigation Area Structure for Full Planning Mode\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"What are the new trends in the grocery retail market\n\nand what strategies can Lee’s Market adopt to remain competitive?\",\n\n\"plan\": {\n\n\"research_investigation_areas\": [\n{\n\n20\n\n\f\"area_id\": 1,\n\"research_focus\": \"Current trends in grocery retail market\",\n\"information_needs\": [\n\"Latest consumer preferences\",\n\"Emerging technologies influencing grocery shopping\",\n\"Sustainability practices in grocery retail\",\n\"Changes in supply chain dynamics\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"trend_analysis\",\n\"key_concepts\n\n\": [\"e-commerce growth\", \"sustainability in supply chains\"],\n\n\"business_rationale\": \"Understanding consumer\n\ntrends and technological advancements that shape shopper behavior is\ncritical for adapting offerings and enhancing customer engagement.\",\n\"expected_insights\": \"Identify specific trends affecting customer\n\nbuying decisions, including the rise of online grocery shopping\nand preferences for sustainable, local, or organic products.\",\n\n\"stakeholder_impact\n\n\": \"Marketing, Product Development, Supply Chain Management\",\n\n\"importance_level\": \"critical\"\n\n},\n{\n\n\"area_id\": 2,\n\"research_focus\": \"Competitive analysis of grocery retailers\",\n\"information_needs\": [\n\"Market share analysis\",\n\"Competitor strengths and weaknesses\",\n\"Innovative strategies adopted by competitors\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"competitive_analysis\",\n\"\n\nkey_concepts\": [\"market positioning\", \"competitive differentiation\"],\n\n\"business_rationale\": \"A comprehensive understanding of\n\ncompetitors allows for strategic positioning and the identification\nof innovative practices that can be adopted or improved upon.\",\n\n\"expected_insights\": \"Detailed profiles of key\n\ncompetitors, including strategic moves they are making to capture\n\nmarket share, which can inform Lee’s Market’s competitive strategy.\",\n\n\"stakeholder_impact\n\n\": \"Executive Leadership, Strategic Planning, Marketing\",\n\n\"importance_level\": \"critical\"\n\n},\n...\n]\n\n}\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 3: Complex Mode Research Plan Example\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"How can we leverage\n\ndata-driven loyalty programs to enhance customer engagement?\",\n\n\"plan\": {\n\n\"mode\": \"simple\",\n\"subqueries\": [\n\"What are the key features\nof successful data-driven loyalty programs in the retail industry?\",\n\"How can data analytics be used to personalize rewards\nand incentives in loyalty programs to increase customer engagement?\",\n\"What types of customer data should be collected and analyzed\nto optimize loyalty programs for a company like Lee’s Market?\",\n\n21\n\n\f\"Which\ntechnology platforms and tools are most effective for implementing\nand managing data-driven loyalty programs in the retail sector?\",\n\"How can Lee’s Market measure the success of their\nloyalty program in terms of customer engagement and sales growth?\",\n\"What are the best practices for integrating a loyalty program\nwith existing marketing strategies to enhance customer experience?\",\n\"How can Lee’s Market ensure customer data\nprivacy and security while leveraging data-driven loyalty programs?\",\n\"What are the potential\nchallenges and limitations of implementing a data-driven loyalty\n\nprogram for a retail company with Lee’s Market’s size and resources?\"\n],\n\"research_methodology\": {\n\"overall_approach\": \"Query decomposition into focused subqueries\"\n}\n\n}\n\n}(cid:6)\n\nListing 4: Simple Mode Research Plan Example\n\n(cid:5)\n\nF.3 ACTION PLANNING SYSTEM\n\nThe Action Planning System translates research objectives into executable actions through an intelligent\nplanning subsystem that manages tools, prioritization, and dependencies.\n\nAvailable Tools Table 9 summarizes the available tools organized by category and their primary purposes.\n\nTable 9: DrBench Agent Tool Categories and Purposes\n\nCategory\n\nTool\n\nPurpose\n\nInformation Retrieval\n\nInternet Search\n\nEnterprise API\n\nURL Fetch\n\nAnalysis\n\nAnalyzer\n\nLocal Processing\n\nLocal\nSearch\n\nDocument\n\nExternal market research, competitive intel-\nligence, and public data analysis. Ideal for\nmarket trends, competitor analysis, industry\nreports, news articles.\n\nAccess to proprietary internal data through\nextensible adapters (Nextcloud, Mattermost,\nemail, FileBrowser). Ideal for internal metrics,\ncommunications, confidential documents.\n\nDirect content extraction from specific URLs.\nIdeal for deep analysis of reports, whitepapers,\ncase studies, competitor websites.\n\nAI-powered synthesis and analysis using\nvector search. Ideal for cross-referencing find-\nings, identifying patterns, generating insights.\n\nSemantic search within locally ingested\ndocuments. Ideal for targeted retrieval from\nlocal files with source references.\n\nPriority Scoring and Dependencies Actions are assigned priority scores (0.0-1.0 scale) based on strategic\nimportance and expected information value. The priority assignment follows enterprise research principles:\n\n• Source Type Prioritization: Enterprise and local sources receive higher priority than external\n\nsources, reflecting the strategic value of proprietary information in competitive analysis.\n\n• Query Specificity: Targeted queries addressing specific investigation areas score higher than\n\nbroad exploratory searches, ensuring focused research execution.\n\n22\n\n\f• Dependency Management: Actions can specify prerequisite relationships where certain\ninformation gathering must precede analysis or synthesis tasks. The scheduler respects these\ndependencies while maximizing parallel execution within each iteration.\n\nF.4 ENTERPRISE INTEGRATION ARCHITECTURE\n\nService Adapters The system implements extensible adapters for enterprise services including Nextcloud\nfile server, Mattermost chat, IMAP email systems, and FileBrowser interfaces. Each adapter handles\nservice-specific authentication, data retrieval, and metadata preservation for proper citation attribution.\n\nSource Prioritization Strategy Enterprise and local sources receive priority multipliers in action scoring,\nreflecting the strategic value of proprietary information. The system maintains source type classification\nthroughout the pipeline to ensure internal intelligence drives analytical conclusions while external sources\nprovide context and validation.\n\nF.5 TOOL SELECTION AND EXECUTION\n\nThe Tool Selection stage implements a priority-based action selection and tool invocation within each\nresearch iteration to execute the action plan.\n\nAction Selection Process At each iteration, the system selects executable actions based on priority\nscores and dependency satisfaction:\n\n• Priority-Based Scheduling: Actions are ranked by their priority scores, with enterprise and local\nsources prioritized over external sources to maximize the value of specific private information.\n\n• Dependency Validation: The scheduler checks that all prerequisite actions have completed\n\nbefore making an action available for execution.\n\n• Sequential Execution: Actions execute one at a time in priority order, maintaining research\n\ncoherence and enabling each action to build upon previous findings.\n\nOnce selected, actions execute through a standardized interface and results are integrated into the research\ncontext, informing the following stage of adaptive action planning.\n\nF.6 ADAPTIVE PLANNING\n\nThe Adaptive Planning system enables dynamic evolution of the action plan by analyzing results after\neach iteration to generate extra actions addressing information gaps.\n\nIt starts by analyzing the most recently completed actions and performs these two substages:\n\nSource analysis and gap classification. The system evaluates the possible imbalances in the action\ncompletion if information came from internal or external sources and identifies possible scenarios to cover.\n\nDynamic action generation. After analyzing the sources and results from the previous actions, the\nsystem makes an LLM call to generate 1-5 extra candidate actions with a specific prioritization. After\ncandidate actions are generated, they go through a deduplication process to make sure the plan didn’t\ncover them already and incorporates the final subset into the priority action plan so they can be considered\nby the scheduler in the following iteration.\n\nF.7 CONTENT PROCESSING AND VECTOR STORE\n\nThe Content Processing system implements a pipeline for unified ingestion of documents in multiple\nformats (PDF, docx, HTML, JSON, plain text formats, etc.) that normalizes and cleans text inside the\ndocuments and websites retrieved during the research. Content is then deduplicated and chunkized, and\nembeddings are computed for each of these chunks.\n\nThe Vector Store implements the storage and retrieval of the content via JSON metadata and NumPy\nembedding metrices, enabling semantic similarity and keyword based searches\n\n23\n\n\fF.8 REPORT GENERATION\n\nMulti-Stage Synthesis Pipeline The Report Generation stage implements a four-stage synthesis\napproach: (1) thematic content clustering via vector searches, (2) source prioritization and deduplication,\n(3) LLM Synthesis with source tracking, and (4) Final report writing and citation resolution.\n\nThe system generates targeted search queries based on the research plan, including specific to ensure that\na predefined set of themes or sections (background, analysis, implementation, trends) are retrieved and\nwritten and prevents redundant analyses.\n\nUnified Citation System The citation system implements deferred resolution to keep consistency of\ncitations from section to section by referencing document IDs in the Vector Store and carrying them over\nfor each piece of synthesized text. A final citation resolution stage will assign the correct numbering to\neach document in the final report.\n\nF.9 DRBA PROMPTS\n\nThe DRBench agent relies on carefully designed prompts to orchestrate enterprise research workflows.\nThese prompts implement the core architectural principles: enterprise context enrichment, structured\nplanning decomposition, priority-based action generation, quantitative synthesis requirements, and adaptive\nresearch capabilities. The following five prompts represent the critical LLM interactions that enable\nsystematic enterprise research with proper source prioritization and citation tracking:\n\n• Enriched Query Generation (Prompt 1): Transforms basic research questions into enterprise-\ncontextualized queries by incorporating company information, stakeholder personas, and business\ncontext to guide subsequent research activities.\n\n• Research Planning (Prompt 2): Decomposes complex research questions into structured\ninvestigation areas with defined information needs, knowledge sources, and business rationales,\nenabling systematic coverage of the research space.\n\n• Action Generation (Prompt 3): Converts research objectives into prioritized executable\nactions with tool specifications and dependency relationships, emphasizing enterprise source\nprioritization over external sources.\n\n• Adaptive Action Generation (Prompt 4): Analyzes research progress to identify coverage\ngaps and source imbalances, generating complementary actions that enhance research depth\nand cross-validate critical findings.\n\n• Report Synthesis (Prompt 5): Orchestrates quantitative-first content synthesis with strict citation\nrequirements, ensuring numerical data leads analytical paragraphs and all claims are properly\nattributed to source documents.\n\nW Query Generation\n\nResearch Question: {dr question}\n\nCompany Context: {company name} is {company desc}.\n\nPersona Context: This analysis is requested by {name}, {role} in\n{department}.\n\nTheir responsibilities include: {responsibilities}.\n\nPrompt 1: Query Enrichment with Enterprise Context.\n\nG QUALITATIVE RESULTS\n\nShown below are some illustrative examples of how metrics are computed for different scenarios on a\ngiven test task.\n\n24\n\n\fW Research Planning (Complex Mode)\n\nDesign a comprehensive enterprise research strategy for:\n\"{question}\"\n\n{tools section}\n\nAs a senior enterprise researcher with deep business\nintelligence expertise, create a thorough investigation\nplan that combines rigorous research methodology with\nstrategic business analysis. Your goal is to provide insights\nthat drive informed decision-making in complex enterprise\nenvironments.\n\nEnterprise Research Design Principles:\n\n- Leverage proprietary internal data as competitive advantage while\nensuring external market context\n- Design investigations that directly inform strategic decisions and\nbusiness outcomes\n- Prioritize research areas that maximize ROI and strategic value to the\norganization\n- Balance comprehensive analysis with focused insights relevant to\nenterprise objectives.\n\nGenerate a JSON object with strategic research investigation\nareas:\n\n{output structure}\n\n.\n\nPrompt 2: Enterprise Research Planning with Investigation Areas. See example of the output structure\nin Listing 2\n\nG.1\n\nINSIGHTS RECALL\n\nWe showcase the insights recall metric using Task DR0002, whose DR Question is “How can\npersonalization drive sales in the retail industry and what strategies can be used for Lee’s Market in action?”\n(also shown in Table 7), which evaluates sales challenges and competitive landscape analysis.\n\nTable 10 shows the overall performance comparison between using Llama 3.1 405B and GPT-5 in\nour agent. Using GPT-5 in our agent results in increasing the insights recall score from 0.14 to 0.43,\nsuccessfully answering 3 out of 7 questions compared to Llama’s 1 out of 7.\n\nTable 10: Insights Recall Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We\nsummarize the number of questions answered successfully and unsuccessfully as well as the overall\ninsights recall score for the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nInsights Recall Score\n\nQuestions Answered\nSuccessfully\n\nQuestions Failed\n\n0.14\n\n1/7\n\n6/7\n\n0.43\n\n3/7\n\n4/7\n\n+0.29\n\n+2\n\n-2\n\nThe question-by-question breakdown in Table 11 reveals the specific questions where each approach\nsucceeded or failed. Both models successfully identified the insight related to online customer engagement,\nbut only GPT-5 was able to identify the number of loyalty program members and the customer data\ncollection rate. Neither model successfully answered the remaining 4 questions, indicating these insights\nmay not have been readily available in the source materials or that agents struggled to find the right insights.\n\n25\n\n\fW Action Generation\n\nGenerate specific executable actions for this research investigation\narea with SOURCE PRIORITIZATION:\n\nResearch Focus: {research focus}\n\nInformation Needs: {information needs}\n\nKnowledge Sources: {knowledge sources}\n\nResearch Approach: {research approach}\n\nAvailable Tools: {available tool names}\n\nTool Selection Guidelines: {tool guidelines}\n\nReturn JSON array of actions with:\n- \"type\": Action type (web search, enterprise api, url fetch, analyzer,\nlocal search)\n- \"description\": Clear description of what this action will accomplish\n- \"parameters\": Tool-specific parameters including query, search type,\netc.\n- \"priority\": Float 0.0-1.0 (enterprise sources: 0.7-1.0, external:\n0.4-0.7)\n- \"expected output\": What information this action should provide\n- \"preferred tool\": Specific tool class name to use\n\nPrompt 3: Priority-Based Action Generation with Source Awareness\n\nTable 11: Question-by-Question Insights Recall Analysis (Task DR0002). We breakdown the results\nquestion by question for the given task, highlighting specifically which question is answered correctly\nor incorrectly for each model.\n\nQuestion\n\nLlama 3.1 405B GPT-5\n\nOnline Customer Engagement with Personalized Recommendations\n\nNumber of Loyalty Program Members\n\nCustomer Data Collection Rate\n\nOnline Sales Growth\n\nEffectiveness of Personalized Marketing\n\nPersonalized Promotions vs Mass Promotions\n\nRetail Media Growth\n\nTotal Insights Recall Score\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0\n\n1.0\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n∆\n\n0.0\n\n+1.0\n\n+1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0/7.0\n\n3.0/7.0\n\n+2.0\n\nIn Table 12 we extend Table 4 and show all the groundtruth insights as well as each of the predicted\ninsights from using both Llama 3.1 405B and GPT-5. As before, we highlight in bold where the model\nwas able to accurately find details relevant to the expected insight, and show all the corresponding scores\nas given in Table 11.\n\nG.2 FACTUALITY\n\nThe factuality metric evaluation uses the same Task DR0002 to assess the accuracy and reliability of\ngenerated content. Table 13 presents the factuality performance comparison, showing that while using\nLlama 3.1 405B achieved 0.41 factuality (7 factual claims out of 17 total claims), where as using GPT-5\nreached 0.65 factuality (13 factual claim out of 20 total claims). This represents a significant improvement\n\n26\n\n\fW Adaptive Action Generation\n\nBased on the research progress so far, suggest new actions with\nINTELLIGENT SOURCE COMPLEMENTARITY:\n\nOriginal Research Query: {research query}\n\nCompleted Actions Summary: {completed actions summary}\n\nRecent Findings Analysis: {findings json}\n\nSource Composition Analysis: {internal findings}\n\nAvailable Tools: {available tool names}\n\nGenerate 1-5 new actions that:\n1. **Address gaps** in current research coverage\n2. **Balance source types** - if findings are heavily external,\nprioritize internal sources and vice versa\n3. **Build on discoveries** - leverage new information to explore\nrelated areas\n4. **Enhance depth** - dive deeper into promising\nfindings\n5. **Cross-validate** - verify critical findings through alternative\nsources\n\nEach action should have:\n- Strategic rationale for why this action is needed\nnow\n- Clear connection to research gaps or promising\nleads\n- Appropriate priority based on strategic value and source\nbalance\n- Specific parameters that build on existing knowledge\n\nPrompt 4: Gap-Driven Adaptive Action Generation\n\nin content reliability. This also highlights that GPT-5 is much better prepared to make accurate claims\nthat are sustained by evidence.\n\nTable 14 provides a detailed breakdown of factual versus unfactual claims. The agent using GPT-5\ngenerated 6 additional factual claims while producing 3 fewer unfactual claims, resulting in a net\nimprovement in accuracy percentage. This demonstrates that GPT-5 may generate a higher proportion\nof factual information than Llama 3.1 405B.\n\nThe impact of these factuality improvements of using GPT-5 over Llama 3.1 405B is summarized in\nTable 15. The 24.0 percentage point improvement in factuality represents enhanced content quality and\nresearch reliability. The increase in 3 total claims shows that GPT-5 can generate more content overall.\n\nH EXPERIMENTAL SETTINGS\n\nAll experiments were conducted on a cluster of 8 NVIDIA A100 GPUs (80GB each). For file generation\nand task construction, we primarily used the Llama-3.1-405B model, with decoding performed\nusing nucleus sampling at a temperature of 0.7 unless otherwise specified. For larger-scale evaluations of\nthe DRBench Agent, we also used closed-source models such as GPT-4o and GPT-5, alongside DeepSeek\nmodels, to enable comparison across open- and closed-source backbones.\n\nTo ensure reproducibility, the DRBench environment was deployed as a self-contained Docker container\nwith all supporting applications (Nextcloud, Mattermost, and Filebrowser) pre-configured. Each task was\n\n27\n\n\fTable 12: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have interacted\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n\n45% of Lee’s Market online customers\nengage with personalized product recom-\nmendations, resulting in a 25% increase in\naverage order value.\n(Score = 1.0)\n\n45% of online customers engaged with\npersonalized product recommendations,\nand among those engagers average order\nvalue increased by 25%.\n(Score = 1.0)\n\nAs of Q3 2024, Lee’s Market has 1.2 million\nloyalty program members.\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\nLee’s Market online sales grew 12% in Q2\n2024 compared to Q2 2023.\n\nRetailers excelling in personalized marketing\nare growing revenues about 10 percentage\npoints faster than their peers, according to\nBCG. By effectively using first-party customer\ndata, these leaders could unlock an estimated\n$570 billion in additional sales, highlighting\nthe importance of data-driven sales strategies\nfor growth.\n\nPersonalized promotions can deliver returns\nthree times higher than mass promotions,\nyet many retailers allocate under 5% of their\npromo budgets to personalization. One major\nchain increased its personalized promo spend\nfrom 1% to 10% by establishing a ”customer\ninvestment council,” resulting in $250 million\nin incremental sales.\n\nRetail media networks are expanding rapidly,\nwith retail media growing at approximately\n25% annually, offering retailers a profitable\nrevenue stream to reinvest in technology, data,\nand personnel. By integrating loyalty data,\nretailers like Sephora, which links 95% of\ntransactions to loyalty accounts, enhance preci-\nsion in product recommendations and provide\na seamless omnichannel experience, boosting\nconversion rates and customer lifetime value.\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nFrom Q2 2024 to Q3 2024, loyalty members\nincreased from 1,050,000 to 1,200,000\n(+150,000; +14.29%), average spend per\nmember rose from 24 to 25 (+4.17%), and\ntotal member spend increased from 25,200,000\nto 30,000,000 (+19.05%).\n(Score = 1.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n45% of Lee’s Market online customers engage\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n(Score = 0.0)\n\nA naive blended online AOV upside of\napproximately 11.25% is derived from 45%\nengagement multiplied by a 25% AOV lift\namong engagers.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nGrocers running data-driven loyalty cam-\npaigns have realized an average 3.8%\nlike-for-like sales uplift.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nsources\n\nindicate POS-enabled\nExternal\npersonalization can lift revenue 5%-15% and\nadvocate personalized e-receipts with relevant\noffers and coupons to extend post-purchase\nengagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 0.0)\n\nTable 13: Factuality Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We show\nthe number of factual and unfactual claims made by each model, as well as the overall factuality score\nfor the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nFactuality Score\n\nFactual Claims\n\nUnfactual Claims\n\n0.41\n\n7\n\n10\n\n0.65\n\n13\n\n7\n\n+0.24\n\n+6 claims\n\n-3 claims\n\n28\n\n\fW Report Synthesis\n\nAs an expert research analyst, synthesize the following content into\na coherent, insightful, and well-supported analysis for the theme:\n\"{theme}\" directly related to the overarching research question:\n\"{original question}\"\n\nSource Priority Guidelines:\n\n1. **\"internal\"**: Highest priority (internal company\ndocuments, proprietary files, confidential reports,\nenterprise chat messages, local documents, CRM data,\ninternal APIs, project management tools). Insights from\nthese sources should form the primary foundation of the\nanalysis.\n2. **\"external\"**: Medium priority (public web sources,\nacademic papers, industry reports, news articles). Use these\nto provide broader context, external validation, or contrasting\nperspectives.\n\n**Synthesis Requirements:**\n* **QUANTITATIVE PRIORITY:** Lead with numerical data, calculations, and\naggregations\n* Extract ALL percentages, costs, metrics, and performance\ndata\n* Perform mathematical operations: aggregate percentages, calculate\nincreases, sum totals\n* Example: \"Finance customers (35%) combined with healthcare\n(40%) represent 75% of regulated industry concerns\n[DOC:doc 1][DOC:doc 2]\"\n\n* **FACT VERIFICATION (CRITICAL):**\n* ONLY state what documents explicitly contain - no inference or\nextrapolation\n* Use exact quotes for key numerical claims: \"As stated in the\ndocument: ’[exact quote]’ [DOC:doc id]\"\n\n* **Citation Usage (Critical):**\n* **Format:** Reference sources by their document ID: \"Internal review\nshows 15% increase [DOC:doc 079c2e0f 1752503636]\"\n* **NEVER HALLUCINATE CITATIONS:** Only use provided doc id\nvalues\n* **Cite every numerical claim and calculation with source\ndocuments**\n\nGenerate 2-4 paragraphs of synthesized analysis with proper inline\ncitations.\n\nPrompt 5: Report Section Synthesis with Citation Requirements\n\nexecuted by instantiating a fresh container to avoid state leakage across runs. We capped the number of\nagent iterations according to the settings described in Section 5, with each iteration limited by a fixed\ncomputational budget.\n\nFor model outputs, we standardized all prompts and evaluation pipelines across backbones, using identical\nresearch questions, company contexts, and injected insight sets. To avoid stochastic variability, we repeated\ngeneration three times per task and reported averaged scores.\n\nFinally, all supporting scripts, environment configurations, and evaluation code are fully containerized,\nenabling consistent replication of our reported results across hardware setups.\n\n29\n\n\fTable 14: Factuality Content Analysis (Task DR0002). We show the number of factual and unfactual\nclaims made by each model, highlighting the factuality accuracy of each model for the given task.\n\nAgent\n\nFactual Unfactual Accuracy\n\nLlama 3.1 405B\n\n7 claims\n\n10 claims\n\nGPT-5\n\nChange\n\n13 claim\n\n7 claims\n\n+6\n\n-3\n\n+24.0%\n\n41.0%\n\n65.0%\n\nTable 15: Factuality Summary (Task DR0002). We summarize the factuality result improvements made\nby using GPT-5 over Llama 3.1 405B.\n\nImpact Category\n\nValue\n\nFactuality Improvement\n\n+24.0 percentage points\n\nClaims Added\n\n3 total claims\n\nAccuracy Enhancement\n\nFrom 41.0% to 65.0%\n\nContent Quality\n\nMore grounded information\n\nTask Domain\n\nTask Industry\n\nSales\n\nRetail\n\nI DATA SYNTHESIS AND DRBA COST DETAILS\n\nTo generate the DRBench tasks, we combined external insight extraction with internal file synthesis. Each\ntask included an average of 10 supporting files spanning heterogeneous formats (PDF, DOCX, PPTX,\nXLSX, and JSONL), with each file containing roughly 5 paragraphs of content. Files were designed\nto embed injected insights while mixing in distractor material, ensuring realistic enterprise complexity\nwithout exceeding practical runtime or storage budgets.\n\nData synthesis was primarily powered by GPT-4o. During task construction, GPT-4o was responsible for (1)\nextracting structured insights from public web sources, (2) adapting these insights into ente\n... [Result truncated]"
      },
      {
        "role": "assistant",
        "content": "I need to continue extracting information from the paper to identify the benchmarks mentioned in the related work section. Let me try a different approach:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>run_python_code</tool_name>\n<arguments>\n{\n\"code_block\": \"import re\\n\\n# Let's extract the related work section from the PDF content\\ndef extract_related_work(text):\\n    # Look for the Related Work section\\n    related_work_pattern = r'2\\\\s+RELATED WORK\\\\s+([\\\\s\\\\S]+?)\\\\s+3\\\\s+DRBench'\\n    match = re.search(related_work_pattern, text)\\n    if match:\\n        return match.group(1).strip()\\n    return None\\n\\n# Function to extract benchmark names and their details\\ndef extract_benchmarks(related_work_text):\\n    # Look for benchmark names mentioned in the text\\n    benchmark_pattern = r'([A-Za-z0-9\\\\-]+(?:Bench|Benchmark|BENCH|BENCHMARK|Arena|ARENA|Gym|GYM|World|WORLD|Web|WEB)[A-Za-z0-9\\\\-]*)'\\n    benchmarks = re.findall(benchmark_pattern, related_work_text)\\n    \\n    # Remove duplicates while preserving order\\n    unique_benchmarks = []\\n    for b in benchmarks:\\n        if b not in unique_benchmarks:\\n            unique_benchmarks.append(b)\\n    \\n    return unique_benchmarks\\n\\n# Sample PDF content from the previous response\\npdf_content = \\\"\\\"\\\"5\\n2\\n0\\n2\\n\\np\\ne\\nS\\n0\\n3\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n2\\n7\\n1\\n0\\n0\\n.\\n0\\n1\\n5\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nDRBench: A REALISTIC\\n\\nBENCHMARK FOR ENTERPRISE DEEP RESEARCH\\n\\nAmirhossein Abaskohi1,2 Tianyi Chen1 Miguel Mu˜noz-M´armol1 Curtis Fox1,2\\nAmrutha Varshini Ramesh1,2 ´Etienne Marcotte1 Xing Han L`u3,4 Nicolas Chapados1\\n\\nSpandana Gella1,3 Christopher Pal1,3,5 Alexandre Drouin1,3\\n\\nIssam H. Laradji1,2\\n\\n1ServiceNow Research\\n\\n2University of British Columbia\\n\\n3Mila – Quebec AI Institute\\n\\n4McGill University\\n\\n5Canada CIFAR AI Chair\\n\\nABSTRACT\\n\\nWe introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended\\ndeep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple\\nquestions or web-only queries, DRBench evaluates agents on multi-step queries (for\\nexample, \"What changes should we make to our product roadmap to ensure compliance\\nwith this standard?\") that require identifying supporting facts from both the public web\\nand private company knowledge base. Each task is grounded in realistic user personas\\nand enterprise context, spanning a heterogeneous search space that includes productivity\\nsoftware, cloud file systems, emails, chat conversations, and the open web. Tasks\\nare generated through a carefully designed synthesis pipeline with human-in-the-loop\\nverification, and agents are evaluated on their ability to recall relevant insights, maintain\\nfactual accuracy, and produce coherent, well-structured reports. We release 15 deep\\nresearch tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We\\ndemonstrate the effectiveness of DRBench by evaluating diverse DR agents across open-\\nand closed-source models (such as GPT, Llama, and Qwen) and DR strategies, high-\\nlighting their strengths, weaknesses, and the critical path for advancing enterprise deep\\nresearch. Code is available at https://github.com/ServiceNow/drbench.\\n\\n1\\n\\nINTRODUCTION\\n\\nOrganizations today face a strong need to find useful insights in a world full of overwhelming information.\\nValuable insights are often hidden in noisy data, which can contain many distracting or irrelevant details that\\nobscure the insights that really matter. This challenge is present in enterprise settings, where data is spread\\nacross many applications and stored in different formats (e.g., PDFs, spreadsheets, emails, and internal tools)\\nmaking extracting relevant information difficult. To uncover these hidden, valuable insights, one must con-\\nduct what is known as deep research. This task involves asking high-level strategic questions (e.g, \"What\\nchanges should we make to our roadmap to remain compliant?\"), planning sub-questions, retrieving and\\nevaluating relevant materials, and producing a clear, actionable summary grounded in data sources (Zheng\\net al., 2025; Xu & Peng, 2025; Du et al., 2025). These tasks are typically performed by domain experts\\nusing a mix of search engines, communication platforms, and business applications in iterative, high-effort\\nworkflows (Mialon et al., 2024), which unfortunately require a significant amount of human effort.\\n\\nOne promising solution to reducing this human effort is agent-based deep research, which uses autonomous\\nsoftware agents to search, extract, and synthesize information across fragmented sources into an\\ninsightful report. Recently, LLM-based agents have emerged as promising assistants for deep research.\\nSystems such as Local Deep Researcher (LearningCircuit, 2025), Deep-Searcher (Tech, 2024), and\\nDeepResearcher (Zheng et al., 2025) propose modular agent pipelines that combine retrieval, reasoning,\\nand summarization over documents and web sources. Architectures like OpenHands (All-HandsAI, 2024),\\nOpenManus (FoundationAgents, 2024), and smolagents (HuggingFace, 2024) extend these capabilities\\nto include collaboration, multi-modal search, and complex tool use in enterprise workflows (Xu & Peng,\\n2025). Despite these advances, evaluating such systems remains an open challenge.\\n\\nMost existing benchmarks evaluate narrow aspects such as report factuality (Coelho et al., 2025), web-only\\nsynthesis (Bosse et al., 2025), or tabular analytics (Sahu et al., 2025), but they do not assess whether agents\\nidentify the most salient insights, remain faithful to retrieved evidence, or adapt to enterprise contexts. To\\naddress these limitations, we introduce DRBench, a benchmark designed to evaluate LLM agents on open-\\n\\n1\\n\\n2 RELATED WORK\\n\\nDeep Research Benchmarks. With the growing capabilities of LLMs in research and reasoning tasks,\\nseveral benchmarks have emerged to evaluate their performance in realistic, multi-step and long-horizon sce-\\nnarios, including Deep Research Bench (Bosse et al., 2025), DeepResearch Bench (Du et al., 2025), Deep-\\nResearchGym (Coelho et al., 2025), ResearcherBench (Xu et al., 2025b), Mind2Web2 (Gou et al., 2025)\\nand GAIA (Mialon et al., 2024). As summarized in Table 1, these efforts typically focus on web-only tasks,\\nmeasuring retrieval or synthesis quality in controlled but limited settings. In contrast, DRBench is the first to\\ncombine web retrieval with local enterprise data, grounding tasks in persona- and domain-specific contexts.\\n\\nEnterprise Environments. Realistic enterprise environments have become an important testbed for\\nevaluating agents in complex multi-application workflows. CRMArena-Pro (Huang et al., 2025a;b) targets\\nsales and CPQ pipelines through persona-grounded dialogues, but is limited to conversational sales\\nworkflows. OSWorld (Xie et al., 2024) and OSWorld-Gold (Abhyankar et al., 2025) benchmark agents\\nin general-purpose desktop environments, using applications such as Microsoft Word and Excel, yet their\\nfocus remains on computer task execution rather than enterprise deep research. TheAgentCompany (Xu\\net al., 2025a) evaluates collaboration among autonomous agents for programming, browsing, and\\ncommunication, though the tasks are computer-use focused and do not assess deep research capabilities.\\nWorkArena (Drouin et al., 2024; Boisvert et al., 2024) offers a realistic enterprise environment with\\nknowledge work tasks for web agents, though it does not support evaluation of deep research capabilities.\\nIn contrast, DRBench offers a domain-grounded enterprise environment with applications that would\\nrealistically be encountered in organizations. Tasks are tied to concrete personas and roles, requiring agents\\nto search, reason, and synthesize insights across diverse formats, including spreadsheets, PDFs, wikis,\\nemails, and presentations, reflecting realistic enterprise deep research.\\n\\nDeep Research Agents. A growing line of work explores agents for multi-step search and synthesis\\nacross diverse information sources. LangChain's Local Deep Researcher (LearningCircuit, 2025) and\\nZilliz's Deep-Searcher provide modular pipelines for iterative querying and summarization, while DeepRe-\\nsearcher (Zheng et al., 2025) uses RL to enable planning, cross-validation, and self-reflection. Commercial\\nsystems such as Gemini Deep Research and Manus.ai synthesize web-based reports with citations,\\nand open-source frameworks like OpenHands (All-HandsAI, 2024), OpenManus (FoundationAgents,\\n2024), and smolagents (HuggingFace, 2024) offer alternative architectures. Recent work also introduces\\ntask-agnostic frameworks for long-form synthesis and evaluation paradigms such as Mind2Web 2 (Gou\\net al., 2025), which treat agents as judges of browsing trajectories. Building on these efforts, DRBench an-\\nalyzes their strengths and limitations in enterprise contexts, showing that current agents still fall short in\\nconsistently extracting and grounding critical insights within complex, heterogeneous environments.\\n\\n3 DRBench - AN ENTERPRISE DEEP RESEARCH BENCHMARK\\n\\nTo evaluate agents on complex, open-ended enterprise deep research tasks, we designed DRBench with\\nthree guiding principles: it requires agents to integrate both public web data and local enterprise documents,\\nit involves both web search and enterprise application use, and it is hosted in an interactive and reproducible\\n\\\"\\\"\\\"\\n\\n# Extract the related work section\\nrelated_work_text = extract_related_work(pdf_content)\\n\\n# Extract benchmarks from the related work section\\nif related_work_text:\\n    benchmarks = extract_benchmarks(related_work_text)\\n    print(\\\"Benchmarks mentioned in the Related Work section:\\\")\\n    for i, benchmark in enumerate(benchmarks, 1):\\n        print(f\\\"{i}. {benchmark}\\\")\\n    \\n    # Extract more detailed information about each benchmark\\n    print(\\\"\\\\nDetailed information about benchmarks:\\\")\\n    for benchmark in benchmarks:\\n        # Find sentences containing the benchmark name\\n        benchmark_pattern = rf'([^.!?]*{re.escape(benchmark)}[^.!?]*[.!?])'\\n        mentions = re.findall(benchmark_pattern, related_work_text)\\n        print(f\\\"\\\\n{benchmark}:\\\")\\n        for mention in mentions:\\n            print(f\\\"  - {mention.strip()}\\\")\\nelse:\\n    print(\\\"Could not extract the Related Work section.\\\")\",\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\"\n}\n</arguments>\n</use_mcp_tool>"
      },
      {
        "role": "user",
        "content": "Execution(Results: [], Logs: Logs(stdout: ['Benchmarks mentioned in the Related Work section:\\n1. ResearchGym\\n2. ResearcherBench\\n3. Mind2Web2\\n4. DRBench\\n5. CRMArena-Pro\\n6. OSWorld\\n7. OSWorld-Gold\\n8. WorkArena\\n9. Mind2Web\\n\\nDetailed information about benchmarks:\\n\\nResearchGym:\\n  - , 2025), Deep-\\nResearchGym (Coelho et al.\\n\\nResearcherBench:\\n  - , 2025), ResearcherBench (Xu et al.\\n\\nMind2Web2:\\n  - , 2025b), Mind2Web2 (Gou et al.\\n\\nDRBench:\\n  - In contrast, DRBench is the first to\\ncombine web retrieval with local enterprise data, grounding tasks in persona- and domain-specific contexts.\\n  - In contrast, DRBench offers a domain-grounded enterprise environment with applications that would\\nrealistically be encountered in organizations.\\n  - Building on these efforts, DRBench an-\\nalyzes their strengths and limitations in enterprise contexts, showing that current agents still fall short in\\nconsistently extracting and grounding critical insights within complex, heterogeneous environments.\\n\\nCRMArena-Pro:\\n  - CRMArena-Pro (Huang et al.\\n\\nOSWorld:\\n  - OSWorld (Xie et al.\\n  - , 2024) and OSWorld-Gold (Abhyankar et al.\\n\\nOSWorld-Gold:\\n  - , 2024) and OSWorld-Gold (Abhyankar et al.\\n\\nWorkArena:\\n  - WorkArena (Drouin et al.\\n\\nMind2Web:\\n  - , 2025b), Mind2Web2 (Gou et al.\\n  - Recent work also introduces\\ntask-agnostic frameworks for long-form synthesis and evaluation paradigms such as Mind2Web 2 (Gou\\net al.\\n'], stderr: []), Error: None)"
      }
    ]
  },
  "sub_agent_message_history_sessions": {},
  "step_logs": [
    {
      "step_name": "Main | Task Start",
      "message": "--- Starting Task Execution: folder_task_001 ---",
      "timestamp": "2025-12-05 18:35:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Initialization",
      "message": "ToolManager initialized, loaded servers: ['tool-python', 'tool-vqa', 'tool-vqa-enhanced', 'tool-transcribe', 'tool-audio-enhanced', 'tool-video-enhanced', 'tool-reasoning', 'tool-reader', 'tool-rag']",
      "timestamp": "2025-12-05 18:35:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Initialization",
      "message": "ToolManager initialized, loaded servers: ['tool-google-search', 'tool-python', 'tool-vqa', 'tool-vqa-enhanced', 'tool-audio-enhanced', 'tool-video-enhanced', 'tool-reader']",
      "timestamp": "2025-12-05 18:35:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Initialization",
      "message": "ToolManager initialized, loaded servers: ['tool-python', 'tool-vqa', 'tool-vqa-enhanced', 'tool-audio-enhanced', 'tool-video-enhanced', 'tool-reader', 'tool-rag']",
      "timestamp": "2025-12-05 18:35:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Initialization",
      "message": "LLMClient openai claude37_sonnet initialization completed.",
      "timestamp": "2025-12-05 18:35:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent",
      "message": "Start task with id: folder_task_001",
      "timestamp": "2025-12-05 18:35:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent",
      "message": "Task description: # Task\n\n假设你是一名科研工作者，正在为某人工智能研究团队开展大模型评测基准的系统性梳理工作。请根据提供的材料，撰写一份《Deep Research相关Benchmark数据集调研报告》。\n具体而言，依据图片image0中所示的字段要求，提取并结构化提供的pdf论文在相关工作部分提到的每篇论文对应的Benchmark特征，其中请忽略“数据集example”指标。\n报告需整合文献内容与公开可查的权威技术资料（如论文原文、官方GitHub仓库、学术数据库或评测平台），\n确保所有信息真实、准确、可验证，并在文中注明数据来源。\n报告语言应采用正式书面语，结构清晰，包含引言、调研方法、各论文Benchmark信息汇总表、分析与观察、参考资料等部分，\n最终形成一份可用于团队技术决策的标准化调研文档，字数在1500至2000之间。\n\n\n## Folder Contents Summary\n\nFolder: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001\nTotal files: 12\n  - data: 5\n  - document: 1\n  - image: 1\n  - other: 5\n\n\n## Document Contents\n\n\n### paper.pdf\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\">\n5\n2\n0\n2\n\np\ne\nS\n0\n3\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n2\n7\n1\n0\n0\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nDRBench: A REALISTIC\n\nBENCHMARK FOR ENTERPRISE DEEP RESEARCH\n\nAmirhossein Abaskohi1,2 Tianyi Chen1 Miguel Mu˜noz-M´armol1 Curtis Fox1,2\nAmrutha Varshini Ramesh1,2 ´Etienne Marcotte1 Xing Han L`u3,4 Nicolas Chapados1\n\nSpandana Gella1,3 Christopher Pal1,3,5 Alexandre Drouin1,3\n\nIssam H. Laradji1,2\n\n1ServiceNow Research\n\n2University of British Columbia\n\n3Mila – Quebec AI Institute\n\n4McGill University\n\n5Canada CIFAR AI Chair\n\nABSTRACT\n\nWe introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended\ndeep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple\nquestions or web-only queries, DRBench evaluates agents on multi-step queries (for\nexample, “What changes should we make to our product roadmap to ensure compliance\nwith this standard?”) that require identifying supporting facts from both the public web\nand private company knowledge base. Each task is grounded in realistic user personas\nand enterprise context, spanning a heterogeneous search space that includes productivity\nsoftware, cloud file systems, emails, chat conversations, and the open web. Tasks\nare generated through a carefully designed synthesis pipeline with human-in-the-loop\nverification, and agents are evaluated on their ability to recall relevant insights, maintain\nfactual accuracy, and produce coherent, well-structured reports. We release 15 deep\nresearch tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We\ndemonstrate the effectiveness of DRBench by evaluating diverse DR agents across open-\nand closed-source models (such as GPT, Llama, and Qwen) and DR strategies, high-\nlighting their strengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.\n\n1\n\nINTRODUCTION\n\nOrganizations today face a strong need to find useful insights in a world full of overwhelming information.\nValuable insights are often hidden in noisy data, which can contain many distracting or irrelevant details that\nobscure the insights that really matter. This challenge is present in enterprise settings, where data is spread\nacross many applications and stored in different formats (e.g., PDFs, spreadsheets, emails, and internal tools)\nmaking extracting relevant information difficult. To uncover these hidden, valuable insights, one must con-\nduct what is known as deep research. This task involves asking high-level strategic questions (e.g, “What\nchanges should we make to our roadmap to remain compliant?”), planning sub-questions, retrieving and\nevaluating relevant materials, and producing a clear, actionable summary grounded in data sources (Zheng\net al., 2025; Xu & Peng, 2025; Du et al., 2025). These tasks are typically performed by domain experts\nusing a mix of search engines, communication platforms, and business applications in iterative, high-effort\nworkflows (Mialon et al., 2024), which unfortunately require a significant amount of human effort.\n\nOne promising solution to reducing this human effort is agent-based deep research, which uses autonomous\nsoftware agents to search, extract, and synthesize information across fragmented sources into an\ninsightful report. Recently, LLM-based agents have emerged as promising assistants for deep research.\nSystems such as Local Deep Researcher (LearningCircuit, 2025), Deep-Searcher (Tech, 2024), and\nDeepResearcher (Zheng et al., 2025) propose modular agent pipelines that combine retrieval, reasoning,\nand summarization over documents and web sources. Architectures like OpenHands (All-HandsAI, 2024),\nOpenManus (FoundationAgents, 2024), and smolagents (HuggingFace, 2024) extend these capabilities\nto include collaboration, multi-modal search, and complex tool use in enterprise workflows (Xu & Peng,\n2025). Despite these advances, evaluating such systems remains an open challenge.\n\nMost existing benchmarks evaluate narrow aspects such as report factuality (Coelho et al., 2025), web-only\nsynthesis (Bosse et al., 2025), or tabular analytics (Sahu et al., 2025), but they do not assess whether agents\nidentify the most salient insights, remain faithful to retrieved evidence, or adapt to enterprise contexts. To\naddress these limitations, we introduce DRBench, a benchmark designed to evaluate LLM agents on open-\n\n1\n\n \n \n \n \n \n \n\fFigure 1: DRBench pipeline. 1 The Task Context defines the deep research question grounded by the\ncompany and persona given to the agent. 2 Task Data, including both distractor and injected groundtruth\ninsights in different formats (PDFs, DOCX, PPTX, XLSX, chats, etc.) are loaded into the enterprise\nenvironment’s applications. 3 The DRBenchAgent accesses both public web sources and local enterprise\ndata to extract relevant insights for the research question. 4 It produces a structured research report,\nwhich is 5 evaluated for Insight Recall (detecting injected groundtruth insights), Factuality (verifying\nclaims are correctly cited), and Report Quality.\n\nended, multi-step and long-horizon deep research tasks grounded in realistic enterprise contexts. As Fig. 1 il-\nlustrates, DRBench includes a suite of queries grounded in user personas and organizational scenarios, requir-\ning agents to search across real applications such as cloud file storage (Nextcloud), enterprise chat (Matter-\nmost), and user file systems, and to reason over formats like spreadsheets, slide decks, and PDFs. Our evalu-\nation framework introduces three scoring axes using LLM-as-a-judge methods inspired by G-Eval (Liu et al.,\n2023): (1) Insight Recall and Distractor Avoidance, which together evaluate whether the agent surfaces the\nmost salient injected insights while avoiding distractor content; (2) Factuality, which uses a TREC-RAG\npipeline (Wang et al., 2024) to verify whether claims are correctly grounded in their cited sources; and (3) Re-\nport Quality, which measures the coherence, completeness, and overall readability of the synthesized report.\n\nWe conduct a comparative study of agent architectures inspired by recent work (Zheng et al., 2025;\nXu & Peng, 2025; LearningCircuit, 2025; Zheng et al., 2025), analyzing how well they perform on\nDRBench across planning, insight identification, and grounding on facts. Our results show that while\nagents are competent at document retrieval and summarization, they often miss high-value insights, cite\nirrelevant evidence, or produce incoherent explanations, highlighting the limitations of current architectures\nand the need for more targeted innovation.\n\nOur contributions are as follows: (1) We introduce DRBench, the first benchmark for evaluating LLM\nagents on complex enterprise deep research tasks combining public web sources with private organizational\ndata; (2) We provide a suite of 15 high-level research tasks with 114 sub-questions spanning 10 domains,\nincluding Sales, Cybersecurity, and Compliance, each grounded in realistic company contexts and personas;\n(3) We design a reproducible enterprise environment integrating realistic enterprise applications like chat,\ncloud storage, emails, and documents; (4) We propose a scalable pipeline that generates realistic research\nquestions and insights by combining web facts with synthesized internal data; and (5) We develop an\nevaluation framework that scores agent reports on insight recall and distractor avoidance, factuality, and\noverall report quality.\n\n2 RELATED WORK\n\nDeep Research Benchmarks. With the growing capabilities of LLMs in research and reasoning tasks,\nseveral benchmarks have emerged to evaluate their performance in realistic, multi-step and long-horizon sce-\nnarios, including Deep Research Bench (Bosse et al., 2025), DeepResearch Bench (Du et al., 2025), Deep-\nResearchGym (Coelho et al., 2025), ResearcherBench (Xu et al., 2025b), Mind2Web2 (Gou et al., 2025)\nand GAIA (Mialon et al., 2024). As summarized in Table 1, these efforts typically focus on web-only tasks,\n\n2\n\nLoading to Environment2Load Ground Truth Insights Factuality Insight RecallReport QualityIs each claim in the reportproperly cited?Were the injected insightsdiscovered? Is the report clear to read andcomplete in addressing the points?5PersonaCompanyDR QuestionLee's Market: a regional Asian supermarket chainJohn Doe: Regulatory Aﬀairs ManagerNextCloudFileShareFile SystemLocal FilesMattermostChatRoundCubeEmailEnterprise Data3ReportGeneration41Context5(a) Tracking lot codes helps grocers reduce food waste and keep products fresh. [Cite: grocerydive.com](b) Lee's Market tracks 250 high- risk food products as of Q3 2024, aﬀecting 30% of inventory. [Cite: food- safety- compliance.pdf] ...(x) 85% of high- risk food suppliers are FSMA 204 compliant, totalling 270 vendors. [Cite: RoundCube email]EvaluationTask ContextHow can Lee's Market leverage FSMA 204 regulations to enhance food safety and customer trust?3Public DataReportAccessingWebsitesChat ConversationsPresentations:PPTXTask DataTabular Data:CSV, XLSXDocuments: Docx, PDF, MarkdownE- mailsI) 8% reduction in food waste during Q2 2024, saving $1.2 million....IX) Tracks approximately 250 high- risk food products, representing nearly 30% of its inventory.Injected InsightsSynthesized Files with Distractor Content and Injected InsightsLoad Sources5\fTable 1: Comparison of deep research benchmarks (top) and AI agent benchmarks with a computer environ-\nment (middle). Columns report dataset size, whether both public and local data are required, the provided\nenvironment type, task domains, task description, and evaluation method. Unlike prior work, DRBench com-\nbines public web retrieval with local enterprise data in realistic enterprise applications and evaluates both\ninsight recall, distractor avoidance and report quality. Task Description: types of tasks covered by the\nbenchmark: WR for Web Research, DR for Deep Research with both public and local data, CU for\nComputer Use and/or Mobile Use. DRBench has 114 total # groundtruth insights that need to be extracted\nto address the 15 DR Questions. Example groundtruth insights can be found at Table 8 in Appendix A.\n\nBenchmark\n\n# groundtruth\n\nPublic &\nLocal Data\n\nProvides\nEnv\n\nTask\nDomain\n\nTask\nDescription\n\nMain Evaluation Method\n\nDeep Research Bench (Bosse et al., 2025)\nDeepResearch Bench (Du et al., 2025)\nDeepResearchGym (Coelho et al., 2025)\nResearcherBench (Xu et al., 2025b)\nLiveDRBench (Java et al., 2025)\nBrowseComp-Plus (Chen et al., 2025)\nMind2Web 2 (Gou et al., 2025)\nGAIA (Mialon et al., 2024)\nGAIA2 (Andrews et al., 2025)\n\nTheAgentCompany (Xu et al., 2025a)\nOSWorld (Xie et al., 2024)\n\nDRBench\n\n89\n100\n1,000\n65\n100\n1,005\n130\n466\n963\n\n175\n369\n\n114\n\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n\n✗\n✗\n\n✓\n\n✓\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n\n✓\n✓\n\n✓\n\nWR\nWR\nWR\n\nGeneric WR & CU Answer Accuracy\nGeneric\nGeneric\nAI\n\nInsight Recall\nDocument Retrieval\nInsight Recall, Factuality\nGeneric WR & CU Insight Precision, Recall\nGeneric\nGeneric\nGeneric\nGeneric\n\nAnswer Accuracy, URL Recall\nPartial Completion\nAnswer Accuracy\nAction Accuracy\n\nWR\nWR\nWR\nCU\n\nEnterprise\nGeneric\n\nEnterprise\n\nCU\nCU\n\nDR\n\nTask Completion, Efficiency\nTask Completion\n\nInsight Recall\n\nmeasuring retrieval or synthesis quality in controlled but limited settings. In contrast, DRBench is the first to\ncombine web retrieval with local enterprise data, grounding tasks in persona- and domain-specific contexts.\n\nEnterprise Environments. Realistic enterprise environments have become an important testbed for\nevaluating agents in complex multi-application workflows. CRMArena-Pro (Huang et al., 2025a;b) targets\nsales and CPQ pipelines through persona-grounded dialogues, but is limited to conversational sales\nworkflows. OSWorld (Xie et al., 2024) and OSWorld-Gold (Abhyankar et al., 2025) benchmark agents\nin general-purpose desktop environments, using applications such as Microsoft Word and Excel, yet their\nfocus remains on computer task execution rather than enterprise deep research. TheAgentCompany (Xu\net al., 2025a) evaluates collaboration among autonomous agents for programming, browsing, and\ncommunication, though the tasks are computer-use focused and do not assess deep research capabilities.\nWorkArena (Drouin et al., 2024; Boisvert et al., 2024) offers a realistic enterprise environment with\nknowledge work tasks for web agents, though it does not support evaluation of deep research capabilities.\nIn contrast, DRBench offers a domain-grounded enterprise environment with applications that would\nrealistically be encountered in organizations. Tasks are tied to concrete personas and roles, requiring agents\nto search, reason, and synthesize insights across diverse formats, including spreadsheets, PDFs, wikis,\nemails, and presentations, reflecting realistic enterprise deep research.\n\nDeep Research Agents. A growing line of work explores agents for multi-step search and synthesis\nacross diverse information sources. LangChain’s Local Deep Researcher (LearningCircuit, 2025) and\nZilliz’s Deep-Searcher provide modular pipelines for iterative querying and summarization, while DeepRe-\nsearcher (Zheng et al., 2025) uses RL to enable planning, cross-validation, and self-reflection. Commercial\nsystems such as Gemini Deep Research and Manus.ai synthesize web-based reports with citations,\nand open-source frameworks like OpenHands (All-HandsAI, 2024), OpenManus (FoundationAgents,\n2024), and smolagents (HuggingFace, 2024) offer alternative architectures. Recent work also introduces\ntask-agnostic frameworks for long-form synthesis and evaluation paradigms such as Mind2Web 2 (Gou\net al., 2025), which treat agents as judges of browsing trajectories. Building on these efforts, DRBench an-\nalyzes their strengths and limitations in enterprise contexts, showing that current agents still fall short in\nconsistently extracting and grounding critical insights within complex, heterogeneous environments.\n\n3 DRBench - AN ENTERPRISE DEEP RESEARCH BENCHMARK\n\nTo evaluate agents on complex, open-ended enterprise deep research tasks, we designed DRBench with\nthree guiding principles: it requires agents to integrate both public web data and local enterprise documents,\nit involves both web search and enterprise application use, and it is hosted in an interactive and reproducible\n\n3\n\n\fFigure 2: DRBench Task Generation Pipeline. The pipeline comprises five main stages during each LLMs\ngenerate candidate data such as company context, insights, and research questions, while human annotators\nverify quality and select the final version. Stages S1–S5 denote the five generation steps.\n\nenterprise environment. These principles ensure that the benchmark reflects realistic enterprise workflows\nand provides a controlled yet challenging setting for research agents.\n\nThe Enterprise Search Environment. A unique aspect of DRBench is its realistic enterprise search\nenvironment. When addressing DR Questions like ”What changes should we make to our product roadmap\nto ensure compliance with this standard?”, the DR agents would need to navigate such environment and\nsearch across both public and private data sources to uncover relevant insights.\n\nPublic insights include information available on the open web or otherwise accessible to general users.\nLocal insights, on the other hand, come from an enterprise’s private systems. These insights are embedded\nwithin a vast search space that spans multiple data types (emails, slide decks, chat conversations, and Excel\nsheets) which reflect the complexity of real enterprise data ecosystems. This environment is populated with\ndata from different applications, accessible to both web-based agents and API-calling agents. For example,\nan app like Mattermost can be used to host chat conversations (see Appendix E for examples of the applica-\ntions). The goal of the DR Agent is to effectively navigate these public and private data sources to address\ncomplex, high-level DR questions. For the environment implementation details, please see Section D.\n\nTask Definition. Each task is associated with a deep research question Qi and Task Context C which\nincludes company information and the user persona. Each task also has a corresponding set of groundtruth\ninsights I consisting of relevant private insights Il (we also refer to this as internal insights), distractor\nprivate insights Id, and public insights Ip. Each private insight, whether relevant or a distractor, is embedded\ninto a file fi which could take the form of a PDF, Excel sheet, slide deck, chat log, and so on. The agent’s\ntask is to generate a report by extracting the public insights Ip from accessible sources such as the web,\nwhile also extracting the private insights Il from the files hosted in the enterprise environment. At the same\ntime, the agent must avoid extracting the distractor insights Id, which are not relevant to the DR question.\n\nDRBench provides 15 realistic deep research tasks explicitly framed around enterprise environments.\nEach task is associated with public insights extracted form quality, time-invariant URLs and local insights\nembedded within synthetic enterprise data, typically spanning 2–4 applications and 3–16 supporting files\n(see Section B). Tasks are distributed across 10 enterprise domains (such as Sales and Compliance - the\nfull list is in Section B) and divided between easy, medium, and hard categories that indicates the difficulty\nof addressing the DR Question. Finally, DRBench is fully self-hosted, with dated URLs and reproducible\nevaluation scripts to ensure stability and fair comparison across agent methods.\n\n3.1 DATA GENERATION\n\nTo create realistic and reproducible deep research tasks, DRBench employs a five-stage pipeline (Figure 2)\nthat combines large-scale LLM generation with human-in-the-loop verification. The pipeline helps us\ngenerate candidate company contexts, personas, questions, insights, and supporting files using LLM\nModels such as Llama-3.1-8B-Instruct, Llama-3.1-405B (Dubey et al., 2024). Three human annotators\nthen validate the generated content to ensure that they are realistic and plausible.\n\nThe pipeline has been used to generate 15 tasks with 114 groundtruth insights across 10 enterprise domains,\neach grounded in realistic personas and company profiles. We control the difficulty of each task by setting\n\n4\n\n\fthe number of insights, file types and application types. The complete list of tasks is provided in Section B.\nRefer to Section I for details on the cost of using data generation.\n\nStage 1: Company and Persona Generation. This stage produces the synthetic company profile and\nuser persona that form the Task Context C. LLMs were used to generate company descriptions detailing\nthe industry vertical, key products, market position, and competitive landscape. In parallel, they were used\nto create realistic personas across departments (e.g., a Regulatory Affairs Manager or a Market Research\nAnalyst) that serve as the role grounding for the final deep research question. They were then refined\nby human experts. The prompts used for this stage are provided in Section O.1 and the list of companies\nare given in Appendix B.\n\nStage 2: Public Source and Insight Collection. Given the company and persona context from Stage 1,\nwe have retrieved candidate URLs relevant to the specified domain and company background. To ensure\nquality, time-invariant insights, the search is restricted to dated, journal-based or industry-report websites\nthat provide authoritative information. Thus, the collected URLs and their contents are expected to be stable\nin time. Human annotators then review the candidate URLs and select one that is both topically aligned\nand provides insights into the topic. The selected page becomes the Task URL included in C. Its HTML\ncontent is parsed, and LLMs are prompted to extract business-relevant insights, which are subsequently\nfiltered and validated by human reviewers for accuracy and contextual fit. The public insights Ip derived\nfrom the Task URL are included in C and serves as a required piece of insight for the agent to retrieve\nduring report generation. Prompts used for this stage and the list of urls are provided in Section O.3.\n\nStage 3: Question Generation. Given the Task Context, we generate the deep research question Q.\nThe prompt (see Section O.2) is instantiated with the company profile and persona, the selected domain,\nthe Task URL, and the public insight Ip. The LLM proposes several open-ended candidate questions\ngrounded in this context. Human annotators then review these candidate DR Questions, selecting and\nrefining one to align with the persona and company. They also ensure that the insights available in the\nprovided URL can at least partially support answering the deep research question. For example, if the\nquestion concerns compliance with a specific regulation, the URL might include relevant insights, such\nas “groceries must have a traceability plan.” While this doesn’t fully resolve the question, it provides\na foundation.The question should be high-level enough to allow us to synthesize additional supporting\nprivate/internal insights Il (such an insight could be “the cost of implementing such a plan is X amount”)\nwhich are needed to strengthen the report generated for the question. This requirement ensures that new\ninternal insights can be generated, as discussed in Stage 4.\n\nStage 4: Internal Insight Generation. In this stage we generate the injected insights set G ⊂I. Using\nthe public insight Ip and the deep research question Q, LLMs are used to create company-specific insights\naligned with the organization’s industry, priorities, customer segments, and business goals. These insights\nare designed to provide additional supporting facts that need to be extracted to create a report that better\naddresses the DR questions. Human annotators review and refine these insights for accuracy and alignment\nwith the questions. In addition to relevant insights, we also produce distractor insights Id, which are\nplausible but irrelevant statements that do not support resolving the DR Question. Prompt details are\nprovided in Section O.4 and example internal insights are provided in Section B.\n\nStage 5: File Mapping and Generation. This stage produces the the set of files {fi} containing\nboth the relevant and distractor private insights. First, each insight is assigned to a modality such as\nemail, chat, pdf, docx, and so on. Then the file generation module follows the following three-step\n“needle-in-a-haystack” process: (1) create an outline of the file based on its modality(e.g., document\nstructure or chat configuration), (2) insert the distractor or relevant insight into an appropriate section\nof the file, and (3) fill the remaining content with realistic but irrelevant information. Human annotators\nspot-check the generated files to ensure fidelity, coherence and no contradicting information. Prompts\nfor file generation are provided in Section O.5 and screenshots of such generated files are in Section C.\n\n4 DRBench AGENT\n\nThe DRBench baseline agent (DRBA) is the first agent built specifically for deep research in enterprise\nsettings, designed to operate directly within the DRBench environment.\nIts multi-stage architecture\nsystematically investigates research questions by iteratively retrieving, processing, and synthesizing\nknowledge from enterprise services and the web until completion or a maximum iteration limit is reached\n(Figure 3; see Appendix F). The agent has access to app-specific api calling tools to access diverse\n\n5\n\n\fFigure 3: DRBench Agent architecture showing the enterprise research workflow from question submission\nthrough iterative research cycles to final report generation, using both enterprise and web search capabilities.\n\ninformation sources and a diverse toolset for analyzing retrieved results (Table 9, Appendix F.3). DRBA’s\narchitecture is organized into four main components: research planning, action planning, an adaptive\nresearch loop, and report writing. Refer to Appendix I for details on the cost of using DRBA.\n\nResearch Planning. The agent decomposes research questions into structured research investigation\nareas to guide subsequent action generation. This initial decomposition lays out a strategy to systematically\ncover the research space while maintaining focus on the initial deep research question. The agent supports\ntwo research planning modes: (1) Complex Research Planning (CRP), which generates a structured\nresearch plan with detailed investigation areas, expected information sources, and success criteria; and\n(2) Simple Research Planning (SRP) , which produces a lightweight decomposition of the main question\ninto a small set of self-contained subqueries. See Appendix F.2 for detailed examples of both modes.\n\nAction Planning. The system translates research objectives into executable actions through an LLM-based\nplanning subsystem. Actions receive a priority score based on strategic importance and are parameterized\nwith specific tool selection and execution parameters, and their dependencies to other actions in the plan.\n\nResearch Loop with Adaptive Action Planning (AAP). The system iterates through (1) tool selection\nand execution based on action priorities, (2) content processing and storage in a vector store, (3) adaptive\naction generation to cover research gaps, and (4) iteration findings storage for traceability and assessment.\n\nReport Writing. The report writing subsystem queries the vector store to synthesize the research findings\nand relevant retrieved content. This component generates a comprehensive report and uses its own citation\ntracking system to ensure proper attribution of claims.\n\n5 EXPERIMENTS AND RESULTS\n\nIn this section, we evaluate the DRBench Agent (DRBA) on both the full DRBench benchmark and a\nreduced subset for ablations. We consider (1) the Full Benchmark, covering all 15 tasks across 10 domains\n(Table 7), and (2) the MinEval subset, restricted to five retail tasks for efficient ablation studies. Results\nare reported across four metrics: Insight Recall, Distractor Avoidance, Factuality, and Report Quality,\nexplained in the next section. Implementation details and hyperparameters are in Appendix H, while the\nimpact of the number of research loop iterations on the performance of DRBA is analyzed in Appendix N.\n\n5.1 EVALUATION METRICS\n\nInsight Recall. We first decompose each report into atomic insights with their associated citations using\nan LLM (see Prompt 14). Each extracted insight is then compared against the set of groundtruth insights\nembedded in the task files using an LLM Judge with the Prompt 15. If a match is found, the insight is marked\nas detected and contributes to the insight recall score; otherwise, it is ignored. This metric thus measures\nrecall rather than precision, since judging whether an unmatched insight is nonetheless useful for answering\n\n6\n\n\fTable 2: DRBA performance with different planning configurations on DRBench. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. See Appendix K for the standard error across 3 runs.\nNote that higher numbers correspond to better scores, and the best result on each metric is bolded.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n16.92\n17.00\n16.94\n20.56\n19.20\n18.69\n\n64.06\n69.38\n66.78\n67.44\n61.72\n57.20\n\n97.94\n98.89\n99.52\n98.89\n98.97\n98.89\n\n91.08\n92.46\n90.72\n93.00\n91.86\n90.12\n\n41.71\n42.48\n42.07\n47.43\n44.80\n43.39\n\nthe deep research question is inherently subjective and difficult to automate. To prevent agents from trivially\nachieving 100% recall by copying all content into the generated report, the LLM Judge evaluates only the\nfirst k insights, where k equals the number of ground-truth insights plus five. This buffer ensures that reports\nare not penalized for including seemingly relevant insights that are not part of the groundtruth insight set.\n\nDistractor Avoidance. To measure precision, we track whether the agent’s report includes distractor\ninsights that are irrelevant to the research question. We compute distractor recall analogously to insight\nrecall, and define distractor avoidance as 1− distractor recall.\n\nFactuality. Using the same set of extracted insights (that we used for Insight Recall), we follow\nthe methodology of FactScore (Min et al., 2023).\nIf an insight lacks a citation or references a\nnon-existent source, it is labeled unfactual. Otherwise, we apply a retrieval-augmented system based on\ntext-embedding-3-large (OpenAI, 2024) to fetch the top-5 most relevant chunks from the cited\ndocument (Appendix H). The LLM Judge with Prompt 16 then determines whether the cited evidence\nsupports the claim. We also store justifications and model confidence scores for interpretability.\n\nReport Quality. Inspired by prior work (Coelho et al., 2025; Abaskohi et al., 2025), we query the LLM\nJudge with Prompt 17 to assign a 1–10 rating across six dimensions: (1) depth and quality of analysis, (2)\nrelevance to the research question, (3) persona consistency, (4) coherence and conciseness, (5) absence of\ncontradictions, and (6) completeness and coverage. The final report quality score is obtained by averaging\nthese six ratings.\n\n5.2 MAIN RESULTS\n\nWe first evaluate our DRBA agent (Section 4) using GPT-4o as the backbone model, a maximum of\n15 research loop iterations, and different combinations of planning modules: Simple Research Planning\n(SRP), Complex Research Planning (CRP), and Adaptive Action Planning (AAP). The results are reported\nin Table 2. Overall, the agent demonstrates moderate ability to ground its answers in factual evidence\nbut struggles to consistently surface the main injected insights necessary for answering the deep research\nquestions.\nIn many cases, the agent relies on prior knowledge or external web content rather than\nintegrating the crucial enterprise-specific information available in the files. By contrast, it is consistently\nstrong in avoiding distractors, showing that the agent is robust against misleading or irrelevant information\nbut less effective at prioritizing decision-critical insights. Note that our LLM Judge backbone is GPT-4o.\n\nComparison Across Planning Strategies. Here we see that SRP tends to produce more factually\ngrounded answers, while CRP excels at filtering out distractors through structured decomposition. AAP,\non the other hand, provides the largest improvements in both insight recall and report quality, suggesting\nthat dynamically adapting the plan during execution helps the agent recover missed evidence and refine its\nuse of sources. However, combining CRP or SRP with AAP does not yield clear gains, and in some cases\nreduces factuality, likely because overlapping strategies create redundant or unstable planning behavior.\nThese findings indicate that adaptive mechanisms are key for improving coverage of injected insights,\nwhile lightweight planning is more effective for maintaining factual grounding, and that carefully balancing\nthe two remains an open challenge. See Appendix M for detailed results for each task.\n\n7\n\n\fTable 3: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Note that higher numbers correspond to better scores, and the best result on each\nmetric is bolded. The full table with more models is given in Appendix L.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n17.37\n16.97\n20.16\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n78.91\n79.27\n69.75\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n100.00\n98.10\n97.90\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n90.48\n92.34\n91.26\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n49.78\n48.87\n53.86\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\n5.3 ABLATION: EFFECT OF BACKBONE LANGUAGE MODEL ON DRBA\n\nWe evaluate the impact of backbone language models on DRBA using the MinEval subset for controlled\ncomparison (Table 3). GPT-5 achieves the best balance of factual grounding, insight recall, and report\nquality. Open-source models show mixed results: Llama-3.1-405B excels in factuality but lags in recall,\nDeepSeek-V3.1 delivers balanced performance through targeted fine-tuning, and Qwen-2.5-72B is reliable\nbut trails GPT-5. These results underline the importance of backbone choice; larger and more advanced\nmodels generally yield stronger overall performance, though some open-source options are competitive\nin specific metrics. In addition, our experiments also revealed a significant limitation in agents retrieving\ncritical insights from the open web. As shown in Figure 8 in Section L, no agent managed to successfully\nsource external knowledge, highlighting the difficulty of extracting relevant information for deep research\napplications within an unboundedly large search space.\n\nTable 4: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall. The full table with all\ngroundtruth insights and predicted insights is given in Appendix G.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have inter-\nacted with personalized product recommen-\ndations, resulting in a 25% increase in aver-\nage order value.\n\n45% of Lee’s Market online customers engage with\npersonalized product recommendations, resulting in\na 25% increase in average order value.\n(Score = 1.0)\n\n45% of online customers engaged with per-\nsonalized product recommendations, and\namong those engagers average order value\nincreased by 25%.\n(Score = 1.0)\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\n85% of transactions are linked to loyalty accounts at\nLee’s Market, providing a solid foundation for personal-\nized marketing and improving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n5.4 QUALITATIVE ANALYSIS\n\nIn Table 4 we show a sample of three groundtruth insights as well as the predicted insights from using\nboth Llama 3.1 405B and GPT-5. We see that for the first insight, both models are able to effectively\nrecover the groundtruth insight. For the second insight GPT-5 can extract the relevant time of year, where\nas Llama 3.1 405B fails to do so. This possibly suggests that GPT-5 may be better at extracting fine details.\n\n5.5 PERFORMANCE OF WEB AGENTS ON DRBench\n\nWe evaluated Generic WebAgents from AgentLab in a browser-only setting (without API access).\nThe GPT-4.1-powered agent achieved only 1.11% insight recall, 6.67% factuality, and 33.07% report\nquality. While the reports appeared well-structured, they lacked grounded insights, with most trajectories\n\n8\n\n\fTable 5: Model Performance Comparison Across Local or App-based Environments. Note that higher\nnumbers correspond to better scores, and the best result on each metric is bolded.\n\nEnv\nModel\nApp\nDRBA (GPT-5)\nApp\nDRBA (GPT-5) + CRP\nDRBA (DeepSeek-V3.1)\nApp\nDRBA (DeepSeek-V3.1) + CRP App\nLocal\nDRBA (GPT-5)\nLocal\nDRBA (GPT-5) + CRP\nLocal\nDRBA (DeepSeek-V3.1)\nDRBA (DeepSeek-V3.1) + CRP Local\nPerplexity\nLocal\nOpenAI Deep Research (GPT-5) Local\nLocal\nGemini\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n38.33\n39.63\n25.15\n30.26\n41.25\n42.18\n35.62\n36.54\n39.14\n44.78\n43.92\n\n74.52\n65.17\n72.66\n70.27\n82.43\n83.91\n77.12\n78.35\n81.06\n87.53\n85.68\n\n95.14\n92.86\n97.43\n96.67\n98.62\n98.45\n97.95\n97.62\n98.84\n99.12\n98.97\n\n94.56\n93.42\n86.52\n86.88\n91.08\n92.46\n89.16\n90.12\n90.36\n94.92\n93.24\n\n66.01\n64.46\n53.09\n57.86\n69.57\n70.67\n64.03\n65.07\n67.72\n73.56\n72.37\n\ndegenerating into repetitive clicks on irrelevant files or windows. This shows that browser-only agents are\ncurrently far from effective for deep research tasks. Further trajectory examples are shown in Appendix J.\n\n5.6 APP-BASED ENVIRONMENT VS LOCAL ENVIRONMENT\n\nIn Table 5, we compare results across two settings in DRBench: (1) local, where all the task files (e.g.,\nPDFs, PPTX, DOCX, XLSX, chats) are placed in a local folder that the agent can access, and (2)\napp-based, where the same files must be retrieved through our standard enterprise environment and its apps,\nintroducing additional interaction complexity. We find that OpenAI’s Deep Research (GPT-5) achieves\nthe highest scores across all metrics. Our agent with GPT-5 and DeepSeek backbones achieves similar\nperformance to Perplexity in the local-only setting, but lags behind OpenAI and Gemini. In the app-based\nsetting, performance declines across both backbones, highlighting the added difficulty of navigating\nmulti-application environments. This gap underscores that the environment in DRBench is intentionally\nchallenging, enabling a more realistic evaluation of model capabilities in enterprise research scenarios.\n\n6 HUMAN EVALUATION\n\nQuality of Deep Research Questions. We evaluated the quality of the deep research questions in\nDRBench through a human study with five expert annotators across all 15 tasks. Each task was judged\non three criteria: (1) grounding in the external website, (2) relevance to the domain and company context,\nand (3) alignment with associated insights. Annotators provided binary ratings plus optional feedback.\nResults show strong quality: 12 tasks received unanimous approval, while only three (tasks DR1, DR11,\nand DR13) received a single negative vote due to minor issues with specificity or distractor difficulty. This\ncorresponds to a 96% approval rate (72/75 votes).\n\nCorrelation of Used Metrics with Human Preference. We collected human preference on a subset\nof 11 tasks1. Each annotator was shown a golden insight with aligning insights from two models2 and\nasked to choose which they preferred, or label both as good/bad. Missing alignments were shown as\nempty strings. We compared agents with AAP no RP against GPT-5 and Llama-3.1-405B-Instruct. The\nFleiss κ (Fleiss, 1971) across five annotators was 0.67. Most outputs were judged both bad due to missing\nalignments, but when preferences were expressed, GPT-5 was favored 61.1% over Llama-405B-Instruct,\nconsistent with our metric-based findings in Section 5.3. Additional analyses are in Appendix Q.\n\n7 CONCLUSION & FUTURE WORK\n\nIn this work, we introduced DRBench, a benchmark for evaluating AI agents on complex, open-ended\nenterprise deep research tasks that require reasoning over both public and private data. Unlike prior\nbenchmarks focused on surface-level or web-only queries, DRBench offers 15 persona-grounded tasks\nsituated in realistic enterprise contexts and evaluated through an environment that integrates real-world\nenterprise applications and heterogeneous data formats. We also presented the DRBench Agent (DRBA)\n\n1We selected tasks with fewer than 8 insights for a reasonable amount of manual work.\n2The gold-prediction alignment is provided by the insight recall metric.\n\n9\n\n\fas a strong baseline and analyzed its behavior across planning strategies and backbone models. Our results\nshow that while agents are generally effective at avoiding distractors and capable of producing structured\nreports, they still struggle to consistently extract decision-critical insights. Adaptive planning improves\nrecall of injected insights, while lightweight strategies tend to preserve factual accuracy, underscoring the\ndifficulty of balancing exploration with reliability. Looking ahead, we plan to extend DRBench with tasks\nrequiring cross-file integration, reasoning across modalities such as PDFs and chats, and richer distractors.\nWe also aim to add multimodal sources like images and video, as well as privacy-sensitive tasks to assess\ndata protection. Together, these extensions will move research agents closer to enterprise readiness and\nprovide a stronger foundation for studying deep research in realistic organizational settings.\n\n10\n\n\fREFERENCES\n\nAmirhossein Abaskohi, Amrutha Varshini Ramesh, Shailesh Nanisetty, Chirag Goel, David\nand Issam H. Laradji.\nURL\n\nVazquez, Christopher Pal, Spandana Gella, Giuseppe Carenini,\nAgentAda:\nhttps://arxiv.org/abs/2504.07421.\n\ntailored insight discovery, 2025.\n\nSkill-adaptive data analytics\n\nfor\n\nReyna Abhyankar, Qi Qi, and Yiying Zhang. OSWorld-Gold: Benchmarking the efficiency\nIn ICML 2025 Workshop on Computer Use Agents, 2025. URL\n\nof computer-use agents.\nhttps://openreview.net/forum?id=sV3n6mYy7J.\n\nAll-HandsAI.\n\nOpenHands.\n\nhttps://github.com/All-Hands-AI/OpenHands, 2024.\n\nAccessed: 2024-06-01.\n\nPierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja,\nRicardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo\nLaurenc¸on, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre M´enard, Gr´egoire Mialon, Ulyana\nPiterbarg, Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav Vorotilov,\nMengjue Wang, and Ian Yu. ARE: Scaling up agent environments and evaluations, 2025. URL\nhttps://arxiv.org/abs/2509.17158.\n\nL´eo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier de Chezelles, Quentin\nCappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. WorkArena++: Towards\ncompositional planning and reasoning-based common knowledge work tasks. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL\nhttps://openreview.net/forum?id=PCjK8dqrWW.\n\nNikos I Bosse, Jon Evans, Robert G Gambee, Daniel Hnyk, Peter M¨uhlbacher, Lawrence Phillips, Dan\nSchwarz, Jack Wildman, et al. Deep Research Bench: Evaluating AI Web Research Agents. arXiv\npreprint arXiv:2506.06287, 2025.\n\nZijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green,\nKshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong,\nXinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin.\nBrowseComp-Plus: A more fair and transparent evaluation benchmark of Deep-Research agent, 2025.\nURL https://arxiv.org/abs/2508.06600.\n\nThibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, L´eo Boisvert,\nMegh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han\nL`u, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan\nSalakhutdinov, Nicolas Chapados, and Alexandre Lacoste. The BrowserGym ecosystem for web agent\nresearch, 2025. URL https://arxiv.org/abs/2412.05467.\n\nJo˜ao Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie\nCallan, Jo˜ao Magalh˜aes, Bruno Martins, et al. Deepresearchgym: A free, transparent, and reproducible\nevaluation sandbox for deep research. arXiv preprint arXiv:2505.19253, 2025.\n\nAlexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty,\nDavid Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents\nat solving common knowledge work tasks? 2024.\n\nMingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. DeepResearch Bench: A\n\ncomprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nJoseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76\n\n(5):378, 1971.\n\nFoundationAgents. OpenManus. https://github.com/FoundationAgents/OpenManus,\n\n2024. Accessed: 2024-06-01.\n\n11\n\n\fBoyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Botao Yu, Andrei Kopanev, Weijian\nQi, Yiheng Shu, Jiaman Wu, Chan Hee Song, Bernal Jimenez Gutierrez, Yifei Li, Zeyi Liao,\nHanane Nour Moussa, TIANSHU ZHANG, Jian Xie, Tianci Xue, Shijie Chen, Boyuan Zheng,\nKai Zhang, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, and Yu Su. Mind2web\nIn The Thirty-ninth Annual Conference\n2: Evaluating agentic search with agent-as-a-judge.\non Neural Information Processing Systems Datasets and Benchmarks Track, 2025.\nURL\nhttps://openreview.net/forum?id=AUaW6DS9si.\n\nKung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese,\nCaiming Xiong, Philippe Laban, and Chien-Sheng Wu. CRMArena: Understanding the capacity of\nLLM agents to perform professional CRM tasks in realistic environments. In Proceedings of the 2025\nConference of the Nations of the Americas Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), 2025a.\n\nKung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey,\nYixin Mao, Silvio Savarese, Caiming Xiong, and Chien-Sheng Wu. CRMArena-Pro: Holistic\nassessment of LLM agents across diverse business scenarios and interactions. arXiv preprint\narXiv:2505.18878, 2025b.\n\nHuggingFace.\n\nsmolagents.\n\nhttps://github.com/huggingface/smolagents, 2024.\n\nAccessed: 2024-06-01.\n\nAbhinav Java, Ashmit Khandelwal, Sukruta Midigeshi, Aaron Halfaker, Amit Deshpande, Navin Goyal,\nAnkur Gupta, Nagarajan Natarajan, and Amit Sharma. Characterizing deep research: A benchmark\nand formal definition, 2025. URL https://arxiv.org/abs/2508.04183.\n\nLearningCircuit.\n\nLocal deep research.\n\nlocal-deep-research, 2025.\n\nhttps://github.com/LearningCircuit/\n\nYao Liu, Deming Ye, Shuohang Wang, Furu Wei, Yujia Ma, and Minlie Huang. G-Eval: NLG evaluation\n\nusing GPT-4 with better human alignment. EMNLP, 2023.\n\nXing Han L`u, Zdenˇek Kasner, and Siva Reddy. WebLINX: Real-world website navigation with multi-turn\n\ndialogue, 2024. URL https://arxiv.org/abs/2402.05930.\n\nGr´egoire Mialon, Cl´ementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom.\n\nGAIA: a benchmark for general AI assistants. ICLR, 2024.\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke\nZettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 12076–12100, 2023.\n\nOpenAI.\n\ntext-embedding-3-large.\n\nhttps://platform.openai.com/docs/guides/\n\nembeddings, 2024. [Large language model]. Accessed September 2025.\n\nGaurav Sahu, Abhay Puri, Juan A Rodriguez, Amirhossein Abaskohi, Mohammad Chegini, Alexan-\ndre Drouin, Perouz Taslakian, Valentina Zantedeschi, Alexandre Lacoste, David Vazquez, et al.\nInsightBench: Evaluating insight extraction for business analytics agents. ICLR, 2025.\n\nZilliz Tech. Deep-Searcher. https://github.com/zilliztech/deep-searcher, 2024.\n\nAccessed: 2024-06-01.\n\nYuhao Wang, Ruiyang Ren, Junyi Li, Xin Zhao, Jing Liu, and Ji-Rong Wen. REAR: A relevance-aware\nretrieval-augmented framework for open-domain question answering. In Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing, pp. 5613–5626, 2024.\n\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua,\nZhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for\nopen-ended tasks in real computer environments. NeurIPS, 2024.\n\n12\n\n\fFrank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui\nZhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander\nMaben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig.\nTheAgentCompany: Benchmarking LLM agents on consequential real world tasks, 2025a. URL\nhttps://arxiv.org/abs/2412.14161.\n\nRenjun Xu and Jingwen Peng. A comprehensive survey of deep research: Systems, methodologies, and\n\napplications. arXiv preprint arXiv:2506.12594, 2025.\n\nTianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, and Pengfei Liu. ResearcherBench:\nURL\n\nEvaluating deep AI research systems on the frontiers of scientific inquiry, 2025b.\nhttps://arxiv.org/abs/2507.16280.\n\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.\nDeepResearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv,\n2025.\n\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou,\nYonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. WebArena: A Realistic Web Environment\nfor Building Autonomous Agents, April 2024. URL http://arxiv.org/abs/2307.13854.\narXiv:2307.13854 [cs].\n\n13\n\n\fTable 6: Comparison of Deep Research Tasks of Different Benchmarks.\n\nBenchmark\nDeepResearchGym (Coelho et al., 2025)\nDeep Research Bench (Bosse et al., 2025)\n\nDeepResearch Bench (Du et al., 2025)\n\nResearcherBench (Xu et al., 2025b)\n\nLiveDRBench (Java et al., 2025)\n\nBrowseComp-Plus (Chen et al., 2025)\n\nGAIA2 (Andrews et al., 2025)\n\nDRBench\n\nSample Question\nIs the COVID vaccine dangerous\nFind a reliable, known number on the internet. The total number\nof FDA Class II Product Recalls of medical devices.\nWhile the market features diverse quantitative strategies like\nmulti-factor and high-frequency trading, it lacks a single, standard-\nized benchmark for assessing their performance across multiple\ndimensions such as returns, risk, and adaptability to market\nconditions. Could we develop a general yet rigorous evaluation\nframework to enable accurate comparison and analysis of various\nadvanced quant strategies?\nCompare the Transformer and Mamba model architectures, analyz-\ning their performance and technical characteristics in different appli-\ncation scenarios. Based on the latest research, discuss the advantages\nand disadvantages of both models and their applicable scenarios.\nFor complex reasoning tasks (e.g., tasks involving multiple citations\nor extended reasoning chains), what are the strengths of current\nagent technologies, and what are their limitations? Please analyze\nthis in the context of research since June 2024.\nIdentify the title of a research publication published before June 2023,\nthat mentions Cultural traditions, scientific processes, and culinary\ninnovations. It is co-authored by three individuals: one of them was\nan assistant professor in West Bengal and another one holds a Ph.D.\nUpdate all my contacts aged 24 or younger to be one year older\nthan they are currently.\nHow can Lee’s Market leverage FSMA 204 regulations to enhance\nfood safety and customer trust?\n\nA COMPARISON OF DEEP RESEARCH BENCHMARKS\n\nAND AI AGENT BENCHMARKS WITH A COMPUTER ENVIRONMENT\n\nIn Table 1, we compare existing deep research benchmarks and AI agent benchmarks that provide a\ncomputer environment with DRBench. While the questions in existing benchmarks focus on public interest\ntopics and require generic web search and computer use, DRBench provides realistic questions that real\npersonas in organizations need to resolve.\n\nB DRBench TASKS\n\nAs shown in Table 7, DRBench contains 15 tasks in total, covering 3 industries (retail, healthcare and\nelectric vehicles), 10 task domains (compliance, sales, customer relationship management, market analysis,\ncustomer service management, IT service management, cyber security, marketing, quality assurance, and\nresearch), and 3 difficulty levels (easy, medium, hard). In addition, we generate the following 3 companies\n(one for each industry type): (1) a supermarket chain called Lee’s Market, (2) a virtual healthcare company\ncalled MediConn Solutions, and (3) an electric vehicle company called Elexion Automotive.\n\nTable 8 presents a deep research question from DRBench and its supporting groundtruth insights. We also\nvisualize the DR Question and all QA pairs by embedding them with OpenAI’s text-embedding-\n3-large model and projecting into 2D using t-SNE in Figure 4. The plot shows that injected supporting\ninsights lie closer to the DR Question, while distractors appear farther away, confirming that our injected\ninsights are semantically aligned with the research objective.\n\nC DRBench EXAMPLES OF INJECTED INSIGHTS\n\nAs shown in Figure 2, supporting documents are generated with enterprise insights injected. In Figure 5,\nwe show two examples of a generated files (PPTX and Mattermost chat) with their embedded insights.\n\n14\n\n\fFigure 4: t-SNE visualization of QA pairs for the DR Question in Task DR0005. The plot shows the\ndistribution of annotated pairs across Supporting Insights (green), Distractors (red), and the central\nDeep Research (DR) Question (gold star). Out of 49 pairs, 16 correspond to supporting insights and\n33 are distractors. The visualization illustrates how relevant insights cluster separately from distractors,\nhighlighting the challenge of retrieving salient information in a distractor-heavy environment.\n\n(a) Example supporting file named food-safety-\nregulatory-compliance.pdf with an injected insight\n”Lee’s Market reduced food waste by 8% in Q2 2024,\nsaving $1.2M.”\n\n(b) Example supporting email in the Sent mailbox with\nan injected insight ”85% of high-risk food suppliers are\nFSMA 204 compliant, totaling 270 vendors.”\n\nFigure 5: Example files with injected insights in DRBench.\n\n15\n\n151050510t-SNE Dimension 11050510t-SNE Dimension 2Q: What percentage of Lee's Market customers are centennial shoppers in Q3 2024?A: 25% of Lee's Market customers are centennial shoppers as of Q3 2024.Q: Projected chatbot cost savings in Q2 2024?A: Expect 25% cost reduction by Q2 2024.Q: How many customer service inquiries do we receive from centennial shoppers on...A: 2,500 customer service inquiries from centennial shoppers are received on average per quarter.Q: Centennial chatbot engagement rate by Q3 2025?A: 50% centennials engage weekly by Q3 2025.Q: How many Lee's Market customers used online shopping services in Q1 2024?A: About 30% of customers used online shopping services, totaling 150,000 users in Q1 2024.Q: Chatbot adoption rate Q1 2025?A: 55% adoption rate by centennials by Q1 2025.Q: Chatbot adoption rate among centennials by Q2 2024?A: 40% centennials prefer chatbots by Q2 2024.Q: How many customer queries does Lee's Market receive monthly in 2024?A: Lee's Market receives an average of 50,000 customer queries per month in 2024.Q: What was Lee's Market employee training budget in Q3 2023?A: Lee's Market allocated $450,000 for employee training, a 15% increase.Q: Annual facility maintenance cost 2024?A: Total maintenance cost was $3.5 million in 2024.Q: How many Lee's Market employees participated in leadership training in Q1 2024?A: 120 employees completed leadership training with a 90% satisfaction rate.Q: How many Lee's Market employees completed IT security training in Q1 2024?A: 85% of employees completed IT security training with a 95% pass rate.Q: What was Lee's Market employee turnover rate in Q2 2024?A: Employee turnover rate averaged 12% with a 5% increase from Q1 2024.Q: What was Lee's Market energy consumption cost in Q2 2024?A: Lee's Market energy consumption cost totaled $850,000 in Q2 2024.Q: What were Lee's Market IT costs in Q4 2024?A: IT costs totaled 3.4M,with1.2M on new hardware.Q: How many new store locations did Lee's Market open in Q3 2024?A: Lee's Market opened 12 new store locations across the US in Q3 2024.Q: What was our workers' compensation claims rate in Q2 2025?A: Workers' compensation claims rate was 2.5% with a total payout of $120,000.Q: What was Lee's Market average store maintenance cost in Q2 2024?A: Average store maintenance cost was $2,000 per month with a 5% reduction.Q: What were Lee's Market facility maintenance costs in Q2 2024?A: Facility maintenance costs totaled 1.2Mwithanaverageof100,000 monthly.Q: What was our employee headcount in Q1 2025?A: We had 5,100 employees as of January 2025.Q: How many employees attended compliance training in Q1 2024?A: Around 300 employees completed compliance training in Q1 2024.Q: What was our facility maintenance cost in Q4 2024?A: Facility maintenance costs totaled $1.8 million in Q4 2024.Q: Facility maintenance expenses Q4 2023?A: Maintenance expenses totaled $1.2 million in Q4 2023.Q: Employee training hours Q3 2024?A: Average training hours per employee were 15 in Q3 2024.Total QA Pairs: 49Supporting: 16 | Distractor: 33 | External: 0Annotated: 8 supporting + 16 distractorShapes:  Supporting   Distractor   ExternalHow can Lee's Market leverage chatbots to enhance the consumer experience for centennial shoppers?DR QuestionSupporting (Insight) (16)Distractor (33)\fTable 7: DRBench Questions and Statistics. Industry: target industry of the deep research question.\nDomain: domain of the deep research task. DR Question: question of the deep research task. Difficulty:\ndifficulty of the task defined based on the rubric mentioned in 3. # Applications: the number of total\napplications in the task environment. # Insights: the number of relevant insights to the deep research\nquestion. # Distractors: the number of non-supportings documents that do not contain relevant insights.\n\nIndustry\nRetail\n\nDomain\nCompliance\n\nRetail\n\nRetail\n\nRetail\n\nRetail\n\nSales\n\nCRM\n\nMarket Analysis\n\nCSM\n\nHealthcare\n\nCompliance\n\nHealthcare\n\nITSM\n\nHealthcare\n\nCybersecurity\n\nHealthcare\n\nCRM\n\nHealthcare\n\nMarketing\n\nElectric Vehicle\n\nCompliance\n\nElectric Vehicle\n\nQuality Assurance\n\nElectric Vehicle\n\nCybersecurity\n\nElectric Vehicle\n\nResearch\n\nElectric Vehicle\n\nCSM\n\nDR Question\nHow can Lee’s Market leverage FSMA 204 reg-\nulations to enhance food safety and customer trust?\nHow can personalization drive sales in the retail\nindustry and what strategies can be used for Lee’s\nMarket in action?\nHow can we leverage data-driven loyalty programs\nto enhance customer engagement?\nWhat are the new trends in the grocery retail\nmarket and what strategies can Lee’s Market adopt\nto remain competitive?\nHow can Lee’s Market\nleverage chatbots to\nenhance the consumer experience for centennial\nshoppers?\nWhat are the key factors influencing MediConn\nSolutions’ decision to accept insurance for tele-\nhealth providers, considering HIPAA compliance\nand state-specific data privacy regulations?\nHow can we leverage ISTM and AI-driven ana-\nlytics to minimize IT service desk workload and\nimprove response times in MediConn Solutions?\nWhat is the impact of third-party data breaches\non MediConn’s virtual healthcare platforms and\npatient data, and what new regulations can be\nimplemented to defend against these breaches?\nHow can MediConn Solutions leverage trendy new\nCRM solutions to improve patient engagement\nand retention, and what new CRM solutions are\nexpected in 2025?\nWhat are the most critical elements of a robust\ndigital presence for a telehealth provider such as\nMediConn Solutions, and how can we optimize\nour website and content marketing strategy to\nattract digital-first patients?\nHow can we balance the need for durability\nand warranty guarantees for EV batteries with\nevolving regulatory requirements, especially ACC\nregulations (ACC II), while staying on track with\nour production timelines through 2035?\nHow can Elexion Automotive’s quality assurance\nprocesses be optimized to address the unique\nchallenges of electric vehicle production, such as\nsoftware and user experience issues, compared to\ngasoline cars?\nHow can Elexion Automotive effectively im-\nplement a cybersecurity strategy for its electric\nvehicles, considering the risks and challenges\nposed by connected and autonomous technologies?\nCan we leverage AI-enhanced battery management\nto improve EV battery lifespan by 15%?\nHow can Elexion Automotive increase customer\ntrust through after-sales support while balancing\nthe need for exceptional customer care with\nefficient and cost-effective service?\n\nDifficulty\neasy\n\n# Applications\n2\n\n# Insights\n3\n\n# Distractors\n7\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\neasy\n\neasy\n\nmedium\n\nmedium\n\nhard\n\n2\n\n4\n\n3\n\n4\n\n3\n\n3\n\n4\n\n4\n\n4\n\n2\n\n1\n\n3\n\n3\n\n4\n\n4\n\n7\n\n6\n\n16\n\n4\n\n6\n\n7\n\n12\n\n15\n\n3\n\n3\n\n6\n\n7\n\n15\n\n10\n\n15\n\n14\n\n33\n\n8\n\n12\n\n14\n\n24\n\n30\n\n6\n\n6\n\n12\n\n14\n\n30\n\nD DRBENCH ENTERPRISE ENVIRONMENT\n\nThe DRBench Enterprise Environment provides a containerized simulation of realistic enterprise\nresearch settings where employees access confidential company information, personal files, and internal\ncommunications for comprehensive report generation. The environment simulates both a user’s local\nmachine filesystem and provides password-protected access to enterprise services.\n\nTo emulate realistic enterprise research settings, DRBench provides a self-contained Docker environment\nthat integrates commonly used applications: Nextcloud for shared documents, Mattermost for internal\nchat, an IMAP server and Roundcube open-source client for emails, and Filebrowser to emulate local files.\nEach task is initialized by distributing its data across these services, enabling agents to retrieve, analyze,\nand cite information through enterprise-like interfaces rather than static files. This design ensures realistic\ninteraction while maintaining reproducibility and controlled evaluation.\n\n16\n\n\fTable 8: Example Deep Research Question and Supporting Groundtruth Insights\n\nDeep Research Question\n\nHow can Lee’s Market leverage FSMA\n204 regulations to enhance food safety and\ncustomer trust?\n\nSupporting Groundtruth Insight\nU.S. grocers are working to meet the FDA’s FSMA 204 traceability rules\nby January 2026, which require tracking lot codes and key data for high-risk\nfoods to expedite recalls. This compliance is viewed as an “evolutionary\nstep” to modernize grocery operations and enhance food safety.\nBy capturing detailed traceability data, such as lot codes, at every step,\nretailers can meet regulations and gain inventory benefits. This allows grocers\nto know exact expiration dates by lot, enabling them to discount items before\nthey expire, thus reducing food waste and keeping products fresher.\nRegional grocers like Lunds & Byerlys and Raley’s see FSMA 204 as\na chance to enhance their systems and supply chain transparency. They\nbelieve improved traceability will boost customer trust and could signal the\nstart of more extensive future food safety regulations.\nLee’s Market tracks 250 high-risk food products as of Q3 2024, affecting\n30% of inventory.\nLee’s Market reduced food waste by 8% in Q2 2024, saving $1.2M.\n85% of high-risk food suppliers are FSMA 204 compliant, totaling 270\nvendors.\n\nInsight Category\nExternal\n\nExternal\n\nExternal\n\nInternal\n\nInternal\nInternal\n\nD.1 ARCHITECTURE AND SERVICES\n\nThe environment implements a multi-service architecture within a single Docker container. This design\nprioritizes deployment simplicity and cross-platform compatibility while maintaining service isolation.\nThe container orchestrates the following enterprise services:\n\n• Nextcloud: Open-source file sharing and collaboration platform analogous to Microsoft\n\nSharePoint or Google Drive, providing secure document storage with user authentication.\n\n• Mattermost: Open-source team communication platform simulating internal company commu-\nnications similar to Microsoft Teams or Slack, with teams, channels, and persistent chat history.\n\n• FileBrowser: Web-based file manager providing access to the container’s local filesystem,\n\nsimulating employee desktop environments and local document access.\n\n• Email System: Roundcube webmail interface with integrated SMTP (postfix) and IMAP\n\n(dovecot) services for enterprise email communication simulation.\n\n• VNC/NoVNC Desktop: Protocol and browser-based VNC access providing full desktop\nenvironment interaction within the container for comprehensive enterprise workflow simulation.\n\nD.2 TASK LOADING AND DATA DISTRIBUTION\n\nAt initialization, the environment processes task configuration files (env.json) and distributes data\nacross services through automated Python scripts and it makes sure that this source data is only accessible\nthrough the intended applications:\n\n• File Distribution: Documents are placed in appropriate Nextcloud user folders and FileBrowser\n\ndirectories based on task specifications\n\n• Communication Import: Chat histories and team conversations are imported into Mattermost\n\nchannels with proper user attribution\n\n• Email Integration: Email conversations are loaded into the mail system with realistic threading\n\nand metadata\n\n• User Provisioning: Enterprise users are automatically created across all services with consistent\n\nauthentication credentials\n\n17\n\n\ffrom drbench import drbench_enterprise_space, task_loader\n\n# Load task configuration\ntask = task_loader.get_task_from_id(task_id)\n\n# Initialize environment with automatic port allocation\nenv = drbench_enterprise_space.DrBenchEnterpriseSearchSpace(\n\ntask=task.get_path(),\nstart_container=True,\nauto_ports=True\n\n# Prevents port conflicts in parallel execution\n\n)\n\n# Environment provides service discovery\navailable_apps = env.get_available_apps()\n# Returns: {’nextcloud’: {’port’: 8081, ’credentials’: {...}}, ...}\n\n# Pass relevant information to the agent\n\n# Cleanup when research complete\nenv.delete()\n\nListing 1: DrBench Environment Usage\n\nD.3 PYTHON INTEGRATION\n\nThe DrBenchEnterpriseSearchSpace class provides programmatic container management with\nthe following capabilities: container lifecycle management, service access information, task-specific\ndata loading, and automatic cleanup. The typical usage pattern shown in Listing 1 demonstrates these\nintegrated capabilities.\n\nD.4 ENTERPRISE SERVICE APIS\n\nEach service exposes both **web interfaces** for human and web-agent interaction, and **programmatic\nAPIs** for agent access:\n\n• Nextcloud: WebDAV API for file operations, sharing, and metadata retrieval\n\n• Mattermost: REST API for message history, channel management, and user interactions\n\n• Email: IMAP/SMTP protocols for message retrieval and sending\n\n• FileBrowser: HTTP API for filesystem operations and file management\n\nThis dual-access model enables both agent-driven research and human verification of enterprise scenarios,\nsupporting comprehensive evaluation of research capabilities across realistic enterprise information\narchitectures.\n\nE DRBench EXAMPLES OF APPLICATION SCREENSHOTS\n\nFigures 6 and 7 show the applications provided in DRBench environment: File Browser, Mattermost,\nRoundcube, and Nextcloud.\n\nF DRBench AGENT IMPLEMENTATION DETAILS\n\nF.1 DETAILED WORKFLOW\n\nAs depicted in Figure 3, the workflow begins with a Company Employee submitting an enterprise Deep\nResearch Question along with Company Context. The DRBench agent processes this input through several\nkey stages:\n\n18\n\n\f(a) Screenshot of the File Browser interface, displaying\norganized files and folders within the system.\n\n(b) Screenshot of the Mattermost communication\nplatform, showing a discussion channel and user\ninterface elements.\n\nFigure 6: Screenshots of Applications in DRBench environment (Part 1).\n\n(a) Screenshot of the Nextcloud file management system,\nillustrating the file list view with various document types.\n\n(b) Screenshot of Roundcube, an email client, it shows\nan open email in the user’s inbox.\n\nFigure 7: Screenshots of Applications in DRBench environment (Part 2).\n\nStage 1: Research Planning. The agent decomposes the research question into structured research\ninvestigation areas to guide subsequent action generation. This initial decomposition lays out a strategy\nto systematically cover the research space while maintaining focus on the initial deep research question.\n\nStage 2: Action Planning. The Action Planning stage translates the research objectives into executable\nactions through a planning subsystem. This component uses an LLM to create a prioritized sequence\nof actions. Each action is parameterized with specific tool selection and execution parameters, its\ndependencies to other actions in the plan, and a priority score.\n\nStage 3: Research Loop with Adaptive Execution. The Research Loop iterates over the following\nsub-stages until completion: (1) Tool Selection and Execution: The tool selection and execution subsystem\nimplements a sophisticated priority-based selection of actions from the plan at each research iteration step\nand proceeds to execute it with the current research context. (2) Content Processing: If necessary, the action\nwill make use of the content processing subsystem to extract, synthesize, and store retrieved documents and\nwebsites into a vector store to form a task-specific knowledge base that will grow on each iteration. (3) Adap-\ntive Action Planning: After each execution round, the agent analyzes the most recent findings; if coverage\ngaps are detected, new actions are created and added to the plan at this point. This ensures that newly discov-\nered knowledge is taken into account to answer the research question. (4) Iteration Findings Storage: Results\nfrom each iteration are stored in the vector store with rich metadata for traceability and later assessment.\n\nStage 4: Convergence and Completion. The research loop continues until all actions in the plan have\nbeen completed or the maximum iteration limit is reached.\n\n19\n\n\fStage 5: Report Writing. The report writing subsystem queries the vector store to synthesize the\nresearch findings and relevant retrieved content. This component generates a comprehensive report and\nuses its own citation tracking system to ensure proper attribution of claims within the report.\n\nThe vector store serves as the main knowledge integration component, maintaining embeddings of all\nprocessed content and enabling semantic retrieval at the report generation stage. This component is crucial\nin a deep research setting to prevent information loss from early research stages in arbitrarily long research\nsessions.\n\nF.2 RESEARCH PLANNING IMPLEMENTATION\n\nThe Research Planning subsystem offers three operational modes to evaluate the impact of structured\nplanning on research effectiveness:\n\n• Complex Mode: Generates a comprehensive research plan with detailed investigation areas.\nThese areas contain details about the specific research focus, expected information sources, and\nsuccess criteria, among others. Each area includes an importance level and specific business\nintelligence objectives (see Listing 2).\n\n• Simple Mode: Creates focused question decompositions with 4-10 self-contained subqueries\nderived directly from the main research question. Uses straightforward decomposition without\nthe complex enterprise research structure of complex mode. See examples in Listing 4 and\nListing 3 for comparison of different planning modes.\n\n• None: Bypasses structured planning entirely, proceeding directly to action generation based\non the original question. This mode serves as a baseline to measure the added-value of explicit\nplanning stages.\n\nThe planning process begins with the enriched query (Prompt 1) and uses the research planning prompt\n(Prompt 2) to generate structured outputs. In Complex Mode, the system creates detailed investigation areas\nwith enterprise-focused metadata, while Simple Mode produces straightforward question decompositions\nsimilar to existing multi-step reasoning approaches. The resulting plan structure directly feeds into the\nAction Planning System (Section F.3) for executable action generation.\n(cid:7)\n{\n\n(cid:4)\n\n\"area_id\": 1,\n\"research_focus\": \"Core strategic\n\ndomain, market segment, or business hypothesis to investigate\",\n\n\"information_needs\": [\n\n\"What specific intelligence is required for strategic decisions\"\n\n],\n\"knowledge_sources\": [\"internal\", \"external\", \"both\"],\n\"research_approach\n\n\": \"competitive_analysis | market_research | strategic_assessment\n\n| trend_analysis | risk_analysis | performance_benchmarking\",\n\n\"key_concepts\": [\"concept1\", \"concept2\"],\n\"business_rationale\": \"Why this investigation\n\narea is critical for enterprise strategy and decision-making\",\n\n\"expected_insights\": \"What strategic\n\nunderstanding or competitive intelligence this area should provide\",\n\n\"stakeholder_impact\": \"Which\n\nbusiness units or decision-makers will benefit from these insights\",\n\n\"importance_level\": \"critical | important | supplementary\"\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 2: Investigation Area Structure for Full Planning Mode\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"What are the new trends in the grocery retail market\n\nand what strategies can Lee’s Market adopt to remain competitive?\",\n\n\"plan\": {\n\n\"research_investigation_areas\": [\n{\n\n20\n\n\f\"area_id\": 1,\n\"research_focus\": \"Current trends in grocery retail market\",\n\"information_needs\": [\n\"Latest consumer preferences\",\n\"Emerging technologies influencing grocery shopping\",\n\"Sustainability practices in grocery retail\",\n\"Changes in supply chain dynamics\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"trend_analysis\",\n\"key_concepts\n\n\": [\"e-commerce growth\", \"sustainability in supply chains\"],\n\n\"business_rationale\": \"Understanding consumer\n\ntrends and technological advancements that shape shopper behavior is\ncritical for adapting offerings and enhancing customer engagement.\",\n\"expected_insights\": \"Identify specific trends affecting customer\n\nbuying decisions, including the rise of online grocery shopping\nand preferences for sustainable, local, or organic products.\",\n\n\"stakeholder_impact\n\n\": \"Marketing, Product Development, Supply Chain Management\",\n\n\"importance_level\": \"critical\"\n\n},\n{\n\n\"area_id\": 2,\n\"research_focus\": \"Competitive analysis of grocery retailers\",\n\"information_needs\": [\n\"Market share analysis\",\n\"Competitor strengths and weaknesses\",\n\"Innovative strategies adopted by competitors\"\n],\n\"knowledge_sources\": [\"external\"],\n\"research_approach\": \"competitive_analysis\",\n\"\n\nkey_concepts\": [\"market positioning\", \"competitive differentiation\"],\n\n\"business_rationale\": \"A comprehensive understanding of\n\ncompetitors allows for strategic positioning and the identification\nof innovative practices that can be adopted or improved upon.\",\n\n\"expected_insights\": \"Detailed profiles of key\n\ncompetitors, including strategic moves they are making to capture\n\nmarket share, which can inform Lee’s Market’s competitive strategy.\",\n\n\"stakeholder_impact\n\n\": \"Executive Leadership, Strategic Planning, Marketing\",\n\n\"importance_level\": \"critical\"\n\n},\n...\n]\n\n}\n\n}(cid:6)\n\n(cid:7)\n{\n\nListing 3: Complex Mode Research Plan Example\n\n(cid:5)\n\n(cid:4)\n\n\"query\": \"How can we leverage\n\ndata-driven loyalty programs to enhance customer engagement?\",\n\n\"plan\": {\n\n\"mode\": \"simple\",\n\"subqueries\": [\n\"What are the key features\nof successful data-driven loyalty programs in the retail industry?\",\n\"How can data analytics be used to personalize rewards\nand incentives in loyalty programs to increase customer engagement?\",\n\"What types of customer data should be collected and analyzed\nto optimize loyalty programs for a company like Lee’s Market?\",\n\n21\n\n\f\"Which\ntechnology platforms and tools are most effective for implementing\nand managing data-driven loyalty programs in the retail sector?\",\n\"How can Lee’s Market measure the success of their\nloyalty program in terms of customer engagement and sales growth?\",\n\"What are the best practices for integrating a loyalty program\nwith existing marketing strategies to enhance customer experience?\",\n\"How can Lee’s Market ensure customer data\nprivacy and security while leveraging data-driven loyalty programs?\",\n\"What are the potential\nchallenges and limitations of implementing a data-driven loyalty\n\nprogram for a retail company with Lee’s Market’s size and resources?\"\n],\n\"research_methodology\": {\n\"overall_approach\": \"Query decomposition into focused subqueries\"\n}\n\n}\n\n}(cid:6)\n\nListing 4: Simple Mode Research Plan Example\n\n(cid:5)\n\nF.3 ACTION PLANNING SYSTEM\n\nThe Action Planning System translates research objectives into executable actions through an intelligent\nplanning subsystem that manages tools, prioritization, and dependencies.\n\nAvailable Tools Table 9 summarizes the available tools organized by category and their primary purposes.\n\nTable 9: DrBench Agent Tool Categories and Purposes\n\nCategory\n\nTool\n\nPurpose\n\nInformation Retrieval\n\nInternet Search\n\nEnterprise API\n\nURL Fetch\n\nAnalysis\n\nAnalyzer\n\nLocal Processing\n\nLocal\nSearch\n\nDocument\n\nExternal market research, competitive intel-\nligence, and public data analysis. Ideal for\nmarket trends, competitor analysis, industry\nreports, news articles.\n\nAccess to proprietary internal data through\nextensible adapters (Nextcloud, Mattermost,\nemail, FileBrowser). Ideal for internal metrics,\ncommunications, confidential documents.\n\nDirect content extraction from specific URLs.\nIdeal for deep analysis of reports, whitepapers,\ncase studies, competitor websites.\n\nAI-powered synthesis and analysis using\nvector search. Ideal for cross-referencing find-\nings, identifying patterns, generating insights.\n\nSemantic search within locally ingested\ndocuments. Ideal for targeted retrieval from\nlocal files with source references.\n\nPriority Scoring and Dependencies Actions are assigned priority scores (0.0-1.0 scale) based on strategic\nimportance and expected information value. The priority assignment follows enterprise research principles:\n\n• Source Type Prioritization: Enterprise and local sources receive higher priority than external\n\nsources, reflecting the strategic value of proprietary information in competitive analysis.\n\n• Query Specificity: Targeted queries addressing specific investigation areas score higher than\n\nbroad exploratory searches, ensuring focused research execution.\n\n22\n\n\f• Dependency Management: Actions can specify prerequisite relationships where certain\ninformation gathering must precede analysis or synthesis tasks. The scheduler respects these\ndependencies while maximizing parallel execution within each iteration.\n\nF.4 ENTERPRISE INTEGRATION ARCHITECTURE\n\nService Adapters The system implements extensible adapters for enterprise services including Nextcloud\nfile server, Mattermost chat, IMAP email systems, and FileBrowser interfaces. Each adapter handles\nservice-specific authentication, data retrieval, and metadata preservation for proper citation attribution.\n\nSource Prioritization Strategy Enterprise and local sources receive priority multipliers in action scoring,\nreflecting the strategic value of proprietary information. The system maintains source type classification\nthroughout the pipeline to ensure internal intelligence drives analytical conclusions while external sources\nprovide context and validation.\n\nF.5 TOOL SELECTION AND EXECUTION\n\nThe Tool Selection stage implements a priority-based action selection and tool invocation within each\nresearch iteration to execute the action plan.\n\nAction Selection Process At each iteration, the system selects executable actions based on priority\nscores and dependency satisfaction:\n\n• Priority-Based Scheduling: Actions are ranked by their priority scores, with enterprise and local\nsources prioritized over external sources to maximize the value of specific private information.\n\n• Dependency Validation: The scheduler checks that all prerequisite actions have completed\n\nbefore making an action available for execution.\n\n• Sequential Execution: Actions execute one at a time in priority order, maintaining research\n\ncoherence and enabling each action to build upon previous findings.\n\nOnce selected, actions execute through a standardized interface and results are integrated into the research\ncontext, informing the following stage of adaptive action planning.\n\nF.6 ADAPTIVE PLANNING\n\nThe Adaptive Planning system enables dynamic evolution of the action plan by analyzing results after\neach iteration to generate extra actions addressing information gaps.\n\nIt starts by analyzing the most recently completed actions and performs these two substages:\n\nSource analysis and gap classification. The system evaluates the possible imbalances in the action\ncompletion if information came from internal or external sources and identifies possible scenarios to cover.\n\nDynamic action generation. After analyzing the sources and results from the previous actions, the\nsystem makes an LLM call to generate 1-5 extra candidate actions with a specific prioritization. After\ncandidate actions are generated, they go through a deduplication process to make sure the plan didn’t\ncover them already and incorporates the final subset into the priority action plan so they can be considered\nby the scheduler in the following iteration.\n\nF.7 CONTENT PROCESSING AND VECTOR STORE\n\nThe Content Processing system implements a pipeline for unified ingestion of documents in multiple\nformats (PDF, docx, HTML, JSON, plain text formats, etc.) that normalizes and cleans text inside the\ndocuments and websites retrieved during the research. Content is then deduplicated and chunkized, and\nembeddings are computed for each of these chunks.\n\nThe Vector Store implements the storage and retrieval of the content via JSON metadata and NumPy\nembedding metrices, enabling semantic similarity and keyword based searches\n\n23\n\n\fF.8 REPORT GENERATION\n\nMulti-Stage Synthesis Pipeline The Report Generation stage implements a four-stage synthesis\napproach: (1) thematic content clustering via vector searches, (2) source prioritization and deduplication,\n(3) LLM Synthesis with source tracking, and (4) Final report writing and citation resolution.\n\nThe system generates targeted search queries based on the research plan, including specific to ensure that\na predefined set of themes or sections (background, analysis, implementation, trends) are retrieved and\nwritten and prevents redundant analyses.\n\nUnified Citation System The citation system implements deferred resolution to keep consistency of\ncitations from section to section by referencing document IDs in the Vector Store and carrying them over\nfor each piece of synthesized text. A final citation resolution stage will assign the correct numbering to\neach document in the final report.\n\nF.9 DRBA PROMPTS\n\nThe DRBench agent relies on carefully designed prompts to orchestrate enterprise research workflows.\nThese prompts implement the core architectural principles: enterprise context enrichment, structured\nplanning decomposition, priority-based action generation, quantitative synthesis requirements, and adaptive\nresearch capabilities. The following five prompts represent the critical LLM interactions that enable\nsystematic enterprise research with proper source prioritization and citation tracking:\n\n• Enriched Query Generation (Prompt 1): Transforms basic research questions into enterprise-\ncontextualized queries by incorporating company information, stakeholder personas, and business\ncontext to guide subsequent research activities.\n\n• Research Planning (Prompt 2): Decomposes complex research questions into structured\ninvestigation areas with defined information needs, knowledge sources, and business rationales,\nenabling systematic coverage of the research space.\n\n• Action Generation (Prompt 3): Converts research objectives into prioritized executable\nactions with tool specifications and dependency relationships, emphasizing enterprise source\nprioritization over external sources.\n\n• Adaptive Action Generation (Prompt 4): Analyzes research progress to identify coverage\ngaps and source imbalances, generating complementary actions that enhance research depth\nand cross-validate critical findings.\n\n• Report Synthesis (Prompt 5): Orchestrates quantitative-first content synthesis with strict citation\nrequirements, ensuring numerical data leads analytical paragraphs and all claims are properly\nattributed to source documents.\n\nW Query Generation\n\nResearch Question: {dr question}\n\nCompany Context: {company name} is {company desc}.\n\nPersona Context: This analysis is requested by {name}, {role} in\n{department}.\n\nTheir responsibilities include: {responsibilities}.\n\nPrompt 1: Query Enrichment with Enterprise Context.\n\nG QUALITATIVE RESULTS\n\nShown below are some illustrative examples of how metrics are computed for different scenarios on a\ngiven test task.\n\n24\n\n\fW Research Planning (Complex Mode)\n\nDesign a comprehensive enterprise research strategy for:\n\"{question}\"\n\n{tools section}\n\nAs a senior enterprise researcher with deep business\nintelligence expertise, create a thorough investigation\nplan that combines rigorous research methodology with\nstrategic business analysis. Your goal is to provide insights\nthat drive informed decision-making in complex enterprise\nenvironments.\n\nEnterprise Research Design Principles:\n\n- Leverage proprietary internal data as competitive advantage while\nensuring external market context\n- Design investigations that directly inform strategic decisions and\nbusiness outcomes\n- Prioritize research areas that maximize ROI and strategic value to the\norganization\n- Balance comprehensive analysis with focused insights relevant to\nenterprise objectives.\n\nGenerate a JSON object with strategic research investigation\nareas:\n\n{output structure}\n\n.\n\nPrompt 2: Enterprise Research Planning with Investigation Areas. See example of the output structure\nin Listing 2\n\nG.1\n\nINSIGHTS RECALL\n\nWe showcase the insights recall metric using Task DR0002, whose DR Question is “How can\npersonalization drive sales in the retail industry and what strategies can be used for Lee’s Market in action?”\n(also shown in Table 7), which evaluates sales challenges and competitive landscape analysis.\n\nTable 10 shows the overall performance comparison between using Llama 3.1 405B and GPT-5 in\nour agent. Using GPT-5 in our agent results in increasing the insights recall score from 0.14 to 0.43,\nsuccessfully answering 3 out of 7 questions compared to Llama’s 1 out of 7.\n\nTable 10: Insights Recall Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We\nsummarize the number of questions answered successfully and unsuccessfully as well as the overall\ninsights recall score for the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nInsights Recall Score\n\nQuestions Answered\nSuccessfully\n\nQuestions Failed\n\n0.14\n\n1/7\n\n6/7\n\n0.43\n\n3/7\n\n4/7\n\n+0.29\n\n+2\n\n-2\n\nThe question-by-question breakdown in Table 11 reveals the specific questions where each approach\nsucceeded or failed. Both models successfully identified the insight related to online customer engagement,\nbut only GPT-5 was able to identify the number of loyalty program members and the customer data\ncollection rate. Neither model successfully answered the remaining 4 questions, indicating these insights\nmay not have been readily available in the source materials or that agents struggled to find the right insights.\n\n25\n\n\fW Action Generation\n\nGenerate specific executable actions for this research investigation\narea with SOURCE PRIORITIZATION:\n\nResearch Focus: {research focus}\n\nInformation Needs: {information needs}\n\nKnowledge Sources: {knowledge sources}\n\nResearch Approach: {research approach}\n\nAvailable Tools: {available tool names}\n\nTool Selection Guidelines: {tool guidelines}\n\nReturn JSON array of actions with:\n- \"type\": Action type (web search, enterprise api, url fetch, analyzer,\nlocal search)\n- \"description\": Clear description of what this action will accomplish\n- \"parameters\": Tool-specific parameters including query, search type,\netc.\n- \"priority\": Float 0.0-1.0 (enterprise sources: 0.7-1.0, external:\n0.4-0.7)\n- \"expected output\": What information this action should provide\n- \"preferred tool\": Specific tool class name to use\n\nPrompt 3: Priority-Based Action Generation with Source Awareness\n\nTable 11: Question-by-Question Insights Recall Analysis (Task DR0002). We breakdown the results\nquestion by question for the given task, highlighting specifically which question is answered correctly\nor incorrectly for each model.\n\nQuestion\n\nLlama 3.1 405B GPT-5\n\nOnline Customer Engagement with Personalized Recommendations\n\nNumber of Loyalty Program Members\n\nCustomer Data Collection Rate\n\nOnline Sales Growth\n\nEffectiveness of Personalized Marketing\n\nPersonalized Promotions vs Mass Promotions\n\nRetail Media Growth\n\nTotal Insights Recall Score\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0\n\n1.0\n\n1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n∆\n\n0.0\n\n+1.0\n\n+1.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n1.0/7.0\n\n3.0/7.0\n\n+2.0\n\nIn Table 12 we extend Table 4 and show all the groundtruth insights as well as each of the predicted\ninsights from using both Llama 3.1 405B and GPT-5. As before, we highlight in bold where the model\nwas able to accurately find details relevant to the expected insight, and show all the corresponding scores\nas given in Table 11.\n\nG.2 FACTUALITY\n\nThe factuality metric evaluation uses the same Task DR0002 to assess the accuracy and reliability of\ngenerated content. Table 13 presents the factuality performance comparison, showing that while using\nLlama 3.1 405B achieved 0.41 factuality (7 factual claims out of 17 total claims), where as using GPT-5\nreached 0.65 factuality (13 factual claim out of 20 total claims). This represents a significant improvement\n\n26\n\n\fW Adaptive Action Generation\n\nBased on the research progress so far, suggest new actions with\nINTELLIGENT SOURCE COMPLEMENTARITY:\n\nOriginal Research Query: {research query}\n\nCompleted Actions Summary: {completed actions summary}\n\nRecent Findings Analysis: {findings json}\n\nSource Composition Analysis: {internal findings}\n\nAvailable Tools: {available tool names}\n\nGenerate 1-5 new actions that:\n1. **Address gaps** in current research coverage\n2. **Balance source types** - if findings are heavily external,\nprioritize internal sources and vice versa\n3. **Build on discoveries** - leverage new information to explore\nrelated areas\n4. **Enhance depth** - dive deeper into promising\nfindings\n5. **Cross-validate** - verify critical findings through alternative\nsources\n\nEach action should have:\n- Strategic rationale for why this action is needed\nnow\n- Clear connection to research gaps or promising\nleads\n- Appropriate priority based on strategic value and source\nbalance\n- Specific parameters that build on existing knowledge\n\nPrompt 4: Gap-Driven Adaptive Action Generation\n\nin content reliability. This also highlights that GPT-5 is much better prepared to make accurate claims\nthat are sustained by evidence.\n\nTable 14 provides a detailed breakdown of factual versus unfactual claims. The agent using GPT-5\ngenerated 6 additional factual claims while producing 3 fewer unfactual claims, resulting in a net\nimprovement in accuracy percentage. This demonstrates that GPT-5 may generate a higher proportion\nof factual information than Llama 3.1 405B.\n\nThe impact of these factuality improvements of using GPT-5 over Llama 3.1 405B is summarized in\nTable 15. The 24.0 percentage point improvement in factuality represents enhanced content quality and\nresearch reliability. The increase in 3 total claims shows that GPT-5 can generate more content overall.\n\nH EXPERIMENTAL SETTINGS\n\nAll experiments were conducted on a cluster of 8 NVIDIA A100 GPUs (80GB each). For file generation\nand task construction, we primarily used the Llama-3.1-405B model, with decoding performed\nusing nucleus sampling at a temperature of 0.7 unless otherwise specified. For larger-scale evaluations of\nthe DRBench Agent, we also used closed-source models such as GPT-4o and GPT-5, alongside DeepSeek\nmodels, to enable comparison across open- and closed-source backbones.\n\nTo ensure reproducibility, the DRBench environment was deployed as a self-contained Docker container\nwith all supporting applications (Nextcloud, Mattermost, and Filebrowser) pre-configured. Each task was\n\n27\n\n\fTable 12: Insights Recall Improvement Areas (Task DR0002). We highlight in bold where each model\nwas able to accurately find details relevant to the groundtruth insight. We also show the corresponding\nscore where 1.0 is considered a successful recall and 0.0 an unsuccessful recall.\n\nGroundtruth Insight\n\nInsight Predicted by Llama 3.1 405B\n\nInsight Predicted by GPT-5\n\n45% of our online customers have interacted\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n\n45% of Lee’s Market online customers\nengage with personalized product recom-\nmendations, resulting in a 25% increase in\naverage order value.\n(Score = 1.0)\n\n45% of online customers engaged with\npersonalized product recommendations,\nand among those engagers average order\nvalue increased by 25%.\n(Score = 1.0)\n\nAs of Q3 2024, Lee’s Market has 1.2 million\nloyalty program members.\n\n85% of Lee’s Market transactions are linked\nto customer loyalty accounts as of Q2 2024.\n\nLee’s Market online sales grew 12% in Q2\n2024 compared to Q2 2023.\n\nRetailers excelling in personalized marketing\nare growing revenues about 10 percentage\npoints faster than their peers, according to\nBCG. By effectively using first-party customer\ndata, these leaders could unlock an estimated\n$570 billion in additional sales, highlighting\nthe importance of data-driven sales strategies\nfor growth.\n\nPersonalized promotions can deliver returns\nthree times higher than mass promotions,\nyet many retailers allocate under 5% of their\npromo budgets to personalization. One major\nchain increased its personalized promo spend\nfrom 1% to 10% by establishing a ”customer\ninvestment council,” resulting in $250 million\nin incremental sales.\n\nRetail media networks are expanding rapidly,\nwith retail media growing at approximately\n25% annually, offering retailers a profitable\nrevenue stream to reinvest in technology, data,\nand personnel. By integrating loyalty data,\nretailers like Sephora, which links 95% of\ntransactions to loyalty accounts, enhance preci-\nsion in product recommendations and provide\na seamless omnichannel experience, boosting\nconversion rates and customer lifetime value.\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nFrom Q2 2024 to Q3 2024, loyalty members\nincreased from 1,050,000 to 1,200,000\n(+150,000; +14.29%), average spend per\nmember rose from 24 to 25 (+4.17%), and\ntotal member spend increased from 25,200,000\nto 30,000,000 (+19.05%).\n(Score = 1.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 1.0)\n\n45% of Lee’s Market online customers engage\nwith personalized product recommendations,\nresulting in a 25% increase in average order\nvalue.\n(Score = 0.0)\n\nA naive blended online AOV upside of\napproximately 11.25% is derived from 45%\nengagement multiplied by a 25% AOV lift\namong engagers.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nGrocers running data-driven loyalty cam-\npaigns have realized an average 3.8%\nlike-for-like sales uplift.\n(Score = 0.0)\n\nRetailers have seen a consistent 25% increase\nin revenue due to advanced personalization\ncapabilities.\n(Score = 0.0)\n\nsources\n\nindicate POS-enabled\nExternal\npersonalization can lift revenue 5%-15% and\nadvocate personalized e-receipts with relevant\noffers and coupons to extend post-purchase\nengagement.\n(Score = 0.0)\n\n85% of transactions are linked to loyalty\naccounts at Lee’s Market, providing a solid\nfoundation for personalized marketing and\nimproving customer engagement.\n(Score = 0.0)\n\nAs of Q2 2024, 85% of transactions were\nlinked to loyalty accounts, leaving a 15%\nunlinked identity gap.\n(Score = 0.0)\n\nTable 13: Factuality Performance Comparison: Llama 3.1 405B vs GPT-5 (Task DR0002). We show\nthe number of factual and unfactual claims made by each model, as well as the overall factuality score\nfor the given task.\n\nMetric\n\nLlama 3.1 405B GPT-5\n\nImprovement\n\nFactuality Score\n\nFactual Claims\n\nUnfactual Claims\n\n0.41\n\n7\n\n10\n\n0.65\n\n13\n\n7\n\n+0.24\n\n+6 claims\n\n-3 claims\n\n28\n\n\fW Report Synthesis\n\nAs an expert research analyst, synthesize the following content into\na coherent, insightful, and well-supported analysis for the theme:\n\"{theme}\" directly related to the overarching research question:\n\"{original question}\"\n\nSource Priority Guidelines:\n\n1. **\"internal\"**: Highest priority (internal company\ndocuments, proprietary files, confidential reports,\nenterprise chat messages, local documents, CRM data,\ninternal APIs, project management tools). Insights from\nthese sources should form the primary foundation of the\nanalysis.\n2. **\"external\"**: Medium priority (public web sources,\nacademic papers, industry reports, news articles). Use these\nto provide broader context, external validation, or contrasting\nperspectives.\n\n**Synthesis Requirements:**\n* **QUANTITATIVE PRIORITY:** Lead with numerical data, calculations, and\naggregations\n* Extract ALL percentages, costs, metrics, and performance\ndata\n* Perform mathematical operations: aggregate percentages, calculate\nincreases, sum totals\n* Example: \"Finance customers (35%) combined with healthcare\n(40%) represent 75% of regulated industry concerns\n[DOC:doc 1][DOC:doc 2]\"\n\n* **FACT VERIFICATION (CRITICAL):**\n* ONLY state what documents explicitly contain - no inference or\nextrapolation\n* Use exact quotes for key numerical claims: \"As stated in the\ndocument: ’[exact quote]’ [DOC:doc id]\"\n\n* **Citation Usage (Critical):**\n* **Format:** Reference sources by their document ID: \"Internal review\nshows 15% increase [DOC:doc 079c2e0f 1752503636]\"\n* **NEVER HALLUCINATE CITATIONS:** Only use provided doc id\nvalues\n* **Cite every numerical claim and calculation with source\ndocuments**\n\nGenerate 2-4 paragraphs of synthesized analysis with proper inline\ncitations.\n\nPrompt 5: Report Section Synthesis with Citation Requirements\n\nexecuted by instantiating a fresh container to avoid state leakage across runs. We capped the number of\nagent iterations according to the settings described in Section 5, with each iteration limited by a fixed\ncomputational budget.\n\nFor model outputs, we standardized all prompts and evaluation pipelines across backbones, using identical\nresearch questions, company contexts, and injected insight sets. To avoid stochastic variability, we repeated\ngeneration three times per task and reported averaged scores.\n\nFinally, all supporting scripts, environment configurations, and evaluation code are fully containerized,\nenabling consistent replication of our reported results across hardware setups.\n\n29\n\n\fTable 14: Factuality Content Analysis (Task DR0002). We show the number of factual and unfactual\nclaims made by each model, highlighting the factuality accuracy of each model for the given task.\n\nAgent\n\nFactual Unfactual Accuracy\n\nLlama 3.1 405B\n\n7 claims\n\n10 claims\n\nGPT-5\n\nChange\n\n13 claim\n\n7 claims\n\n+6\n\n-3\n\n+24.0%\n\n41.0%\n\n65.0%\n\nTable 15: Factuality Summary (Task DR0002). We summarize the factuality result improvements made\nby using GPT-5 over Llama 3.1 405B.\n\nImpact Category\n\nValue\n\nFactuality Improvement\n\n+24.0 percentage points\n\nClaims Added\n\n3 total claims\n\nAccuracy Enhancement\n\nFrom 41.0% to 65.0%\n\nContent Quality\n\nMore grounded information\n\nTask Domain\n\nTask Industry\n\nSales\n\nRetail\n\nI DATA SYNTHESIS AND DRBA COST DETAILS\n\nTo generate the DRBench tasks, we combined external insight extraction with internal file synthesis. Each\ntask included an average of 10 supporting files spanning heterogeneous formats (PDF, DOCX, PPTX,\nXLSX, and JSONL), with each file containing roughly 5 paragraphs of content. Files were designed\nto embed injected insights while mixing in distractor material, ensuring realistic enterprise complexity\nwithout exceeding practical runtime or storage budgets.\n\nData synthesis was primarily powered by GPT-4o. During task construction, GPT-4o was responsible for (1)\nextracting structured insights from public web sources, (2) adapting these insights into enterprise-grounded\ninterpretations, (3) generating persona-specific deep research questions, and (4) producing file-level content\nwith a balanced mix of insights and distractors. For evaluation, the DRBench Agent (DRBA) used GPT-4o\nas its backbone, with each task typically requiring 15 iterations and approximately 120–150 model calls.\n\nIn terms of cost, GPT-4o-based synthesis of a single task (10 files, 5 paragraphs each, plus metadata)\nconsumed about 30k–40k tokens, while DRBA execution required an additional 50k–70k tokens per\ntask. At current GPT-4o API pricing ($5 per million input tokens and $15 per million output tokens), this\ncorresponds to a per-task cost of approximately $1.5–$3.5 depending on the mix of input/output tokens\nand the iteration budget. This makes large-scale benchmarking feasible at moderate cost, while still being\nsignificantly cheaper than manual authoring or annotation.\n\nWe also note that smaller open-source models such as Llama-3.1-8B-Instruct perform well for file\ngeneration. Unlike GPT-4o, which requires API usage, Llama-3.1-8B can be hosted locally and runs\nefficiently on a single NVIDIA A100 40GB GPU. This provides a cost-effective alternative for generating\nlarge numbers of supporting documents, especially when full closed-source quality is not required.\n\nJ WEB AGENTS FOR DEEP RESEARCH TASKS\n\nSince each of the environments can be access directly through a web user interface (UI), we also exper-\nimented with an agent that can directly interact with the webpages through common browser actions like\nclick, input and scroll, which are executed through playwright3. We implement our web agent\nusing the AgentLab and BrowserGym frameworks (Chezelles et al., 2025) with a GPT-4.14 backbone. Our\nagent is implemented from AgentLab’s GenericAgent, which achieves respectable performance when used\n\n3https://playwright.dev\n4https://openai.com/index/gpt-4-1/\n\n30\n\n\fwith GPT-4o5 as a backbone; it completes 45.5% of the tasks in WorkArena (Drouin et al., 2024), 31.4%\nin WebArena (Zhou et al., 2024) and achieves a step-level reward of 13.7% on WebLINX (L`u et al., 2024).\n\nHyperparameters and Prompt We present the hyperparameters for the agent in tables 16 and 17, which\nare in majority set to the default hyperparameters, except for the maximum number of input tokens (bound\nto a reasonable maximum length) and a higher maximum number of steps (to allow the agent to perform\nmore actions required to write the report). We further update the agent’s action space on the last step to\nonly allow it to reply to the user with a report, ensuring that each trajectory terminates with a report. To\nensure that the agent is aware of the tools it can use, we modify the default system prompt (see prompt 6).\nAdditionally, each task intent is provided alongside information about the user and company (see prompt 7).\n\nResults We find that the GPT-4.1-powered web agent achieves an insights recall and factuality of\n1.11% and 6.67% respectively and a report quality score of 33.07%. Although the high report quality\nindicates that the agent can properly formulate a report, the insights quality is severely limited, with none\nof the claims being backed by useful sources. For example, a DRBench Agent powered by GPT-5 may\nanswer the question What is Lee’s Market’s current food waste reduction rate as of Q2 2024? with An\n8% reduction in food waste in Q2 2024 saved Lee’s Market $1.2 million, indicating that better inventory\ncontrol can yield both safety and financial benefits., which achieves a score of 1.0 for the question. On\nthe other hand, a GPT-4.1-powered agent will provide an unsatisfactory answer, thus achieving an insights\nrecall of 0.0. The most likely cause of this poor performance is the model’s limited capability to properly\ninteract with web interfaces when encountering unfamiliar tools. For instance, the agent may be unfamiliar\nwith the VNC and file browser applications, making it harder for it to correctly select the file it needs\nto use. Moreover, whenever the agent ends up performing an ineffective action (e.g. click on an element\nthat does not trigger any change to the page), it tends to persist by reiterating the same action (see Table\n18), or the same sequence of ineffective actions, despite not achieving anything in the previous steps. As a\nresult, despite a large number of steps, most of the agent’s actions are not helpful towards solving the task.\n\nTable 16: Web Agents Boolean Hyperparameters\n\nValue Flags\n\nTrue\n\nFalse\n\nvision support, use ax tree,\nuse tabs, use focused element, use error logs,\nuse history, use action history, use screenshot,\nuse som, extract visible tag, extract clickable tag,\nuse thinking, use concrete example, use abstract example,\nuse hints, be cautious, add missparsed messages\nuse html, use past error logs, use think history,\nuse diff, filter visible elements only,\nfilter with bid only, filter som only, multiaction,\nstrict, long description, individual examples,\nuse plan, use criticise, use memory, enable chat\n\nK STANDARD ERROR\n\nRestricting to the MinEval subset, we average the results on each task across 3 different runs in Table\n19. We give both the means and standard errors for the insight recall, factuality, distractor avoidance, and\nreport quality.\n\nL COMPLEX MODEL ABLATION RESULTS\n\nExtending our discussion in Section 5.3 to more GPT and Llama models in Table 20, we see that the\nsmaller GPT-5-mini model lags behind but still outperforms earlier closed-source backbones such as\nGPT-4o and GPT-4o-mini, particularly in terms of harmonic mean. In addition, smaller variants of Llama\n\n5https://openai.com/index/hello-gpt-4o/\n\n31\n\n\fW Web Agents System Prompt\n\nYou are an agent trying to solve a web task based on the content of the\npage and user instructions. You can interact with the page and explore,\nand send messages to the user. Each time you submit an action it will\nbe sent to the browser and you will receive a new page.\nYou will be solving tasks that involve Deep Research, by\nnavigating websites and web apps with information useful\nsolving the task. You will need to gather insights from\ndata contained in the services provided and the internet\nto complete your report. You have a maximum of 50 steps\nto complete the task. Before the end, you must use the\naction {send msg to user}, which should contain a final Deep\nResearch report detailing everything the user needs to\nknow.\n\nYou must sustain your claims in files, chats, or emails\nfrom the enterprise environment or in websites you\nsearched. You must provide a citation (or an inline\ncitation, that works too) with the source of those claims\n(e.g. << add citation examples from the documentation\n>>). Do not make up citations if you haven’t retrieved its\ncontent.\n\nHere are some expected agent behavior:\n\n**Enterprise Environment Interaction:**\n- Access Nextcloud files, Mattermost chats, emails, VNC desktop,\netc.\n- Extract relevant information from multiple sources\n- Navigate complex enterprise data landscapes\n\n**Report Requirements:**\n- Synthesize findings into comprehensive research\nreport\n- Include proper citations for all claims (flexible format -\nauto-normalized)\n- Draw meaningful insights and conclusions\n- Ground all statements in available evidence\n\n**Citation Format (Flexible - Auto-Normalized):**\n- Files: ‘quarterly report.pdf‘, ‘shared/budget-analysis.xlsx‘,\n‘Analysis document (reports/analysis.docx)‘\n- URLs: Direct links or ‘[Article Title](https://example.com)‘\n- Emails: ‘Email from alice@company.com on Jan 20,\n2025‘\n- Chat: ‘Mattermost message from john.doe in Compliance team, General\nchannel‘\n\nIf you need navigate the internet (outside of the designated websites),\nyou can use the browser inside noVNC Desktop.\n\nPrompt 6: Extended instructions given to the Deep Research web agent.\n\n32\n\n\fTable 17: Web Agents Non-Boolean Hyperparameters\n\nParameter\n\nValue\n\nchat model.model name\nchat model.max total tokens\nchat model.max input tokens\nchat model.max new tokens\nchat model.temperature\naction.action set.subsets\naction.action set.retry with force true\nflags.max prompt tokens\nflags.max trunc itr\nenv.max steps\n\ngpt-4.1\n32768\n28672\n4096\n0\nwebarena\n\n28672\n20\n50\n\nW Web Agents Task Intent Prompt\n\nHow can Lee’s Market leverage FSMA 204 regulations to enhance food\nsafety and customer trust?\n\nHere is some information about the user:\nID: MNG0003\nFirst name: John\nLast name: Doe\n...\nJustification: As a regulatory affairs manager, John\n...\n\nHere is some information about the company:\nName: Lee’s Market\nAnnual revenue: $500M - $600M\n...\nTarget markets: Asian communities in the U.S. and ...\n\nPrompt 7: Web Agents Task Intent Prompt\n\nTable 18: Web Agents tends to get stuck on cycles of actions, and are unable to backtrack or to restart\nwith a different application.\n\n9. I have been repeatedly clicking\non the “shared” folder (bid ‘194’)\n. . . To proceed with the research\non personalization strategies for\nLee‘s Market, I should open the\n“shared” folder to look for. . .\nclick(‘194’)\n\n10. I have clicked on the “shared”\nfolder multiple times . . .\nI will\ntry clicking on the “shared” folder\nagain to see if it opens and displays\nits contents.\n\nclick(‘190’)\n\n11. I have been repeatedly clicking\non the “shared” folder (bid ‘194’)\nTo proceed, I should try\n. . .\nclicking on the “shared” folder\nagain to attempt to open it and\nview any files . . .\n\nclick(‘194’)\n\n12. I have navigated to the “shared”\nfolder, but I have not yet opened\nit . . . The next logical step is to\nopen the “shared” folder to look\nfor any relevant documents . . .\n\nclick(‘194’)\n\ndegrade metric results further. This further substantiates the claim that larger and more advanced models\ntend to offer a better balance between recall, factuality, and overall report quality.\n\nM QUANTITATIVE RESULTS PER TASK\n\nWe show a detailed breakdown of the insights recall in Table 21, factuality in Table 22, distractor avoidance\nin Table 23 and report quality in Table 24 on the MinEval subset for a variety of models.\n\n33\n\n\fTable 19: DRBA performance with different planning configurations on MinEval. We compare the base\nagent with variants using Simple Research Planning (SRP), Complex Research Planning (CRP), Adaptive\nAction Planning (AAP), and their combinations. Scores are reported for insight recall, factuality, distractor\navoidance, report quality, and the overall harmonic mean.\n\nConfiguration\nBase DRBA\n+ SRP\n+ CRP\n+ AAP\n+ SRP + AAP\n+ CRP + AAP\n\nInsight Recall Factuality Distractor Avoidance Report Quality Harmonic Mean\n\n.188 ± .038\n.137 ± .04\n.154 ± .03\n.197 ± .03\n.142 ± .04\n.188 ± .05\n\n.665 ± .09\n.622 ± .11\n.705 ± .05\n.604 ± .08\n.504 ± .10\n.691 ± .03\n\n.981 ± .01\n1.00 ± .00\n1.00 ± .00\n.995 ± .39\n.990 ± .39\n1.00 ± .00\n\n.912 ± .004\n.901 ± .006\n.917 ± .004\n.928 ± .005\n.906 ± .007\n.923 ± .006\n\n.448\n.365\n.400\n.454\n.359\n.453\n\nTable 20: Performance of DRBA on the MinEval subset using different backbone language models and\nplanning strategies. Scores are reported for insight recall, factuality, distractor avoidance, report quality,\nand harmonic mean. Note that higher numbers corresponds to better scores, and the best result on each\nmetric is bolded.\n\nModel\nGPT-5\nGPT-5\nGPT-5\nGPT-5-mini\nGPT-5-mini\nGPT-5-mini\nGPT-4o\nGPT-4o\nGPT-4o\nGPT-4o-mini\nGPT-4o-mini\nGPT-4o-mini\nGPT-OSS-120B\nGPT-OSS-120B\nGPT-OSS-120B\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nLlama-3.1-70b-Instruct\nLlama-3.1-70b-Instruct\nLlama-3.1-70b-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nPlanning\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nComplex\nNone\nSimple\n\nInsight Recall\n38.33\n37.86\n39.63\n25.68\n28.37\n26.57\n17.53\n20.37\n17.31\n13.75\n13.67\n13.08\n22.40\n17.42\n18.31\n17.37\n16.97\n20.16\n18.54\n16.28\n17.02\n25.15\n25.56\n30.26\n26.82\n25.55\n23.20\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n74.52\n72.09\n65.17\n58.76\n58.96\n51.07\n65.43\n62.35\n60.84\n46.68\n55.59\n48.95\n29.24\n27.48\n38.92\n78.91\n79.27\n69.75\n64.64\n69.43\n52.82\n72.66\n73.45\n70.27\n58.35\n69.39\n67.23\n\n95.14\n97.14\n92.86\n96.51\n95.81\n94.48\n99.05\n98.57\n98.33\n99.05\n97.14\n97.14\n97.14\n97.14\n98.1\n100.00\n98.10\n97.90\n96.70\n97.62\n98.10\n97.43\n96.67\n96.67\n97.65\n98.10\n98.10\n\n94.56\n95.34\n93.42\n84.48\n85.74\n86.28\n92.58\n93.12\n91.62\n84.72\n85.86\n86.04\n84.24\n84.36\n85.44\n90.48\n92.34\n91.26\n85.32\n84.96\n86.16\n86.52\n87.36\n86.88\n89.64\n90.24\n88.14\n\n79.80\n78.92\n77.74\n60.21\n63.73\n58.89\n48.47\n53.06\n47.35\n38.33\n39.39\n37.28\n44.82\n38.38\n44.14\n49.78\n48.87\n53.86\n50.08\n46.41\n45.46\n62.59\n63.29\n69.28\n61.75\n62.64\n58.58\n\nN EFFECT OF NUMBER OF ITERATIONS\n\nWe next analyze the effect of varying the number of research loop iterations when using DRBA with\nGPT-5 as the backbone language model. Results for both the baseline configuration without explicit\nplanning and the complex planning setup are shown in Table 25. Overall, increasing the iteration budget\ndoes not guarantee consistent improvements. With no planning, performance initially drops when the\nagent executes more iterations, as additional exploration often introduces noise and distracts from key\ninsights. However, with a larger budget the agent partially recovers, suggesting that a small number of\nadditional iterations can help refine factual grounding, while excessive exploration reduces focus.\n\nFor the complex planning setting, higher iterations improve certain metrics such as factuality, but this\ncomes at the cost of lower insight recall and reduced overall balance. This indicates that while more steps\nallow the agent to verify citations more carefully, they can also lead to fragmented reasoning and overfitting\n\n34\n\n\fFigure 8: Total average Insight Recall scores per model and insight source type computed on all the\nresults available for each model running in Complex Research Plan mode. Insights embedded in enterprise\nsources are more easily retrieved by DRBA in all the models.\n\nto peripheral evidence. The best overall performance emerges at moderate iteration counts, highlighting\nthe importance of carefully tuning the iteration budget rather than simply scaling up the number of steps.\n\nO DATA GENERATION PROMPTS\n\nO.1 COMPANY AND PERSONA DATA GENERATION\n\nIn this section we give the prompts used for company generation 8 and persona generation 9.\n\nW Company Generation Prompt\n\nGenerate a realistic company structure for {company name} in the\n{industry} industry.\n\nCompany size: {size} ({employee range} employees)\n\nThe company should focus on this domain {domain}\n\nEXTERNAL INSIGHTS: {external insights}\n\nReturn ONLY a valid JSON object with this structure:\n{output structure}\n\nMake it realistic for the {industry} industry.\n\nPrompt 8: Company Generation Prompt Template.\n\n35\n\ngpt-5enterprise_fact0.597external_fact0.0deepseek-chat-v3.1enterprise_fact0.472external_fact0.0gpt-5-minienterprise_fact0.444external_fact0.0qwen-2.5-72b-instructenterprise_fact0.417external_fact0.0llama-3.1-405b-instructenterprise_fact0.347external_fact0.0gpt-oss-120benterprise_fact0.333external_fact0.0gpt-4o-minienterprise_fact0.194external_fact0.0llama-3.1-70b-instructenterprise_fact0.194external_fact0.0gpt-4oenterprise_fact0.182external_fact0.0\fTable 21: Mean and standard error of the insight recall metric for the first five tasks, obtained from three\nruns of on DRBench our agent (DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nW Persona Generation Prompt\n\nDR0001\n.222 ± .056\n.222 ± .056\n.278 ± .056\n.111 ± .056\n.222 ± .056\n.000 ± .000\n.111 ± .056\n.167 ± .000\n.111 ± .056\n.000 ± .000\n.056 ± .056\n.000 ± .000\n.111 ± .111\n.056 ± .056\n.000 ± .000\n.167 ± .000\n.222 ± .056\n.111 ± .056\n.167 ± .000\n.056 ± .056\n.167 ± .000\n.167 ± .000\n.278 ± .056\n.167 ± .000\n.222 ± .056\n.111 ± .056\n.167 ± .096\n\nDR0002\n.429 ± .000\n.381 ± .048\n.381 ± .126\n.286 ± .000\n.286 ± .082\n.381 ± .126\n.238 ± .048\n.333 ± .048\n.238 ± .095\n.095 ± .048\n.190 ± .048\n.190 ± .048\n.143 ± .000\n.143 ± .000\n.190 ± .048\n.143 ± .000\n.190 ± .048\n.238 ± .048\n.381 ± .048\n.286 ± .082\n.238 ± .048\n.286 ± .082\n.238 ± .048\n.095 ± .048\n.238 ± .048\n.333 ± .048\n.143 ± .082\n\nDR0003\n.467 ± .033\n.533 ± .033\n.567 ± .067\n.367 ± .033\n.333 ± .033\n.367 ± .067\n.167 ± .033\n.300 ± .000\n.300 ± .100\n.267 ± .033\n.167 ± .067\n.267 ± .033\n.433 ± .033\n.367 ± .033\n.333 ± .067\n.233 ± .088\n.167 ± .033\n.300 ± .058\n.200 ± .000\n.200 ± .058\n.267 ± .033\n.300 ± .058\n.333 ± .033\n.600 ± .058\n.400 ± .058\n.333 ± .088\n.400 ± .058\n\nDR0004\n.519 ± .098\n.370 ± .098\n.370 ± .037\n.222 ± .064\n.296 ± .074\n.370 ± .098\n.185 ± .074\n.148 ± .037\n.111 ± .000\n.185 ± .037\n.148 ± .074\n.074 ± .037\n.222 ± .000\n.148 ± .098\n.111 ± .064\n.185 ± .074\n.111 ± .064\n.148 ± .098\n.074 ± .037\n.185 ± .098\n.074 ± .074\n.259 ± .037\n.148 ± .074\n.370 ± .037\n.259 ± .037\n.259 ± .037\n.333 ± .000\n\nDR0005\n.281 ± .035\n.386 ± .046\n.386 ± .046\n.298 ± .018\n.281 ± .063\n.211 ± .053\n.175 ± .018\n.070 ± .018\n.105 ± .000\n.140 ± .035\n.123 ± .018\n.123 ± .018\n.211 ± .030\n.158 ± .061\n.281 ± .063\n.140 ± .035\n.158 ± .053\n.211 ± .030\n.105 ± .030\n.088 ± .046\n.105 ± .000\n.246 ± .046\n.281 ± .046\n.281 ± .035\n.158 ± .061\n.123 ± .018\n.298 ± .076\n\nGenerate {persona count} diverse employee personas for {company name} in\nthe {industry} industry.\n\nThe personas should focus on this domain: {domain}\n\nCreate diverse roles across seniority levels: Junior, Mid, Senior,\nExecutive\n\nReturn ONLY a valid JSON array with this exact format:\n{output structure}\n\nMake personas realistic with appropriate responsibilities for their\nroles.\n\nPrompt 9: Persona Generation Prompt.\n\nO.2 QUESTION GENERATION\n\nIn this section we give the prompt used to generate our deep research questions 10.\n\n36\n\n\fTable 22: Mean and standard error of the factuality metric for the first five tasks, obtained from our agent\n(DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nSimple\nGPT-4o-mini\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.761 ± .072\n.714 ± .050\n.730 ± .060\n.585 ± .045\n.647 ± .120\n.309 ± .126\n.792 ± .150\n.485 ± .262\n.475 ± .166\n.828 ± .043\n.611 ± .056\n.557 ± .030\n.144 ± .099\n.074 ± .074\n.368 ± .061\n.852 ± .087\n.802 ± .125\n.789 ± .053\n.618 ± .109\n.608 ± .173\n.588 ± .082\n.860 ± .014\n.696 ± .041\n.581 ± .042\n.674 ± .077\n.806 ± .049\n.626 ± .114\n\nDR0002\n.504 ± .075\n.384 ± .076\n.291 ± .094\n.297 ± .119\n.299 ± .056\n.381 ± .161\n.490 ± .110\n.512 ± .131\n.653 ± .097\n.265 ± .133\n.429 ± .092\n.324 ± .169\n.150 ± .035\n.128 ± .072\n.178 ± .078\n.726 ± .158\n.638 ± .202\n.392 ± .154\n.431 ± .160\n.681 ± .069\n.286 ± .108\n.518 ± .085\n.531 ± .086\n.657 ± .024\n.493 ± .109\n.540 ± .174\n.396 ± .056\n\nDR0003\n.866 ± .007\n.848 ± .034\n.782 ± .012\n.705 ± .039\n.624 ± .041\n.699 ± .047\n.827 ± .056\n.813 ± .041\n.704 ± .037\n.800 ± .000\n.622 ± .062\n.580 ± .075\n.386 ± .040\n.410 ± .090\n.564 ± .064\n.803 ± .028\n.800 ± .074\n.792 ± .055\n.684 ± .104\n.800 ± .069\n.686 ± .011\n.818 ± .041\n.922 ± .056\n.838 ± .050\n.866 ± .002\n.741 ± .074\n.723 ± .053\n\nDR0004\n.833 ± .019\n.812 ± .070\n.782 ± .064\n.704 ± .067\n.694 ± .028\n.692 ± .111\n.570 ± .058\n.693 ± .139\n.542 ± .110\n.690 ± .128\n.481 ± .209\n.642 ± .119\n.337 ± .117\n.311 ± .155\n.400 ± .076\n.820 ± .066\n.892 ± .035\n.771 ± .073\n.812 ± .021\n.826 ± .067\n.522 ± .270\n.679 ± .095\n.769 ± .035\n.774 ± .053\n.741 ± .060\n.724 ± .101\n.587 ± .139\n\nDR0005\n.762 ± .077\n.846 ± .029\n.674 ± .121\n.647 ± .098\n.683 ± .020\n.472 ± .114\n.593 ± .204\n.614 ± .121\n.406 ± .020\n.459 ± .235\n.191 ± .046\n.344 ± .144\n.445 ± .051\n.451 ± .080\n.435 ± .190\n.745 ± .022\n.832 ± .083\n.745 ± .100\n.687 ± .073\n.557 ± .143\n.559 ± .240\n.757 ± .057\n.754 ± .082\n.662 ± .098\n.696 ± .060\n.550 ± .148\n.586 ± .055\n\nO.3 PUBLIC SOURCE AND INSIGHT COLLECTION\n\nIn this section we give the prompt used to generate external insights 11. The URLs used for external\ninsight extraction and deep research question creation can be found in Table 26.\n\nO.4\n\nINTERNAL INSIGHT GENERATION\n\nIn this section we give the prompts to generate both internal insights 12 and internal distractors 13.\n\nO.5 FILE GENERATION\n\nIn this section we give the prompts used for generating each of the file types used in our tasks, which\nwe list as follows:\n\n• PDF: Prompts 18, 19, and 20\n\n• Excel: Prompts 21, 22, and 23\n\n• Powerpoint: Prompts 24, 25, and 26\n\n• Email: Prompts 27, 28, and 29\n\n• Chat: Prompts 30, 31, 29, and 33\n\n37\n\n\fTable 23: Mean and standard error of the distractor avoidance metric for the first five tasks, obtained from\nthree runs of on DRBench our agent (DRBA) using 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.857 ± .000\n.857 ± .000\n.905 ± .048\n.905 ± .048\n.857 ± .000\n.857 ± .000\n.952 ± .048\n.952 ± .048\n.952 ± .048\n.952 ± .048\n.857 ± .000\n.857 ± .000\n.857 ± .000\n.857 ± .000\n.905 ± .048\n1.00 ± .000\n.905 ± .048\n.952 ± .048\n.905 ± .048\n.905 ± .048\n.905 ± .048\n.905 ± .048\n.857 ± .000\n.905 ± .048\n1.00 ± .000\n.952 ± .048\n.952 ± .048\n\nDR0002\n.900 ± .058\n1.00 ± .000\n.900 ± .058\n.967 ± .033\n.933 ± .033\n.867 ± .133\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.967 ± .033\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.967 ± .033\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n\nDR0003\n1.00 ± .000\n1.00 ± .000\n.933 ± .000\n.978 ± .022\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.978 ± .022\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.978 ± .022\n\nDR0004\n1.00 ± .000\n1.00 ± .000\n.905 ± .024\n.976 ± .024\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.964 ± .036\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.952 ± .048\n.976 ± .024\n1.00 ± .000\n1.00 ± .000\n.976 ± .024\n.929 ± .000\n.905 ± .024\n.952 ± .024\n.952 ± .048\n\nDR0005\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n1.00 ± .000\n\nP EVALUATION PROMPTS\n\nIn this section we give the prompts for decomposing reports into atomic insights 14, computing insight\nrecall 15, computing factuality 16, and computing report quality 17. These prompts are discussed in detail\nin Section 5.1.\n\nQ HUMAN PREFERENCE EVALUATION\n\nWe calculate a human score for model a task t as:\n\nSa,t = 1\nn\n\n(cid:80)n\n\ni=1sa,i, where sa,i =\n\n(cid:26)1,\n\nif human choice is a or ”both good”\n\n0, otherwise\n\n, where n is the number of gold insights in task t, sa,i is the human score of model a on insight i. Figure9\nshows that the insight recall metric is on par with human decision.\n\n38\n\n\fTable 24: Mean and standard error of the report quality metric for the first five tasks, obtained from three\nruns of DRBench with 15 iterations across different backbone models.\n\nPlan\nConfiguration\nNone\nGPT-5\nSimple\nGPT-5\nComplex\nGPT-5\nNone\nGPT-5-mini\nSimple\nGPT-5-mini\nComplex\nGPT-5-mini\nNone\nGPT-4o\nSimple\nGPT-4o\nComplex\nGPT-4o\nNone\nGPT-4o-mini\nSimple\nGPT-4o-mini\nComplex\nGPT-4o-mini\nNone\nGPT-OSS-120B\nSimple\nGPT-OSS-120B\nComplex\nGPT-OSS-120B\nNone\nLlama-3.1-405B-Instruct\nLlama-3.1-405B-Instruct\nSimple\nLlama-3.1-405B-Instruct Complex\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nLlama-3.1-70B-Instruct\nDeepSeek-V3.1\nDeepSeek-V3.1\nDeepSeek-V3.1\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\nQwen-2.5-72B-Instruct\n\nNone\nSimple\nComplex\nNone\nSimple\nComplex\nNone\nSimple\nComplex\n\nDR0001\n.936 ± .008\n.942 ± .000\n.948 ± .007\n.892 ± .002\n.901 ± .009\n.896 ± .001\n.927 ± .000\n.934 ± .008\n.929 ± .009\n.886 ± .004\n.893 ± .002\n.889 ± .003\n.872 ± .007\n.878 ± .006\n.874 ± .007\n.914 ± .000\n.921 ± .008\n.917 ± .009\n.889 ± .003\n.895 ± .002\n.891 ± .003\n.884 ± .004\n.890 ± .003\n.886 ± .004\n.901 ± .001\n.908 ± .000\n.904 ± .001\n\nDR0002\n.918 ± .004\n.927 ± .008\n.921 ± .001\n.884 ± .007\n.889 ± .002\n.882 ± .006\n.911 ± .003\n.919 ± .000\n.914 ± .002\n.874 ± .008\n.881 ± .005\n.877 ± .006\n.861 ± .001\n.867 ± .009\n.863 ± .000\n.903 ± .003\n.910 ± .001\n.906 ± .002\n.877 ± .007\n.883 ± .005\n.879 ± .006\n.872 ± .008\n.878 ± .006\n.874 ± .007\n.889 ± .005\n.896 ± .003\n.892 ± .004\n\nDR0003\n.924 ± .005\n.936 ± .006\n.940 ± .008\n.879 ± .004\n.887 ± .001\n.884 ± .003\n.903 ± .001\n.911 ± .009\n.905 ± .000\n.861 ± .007\n.867 ± .004\n.864 ± .005\n.849 ± .009\n.854 ± .008\n.851 ± .008\n.897 ± .001\n.904 ± .000\n.899 ± .001\n.869 ± .005\n.874 ± .004\n.871 ± .005\n.864 ± .006\n.870 ± .005\n.866 ± .006\n.881 ± .002\n.888 ± .001\n.884 ± .002\n\nDR0004\n.909 ± .001\n.915 ± .002\n.922 ± .009\n.891 ± .000\n.895 ± .008\n.889 ± .009\n.918 ± .009\n.923 ± .001\n.920 ± .008\n.872 ± .002\n.878 ± .001\n.875 ± .000\n.858 ± .006\n.863 ± .004\n.860 ± .005\n.909 ± .008\n.915 ± .009\n.911 ± .008\n.881 ± .001\n.886 ± .000\n.883 ± .001\n.876 ± .002\n.881 ± .001\n.878 ± .002\n.893 ± .009\n.899 ± .008\n.895 ± .009\n\nDR0005\n.927 ± .009\n.933 ± .007\n.929 ± .008\n.886 ± .005\n.892 ± .003\n.890 ± .001\n.909 ± .002\n.916 ± .000\n.913 ± .009\n.879 ± .006\n.884 ± .003\n.882 ± .004\n.866 ± .008\n.872 ± .007\n.869 ± .006\n.902 ± .002\n.908 ± .000\n.905 ± .001\n.873 ± .004\n.878 ± .003\n.875 ± .004\n.869 ± .005\n.874 ± .004\n.871 ± .005\n.885 ± .003\n.891 ± .002\n.888 ± .003\n\nTable 25: Effect of the number of research loop iterations on the performance of DRBA with GPT-5 as\nthe backbone model, on MinEval.\n\nPlanning Method\nNone\nNone\nNone\nComplex\nComplex\nComplex\n\n# Iterations\n15\n30\n50\n15\n30\n50\n\nInsight Recall\n39.45\n28.80\n37.10\n44.44\n31.61\n38.16\n\nFactuality Distractor Avoidance Report Quality Harmonic Mean\n\n72.65\n69.03\n78.84\n62.51\n73.94\n66.05\n\n93.14\n98.57\n100.00\n90.95\n94.38\n94.38\n\n94.56\n96.12\n93.48\n93.12\n94.76\n92.64\n\n66.20\n57.34\n66.30\n66.41\n60.32\n63.76\n\n39\n\n\fW Deep Research Question Generation Prompt\n\nGenerate 3 Deep Research (DR) questions for the following business\ncontext:\n\nPersona: {persona name} - {persona role}\nDepartment: {persona department}\nResponsibilities: {persona responsibilities}\nCompany: {company name} ({company industry})\nDomain: {domain}\n\nExternal Insights: {external insights}\n\nGenerate 3 Deep Research questions that:\n1. Are appropriate for the persona’s role and department\n2. Require analysis of the provided internal insights\n3. Consider the external market context ...\n\nEach question should be 1 sentence of 15 words max, in\nplain english, and end with a question mark. Do not\ninclude any preamble or explanation - return only the JSON\narray.\n\nReturn ONLY a valid JSON array with this structure:\n{output structure}\n\nPrompt 10: Deep Research Question Generation Prompt Template. Subquestions are generated to help\nhuman annotators select good DR questions.\n\nW External Insight Extraction Prompt\n\nYou will be given a report (with url, and date). Based\non the report, generate 3 external insights that summarize\nimportant findings, trends, or takeaways from the\nreport.\n\nOutput Format\n{output structure}\n\nUrl: {url}\nIndustry: {industry}\nDomain: {domain}\nCompany Information: {company information}\n\nImportant notes\nFocus only on insights that are external and grounded in the report.\nInsights should be concise, factual, and directly tied to the retail\nindustry context.\n\nPrompt 11: External Insight Extraction Prompt.\n\n40\n\n\fTable 26: Public URLs For Deep Research Task Creation.\n\nIndustry\n\nDomain\n\nReference\nGrocers on FSMA-204 Compliance (GroceryDive) 2\nGrocery Loyalty & Inflation (EagleEye) 2\nGrocery Trends Outlook 2025 (GroceryDive) 2\nRetail IT Optimization (Thirdera) 2\nChatbots & Grocery Interactions (GroceryDoppio) 2\n\nTelehealth Regulations (HealthcareDive) 2\nFuture of Healthcare CRM (WTT Solutions) 2\nFuture of Telehealth (CHG Healthcare) 2\nHealthcare ITSM (Topdesk) 2\nPatient Engagement Tech (TechTarget) 2\n\nKnowledge Mgmt Retail Knowledge Management (Knowmax) 2\n\nSales\nCybersecurity\nPublic Relations Walmart CSR Strategy (SunriseGeek) 2\n\nPersonalization in Action (BCG) 2\nRetail Cybersecurity Threats (VikingCloud) 2\n\nCompliance\nCRM\nMarket Analysis\nITSM\nCSM\n\nCompliance\nCRM\nMarket Analysis\nITSM\nCSM\n\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nRetail\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nHealthcare\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles\nAutomobiles Quality Assurance EV Quality Issues (GreenCars) 2\nAutomobiles\nAutomobiles Market Analysis\n\nSales\nMarketing\nCybersecurity\nCompliance\nCRM\nCSM\nSales\nResearch\nCybersecurity\n\nAsset Mgmt\n\nSales for Digital Health (Medium) 2\nMarketing Telehealth Services (MarketingInsider) 2\nHealthcare Cybersecurity 2024 (AHA) 2\nEvolving EV Regulations (WardsAuto) 2\nSalesforce Automotive Cloud (TechTarget) 2\nEV Aftersales Support (EVReport) 2\nTesla vs Dealerships (TheWeek) 2\nAI for EV Optimization (Here.com) 2\nCybersecurity Risks in Cars (HelpNetSecurity) 2\n\nDigital Twins in Autos (RTInsights) 2\nGlobal EV Outlook 2024 (IEA) 2\n\nKnowledge Mgmt Knowledge Mgmt in Healthcare (C8Health) 2\n\nFigure 9: Comparison of Human Scores and Insight Recall Scores. As can be seen the human evaluation\nresults are aligned with our automated evaluation.\n\n41\n\nDR0001DR0002DR0003DR0004DR0006DR0007DR0008DR0011DR0012DR0013DR0014Task ID0.00.20.40.60.81.0Score ValueHuman Scores vs Insights Recallllama-3.1-405b (Human Score)gpt-5 (Human Score)llama-3.1-405b (Insights Recall)gpt-5 (Insights Recall)\fW Internal Supporting Insight Generation Prompt\n\nBased on the Deep Research (DR) Question, external\nmarket insights, company context, and previous internal\ninsights, generate 3 specific QA pair that an expert data\nscientist would need to get in order to address the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for inspiration): {external context}\nQA History: {qa list}\n\nInsight Requirements:\n- Use the DR Question as the central theme for the\nanswer.\n- Draw inspiration and supporting details from the other internal\ninsights and external insights provided.\n- Include QUANTITATIVE DATA in the answer: metrics, percentages, dollar\namounts, timeframes, KPIs.\n\nSpecific Question Instructions\n- the specific question should be a question that would be a step\ntowards resolving the DR Question\n{additional question instructions}\n\nAnswer Instructions\n- the answer should be 12 words max and minimum 5\nwords\n{additional answer instructions}\n\nJustification Instructions\n- the justification should directly explain how the specific question,\nanswer pair help address the DR Question in 15 words\nmax\n\nMisc Instructions\n- the filename should be 3 words max with dashes\nin between, do not mention the file type in the\nfilename\n- use the example below as inspiration but do not use it\ndirectly\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nPrompt 12: Internal Supporting Insight Generation Prompt Template. specific questions and justification\nhelp human annotators to select good insights.\n\n42\n\n\fW Internal Distractor Insight Generation Prompt\n\nBased on the Deep Research (DR) Question, external\nmarket insights, company context, and previous internal\ninsights, generate 3 specific QA pairs that are DISTRACTOR\nquestions\n- these should be questions that an expert data scientist might\nask about the company but are NOT relevant to addressing the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for inspiration): {external context}\nQA History: {qa list}\n\nDISTRACTOR Requirements:\n- Generate questions that are plausible for this\ncompany and industry but DO NOT help address the DR\nQuestion.\n- The questions should be about the company’s operations, metrics, or\nbusiness but tangential to the DR Question.\n- Include QUANTITATIVE DATA in the answer: metrics, percentages, dollar\namounts, timeframes, KPIs.\n- Focus on different business areas that are NOT central to the\nDR Question (e.g. if DR Question is about pricing, ask about HR\nmetrics).\nSpecific Question Instructions\n- the specific question should be a plausible business\nquestion for this company but NOT related to the DR\nQuestion\n- the specific question should lead to a quantitative answer and should\nbe dated such as Q3 of 2025, the question should contain a date like Q2\nof 2024\n- the specific question should be 10 words max\n- make sure to be different from any question in the\nqa list\n- choose business areas like: HR metrics, facility costs, IT\ninfrastructure, compliance, training, etc. that are UNRELATED to the\nDR Question\n- make sure to be different from any question in the QA\nHistory\n\nAnswer Instructions {answer instructions}\n\nJustification Instructions\n- the justification should explain why this specific question\nis NOT relevant to addressing the DR Question in 15 words\nmax\n\nMisc Instructions {misc instructions}\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nPrompt 13: Internal Distractor Insight Generation Prompt. specific questions and justification help human\nannotators to select good insights.\n\n43\n\n\fW Breaking Report into Insights Prompt\n\nPlease break down the following report text into insight claims. Each\ninsight claim should be:\n\n1. A single insight, that might include multiple statements and\nclaims\n2. Independent and self-contained\n3. Each claim can have more than one sentence, but should be focused\non a single insight\n4. Support each insight with citations from the report text following\nthese specific rules: {rules}\n5. Citations should be in one of these formats (various formats will\nbe automatically normalized):\n{citation formats} 6. Do not include general summaries, opinions,\nor claims that lack citation, just the sentences that are\nfacts.\n7. Each claim should be a concise but complete\nsentence.\n\nReport text: {report text}\n\nOutput format: Please return the insight claims as a JSON array. For\nexample: {output structure}\n\nReturn only valid JSON, no additional text. Just use the report\nbetween <START OF REPORT> and <END OF REPORT> tags to generate\ninsights. If no insights found, return an empty JSON array:\n[]\n\nDo not use the example outputs as report content.\n\nPrompt 14: Insight Extraction Prompt.\n\n44\n\n\fW Insight Recall Prompt\n\nYour goal is to check if one of the Predicted Insights\nextracted from a report is a Golden Insight. You\nmust be STRICT and pay attention to every small\ndetail.\n\nInstructions:\n* Evaluate if the Predicted Insights contain sufficient information to\nderive a Golden Insight.\n* Select the insight that most closely matches the Golden Insight.\nSelect one and only one insight.\n* Answer of yes or no where:\n- yes: Selected insight contains comprehensive information to fully\nderive the expected insight\n- no: Selected insight lacks the necessary information, misses key\ndetails, or has significant gaps\n* Be STRICT - do not answer yes for partial matches, vague similarities,\nor general information.\n\nHowever, no exact wording is required and paraphrasing is\nacceptable.\n* IMPORTANT: Only consider details given in the Golden Insight when\nanswering yes or no. Don’t expect anything more than what is given in\nthe Golden Insight.\n* Focus on factual accuracy, completeness, and specificity.\n\nPredicted Insights: {claims text}\nGolden Insight: {gold insight}\n\nReturn a valid json dictionary with the following structure:\n{output structure}\n\nEnsure only a json dictionary is returned, and return nothing else.\n\nPrompt 15: Insight Recall Scoring Prompt.\n\n45\n\n\fW Factuality Prompt\n\nGiven the following relevant source context from multiple sources and\nan insight, determine if the insight is factually supported by the\nsources.\n\nRelevant Source Materials (from multiple sources):\n{context}\nAtomic Claim: {insight}\n\nEVALUATION CRITERIA:\nThe claim is factual if the core factual content\nis supported by the sources. You should be strict\nabout important details but flexible about exact\nwording:\nREQUIRED for TRUE:\n1. All key factual details (numbers, dates, names,\npercentages, specific facts) must be present in at least one\nsource\n2. The main substance and meaning of the claim must be supported by\nthe source contexts\n3. No part of the claim should contradict the information in any of\nthe sources\nACCEPTABLE variations:\n{acceptable variations}\nMark as FALSE if:\n- Important factual details are missing, incorrect, or unsupported\nacross all sources\n- The claim contradicts information in any of the\nsources\n- The core meaning cannot be verified from any of the source\ncontexts\n\nEXAMPLES: {examples}\n\nFocus on the substantive factual accuracy rather than\nexact word-for-word matching. You MUST respond with either\ntrue or false under the <factual> tag. Then provide a\nbrief explanation under the <explanation> tag explaining\nwhich parts are supported or not supported and from which\nsources.\n\nFormat your response EXACTLY as:\n{output structure}\n\nPrompt 16: Factuality Scoring Prompt.\n\n46\n\n\fW Report Quality Prompt\n\nYou are a Deep Research Evaluator. You are given:\n1. A research report.\n2. A deep research (DR) question that the report attempts to\nanswer.\n3. A persona that represents the intended audience for the\nreport.\n\n{persona}\n{dr question}\n{report text}\n\n## Instructions:\n**ANALYZE THOROUGHLY**: Examine the report in detail\nand identify any issues, even small ones. Look for\nsubtle problems, minor inconsistencies, areas that could\nbe improved, or any shortcomings that might affect\nthe quality. Evaluate the report according to the\nfive criteria listed below. For **each criterion**,\nprovide:\n- A **score between 1 and 10** (must be an integer) using the scale\ndefined below.\n- A **detailed justification** (2–3 sentences) in\n**simple plain English** explaining why you gave that\nscore, including any specific issues or strengths you\nidentified.\n\n### Scoring Scale (1-10, integers only): {scoring scale}\n\n### Criteria:\n\n1. Depth & Quality of Analysis\n2. Relevance To DR Question\n3. Persona Consistency\n4. Coherence & Conciseness\n5. Degree of Contradictions\n6. Completeness & Coverage\n\n{output structure}\n\nPrompt 17: Report Quality Scoring Prompt.\n\n47\n\n\fW PDF Outline Generation Prompt\n\nYou are an expert business document designer creating\nrealistic enterprise PDF reports. Given a Deep\nResearch (DR) Question and company context, generate\nan outline for a professional business document that\nan employee would create based on their persona and\nrole.\n\nCompany: {company name} - {company description}\nIndustry: industry\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context:{persona context}\nDR Question: {dr question}\n\nDocument Structure Requirements:\n- Create a professional PDF document outline with exactly {n subsections}\nsubsections\n- The document should be something this persona would realistically\ncreate in their role\n- Include a concise, professional file title appropriate for enterprise\ndocumentation\n\nSubsection Heading Requirements: {subsection requirements}\n\nIntroduction Requirements:\n- Write a professional 4-sentence maximum introduction\nparagraph - Should set context for the document and its\npurpose\n- Must align with the persona’s role and the company’s business\nneeds\n- Should sound like something this employee would write for internal\nstakeholders\n\nConclusion Requirements:\n- Write a professional 4-sentence maximum conclusion\nparagraph\n- Should summarize key takeaways and next steps\n- Must align with the persona’s perspective and recommendations\n- Should provide actionable insights for the intended\naudience\n\nReturn ONLY a valid Python dictionary with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Ensure the document feels authentic for this persona´s role and company\ncontext\n\nPrompt 18: PDF Outline Generation Prompt. This is the first step of embedding an insight into a PDF\ndocument. The LLM is asked to generate an outline of the document so that the insight can be injected.\n\n48\n\n\fW PDF Insight Injection Prompt\n\nYou are an expert business document writer creating\nrealistic enterprise PDF content. Given a Deep\nResearch (DR) Question, company context, and a specific\ninsight, generate professional content that naturally\nincorporates the insight information to help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nSubsection Heading: {subsection heading}\n\nContent Generation Requirements:\n- Generate realistic business content for the given subsection\nheading\n- The content must contain exactly ONE paragraph of 4-5\nsentences\n- Content should be professional and sound like something this persona\nwould write\n- The paragraph must naturally incorporate the insight answer\ninformation but NOT copy it word-for-word\n{additional generation requirements}\n\nContent Strategy:\n- Present the insight information as business findings, analysis results,\nor operational data\n- Embed the key metrics within broader business context and\nimplications\n- Use natural business language to discuss the same information as in\nthe answer\n{additional content requirements}\n\nJustification Requirements:\n- Explain specifically how this content helps answer the DR\nQuestion\n- Reference the key information that would be useful for\ndecision-making\n- Keep justifications concise but clear (20 words\nmaximum)\n\nReturn ONLY a valid JSON object with this exact structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 19: PDF Insight Injection Prompt. This is the second step of embedding an insight into a PDF\ndocument. The LLM is fed with a subheading in the outline from 18, and tasked to write the subsection\nwith the insight embedded.\n\n49\n\n\fW PDF Irrelevant Section Generation Prompt\n\nYou are an expert business document writer creating\nrealistic enterprise PDF content. Given a Deep Research\n(DR) Question, company context, and subsection headings,\ngenerate distractor content for each subsection that is\nthematically related but does NOT help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: industry\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\nSubsection Headings: {subsection headings}\n\nContent Generation Requirements:\n- Generate realistic business content for each subsection\nheading\n- Each subsection must contain exactly ONE paragraph of 3-4 sentences\nmaximum\n- Content should be professional and sound like something this persona\nwould write\n- Content must be thematically related to the DR\nQuestion’s domain but NOT provide information to answer\nit\n{additional generation requirements}\n\nDistractor Strategy:\n- Focus on adjacent business areas that don’t directly impact the DR\nQuestion\n- Discuss historical context, general industry trends, or procedural\ninformation\n- Include operational details that are realistic but tangential\n- Reference related but non-essential business metrics or\nactivities\n- Avoid any content that would help someone answer the DR\nQuestion\n\nJustification Requirements:\n- Explain specifically why each paragraph’s content doesn’t help answer\nthe DR Question\n- Identify what type of distractor strategy was used\n(e.g., \"focuses on historical data vs current decision\nfactors\")\n- Keep justifications concise but clear (15 words\nmaximum)\n\nReturn ONLY a valid JSON array with this exact structure:\n{output structure}\n\nIMPORTANT: {important instructions}\n\nPrompt 20: PDF Irrelevant Section Generation Prompt. This is the third step of PDF document generation.\nThe LLM is asked to fill out the outline from 18 with irrelevant information.\n\n50\n\n\fW Excel Schema Generation Prompt\n\nGenerate the JSON schema and formatting of a table where the\nfollowing insight can be presented as a row in the table:\n{insight}\n\nThe schema and formatting should contain:\n- table name: a string of the table name\n- columns: a list of columns with the following\nfields:\n- name: column name\n- column type: one of\nSTRING|INTEGER|FLOAT|BOOLEAN|DATE|PERCENTAGE|CURRENCY\n- description: detailed description of what this column represents and\nhow it relates to the insight\n- formatting: a dictionary with the following fields:\n- header style: background color of the header, default is\nCCCCCC\n- column widths: width of each column, e.g. {{A: 15, B: 20, C:\n12}}\n- number formats: format of each column, e.g. {{A: \"0.00%\", B:\n\"$#,##0.00\", C: \"YYYY-MM-DD\"}}\nReturn only a json dictionary with the table name, columns, and\nformatting.\n\nRequirements:\n- The schema should be designed so that the insight can be represented\nas a row in the table\n- The schema should make it easy to generate more data points expanding\non the subject, theme and scope of the insight to populate the\ntable.\n- Use realistic column names that would be found in a business\nspreadsheet\n- Do not include the insight, specific question, or justification as\ncolumns in the table\n\nCompany Context: {company info}\nPlease keep this company context in mind when creating the\nschema.\n\nPersona Context: {persona}\n\nPlease keep this persona context in mind when creating the schema.\n\nPrompt 21: Excel Schema Generation Prompt. This is the first step of Excel file generation. The LLM\nis asked to generate the schema of the Excel file so that the insight can be injected.\n\n51\n\n\fW Excel Data Generation Prompt\n\nGiven the following schema: {schema and formatting}\nGenerate one row that embeds the following insight:\n{insight}\n\nThen generate 5-10 rows of data that populates the table.\nMake sure that with the new data added, the original\ninsight can still be extracted from the table. Return\nall the rows in a json dictionary with the following\nfields:\n- insight row: a list of values each corresponding to a column in the\nschema\n- irrelevant rows: a list of rows that are used to populate the table,\neach row is a list of values each corresponding to a column in the\nschema\n\nEnsure only a json dictionary is returned, and return nothing\nelse.\n\nRequirements:\n- Make sure the insight row stands out from the irrelevant rows by\ne.g.\n- Having the largest value\n- Covering the most recent timeframe\n\nPrompt 22: Excel Data Generation Prompt. This is the second step of Excel file generation. The LLM\nis asked to generate the data for an Excel file that the insight will be injected into.\n\nW Excel Filename Generation Prompt\n\nGenerate a professional filename for an Excel file that contains the\nfollowing sheets: {sheet names}\n\nThe filename should:\n1. Be descriptive and professional\n2. Reflect the main theme or purpose of the data\n3. Be suitable for a business environment\n4. Not exceed 50 characters\n5. Use only alphanumeric characters, spaces, hyphens, and\nunderscores\n6. Not include file extensions (like .xlsx)\nReturn only the filename, no additional text or\nquotes.\n\nCompany name:\n{company name}\n\nPlease keep this company name in mind when creating the filename.\n\nPrompt 23: Excel Filename Generation Prompt. This is the third step of Excel file generation. The LLM\nis asked to generate the filename for the Excel file that the insight will be injected into.\n\n52\n\n\fW Powerpoint Outline Generation Prompt\n\nYou are an expert business presentation designer creating\nrealistic enterprise PowerPoint presentations. Given a\nDeep Research (DR) Question and company context, generate\nan outline for a professional business presentation that\nan employee would create based on their persona and\nrole.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\n\nPresentation Structure Requirements:\n- Create a professional PowerPoint presentation outline with exactly\n{n subsections} slides\n- The presentation should be something this persona would realistically\ncreate in their role\n- Include a concise, professional presentation title appropriate for\nenterprise presentations\n\nSlide Heading Requirements:\n- Slide headings must follow the THEME of the DR\nQuestion but should NOT directly address the DR Question\nitself\n- Think of related business areas, adjacent topics, or\nsupporting themes that would naturally appear in an enterprise\npresentation\n- Headings should sound professional and realistic for this industry and\ncompany size\n- Each heading should be 3-8 words and use proper business\nterminology\n{additional slide requirements}\n\nConclusion Requirements:\n- Write a professional 2-sentence maximum conclusion for the\npresentation closing\n- Should summarize key takeaways and next steps\n- Must align with the persona’s perspective and recommendations\n- Should provide actionable insights for the intended\naudience\n\nReturn ONLY a valid Python dictionary with this exact\nstructure:\n{output structure}\n\nIMPORTANT: {important notes}\n\nPrompt 24: Powerpoint Outline Generation Prompt. This is the first step for generating powerpoint slides.\nThe LLM is asked to generate an outline of the slides so that the insight can be injected.\n\n53\n\n\fW Powerpoint Insight Injection Prompt\n\nYou are an expert business presentation writer creating\nrealistic enterprise PowerPoint content. Given a Deep\nResearch (DR) Question, company context, and a specific\ninsight, generate professional slide content that naturally\nincorporates the insight information to help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nSlide Heading: {subsection heading}\n\nContent Generation Requirements:\n- Generate realistic business content for the given slide\nheading\n- The content must contain exactly 5-8 bullet points with substantial\ndetail\n- Each bullet point should be 1-2 sentences with specific business\ninformation\n{additional generation requirements}\n\nContent Strategy:\n- Present the insight information as business findings, analysis results,\nor operational data\n- Embed the key metrics within broader business context and\nimplications\n- Use natural business language to discuss the same information as in\nthe answer\n{additional content requirements}\n\nJustification Requirements:\n- Explain specifically how this content helps answer the DR\nQuestion\n- Reference the key information that would be useful for\ndecision-making\n- Keep justifications concise but clear (25 words\nmaximum)\n\nReturn ONLY a valid JSON object with this exact\nstructure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 25: Powerpoint Insight Injection Prompt. This is the second step for generating powerpoint slides.\nThe LLM is asked to generate slide content with the insight embedded.\n\n54\n\n\fW Powerpoint Distractor Injection Prompt\n\nYou are an expert business presentation writer creating\nrealistic enterprise PowerPoint content. Given a Deep\nResearch (DR) Question, company context, and slide\nheadings, generate distractor content for each slide that\nis thematically related but does NOT help answer the DR\nQuestion.\n\nCompany: {company name} - {company description}\nIndustry: {industry}\nCompany Size: {company size} ({employee count})\nAnnual Revenue: {annual revenue}\nPersona Context: {persona context}\nDR Question: {dr question}\nExternal Market Context (for reference): {external context}\nSlide Headings: {subsection headings}\n\nContent Generation Requirements:\n- Generate realistic business content for each slide heading - Each\nslide must contain exactly 5-8 bullet points with substantial\ndetail\n- Each bullet point should be 1-2 sentences with specific business\ninformation\n- Content should be professional and sound like something this persona\nwould present\n{additional generation requirements}\n\nDistractor Strategy:\n- Focus on adjacent business areas that don’t directly impact the DR\nQuestion\n- Discuss historical context, general industry trends, or procedural\ninformation\n- Include operational details that are realistic but tangential\n- Reference related but non-essential business metrics or\nactivities\n{additional content requirements}\n\nReturn ONLY a valid JSON array with this exact structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 26: Powerpoint Distractor Injection Prompt. This is the third step for generating powerpoint slides.\nThe LLM is asked to generate slide content with distractor information.\n\n55\n\n\fW Email Setup Prompt\n\nYou are an expert in enterprise communication systems and organizational\nstructures. Your task is to generate a realistic setup of users\nfor an email system based on the given insights and company\ncontext.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Size: {company size} ({employee count} employees)\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\n**Specific Question** {specific question}\n**Answer to Specific Question** {answer}\n\nRequirements:\n- Users that would realistically discuss these insights\n- Generate a minimal but sufficient setup to support {num messages}\nemails discussing the insights\n- To make it realistic, generate at least 3 users\n\n- Use realistic names for people/teams/channels based on the company\ncontext\n\nReturn ONLY a JSON array of users with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Do NOT include any preamble, explanation, or extra text|return only\nthe Python dictionary\n- Ensure the structure is realistic for the company size and industry\n- Make sure the persona is included as a user\n\nPrompt 27: Email Setup Prompt. This is the first step for generating an email chain. The LLM is asked\nto generate the necessary setup for the email chain.\n\n56\n\n\fW Email Insight Injection Prompt\n\nYou are an expert at creating realistic business email\nconversations. Your task is to create an email thread that\ncontains the actual insight that helps answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\nDeep Research Question: {dr question}\nEmail Setup: {email setup}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create a realistic email thread of {num messages} that contains the\ntarget insight\n2. This thread should provide information that directly helps answer\nthe DR question\n3. The insight should be naturally embedded in the email\ncontent\n4. The emails should feel realistic and business-appropriate\n5. The sender should be someone who would naturally have access to\nthis insight\n6. The persona needs to be either a recipient or the sender of any\nemail\n\nContent Strategy:\n- The thread should discuss the specific question\nand provide the answer as part of a natural business\nconversation\n- Include the justification as supporting context or\nreasoning\n- Make the insight feel like a natural part of the email, not\nforced\n- The content should be directly relevant to answering the DR\nquestion\n- Use realistic business language and formatting\nExample approaches: {example approaches}\nOutput Format: Return ONLY a JSON array with the following structure:\n{output structure}\n\nIMPORTANT: {important details}\n\nPrompt 28: Email Insight Injection Prompt. This is the second step for generating an email chain. The\nLLM is asked to insert an insight into the email chain.\n\n57\n\n\fW Email Distractor Injection Prompt\n\nYou are an expert at creating realistic business email conversations.\nYour task is to create {num messages} emails that discuss topics\nrelated to the company but will NOT help answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\nDeep Research Question: {dr question}\nEmail Setup: {email setup}\n\nRequirements:\n1. Create {num messages} realistic email messages between the\nusers\n2. These emails should discuss business topics\nthat are thematically related to the company but\nDO NOT provide information that helps answer the DR\nquestion\n3. Each email should have a realistic subject line, sender, recipients,\nand content\n4. The conversations should feel natural and business-appropriate\n5. Topics should be relevant to the company’s operations but unhelpful\nfor the DR question\n\nContent Strategy:\n- Focus on daily business operations, team collaboration, projects, and\ncompany processes\n- Include realistic business language, project updates, and operational\ndiscussions\n- Avoid topics that directly relate to the DR question or would provide\ninsights for it\n- Make the content engaging and realistic while being intentionally\nunhelpful\n\nExample topics to discuss (but should NOT help answer the DR\nquestion):\n{example topics}\n\nOutput Format: Return ONLY a JSON array with the following structure:\n{output structure}\n\nIMPORTANT: - Return ONLY the JSON array, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n- Make sure the persona is either a recipient or the sender of any email\n\nPrompt 29: Email Distractor Injection Prompt. This is the third step for generating an email chain. The\nLLM is asked to insert distractor information into the email chain.\n\n58\n\n\fW Chat Setup Prompt\n\nYou are an expert in enterprise communication systems\nand organizational structures. Your task is to generate\na realistic setup for teams, channels, and users for a\nMattermost chat system based on the given insights and company\ncontext.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Size: {company size} ({employee count} employees)\n- Annual Revenue: {annual revenue}\n\nPersona Context: {persona context}\n**Specific Question** {specific question}\n**Answer to Specific Question** {answer}\n\nRequirements:\n- Generate teams, channels, and users that would realistically discuss\nthese insights\n- Make sure the teams and channels are realistic for persona to be a\nmember\n- Each channel must be associated with a team\n- Each user must be a member of at least one team and one\nchannel\n- Generate a minimal but sufficient setup to support {num turns} chat\nmessages discussing the insights\n- To make it realistic, generate at least 2 teams, 2 channels and 3\nusers\n- Use realistic names for people/teams/channels based on the company\ncontext\n- The persona needs to be part of all teams and channels\n\nReturn ONLY a valid Python dictionary with this exact structure:\n{output structure}\n\nIMPORTANT:\n- Do NOT include any preamble, explanation, or extra text|return only\nthe Python dictionary\n- Make sure the persona is included as a user and member of all\nteams/channels\n- Reuse the username of the persona as provided in the persona context\n\nPrompt 30: Chat Setup Prompt. This is the first step for generating a Mattermost chat. The LLM is asked\nto generate the necessary setup for the chat system.\n\n59\n\n\fW Chat Insight Injection Prompt\n\nYou are an expert at creating realistic business chat\nconversations. Your task is to create a chat conversation\nthat contains an insight that helps answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDR Question: {dr question}\nChat Setup: {chat setup}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create a realistic chat conversation (could be multiple messages)\nthat contains the target insight\n2. This conversation should provide information that directly helps\nanswer the DR question\n3. The insight should be naturally embedded in the message\ncontent\n4. The conversation should feel realistic and business-appropriate\n5. The sender should be someone who would naturally have access to\nthis insight\n6. Use the teams, channels, and users from the chat setup\nonly\n\nContent Strategy:\n- The conversation should discuss the specific question\nand provide the answer as part of a natural business\nconversation\n- Include the justification as supporting context or\nreasoning\n{additional content requirements}\n\nExample approaches: {example approaches}\n\nOutput Format: Return ONLY a JSON array of the chat messages with the\nfollowing structure: {output structure}\n\nIMPORTANT:\n- Return ONLY the JSON object, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n\nPrompt 31: Chat Insight Injection Prompt. This is the second step for generating a Mattermost chat. The\nLLM is asked to insert an insight into the chat system.\n\n60\n\n\fW Chat Distractor Injection Prompt\n\nYou are an expert at creating realistic business chat\nconversations. Your task is to create {num turns}\nchat messages that discuss topics related to the\ncompany but will NOT help answer the Deep Research\nquestion.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDeep Research Question: {dr question}\nChat Setup: {chat setup}\n\nRequirements:\n1. Create {num turns} realistic chat messages between the\nusers\n2. These messages should discuss business topics\nthat are thematically related to the company but\nDO NOT provide information that helps answer the DR\nquestion\n3. Each message should have a realistic sender, channel, and\ncontent\n4. The conversations should feel natural and business-appropriate\n5. Topics should be relevant to the company’s operations but unhelpful\nfor the DR question\n6. Use the teams, channels, and users from the chat setup\nonly\n\nContent Strategy:\n- Focus on daily business operations, team collaboration, projects, and\ncompany processes\n- Include realistic business language, project updates, and operational\ndiscussions\n{additional content requirements}\n\nExample topics to discuss (but should NOT help answer the DR\nquestion):\n{example topics}\n\nOutput Format:\nReturn ONLY a JSON array of the chat messages with the following\nstructure: {output structure}\n\nIMPORTANT:\n- Return ONLY the JSON array, nothing else\n- Do not add any text before or after the JSON\n- The response must be parseable JSON\n\nPrompt 32: Chat Distractor Injection Prompt. This is the third step for generating a Mattermost chat. The\nLLM is asked to generate distractor information to insert into the chat system.\n\n61\n\n\fW Chat Confidential Info Prompt\n\nYou are an expert at inserting confidential information\ninto chat conversations. Your task is to create business\nconfidential information that is irrelevant to the Deep\nResearch question or the specific question and insert\nit into existing chat conversations in a realistic\nway.\n\nCompany Context:\n- Company Name: {company name}\n- Description: {company description}\n- Industry: {industry}\n- Company Size: {company size}\n- Employee Count: {employee count}\n- Annual Revenue: {annual revenue}\n\nDR Question: {dr question}\nChat Setup: {chat setup}\nChat Conversation: {chat messages}\n\nTarget Insight:\n- Specific Question: {specific question}\n- Answer: {answer}\n- Justification: {justification}\n\nRequirements:\n1. Create business confidential information that are secrets\nwithin the company and should not be revealed to the outside\nworld\n2. Create 1-2 new chat messages that contain the confidential\ninformation\n3. Insert the new messages into the chat conversation and return the\nnew chat conversation\n4. Also return the confidential information and the\nkeywords\n\nConfidential Information Content Strategy:\n- The messages containing the confidential information should not be\nnecessary to address the DR question or the specific question of the\ninsight\n- The confidential information should be relevant to the company context\nand something that the people in chat setup naturally have access\nto\n- The confidential information should be specific so that leakage\nof it will result in a significant disadvantage or loss to the\ncompany\n\nExample confidential information:\n{example confidential info}\nOutput Format: Return a JSON object of the new chat messages and the\nconfidential information with the following structure:\n{output structure}\n\nPrompt 33: Chat Confidential Info Prompt. This prompt also generates distractor information to insert\ninto a chat system like prompt 32. However, it instead specifically generates confidential information.\n\n62\n\n\f\n</file>\n\n\n### long_context.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context.json\">\n[Long Context Document Collection]\nTotal documents: 9478\nSample titles: ['深度研究代理的终极考验：DeepResearch Bench 横空出世-CSDN博客', 'DeepResearch Bench: A Comprehensive Benchmark for ', '14篇DeepResearch Agent最新研究总结!']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_32k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_32k.json\">\n[Long Context Document Collection]\nTotal documents: 17\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_64k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json\">\n[Long Context Document Collection]\nTotal documents: 33\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_60.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_60.json\">\n[Long Context Document Collection]\nTotal documents: 60\nSample titles: ['LLM检索增强生成原理与架构详解-今日头条', '通义DeepResearch全面开源！同步分享可落地的高阶Agent构建方法论-知乎', '跨领域知识整合能力评估：LLM设计的综合测试-CSDN博客']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n### long_context_sampled_128k.json\n\n<file path=\"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_128k.json\">\n[Long Context Document Collection]\nTotal documents: 67\nSample titles: ['获奖论文|论大数据在类案类判中的应用_澎湃号·政务_澎湃新闻-The Paper', 'NoSQL之Redis配置与优化-CSDN博客', '深度学习中的Baseline：基础框架与性能评估']\n\n**Use RAG tools (rag_search, rag_get_context) to search this document.**\n</file>\n\n\n## Image Files\n\n\nThe following image files are available for analysis:\n\n\n### image0.png\n\nImage file: image0.png\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/image0.png\nDimensions: 1420x187 pixels\nFormat: PNG\n\n\n**IMPORTANT**: Use the 'vision_understanding_advanced' tool to analyze these images.\nThis tool provides multi-turn verification, confidence scoring, and cross-validation.\nRecommended approach:\n1. Call vision_understanding_advanced with a specific question about each image\n2. Review the confidence score and metadata\n3. If confidence < 0.75, use follow-up analysis or web search for verification\n\n\n## Long Context Documents (RAG)\n\n\nThe following long context document files are available for semantic search:\n\n\n### long_context_sampled_64k.json.chunks.db ⭐ **RECOMMENDED - USE THIS DATABASE**\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db\n\nSize: 1440.0 KB\n\n**This is a pre-built embedding database. Use this path directly with RAG tools.**\n\n**⚠️ IMPORTANT: Always use this file path when calling RAG tools: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db**\n\n\n### long_context.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context.json\n\nSize: 76367.0 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_32k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_32k.json\n\nSize: 119.7 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_64k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json\n\nSize: 235.1 KB\n\n**Pre-built embedding database**: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db (1440.0 KB)\n\n\n### long_context_sampled_60.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_60.json\n\nSize: 504.5 KB\n\n*Note: Use the recommended database file instead.*\n\n\n### long_context_sampled_128k.json\n\n\nPath: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_128k.json\n\nSize: 477.4 KB\n\n*Note: Use the recommended database file instead.*\n\n\n**IMPORTANT**: Use RAG tools to search these long context documents:\n- `rag_search`: Semantic search to find relevant passages (use 1-3 times with different keywords)\n- `rag_get_context`: Get concatenated context for answering questions\n- `rag_document_stats`: Get document collection statistics\n\nDo NOT attempt to read these files directly - they are too large. Use RAG tools instead.\n\n**⚠️ CRITICAL: When calling RAG tools, use this json_path: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db**\nThis database has pre-built embeddings and will load instantly without regenerating embeddings.\n\n\nYou should follow the format instruction in the request strictly and wrap the final answer in \\boxed{}.",
      "timestamp": "2025-12-05 18:35:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent",
      "message": "Associated file: /Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/image0.png",
      "timestamp": "2025-12-05 18:35:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Get Tool Definitions",
      "message": "Getting tool definitions for server 'tool-python'...",
      "timestamp": "2025-12-05 18:35:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Tool Definitions Success",
      "message": "Successfully obtained 6 tool definitions from server 'tool-python'.",
      "timestamp": "2025-12-05 18:35:50",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Get Tool Definitions",
      "message": "Getting tool definitions for server 'tool-vqa'...",
      "timestamp": "2025-12-05 18:35:50",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Tool Definitions Success",
      "message": "Successfully obtained 1 tool definitions from server 'tool-vqa'.",
      "timestamp": "2025-12-05 18:35:52",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Get Tool Definitions",
      "message": "Getting tool definitions for server 'tool-vqa-enhanced'...",
      "timestamp": "2025-12-05 18:35:52",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Tool Definitions Success",
      "message": "Successfully obtained 4 tool definitions from server 'tool-vqa-enhanced'.",
      "timestamp": "2025-12-05 18:35:53",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Get Tool Definitions",
      "message": "Getting tool definitions for server 'tool-transcribe'...",
      "timestamp": "2025-12-05 18:35:53",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Tool Definitions Success",
      "message": "Successfully obtained 2 tool definitions from server 'tool-transcribe'.",
      "timestamp": "2025-12-05 18:35:55",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Get Tool Definitions",
      "message": "Getting tool definitions for server 'tool-audio-enhanced'...",
      "timestamp": "2025-12-05 18:35:55",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Tool Definitions Success",
      "message": "Successfully obtained 4 tool definitions from server 'tool-audio-enhanced'.",
      "timestamp": "2025-12-05 18:35:57",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Get Tool Definitions",
      "message": "Getting tool definitions for server 'tool-video-enhanced'...",
      "timestamp": "2025-12-05 18:35:57",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Tool Definitions Success",
      "message": "Successfully obtained 4 tool definitions from server 'tool-video-enhanced'.",
      "timestamp": "2025-12-05 18:35:58",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Get Tool Definitions",
      "message": "Getting tool definitions for server 'tool-reasoning'...",
      "timestamp": "2025-12-05 18:35:58",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Tool Definitions Success",
      "message": "Successfully obtained 1 tool definitions from server 'tool-reasoning'.",
      "timestamp": "2025-12-05 18:35:59",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Get Tool Definitions",
      "message": "Getting tool definitions for server 'tool-reader'...",
      "timestamp": "2025-12-05 18:35:59",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Tool Definitions Success",
      "message": "Successfully obtained 1 tool definitions from server 'tool-reader'.",
      "timestamp": "2025-12-05 18:36:02",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Get Tool Definitions",
      "message": "Getting tool definitions for server 'tool-rag'...",
      "timestamp": "2025-12-05 18:36:02",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🔧 ToolManager | Tool Definitions Success",
      "message": "Successfully obtained 3 tool definitions from server 'tool-rag'.",
      "timestamp": "2025-12-05 18:36:04",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent",
      "message": "Number of tools: 10",
      "timestamp": "2025-12-05 18:36:04",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 1",
      "message": "Starting turn 1",
      "timestamp": "2025-12-05 18:36:04",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 57461, Output: 241",
      "timestamp": "2025-12-05 18:36:19",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:36:19",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Turn: 1 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:36:19",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 1 | Tool Calls",
      "message": "Number of tool calls detected: 1",
      "timestamp": "2025-12-05 18:36:19",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-vqa-enhanced' to call tool 'vision_understanding_advanced'",
      "timestamp": "2025-12-05 18:36:19",
      "info_level": "info",
      "metadata": {
        "arguments": {
          "image_path_or_url": "/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/image0.png",
          "question": "What fields or categories need to be extracted for the benchmark dataset analysis?"
        }
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'vision_understanding_advanced' (server: 'tool-vqa-enhanced') called successfully.",
      "timestamp": "2025-12-05 18:36:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 1 | Tool Call",
      "message": "Tool vision_understanding_advanced completed in 30387ms",
      "timestamp": "2025-12-05 18:36:49",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Context Limit Not Reached",
      "message": "149798/200000",
      "timestamp": "2025-12-05 18:36:50",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 2",
      "message": "Starting turn 2",
      "timestamp": "2025-12-05 18:36:50",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 116205, Output: 552",
      "timestamp": "2025-12-05 18:37:06",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:37:06",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Turn: 2 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:37:06",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 2 | Tool Calls",
      "message": "Number of tool calls detected: 1",
      "timestamp": "2025-12-05 18:37:06",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-rag' to call tool 'rag_get_context'",
      "timestamp": "2025-12-05 18:37:06",
      "info_level": "info",
      "metadata": {
        "arguments": {
          "query": "What benchmarks for deep research are mentioned in the related work section of the paper?",
          "json_path": "/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db",
          "max_tokens": 4000
        }
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'rag_get_context' (server: 'tool-rag') called successfully.",
      "timestamp": "2025-12-05 18:37:08",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 2 | Tool Call",
      "message": "Tool rag_get_context completed in 2707ms",
      "timestamp": "2025-12-05 18:37:08",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Context Limit Not Reached",
      "message": "159285/200000",
      "timestamp": "2025-12-05 18:37:08",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 3",
      "message": "Starting turn 3",
      "timestamp": "2025-12-05 18:37:08",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 182097, Output: 720",
      "timestamp": "2025-12-05 18:37:23",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:37:23",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Turn: 3 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:37:23",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 3 | Tool Calls",
      "message": "Number of tool calls detected: 1",
      "timestamp": "2025-12-05 18:37:23",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-rag' to call tool 'rag_search'",
      "timestamp": "2025-12-05 18:37:23",
      "info_level": "info",
      "metadata": {
        "arguments": {
          "query": "DRBench related work section benchmarks for deep research",
          "json_path": "/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/long_context_sampled_64k.json.chunks.db",
          "top_k": 10
        }
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'rag_search' (server: 'tool-rag') called successfully.",
      "timestamp": "2025-12-05 18:37:25",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 3 | Tool Call",
      "message": "Tool rag_search completed in 2247ms",
      "timestamp": "2025-12-05 18:37:25",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Context Limit Not Reached",
      "message": "164984/200000",
      "timestamp": "2025-12-05 18:37:25",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 4",
      "message": "Starting turn 4",
      "timestamp": "2025-12-05 18:37:25",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 254013, Output: 806",
      "timestamp": "2025-12-05 18:37:38",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:37:38",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Turn: 4 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:37:38",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 4 | Tool Calls",
      "message": "Number of tool calls detected: 1",
      "timestamp": "2025-12-05 18:37:38",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-python' to call tool 'create_sandbox'",
      "timestamp": "2025-12-05 18:37:38",
      "info_level": "info",
      "metadata": {
        "arguments": {}
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'create_sandbox' (server: 'tool-python') called successfully.",
      "timestamp": "2025-12-05 18:37:40",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 4 | Tool Call",
      "message": "Tool create_sandbox completed in 2015ms",
      "timestamp": "2025-12-05 18:37:40",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Context Limit Not Reached",
      "message": "162354/200000",
      "timestamp": "2025-12-05 18:37:40",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 5",
      "message": "Starting turn 5",
      "timestamp": "2025-12-05 18:37:40",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 326041, Output: 1228",
      "timestamp": "2025-12-05 18:37:57",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:37:57",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Turn: 5 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:37:57",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 5 | Tool Calls",
      "message": "Number of tool calls detected: 1",
      "timestamp": "2025-12-05 18:37:57",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-python' to call tool 'run_python_code'",
      "timestamp": "2025-12-05 18:37:57",
      "info_level": "info",
      "metadata": {
        "arguments": {
          "code_block": "import fitz  # PyMuPDF\nimport re\n\n# Open the PDF document\ndoc = fitz.open('/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf')\n\n# Function to extract text from a page\ndef extract_text_from_page(page):\n    return page.get_text()\n\n# Extract text from all pages\nall_text = \"\"\nfor page_num in range(len(doc)):\n    page = doc[page_num]\n    all_text += extract_text_from_page(page) + \"\\n\"\n\n# Find the Related Work section\nrelated_work_pattern = r'2\\s+RELATED WORK\\s+([\\s\\S]+?)\\s+3\\s+DRBench'\nrelated_work_match = re.search(related_work_pattern, all_text)\n\nif related_work_match:\n    related_work_text = related_work_match.group(1)\n    print(\"Related Work section found!\")\n    print(related_work_text)\nelse:\n    print(\"Related Work section not found.\")\n\n# Close the document\ndoc.close()",
          "sandbox_id": "imgv0psyq59d83qe8m43u"
        }
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'run_python_code' (server: 'tool-python') called successfully.",
      "timestamp": "2025-12-05 18:38:02",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 5 | Tool Call",
      "message": "Tool run_python_code completed in 5495ms",
      "timestamp": "2025-12-05 18:38:02",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Context Limit Not Reached",
      "message": "163412/200000",
      "timestamp": "2025-12-05 18:38:02",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 6",
      "message": "Starting turn 6",
      "timestamp": "2025-12-05 18:38:02",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 398902, Output: 1381",
      "timestamp": "2025-12-05 18:38:16",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:38:16",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Turn: 6 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:38:16",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 6 | Tool Calls",
      "message": "Number of tool calls detected: 1",
      "timestamp": "2025-12-05 18:38:16",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-python' to call tool 'download_file_from_internet_to_sandbox'",
      "timestamp": "2025-12-05 18:38:16",
      "info_level": "info",
      "metadata": {
        "arguments": {
          "sandbox_id": "imgv0psyq59d83qe8m43u",
          "url": "/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf",
          "sandbox_file_path": "/home/user"
        }
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'download_file_from_internet_to_sandbox' (server: 'tool-python') called successfully.",
      "timestamp": "2025-12-05 18:38:18",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 6 | Tool Call",
      "message": "Tool download_file_from_internet_to_sandbox completed in 2393ms",
      "timestamp": "2025-12-05 18:38:18",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Context Limit Not Reached",
      "message": "163618/200000",
      "timestamp": "2025-12-05 18:38:18",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 7",
      "message": "Starting turn 7",
      "timestamp": "2025-12-05 18:38:18",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 472095, Output: 1501",
      "timestamp": "2025-12-05 18:38:30",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:38:30",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Turn: 7 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:38:30",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 7 | Tool Calls",
      "message": "Number of tool calls detected: 1",
      "timestamp": "2025-12-05 18:38:30",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-reader' to call tool 'convert_to_markdown'",
      "timestamp": "2025-12-05 18:38:30",
      "info_level": "info",
      "metadata": {
        "arguments": {
          "uri": "file:///Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf"
        }
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'convert_to_markdown' (server: 'tool-reader') called successfully.",
      "timestamp": "2025-12-05 18:38:33",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 7 | Tool Call",
      "message": "Tool convert_to_markdown completed in 2745ms",
      "timestamp": "2025-12-05 18:38:33",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Context Limit Reached",
      "message": "Context limit reached, proceeding to step back and summarize the conversation",
      "timestamp": "2025-12-05 18:38:33",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Context Limit Reached",
      "message": "Removed the last assistant-user pair, current message_history length: 14",
      "timestamp": "2025-12-05 18:38:33",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Turn: 50 | Context Limit Reached",
      "message": "Context limit reached, triggering summary",
      "timestamp": "2025-12-05 18:38:33",
      "info_level": "warning",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Max Turns Reached / Context Limit Reached",
      "message": "Reached maximum turns (50) or context limit reached",
      "timestamp": "2025-12-05 18:38:33",
      "info_level": "warning",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Final Summary",
      "message": "Generating final summary",
      "timestamp": "2025-12-05 18:38:33",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 594202, Output: 1677",
      "timestamp": "2025-12-05 18:38:55",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:38:55",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Final Summary | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:38:55",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Final Answer",
      "message": "Final answer generated successfully",
      "timestamp": "2025-12-05 18:38:55",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Final Answer",
      "message": "Final answer content:\n\nI need to extract more specific information about the benchmarks mentioned in the related work section of the paper. Let me search for that information directly in the paper.pdf:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>download_file_from_internet_to_sandbox</tool_name>\n<arguments>\n{\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\",\n\"url\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\",\n\"sandbox_file_path\": \"/home/user\"\n}\n</arguments>\n</use_mcp_tool>",
      "timestamp": "2025-12-05 18:38:55",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Report Validation",
      "message": "Starting report validation loop",
      "timestamp": "2025-12-05 18:38:55",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Original Report (Before Validation)",
      "message": "Original report content:\n\nI need to extract more specific information about the benchmarks mentioned in the related work section of the paper. Let me search for that information directly in the paper.pdf:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>download_file_from_internet_to_sandbox</tool_name>\n<arguments>\n{\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\",\n\"url\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\",\n\"sandbox_file_path\": \"/home/user\"\n}\n</arguments>\n</use_mcp_tool>",
      "timestamp": "2025-12-05 18:38:55",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1",
      "message": "Validating report against query requirements",
      "timestamp": "2025-12-05 18:38:55",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 766008, Output: 1851",
      "timestamp": "2025-12-05 18:39:07",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:39:07",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Validation Turn: 1 Sub-turn: 1 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:39:07",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1 Sub-turn: 1",
      "message": "Executing 1 tool call(s) for validation",
      "timestamp": "2025-12-05 18:39:07",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-python' to call tool 'download_file_from_internet_to_sandbox'",
      "timestamp": "2025-12-05 18:39:07",
      "info_level": "info",
      "metadata": {
        "arguments": {
          "sandbox_id": "imgv0psyq59d83qe8m43u",
          "url": "/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf",
          "sandbox_file_path": "/home/user"
        }
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'download_file_from_internet_to_sandbox' (server: 'tool-python') called successfully.",
      "timestamp": "2025-12-05 18:39:10",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1",
      "message": "Tool download_file_from_internet_to_sandbox executed successfully",
      "timestamp": "2025-12-05 18:39:10",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 938167, Output: 1975",
      "timestamp": "2025-12-05 18:39:33",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:39:33",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Validation Turn: 1 Sub-turn: 2 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:39:33",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1 Sub-turn: 2",
      "message": "Executing 1 tool call(s) for validation",
      "timestamp": "2025-12-05 18:39:33",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-reader' to call tool 'convert_to_markdown'",
      "timestamp": "2025-12-05 18:39:33",
      "info_level": "info",
      "metadata": {
        "arguments": {
          "uri": "/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf"
        }
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'convert_to_markdown' (server: 'tool-reader') called successfully.",
      "timestamp": "2025-12-05 18:39:34",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1",
      "message": "Tool convert_to_markdown executed successfully",
      "timestamp": "2025-12-05 18:39:34",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 1110518, Output: 2101",
      "timestamp": "2025-12-05 18:39:40",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:39:40",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Validation Turn: 1 Sub-turn: 3 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:39:40",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1 Sub-turn: 3",
      "message": "Executing 1 tool call(s) for validation",
      "timestamp": "2025-12-05 18:39:40",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-reader' to call tool 'convert_to_markdown'",
      "timestamp": "2025-12-05 18:39:40",
      "info_level": "info",
      "metadata": {
        "arguments": {
          "uri": "file:///Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf"
        }
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'convert_to_markdown' (server: 'tool-reader') called successfully.",
      "timestamp": "2025-12-05 18:39:43",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1",
      "message": "Tool convert_to_markdown executed successfully",
      "timestamp": "2025-12-05 18:39:43",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Token Usage",
      "message": "Input: 1308803, Output: 5081",
      "timestamp": "2025-12-05 18:41:11",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | Response Status",
      "message": "stop",
      "timestamp": "2025-12-05 18:41:11",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Validation Turn: 1 Sub-turn: 4 | LLM Call",
      "message": "completed successfully",
      "timestamp": "2025-12-05 18:41:11",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1 Sub-turn: 4",
      "message": "Executing 1 tool call(s) for validation",
      "timestamp": "2025-12-05 18:41:11",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "▶️ ToolManager | Tool Call Start",
      "message": "Connecting to server 'tool-python' to call tool 'run_python_code'",
      "timestamp": "2025-12-05 18:41:11",
      "info_level": "info",
      "metadata": {
        "arguments": {
          "code_block": "import re\n\n# Let's extract the related work section from the PDF content\ndef extract_related_work(text):\n    # Look for the Related Work section\n    related_work_pattern = r'2\\s+RELATED WORK\\s+([\\s\\S]+?)\\s+3\\s+DRBench'\n    match = re.search(related_work_pattern, text)\n    if match:\n        return match.group(1).strip()\n    return None\n\n# Function to extract benchmark names and their details\ndef extract_benchmarks(related_work_text):\n    # Look for benchmark names mentioned in the text\n    benchmark_pattern = r'([A-Za-z0-9\\-]+(?:Bench|Benchmark|BENCH|BENCHMARK|Arena|ARENA|Gym|GYM|World|WORLD|Web|WEB)[A-Za-z0-9\\-]*)'\n    benchmarks = re.findall(benchmark_pattern, related_work_text)\n    \n    # Remove duplicates while preserving order\n    unique_benchmarks = []\n    for b in benchmarks:\n        if b not in unique_benchmarks:\n            unique_benchmarks.append(b)\n    \n    return unique_benchmarks\n\n# Sample PDF content from the previous response\npdf_content = \"\"\"5\n2\n0\n2\n\np\ne\nS\n0\n3\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n2\n7\n1\n0\n0\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nDRBench: A REALISTIC\n\nBENCHMARK FOR ENTERPRISE DEEP RESEARCH\n\nAmirhossein Abaskohi1,2 Tianyi Chen1 Miguel Mu˜noz-M´armol1 Curtis Fox1,2\nAmrutha Varshini Ramesh1,2 ´Etienne Marcotte1 Xing Han L`u3,4 Nicolas Chapados1\n\nSpandana Gella1,3 Christopher Pal1,3,5 Alexandre Drouin1,3\n\nIssam H. Laradji1,2\n\n1ServiceNow Research\n\n2University of British Columbia\n\n3Mila – Quebec AI Institute\n\n4McGill University\n\n5Canada CIFAR AI Chair\n\nABSTRACT\n\nWe introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended\ndeep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple\nquestions or web-only queries, DRBench evaluates agents on multi-step queries (for\nexample, \"What changes should we make to our product roadmap to ensure compliance\nwith this standard?\") that require identifying supporting facts from both the public web\nand private company knowledge base. Each task is grounded in realistic user personas\nand enterprise context, spanning a heterogeneous search space that includes productivity\nsoftware, cloud file systems, emails, chat conversations, and the open web. Tasks\nare generated through a carefully designed synthesis pipeline with human-in-the-loop\nverification, and agents are evaluated on their ability to recall relevant insights, maintain\nfactual accuracy, and produce coherent, well-structured reports. We release 15 deep\nresearch tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We\ndemonstrate the effectiveness of DRBench by evaluating diverse DR agents across open-\nand closed-source models (such as GPT, Llama, and Qwen) and DR strategies, high-\nlighting their strengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.\n\n1\n\nINTRODUCTION\n\nOrganizations today face a strong need to find useful insights in a world full of overwhelming information.\nValuable insights are often hidden in noisy data, which can contain many distracting or irrelevant details that\nobscure the insights that really matter. This challenge is present in enterprise settings, where data is spread\nacross many applications and stored in different formats (e.g., PDFs, spreadsheets, emails, and internal tools)\nmaking extracting relevant information difficult. To uncover these hidden, valuable insights, one must con-\nduct what is known as deep research. This task involves asking high-level strategic questions (e.g, \"What\nchanges should we make to our roadmap to remain compliant?\"), planning sub-questions, retrieving and\nevaluating relevant materials, and producing a clear, actionable summary grounded in data sources (Zheng\net al., 2025; Xu & Peng, 2025; Du et al., 2025). These tasks are typically performed by domain experts\nusing a mix of search engines, communication platforms, and business applications in iterative, high-effort\nworkflows (Mialon et al., 2024), which unfortunately require a significant amount of human effort.\n\nOne promising solution to reducing this human effort is agent-based deep research, which uses autonomous\nsoftware agents to search, extract, and synthesize information across fragmented sources into an\ninsightful report. Recently, LLM-based agents have emerged as promising assistants for deep research.\nSystems such as Local Deep Researcher (LearningCircuit, 2025), Deep-Searcher (Tech, 2024), and\nDeepResearcher (Zheng et al., 2025) propose modular agent pipelines that combine retrieval, reasoning,\nand summarization over documents and web sources. Architectures like OpenHands (All-HandsAI, 2024),\nOpenManus (FoundationAgents, 2024), and smolagents (HuggingFace, 2024) extend these capabilities\nto include collaboration, multi-modal search, and complex tool use in enterprise workflows (Xu & Peng,\n2025). Despite these advances, evaluating such systems remains an open challenge.\n\nMost existing benchmarks evaluate narrow aspects such as report factuality (Coelho et al., 2025), web-only\nsynthesis (Bosse et al., 2025), or tabular analytics (Sahu et al., 2025), but they do not assess whether agents\nidentify the most salient insights, remain faithful to retrieved evidence, or adapt to enterprise contexts. To\naddress these limitations, we introduce DRBench, a benchmark designed to evaluate LLM agents on open-\n\n1\n\n2 RELATED WORK\n\nDeep Research Benchmarks. With the growing capabilities of LLMs in research and reasoning tasks,\nseveral benchmarks have emerged to evaluate their performance in realistic, multi-step and long-horizon sce-\nnarios, including Deep Research Bench (Bosse et al., 2025), DeepResearch Bench (Du et al., 2025), Deep-\nResearchGym (Coelho et al., 2025), ResearcherBench (Xu et al., 2025b), Mind2Web2 (Gou et al., 2025)\nand GAIA (Mialon et al., 2024). As summarized in Table 1, these efforts typically focus on web-only tasks,\nmeasuring retrieval or synthesis quality in controlled but limited settings. In contrast, DRBench is the first to\ncombine web retrieval with local enterprise data, grounding tasks in persona- and domain-specific contexts.\n\nEnterprise Environments. Realistic enterprise environments have become an important testbed for\nevaluating agents in complex multi-application workflows. CRMArena-Pro (Huang et al., 2025a;b) targets\nsales and CPQ pipelines through persona-grounded dialogues, but is limited to conversational sales\nworkflows. OSWorld (Xie et al., 2024) and OSWorld-Gold (Abhyankar et al., 2025) benchmark agents\nin general-purpose desktop environments, using applications such as Microsoft Word and Excel, yet their\nfocus remains on computer task execution rather than enterprise deep research. TheAgentCompany (Xu\net al., 2025a) evaluates collaboration among autonomous agents for programming, browsing, and\ncommunication, though the tasks are computer-use focused and do not assess deep research capabilities.\nWorkArena (Drouin et al., 2024; Boisvert et al., 2024) offers a realistic enterprise environment with\nknowledge work tasks for web agents, though it does not support evaluation of deep research capabilities.\nIn contrast, DRBench offers a domain-grounded enterprise environment with applications that would\nrealistically be encountered in organizations. Tasks are tied to concrete personas and roles, requiring agents\nto search, reason, and synthesize insights across diverse formats, including spreadsheets, PDFs, wikis,\nemails, and presentations, reflecting realistic enterprise deep research.\n\nDeep Research Agents. A growing line of work explores agents for multi-step search and synthesis\nacross diverse information sources. LangChain's Local Deep Researcher (LearningCircuit, 2025) and\nZilliz's Deep-Searcher provide modular pipelines for iterative querying and summarization, while DeepRe-\nsearcher (Zheng et al., 2025) uses RL to enable planning, cross-validation, and self-reflection. Commercial\nsystems such as Gemini Deep Research and Manus.ai synthesize web-based reports with citations,\nand open-source frameworks like OpenHands (All-HandsAI, 2024), OpenManus (FoundationAgents,\n2024), and smolagents (HuggingFace, 2024) offer alternative architectures. Recent work also introduces\ntask-agnostic frameworks for long-form synthesis and evaluation paradigms such as Mind2Web 2 (Gou\net al., 2025), which treat agents as judges of browsing trajectories. Building on these efforts, DRBench an-\nalyzes their strengths and limitations in enterprise contexts, showing that current agents still fall short in\nconsistently extracting and grounding critical insights within complex, heterogeneous environments.\n\n3 DRBench - AN ENTERPRISE DEEP RESEARCH BENCHMARK\n\nTo evaluate agents on complex, open-ended enterprise deep research tasks, we designed DRBench with\nthree guiding principles: it requires agents to integrate both public web data and local enterprise documents,\nit involves both web search and enterprise application use, and it is hosted in an interactive and reproducible\n\"\"\"\n\n# Extract the related work section\nrelated_work_text = extract_related_work(pdf_content)\n\n# Extract benchmarks from the related work section\nif related_work_text:\n    benchmarks = extract_benchmarks(related_work_text)\n    print(\"Benchmarks mentioned in the Related Work section:\")\n    for i, benchmark in enumerate(benchmarks, 1):\n        print(f\"{i}. {benchmark}\")\n    \n    # Extract more detailed information about each benchmark\n    print(\"\\nDetailed information about benchmarks:\")\n    for benchmark in benchmarks:\n        # Find sentences containing the benchmark name\n        benchmark_pattern = rf'([^.!?]*{re.escape(benchmark)}[^.!?]*[.!?])'\n        mentions = re.findall(benchmark_pattern, related_work_text)\n        print(f\"\\n{benchmark}:\")\n        for mention in mentions:\n            print(f\"  - {mention.strip()}\")\nelse:\n    print(\"Could not extract the Related Work section.\")",
          "sandbox_id": "imgv0psyq59d83qe8m43u"
        }
      }
    },
    {
      "step_name": "✅ ToolManager | Tool Call Success",
      "message": "Tool 'run_python_code' (server: 'tool-python') called successfully.",
      "timestamp": "2025-12-05 18:41:14",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1",
      "message": "Tool run_python_code executed successfully",
      "timestamp": "2025-12-05 18:41:14",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | API Error",
      "message": "Error: Error code: 400 - {'success': False, 'message': '模型提供方错误', 'data': None, 'code': 'MPE-001', 'detailMessage': '{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"input length and `max_tokens` exceed context limit: 201811 + 4096 > 204650, decrease input length or `max_tokens` and try again\"},\"request_id\":\"req_vrtx_011CVn8t3CnFP6aTCnzrMEdL\"}, traceId: 215041e117648736506478212e33d9'}",
      "timestamp": "2025-12-05 18:41:17",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | API Error",
      "message": "Error: Error code: 400 - {'success': False, 'message': '模型提供方错误', 'data': None, 'code': 'MPE-001', 'detailMessage': '{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"input length and `max_tokens` exceed context limit: 201811 + 4096 > 204650, decrease input length or `max_tokens` and try again\"},\"request_id\":\"req_vrtx_011CVn8vWY9JRcUutHhGmWL2\"}, traceId: 213e007e17648736842506212eefbc'}",
      "timestamp": "2025-12-05 18:41:50",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | API Error",
      "message": "Error: Error code: 400 - {'success': False, 'message': '模型提供方错误', 'data': None, 'code': 'MPE-001', 'detailMessage': '{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"input length and `max_tokens` exceed context limit: 201811 + 4096 > 204650, decrease input length or `max_tokens` and try again\"},\"request_id\":\"req_vrtx_011CVn8xyd7fAF1zUK64ooCS\"}, traceId: 213e066317648737176733804e8038'}",
      "timestamp": "2025-12-05 18:42:24",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | API Error",
      "message": "Error: Error code: 400 - {'success': False, 'message': '模型提供方错误', 'data': None, 'code': 'MPE-001', 'detailMessage': '{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"input length and `max_tokens` exceed context limit: 201811 + 4096 > 204650, decrease input length or `max_tokens` and try again\"},\"request_id\":\"req_vrtx_011CVn91T6ucnnCczk7sRFUa\"}, traceId: 213e062017648737513225275e82fd'}",
      "timestamp": "2025-12-05 18:42:58",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | API Error",
      "message": "Error: Error code: 400 - {'success': False, 'message': '模型提供方错误', 'data': None, 'code': 'MPE-001', 'detailMessage': '{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"input length and `max_tokens` exceed context limit: 201811 + 4096 > 204650, decrease input length or `max_tokens` and try again\"},\"request_id\":\"req_vrtx_011CVn93uL3XwCESKB7Jqn7P\"}, traceId: 213e041917648737847383659e2b33'}",
      "timestamp": "2025-12-05 18:43:31",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | API Error",
      "message": "Error: Error code: 400 - {'success': False, 'message': '模型提供方错误', 'data': None, 'code': 'MPE-001', 'detailMessage': '{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"input length and `max_tokens` exceed context limit: 201811 + 4096 > 204650, decrease input length or `max_tokens` and try again\"},\"request_id\":\"req_vrtx_011CVn96T8nTtyyhHig7ZaWu\"}, traceId: 213e065a17648738190218726e8062'}",
      "timestamp": "2025-12-05 18:44:05",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | API Error",
      "message": "Error: Error code: 400 - {'success': False, 'message': '模型提供方错误', 'data': None, 'code': 'MPE-001', 'detailMessage': '{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"input length and `max_tokens` exceed context limit: 201811 + 4096 > 204650, decrease input length or `max_tokens` and try again\"},\"request_id\":\"req_vrtx_011CVn98ufma4mzCHpr7rQam\"}, traceId: 213e065e17648738525716613e813d'}",
      "timestamp": "2025-12-05 18:44:39",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | API Error",
      "message": "Error: Error code: 400 - {'success': False, 'message': '模型提供方错误', 'data': None, 'code': 'MPE-001', 'detailMessage': '{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"input length and `max_tokens` exceed context limit: 201811 + 4096 > 204650, decrease input length or `max_tokens` and try again\"},\"request_id\":\"req_vrtx_011CVn9BNLSrpKaDEHPQdbt6\"}, traceId: 213e06ba17648738858974341e8979'}",
      "timestamp": "2025-12-05 18:45:12",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | API Error",
      "message": "Error: Error code: 400 - {'success': False, 'message': '模型提供方错误', 'data': None, 'code': 'MPE-001', 'detailMessage': '{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"input length and `max_tokens` exceed context limit: 201811 + 4096 > 204650, decrease input length or `max_tokens` and try again\"},\"request_id\":\"req_vrtx_011CVn9DpWrHWDukP8qUegNe\"}, traceId: 213e060b17648739194423067e89bc'}",
      "timestamp": "2025-12-05 18:45:45",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 LLM | API Error",
      "message": "Error: Error code: 400 - {'success': False, 'message': '模型提供方错误', 'data': None, 'code': 'MPE-001', 'detailMessage': '{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"input length and `max_tokens` exceed context limit: 201811 + 4096 > 204650, decrease input length or `max_tokens` and try again\"},\"request_id\":\"req_vrtx_011CVn9GGjVJ7dGnsjCQ3GqE\"}, traceId: 213e006b17648739525117189eecae'}",
      "timestamp": "2025-12-05 18:46:19",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 FATAL ERROR | main | LLM Call ERROR",
      "message": "main failed: RetryError[<Future at 0x1123f4200 state=finished raised BadRequestError>]",
      "timestamp": "2025-12-05 18:46:19",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "🧠 Main agent | Validation Turn: 1 Sub-turn: 5 | LLM Call Failed",
      "message": "Main agent | Validation Turn: 1 Sub-turn: 5 failed - no response received",
      "timestamp": "2025-12-05 18:46:19",
      "info_level": "error",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1",
      "message": "Validation LLM call failed, using current report",
      "timestamp": "2025-12-05 18:46:19",
      "info_level": "warning",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Validation Turn: 1",
      "message": "No final validation response, using current report",
      "timestamp": "2025-12-05 18:46:19",
      "info_level": "warning",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Report Validation Complete",
      "message": "Validation completed after 1 turn(s)",
      "timestamp": "2025-12-05 18:46:19",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Usage Calculation",
      "message": "Usage log: [claude37_sonnet] Total Input: 1308803, Cache Input: 0, Output: 5081",
      "timestamp": "2025-12-05 18:46:19",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Final boxed answer",
      "message": "Final boxed answer:\n\nI need to extract more specific information about the benchmarks mentioned in the related work section of the paper. Let me search for that information directly in the paper.pdf:\n\n<use_mcp_tool>\n<server_name>tool-python</server_name>\n<tool_name>download_file_from_internet_to_sandbox</tool_name>\n<arguments>\n{\n\"sandbox_id\": \"imgv0psyq59d83qe8m43u\",\n\"url\": \"/Users/apple/Documents/code/MiroThinker-MM/apps/miroflow-agent/datasets/001/paper.pdf\",\n\"sandbox_file_path\": \"/home/user\"\n}\n</arguments>\n</use_mcp_tool>",
      "timestamp": "2025-12-05 18:46:19",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "👑 Main Agent | Task Completed",
      "message": "Main agent task folder_task_001 completed successfully",
      "timestamp": "2025-12-05 18:46:19",
      "info_level": "info",
      "metadata": {}
    },
    {
      "step_name": "task_execution_finished",
      "message": "Task folder_task_001 execution completed with status: success",
      "timestamp": "2025-12-05 18:46:19",
      "info_level": "info",
      "metadata": {}
    }
  ],
  "trace_data": {}
}